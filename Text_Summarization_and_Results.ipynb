{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization and Results.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fde2e5c5de9e46329ffefb1d4f6166f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e04c35ae1911484193bcf1329082d91a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18976a3185b14210af4512e5da86e4f7",
              "IPY_MODEL_3f2e53b77edf4006a2637d92c39bc39d"
            ]
          }
        },
        "e04c35ae1911484193bcf1329082d91a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18976a3185b14210af4512e5da86e4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac53b686560c4cc591694a549d52bd72",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1912529,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1912529,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb507455410142a5a8c3f396b0a52097"
          }
        },
        "3f2e53b77edf4006a2637d92c39bc39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e8ad901f84db4342883ff3f854879ea0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.91M/1.91M [02:23&lt;00:00, 13.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0516be42fefe428e9c8735a280c905da"
          }
        },
        "ac53b686560c4cc591694a549d52bd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb507455410142a5a8c3f396b0a52097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8ad901f84db4342883ff3f854879ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0516be42fefe428e9c8735a280c905da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3a16be9b28245e5ba0166953dbe263b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04604891240a469695169e8e0c4c23be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6749f713499e42ef9dc32fed5ea40341",
              "IPY_MODEL_28dce52515374383ad6831bfbf9cc24a"
            ]
          }
        },
        "04604891240a469695169e8e0c4c23be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6749f713499e42ef9dc32fed5ea40341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9cee349163c24884ae6ea5b46539fb84",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 65,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 65,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d01c7bcbfd3248d2b2ca8fd5155cc14d"
          }
        },
        "28dce52515374383ad6831bfbf9cc24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_835f373bcda14a1fbcba85196fec89bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 65.0/65.0 [00:01&lt;00:00, 46.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f79e67fcdd3d4eb7b407996922d5c9b9"
          }
        },
        "9cee349163c24884ae6ea5b46539fb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d01c7bcbfd3248d2b2ca8fd5155cc14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "835f373bcda14a1fbcba85196fec89bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f79e67fcdd3d4eb7b407996922d5c9b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f134c294d7604531a589277f8a23f7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e2eabdd59a7e425a9d1dc3ae4fa7e8c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5b661321a4c4f9fabc3596c6922d7b8",
              "IPY_MODEL_24348a33e2004af38e9030141168573e"
            ]
          }
        },
        "e2eabdd59a7e425a9d1dc3ae4fa7e8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5b661321a4c4f9fabc3596c6922d7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_44c9608dbe474bf9968dbcb73f897dce",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 88,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 88,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7e134ab862348ec9da36de84d65fc55"
          }
        },
        "24348a33e2004af38e9030141168573e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4eda6f0cd23841efbff3dc848cc90ff6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 88.0/88.0 [00:00&lt;00:00, 92.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d7f48d13fd94da3b747eadf63dc961c"
          }
        },
        "44c9608dbe474bf9968dbcb73f897dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7e134ab862348ec9da36de84d65fc55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4eda6f0cd23841efbff3dc848cc90ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d7f48d13fd94da3b747eadf63dc961c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e82ddf69fd541868ed80a662c8578dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e69006e78aa34e5ba19f1029c6dc6071",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_917ace664f4943e8813bc7f0e47448f6",
              "IPY_MODEL_404097575bb14739a585af44180bc83c"
            ]
          }
        },
        "e69006e78aa34e5ba19f1029c6dc6071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "917ace664f4943e8813bc7f0e47448f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7f59289080443c1ab7d28b480e57ce4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1120,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1120,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_54467de49847407f8fc995ffb8a12343"
          }
        },
        "404097575bb14739a585af44180bc83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c3b4bdcad5894e768ef2699d81727a29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.12k/1.12k [02:21&lt;00:00, 7.92B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9736c584b3d4536af2e1016b4a0a86a"
          }
        },
        "e7f59289080443c1ab7d28b480e57ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "54467de49847407f8fc995ffb8a12343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3b4bdcad5894e768ef2699d81727a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9736c584b3d4536af2e1016b4a0a86a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75481d0da5e448b8b3a0ab5f46df8a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_daa46470f17b45aba225106d655221ad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41ef7656c7c74e0194f5178c793142cd",
              "IPY_MODEL_686da3701d4643e8bbd1f1394fb0be77"
            ]
          }
        },
        "daa46470f17b45aba225106d655221ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41ef7656c7c74e0194f5178c793142cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dfa8062755454e71987304b42a2845ad",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2275327883,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2275327883,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_289a475d651d4f1e85b73d2b48e35aff"
          }
        },
        "686da3701d4643e8bbd1f1394fb0be77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c027c1c80fe246e78d70d8f3ce54e5ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.28G/2.28G [02:20&lt;00:00, 16.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_25e836b22b3648ee9f95f3cc003f5f68"
          }
        },
        "dfa8062755454e71987304b42a2845ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "289a475d651d4f1e85b73d2b48e35aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c027c1c80fe246e78d70d8f3ce54e5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "25e836b22b3648ee9f95f3cc003f5f68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50804d01e0d6498d96acd88cd61f1290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed400546a28f48ab8e86d1290d4e6c62",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_25e71d93fd024c2cabe32327ca6fc516",
              "IPY_MODEL_4b505e270e15464fbbe806a44e0159dd"
            ]
          }
        },
        "ed400546a28f48ab8e86d1290d4e6c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25e71d93fd024c2cabe32327ca6fc516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_10c9b3efe29643e6b4097ca23030bf8b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1197,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1197,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86990031b52d43a6ba7aa4d06428a33f"
          }
        },
        "4b505e270e15464fbbe806a44e0159dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e604153598bf461ebbf986635b0bddca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:13&lt;00:00, 86.7B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61996efd17f84c24a002cdabe5c5e2d6"
          }
        },
        "10c9b3efe29643e6b4097ca23030bf8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86990031b52d43a6ba7aa4d06428a33f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e604153598bf461ebbf986635b0bddca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61996efd17f84c24a002cdabe5c5e2d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d0e241f496e4819b43e8d4ed3a8c5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6d4226bf62c480ab9007458b0f02d8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49646509194a470fbf533866084888c3",
              "IPY_MODEL_20711695cf404a3684512df984c0024b"
            ]
          }
        },
        "e6d4226bf62c480ab9007458b0f02d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49646509194a470fbf533866084888c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e7588d0c3ef4d5a80e6bbec8f725a86",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242065649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242065649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd57bcba915c46e7944216701386623c"
          }
        },
        "20711695cf404a3684512df984c0024b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd8ad292b18247e08a52ee5d69224e74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242M/242M [00:13&lt;00:00, 18.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d3925c311de244888be188e571560427"
          }
        },
        "4e7588d0c3ef4d5a80e6bbec8f725a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd57bcba915c46e7944216701386623c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd8ad292b18247e08a52ee5d69224e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d3925c311de244888be188e571560427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "994b910603284776bba5a75ae6403ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c0b0f72e6a194d18be90a39ca39b777f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_185b3fa8b73847cd97aba364834a29b1",
              "IPY_MODEL_05f606f386c248b3bf9c52197c80d59c"
            ]
          }
        },
        "c0b0f72e6a194d18be90a39ca39b777f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "185b3fa8b73847cd97aba364834a29b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_42bc8195e862451d8b9e198d4648f26f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9bbe9bab53a4b30b3e05dc133626905"
          }
        },
        "05f606f386c248b3bf9c52197c80d59c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_09a127813df946f9b1ce222e40605258",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [00:02&lt;00:00, 395kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69478767346947dfa2bbfc7f09973c67"
          }
        },
        "42bc8195e862451d8b9e198d4648f26f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9bbe9bab53a4b30b3e05dc133626905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09a127813df946f9b1ce222e40605258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69478767346947dfa2bbfc7f09973c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d5923d3b08348cc93ea75747c5e5aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_389c47da17b54c489c685de44a9621fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_163c5fc50af64a12b04bebe8d258a03c",
              "IPY_MODEL_d0d08097b489431aa1945f66b252b6ac"
            ]
          }
        },
        "389c47da17b54c489c685de44a9621fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "163c5fc50af64a12b04bebe8d258a03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1bc744e920214d83bf474aea631dbbe5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1389353,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1389353,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3288340a5844820819deaa35bdffb3d"
          }
        },
        "d0d08097b489431aa1945f66b252b6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2816b00d74ae4321a132b885616c2886",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 2.29MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5be7ab085ecb418faa5ec420b5650227"
          }
        },
        "1bc744e920214d83bf474aea631dbbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3288340a5844820819deaa35bdffb3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2816b00d74ae4321a132b885616c2886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5be7ab085ecb418faa5ec420b5650227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xfhiuS_i1Fg",
        "outputId": "8addd23e-11b1-47b8-919a-7acfa945320b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igvG6Em-jBdg"
      },
      "source": [
        "drive = '/content/drive/MyDrive/cmplg-xml'\n",
        "import glob\n",
        "files = glob.glob(drive+'/*.xml')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MHyuUL5e5C-R",
        "outputId": "04dc49ff-c9b1-4734-d1d0-20e41972da64"
      },
      "source": [
        "files[94]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/cmplg-xml/9604004.xml'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iHdk7E7lOBA"
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def Split_Data(file):\n",
        "  soup = BeautifulSoup(open(file),'xml')\n",
        "  a = soup.find('ABSTRACT').text\n",
        "  b =  soup.get_text()\n",
        "  c = b.replace(a, '####')\n",
        "  return c.split('####')[1], a\n",
        "\n",
        "\n",
        "def Dataset(files, fn):\n",
        "  article = []\n",
        "  summary = []\n",
        "  for file in files:\n",
        "    a, b = fn(file)\n",
        "    article.append(a)\n",
        "    summary.append(b)\n",
        "  df = pd.DataFrame({'Articles':article, 'Summary':summary})\n",
        "  return df\n",
        "\n",
        "data = Dataset(files, Split_Data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "GI81iQu_m2GK",
        "outputId": "f9b44698-1504-4316-de2b-de6b38ab0955"
      },
      "source": [
        "data"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Articles</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n\\n  Introduction \\n\\n The formalism of synch...</td>\n",
              "      <td>\\n\\nThe formalism of synchronous tree-adjoinin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n  Theoretical Background \\n\\nAs a prelimin...</td>\n",
              "      <td>\\n\\nWe present an analysis of the semantic int...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nIn computational lexico...</td>\n",
              "      <td>\\n\\nCurrent approaches to computational lexico...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n\\n   INTRODUCTION \\n\\nGeneral-purpose natura...</td>\n",
              "      <td>\\n\\nThe paper demonstrates that exponential co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nPsycholinguistic knowle...</td>\n",
              "      <td>\\n\\nWe introduce the bilingual dual-coding the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>\\n\\n  Introduction \\n\\n For an agent  to be ab...</td>\n",
              "      <td>\\n\\nIf an agent does not possess the knowledge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nWithin the cooperative ...</td>\n",
              "      <td>\\n\\nThis paper focuses on two disparate aspect...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nTyped feature structure...</td>\n",
              "      <td>\\n\\nThis paper describes an abstract machine f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nThis work is part of an...</td>\n",
              "      <td>\\n\\nWe describe an approach to robust domain-i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nHidden Markov Models ar...</td>\n",
              "      <td>\\n\\nThis paper investigates model merging, a t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Articles                                            Summary\n",
              "0    \\n\\n  Introduction \\n\\n The formalism of synch...  \\n\\nThe formalism of synchronous tree-adjoinin...\n",
              "1    \\n\\n  Theoretical Background \\n\\nAs a prelimin...  \\n\\nWe present an analysis of the semantic int...\n",
              "2    \\n\\n  Introduction \\n\\nIn computational lexico...  \\n\\nCurrent approaches to computational lexico...\n",
              "3    \\n\\n   INTRODUCTION \\n\\nGeneral-purpose natura...  \\n\\nThe paper demonstrates that exponential co...\n",
              "4    \\n\\n  Introduction \\n\\nPsycholinguistic knowle...  \\n\\nWe introduce the bilingual dual-coding the...\n",
              "..                                                 ...                                                ...\n",
              "173  \\n\\n  Introduction \\n\\n For an agent  to be ab...  \\n\\nIf an agent does not possess the knowledge...\n",
              "174  \\n\\n  Introduction \\n\\nWithin the cooperative ...  \\n\\nThis paper focuses on two disparate aspect...\n",
              "175  \\n\\n  Introduction \\n\\nTyped feature structure...  \\n\\nThis paper describes an abstract machine f...\n",
              "176  \\n\\n  Introduction \\n\\nThis work is part of an...  \\n\\nWe describe an approach to robust domain-i...\n",
              "177  \\n\\n  Introduction \\n\\nHidden Markov Models ar...  \\n\\nThis paper investigates model merging, a t...\n",
              "\n",
              "[178 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ir2oziqQ3G"
      },
      "source": [
        "train = data[:110]\n",
        "test = data[110:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ht2rZ364Adic",
        "outputId": "6dab3981-36bf-4922-e4ec-d4bce6913296"
      },
      "source": [
        "test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Articles</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nRecent years have seen ...</td>\n",
              "      <td>\\n\\nThis paper presents a grammar formalism de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nAlthough a universal fe...</td>\n",
              "      <td>\\n\\nThe recognition problem for attribute-valu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nRecently, there has bee...</td>\n",
              "      <td>\\n\\nIn this paper, we assess the complexity re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nCompositionality is def...</td>\n",
              "      <td>\\n\\nWe prove a theorem stating that any semant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nA number of researchers...</td>\n",
              "      <td>\\n\\nWe conducted an empirical analysis into th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>\\n\\n  Introduction \\n\\n For an agent  to be ab...</td>\n",
              "      <td>\\n\\nIf an agent does not possess the knowledge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nWithin the cooperative ...</td>\n",
              "      <td>\\n\\nThis paper focuses on two disparate aspect...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nTyped feature structure...</td>\n",
              "      <td>\\n\\nThis paper describes an abstract machine f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nThis work is part of an...</td>\n",
              "      <td>\\n\\nWe describe an approach to robust domain-i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>\\n\\n  Introduction \\n\\nHidden Markov Models ar...</td>\n",
              "      <td>\\n\\nThis paper investigates model merging, a t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Articles                                            Summary\n",
              "110  \\n\\n  Introduction \\n\\nRecent years have seen ...  \\n\\nThis paper presents a grammar formalism de...\n",
              "111  \\n\\n  Introduction \\n\\nAlthough a universal fe...  \\n\\nThe recognition problem for attribute-valu...\n",
              "112  \\n\\n  Introduction \\n\\nRecently, there has bee...  \\n\\nIn this paper, we assess the complexity re...\n",
              "113  \\n\\n  Introduction \\n\\nCompositionality is def...  \\n\\nWe prove a theorem stating that any semant...\n",
              "114  \\n\\n  Introduction \\n\\nA number of researchers...  \\n\\nWe conducted an empirical analysis into th...\n",
              "..                                                 ...                                                ...\n",
              "173  \\n\\n  Introduction \\n\\n For an agent  to be ab...  \\n\\nIf an agent does not possess the knowledge...\n",
              "174  \\n\\n  Introduction \\n\\nWithin the cooperative ...  \\n\\nThis paper focuses on two disparate aspect...\n",
              "175  \\n\\n  Introduction \\n\\nTyped feature structure...  \\n\\nThis paper describes an abstract machine f...\n",
              "176  \\n\\n  Introduction \\n\\nThis work is part of an...  \\n\\nWe describe an approach to robust domain-i...\n",
              "177  \\n\\n  Introduction \\n\\nHidden Markov Models ar...  \\n\\nThis paper investigates model merging, a t...\n",
              "\n",
              "[68 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnE9lHrC1mJJ"
      },
      "source": [
        "train_data = []\n",
        "for i in range(len(train)):\n",
        "  ar = train['Articles'][i]\n",
        "  su = train['Summary'][i]\n",
        "  train_data.append([ar,su])\n",
        "\n",
        "test_data = []\n",
        "for j in range(110, len(data)):\n",
        "  ar = test['Articles'][j]\n",
        "  su = test['Summary'][j]\n",
        "  test_data.append([ar,su])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "yBWuS_64Ajfb",
        "outputId": "d3594520-af28-4462-f869-6d9f5b6e1200"
      },
      "source": [
        "test['Articles'][111]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\n  Introduction \\n\\nAlthough a universal feature theory does not exist, there is\\na general understanding of its objects. The objects of feature\\ntheories are abstract linguistic objects, e.g., an object ``sentence,''\\nan object ``masculine third person singular,'' an object ``verb,'' an\\nobject ``noun phrase.'' These abstract objects have properties like\\n``tense,'' ``number,'' ``predicate,'' ``subject.''\\nThe values of these properties\\nare either atomic, like ``present'' and ``singular,'' or abstract\\nobjects, like ``verb'' and ``noun-phrase.''\\nThe abstract objects are fully described by their properties and\\ntheir values. Multiple descriptions for the properties and values\\nof the abstract linguistic objects are presented in the literature.\\nExamples are:\\n\\n\\n1.\\nFeature graphs, which are labeled rooted directed acyclic graphs\\nG=(V,A), where\\nF is a collection of labels,\\na sink\\nin the graph represents an atomic value and the labeling function\\nis an injective function \\n\\n\\n\\n\\n .\\n2.\\nAttribute-value matrices,\\nwhich are matrices in which the\\nentries consist of an attribute and a value or\\na reentrance symbol. The values are either atomic or\\nattribute-value matrices.\\n\\n\\nFrom a computational point of view, all descriptions that are used in\\npractical problems are equivalent. Though there exist some theories\\n with a considerably higher expressive power . For this paper we adopt the feature graph description, which we will\\ndefine somewhat more formal in the next section.\\n Attribute Value Languages(AVL)  consist of sets of logical formulas that describe classes of feature graphs,\\nby expressing constraints on the type of paths that can exist within\\nthe graphs. To wit: In a sentence like ``a man walks'' the edges\\nlabeled with ``person'' that leave the nodes\\nlabeled ``a man'' and ``walks'' should both\\nend in a node labeled ``singular.'' Such a constraint is called\\na ``path equation'' in the attribute-value language.\\n\\n\\n A rewrite grammar  can be enriched with an AVL to construct an Attribute Value\\nGrammar(AVG), which consists of pairs of rewrite-rules and logical\\nformulas. The rewrite rule is applicable to a production (nonterminal)\\nonly if the logical formula that expresses the relation between\\nleft- and right-hand side of the rule evaluates to true.\\nThe recognition problem for attribute-value grammars can be\\nstated as: Given a grammar G and a string w does there\\nexist a derivation in G, that respects the constraints\\ngiven by its AVL, and that ends in w.\\nAs the intermediate productions correspond\\nto feature graphs this question can also be formulated as\\na question about the existence of a consistent sequence of\\nfeature graphs that results in a feature graph describing w.\\nFor the rewrite grammar, any formalism in the Chomsky hierarchy\\n(from regular to type 0) can be chosen. From a computational point\\nof view it is of course most desirable to restrict oneself to\\na formalism that on the one hand gives enough expressibility\\nto describe a large fragment of the (natural) language, and\\non the other hand is restrictive enough to preserve feasibility.\\nFor a discussion on the linguistic significance of such restrictions,\\n see . \\n\\n\\n Johnson  proved that attribute-value grammars that are as restrictive as being equipped with a rewrite grammar\\nthat is regular can already give rise to an undecidable recognition\\nproblem.\\nObviously, to be of any practical use, the\\nrewrite grammar or the attribute-value language must be more restrictive.\\nJohnson proposed to add the off-line parsability constraint,\\nwhich is respected if the rewrite grammar has no chain- or\\n\\n\\n\\n -rules. Then, the number of applications in a production\\nis linear and the size of the structure corresponding to the\\npartial productions is polynomial. Hence as by a modification of\\n Smolka's algorithm  consistency of intermediate steps can be checked in quadratic time, the complexity of\\nthe recognition problem can at most be (nondeterministic) polynomial\\n time. This observation was made in , which also has an \\n\\n\\n\\n\\n -hardness proof of the recognition problem.\\n\\n\\nWe further investigate the properties of these restricted AVGs (R-AVGs).\\nIn the next section, we give some more formal definitions and notations.\\n In Section  we show that the class of languages generated by an R-AVG (R-AVGL) includes\\nthe class of context free languages (CFL). It follows that any easily\\nparsable class of languages (like CFL) is a proper\\nsubset of R-AVGL, unless \\n\\n\\n\\n\\n .\\nLikewise, R-AVGL is a proper subset of the class of context\\nsensitive languages, unless \\n\\n\\n\\n\\n .\\n In Section  we propose a further refinement on the off-line parsability constraint,\\nwhich allows R-AVGs that respect this constraint to capture\\nprecisely complexity classes like \\n\\n\\n\\n\\nor \\n\\n\\n\\n\\n .\\nThat is, for any language L that has an \\n\\n\\n\\n\\n -parser, there exists\\nan R-AVG, say G, such that L=L(G). Though our refinement, the\\nhonest parsability constraint is probably not a property that\\ncan be decided for arbitrary R-AVGs,\\nwe show that R-AVGs can be equipped\\nwith restricting mechanisms that enforce this property.\\n The techniques that prove Theorem  and  Theorem  result from Johnson's work. Therefore, the proofs of these theorems are deferred\\nto the appendices.\\n\\n\\n  Definitions and Notation \\n  Attribute-Value Grammars \\n\\nThe definitions in this section are in the spirit\\n of , Section 3.2]  and , Sections 3-4]. Consider three sets of pairwise disjoint symbols.\\n\\nA, the finite set of constants, denoted (\\n\\n\\n )\\n\\nV, the countable set of variables, denoted (\\n\\n\\n )\\n\\nL, the finite set of attributes, also called features,\\ndenoted (\\n\\n\\n )\\n\\n\\n\\n\\nDefinition thedefctr:\\nAn f-edge from x to s is a triple (x,f,s) such that x is\\na variable, f is an attribute, and s is a constant or a variable.\\nA path, p, is a, possibly empty, sequence of f-edges\\n\\n\\n\\n\\n  in which the xi are\\nvariables and s is either a variable or a constant.\\nOften a path is denoted by the sequence of its edges' attributes,\\nin reversed order, e.g., \\n\\n\\n\\n\\n .\\nLet p be a path, ps denotes the path that starts from s,\\nwhere s is a constant only if p is the empty path. If the path\\nis nonempty, \\n\\n\\n\\n\\n ,\\nthen s is a variable.\\nFor paths\\nps and qt we write \\n\\n\\n\\n\\niff p and q start in sand t respectively and end in the same variable or constant.\\nThe expression \\n\\n\\n\\n\\nis called a path equation.\\nA feature graph is either a pair \\n\\n\\n\\n\\n ,\\nor a pair (x,E) where x is the root and E a finite set of\\nf-edges such that:\\n1.\\nif (y,f,s) and (y,f,t) are in E, then s=t;\\n2.\\nif (y,f,s) is in E, then there\\nis a path from x to y in E.\\n\\n\\nDefinition thedefctr:\\nAn attribute-value language \\n\\n\\n\\n\\nconsists of\\nsets of logical formulas that describe feature graphs,\\nby expressing constraints on the type of paths that can exist within\\nthe graphs.\\n\\nThe terms of an\\nattribute-value language \\n\\n\\nare the\\nconstants and the variables \\n\\n\\n .\\n\\nThe formulas of an attribute-value language \\n\\n\\n are path equations and Boolean combinations\\nof path equations. Thus all formulas are either \\n\\n\\n ,\\nwhere\\nps and qt are paths, or\\n\\n\\n ,\\n\\n\\n ,\\nor \\n\\n ,\\nwhere \\n\\nand\\n\\nare formulas.\\n\\n\\n\\n\\nAssume a finite set \\n\\n\\n\\n\\n(of lexical forms) and a finite set \\n\\n\\n\\n\\n (of categories).\\n\\n\\n\\n\\nwill play the role of the set of terminals and \\n\\n\\n\\n\\nwill play the\\nrole of the set of nonterminals in the productions.\\n\\n\\nDefinition thedefctr:\\nA constituent structure tree (CST) is a labeled\\ntree in which the internal nodes are labeled with elements of Cat and the\\nleaves are labeled with elements of Lex.\\n\\n\\nDefinition thedefctr:\\nLet T be a constituent structure tree and F be\\na set of formulas in an attribute-value language  \\n\\n\\n\\n\\n .\\nAn annotated constituent structure tree is a triple\\n\\n\\n\\n\\n ,\\nwhere h is a function that maps\\ninternal nodes in T onto variables in F.\\n\\n\\nDefinition thedefctr:\\nA lexicon is a finite subset of \\n\\n\\n\\n\\nA set of\\nsyntactic rules is a finite subset of\\n\\n\\n\\n\\n .\\nAn attribute-value grammar is a triple\\n\\n\\n\\n\\n>,\\nwhere lexicon is a lexicon, rules is a set of\\nsyntactic rules and start is an element of \\n\\n\\n\\n\\n .\\n\\n\\nDefinition thedefctr:\\n1.\\n , p .150]  A class \\n\\n\\n\\nof sets is recursively presentable iff there is an\\neffective enumeration \\n\\n\\n\\n\\nof deterministic Turing\\nmachines which halt on all their inputs, and such that \\n\\n\\n\\n\\n2.\\nWe say that a class of grammars \\n\\n\\n\\nis recursively\\npresentable iff the class of sets \\n\\n\\n\\n\\n is recursively presentable.\\n\\n\\n  Restricted Attribute-Value Grammars \\n\\nThe only formulas that are allowed in the attribute-value language\\nof restricted attribute-value grammars (R-AVGs)\\nare path-equations and conjunctions\\nof path-equations (i.e. disjunctions and negations are out). We\\nwill denote the attribute-value language of an R-AVG by\\n\\n\\n\\n\\nto make the distinction clear.\\nThe CST of an R-AVG is produced by a chain- and \\n\\n\\n\\n -rule free\\nregular grammar. The CST of an R-AVG can be either a left-branching\\nor a right-branching tree, since the grammar contains\\nat most one nonterminal in each rule.\\nDefinition thedefctr:\\nThe set of syntactic rules of a restricted attribute-value\\ngrammar is a subset of \\n\\n\\n\\n\\n>.\\nA restricted attribute-value grammar is a pair\\n\\n\\n\\n\\n>,\\nwhere rules is a set of syntactic rules and start is\\nan element of \\n\\n\\n\\n\\n .\\n\\n\\nDefinition thedefctr:\\nAn R-AVG \\n\\n\\n\\n\\n>\\ngenerates an annotated constituent structure\\ntree \\n\\n\\n\\n\\niff\\n1.\\nthe root node of T is start, and\\n2.\\nevery internal node of T is licensed by a syntactic rule, and\\n3.\\nthe set F is consistent, i.e., describes a feature graph.\\nLet \\n\\n\\n\\nstand for the formula \\n\\n\\n\\nin which all\\nvariable y is substituted for variable x.\\nAn internal node v of an annotated constituent structure tree is\\nlicensed by a syntactic rule \\n\\n\\n\\n\\n iff\\n1.\\nthe node v is labeled with category c0, \\n\\nh(v) = n0, and\\n2.\\nall daughters of v are leaves, which are labeled with\\n\\n\\n\\n\\n ,\\nand\\n3.\\n\\n\\n\\n\\nis in the set F.\\nAn internal node v of an annotated constituent structure tree is\\nlicensed by a syntactic rule \\n\\n\\n\\n\\n iff\\n1.\\nthe node v is labeled with category c0, \\n\\nh(v) = n0, and\\n2.\\none of v's daughters is an internal node, v1, which is\\nlabeled with category c1, and \\n\\nh(v1) = n1, and\\n3.\\nthe daughters of v that are leaves are labeled with\\n\\n\\n\\n\\n ,\\nand\\n4.\\n\\n\\n\\n\\nis in the set F.\\n\\n\\n\\n  Weak Generative Capacity\\n\\n\\n In , it is shown that the recognition problem for\\nR-AVGs is \\n\\n\\n\\n\\n -complete. This seems to indicate that although\\nthe mechanism for generating CSTs in R-AVGs is extremely simple, the\\ngenerative capacity of R-AVGs is different from the generative\\ncapacity of e.g., context free languages (CFLs), which have a polynomial\\n time parsing algorithm . Yet, a priori, there may exist CFLs that do not have an R-AVG.\\n\\n\\n    Let L be a context free language. There exists an\\nR-AVG G such that L=L(G).\\n\\n\\nProof.If L is a context free language, then there exists\\na context free grammar G' in Greibach normal form such that L=L(G').\\nFrom this grammar G', we can construct a pushdown store\\nM that accepts exactly the words in L(G')=L. Such a pushdown store\\nM is actually a finite state automaton M' with a stack S.\\nThe finite state automaton M' may be simulated by a\\nchain- and \\n\\n\\n\\n -rule free regular grammar.\\nFurthermore, we can construct an attribute-value language\\n\\n\\n\\n\\nthat simulates the stack S.\\nThus it should be clear that there exists an R-AVG G that\\nproduces word w iff \\n\\n\\n\\n\\n .\\n Details of this construction are deferred to Appendix . \\n\\n\\n\\n  The Honest Parsability Constraint and Consequences\\n\\n\\n According to Theorem , it is unlikely that the languages generated by R-AVGs can be limited to those languages with a\\npolynomial time recognition algorithm.\\n Trautwein  showed that all R-AVGs have nondeterministic polynomial time algorithms. Is it perhaps\\nthe case that any language that has a nondeterministic polynomial\\ntime recognition algorithm can be generated by an R-AVG. Does\\nthere exist a tight relation between time bounded machines and R-AVGs\\nas e.g., between LBAs and CSLs? The answer is that the off-line\\nparsability constraint that forces the R-AVG to have no chain-\\nor \\n\\n\\n\\n -rules\\nis just too restrictive to allow such a connection.\\nThe following trick to alleviate this problem has been observed earlier\\nin complexity theory. The off-line\\n parsability constraint(OLP)  relates the amount of ``work'' done by the grammar to produce a string\\nlinearly to the number of terminal symbols produced. It is therefore\\na sort of honesty constraint that is also demanded of functions\\nthat are used in e.g., cryptography. There the deal is, for each\\npolynomial amount of work done to compute the function at least\\none bit of output must be produced. In such a way, for polynomial\\ntime computable functions one can guarantee that the inverse of\\nthe function is computable in nondeterministic polynomial time.\\n\\n\\nAs a more liberal constraint on R-AVGs we propose an analogous\\nvariation on the OLP\\nDefinition thedefctr:\\nA grammar G satisfies the Honest Parsability\\nConstraint(HPC) iff there exists a polynomial p s.t. for each win L(G) there exists a derivation with at most \\n\\n\\n\\n\\n steps.\\n\\n\\nFrom Smolka's algorithm and Trautwein's observation it trivially\\nfollows that any attribute-value grammar that satisfies the\\nHPC (HP-AVG) has an \\n\\n\\n\\n\\nrecognition algorithm. The problem with the\\nHPC is of course that it is not a syntactic property of grammars.\\nThe question whether a given AVG satisfies the HPC (or the OLP for\\nthat matter) may well be undecidable.\\nNonetheless, we can produce a set of rules that,\\nwhen added to an attribute-value grammar enforces the\\nHPC. The newly produced language is then a subset of the old\\nproduced language with an \\n\\n\\n\\n\\nrecognition algorithm.\\nBecause of the fact that our addition may simulate any polynomial\\nrestriction, we regain the full class of AVG's that satisfy the HPC.\\nIn fact\\n\\n\\nTheorem  4.1   \\nThe class, P-AVGL, of languages produced by the HP-AVGs\\nis recursively presentable.\\n\\n\\nWe will give\\na detailed construction of such a set of rules in\\n Appendix . The existence of such a set of rules and the work of Johnson now gives the following theorem.\\n\\n\\n    For any language L that has an \\n\\n\\n\\n\\nrecognition\\nalgorithm, there exists a\\nrestricted attribute-value grammar G that respects\\nthe HPC and such that L=L(G).\\n\\n\\nProof.(Sketch)\\nLet M be the Turing machine that decides \\n\\n\\n\\n .\\nUse\\na variation of Johnson's construction of a Turing machine\\nto create an R-AVG\\nthat can produce any string w that is recognized by M. Add the\\nset of rules that guarantee that only strings that can be produced\\nwith a polynomial number of rules can be produced by the grammar.\\n\\n\\n\\n  Veer out the HPC \\n\\nInstead of creating a counter of logarithmic size as we do in\\n Appendix , it is quite straightforward to construct a counter of linear size (or exponential size if there is enough\\ntime). In fact, for well-behaved functions, the construction of a\\ncounter gives a method to enforce any desired time bound constraint on\\nthe recognition problem for attribute-value grammars.\\nFor instance, for nondeterministic exponential time we could define\\nthe Linear Dishonest Parsability Constraint (LDP) (allowing a linear\\nexponential number of steps) which would give.\\n\\n\\nTheorem  5.1   \\nThe class of languages generated by R-AVGs obeying the\\nLDP condition is exactly \\n\\n\\n\\n\\n .\\n\\n\\n  Acknowledgements \\n\\nWe are indebted to E. Aarts and W.C. Rounds for their valuable\\nsuggestions on an early presentation of this work.\\n\\nBibliography \\n\\nJ. Balczar, J. Daz, and J. Gabarr.\\nStructural Complexity I.\\nSpringer-Verlag, New York, 1988.\\n\\n\\nP. Blackburn and E. Spaan.\\nA modal perspective on the computational complexity of attribute\\n  value grammar.\\nJournal of Logic, Language and Information, 2(2):129-169,\\n  1993.\\n\\n\\nN. Chomsky.\\nThree models for the description of language.\\nIRE Transactions on Information Theory, 2(3):113-124, 1956.\\n\\n\\nJ. Earley.\\nAn efficient context-free parsing algorithm.\\nCommunications of the Association for Computing Machinery,\\n  13(2):94-102, February 1970.\\n\\n\\nJ. Hopcroft and J. Ullman.\\nIntroduction to Automata Theory, Languages, and Computation.\\nAddison Wesley, Reading, MA, 1979.\\n\\n\\nM. Johnson.\\nAttribute-Value Logic and the Theory of Grammar, volume 16 of\\n  CSLI Lecture Notes.\\nCSLI, Stanford, 1988.\\n\\n\\nC. Perrault.\\nOn the mathematical properties of linguistic theories.\\nComputational Linguistics, 10(3-4):165-176, 1984.\\n\\n\\nG. Smolka.\\nFeature-constraint logics for unification grammars.\\nJournal of Logic Programming, 12(1):51-87, 1992.\\n\\n\\nT. Sudkamp.\\nLanguages and Machines: An introduction to the Theory of\\n  Computer Science.\\nAddison Wesley, Reading, MA, 1988.\\n\\n\\nM. Trautwein.\\nAssessing complexity results in feature theories.\\nILLC Research Report and Technical Notes Series LP-95-01,\\n  University of Amsterdam, Amsterdam, 1995.\\nSubmitted to the CMP-LG archive.\\n\\n\\n\\n  Simulating a Context Free Grammar in GNF\\n\\n\\nA context free grammar (CFG) is a quadruple\\n\\n\\n\\n\\n ,\\nwhere N is a set of nonterminals,\\n\\n\\n\\nis a set of terminals, P is a set of productions,\\nand \\n\\n\\n\\nis the start nonterminal.\\nA CFG is in Greibach normalform (GNF) if, and only if, the\\nproductions are of one of the following\\nforms, where \\n\\n\\n\\n\\n and\\n\\n\\n\\n the empty string (c.f., ,  ): \\n\\n\\nGiven a GNF \\n\\n\\n\\n\\n ,\\nwe can construct a restricted attribute-value grammar (R-AVG)\\nG' that simulates grammar G. R-AVG G' consists\\nof the same set of nonterminals and terminals as GNF G.\\nThe productions of R-AVG G' are described by\\n Table . The only two attributes of R-AVG G' are  TOP and\\n REST. R-AVG G' contains |N| + 1 atomic values, one atomic\\nvalue for each nonterminal and the special atomic value $.\\nThe R-AVG G' uses the feature graph to\\nencode a push-down stack, similar to the encoding of a list.\\nThe stack will be used to store the\\nnonterminals that still have to be rewritten.\\n\\n\\nThe three syntactic abbreviations below are used to clarify the simulation.\\nWe use represent a stack by a Greek letter, or a string of symbols;\\nthe top of the stack is the leftmost symbol of the string.\\nLet x0 encode a stack \\n\\n\\n\\n ,\\nthen the formulas in\\nthe abbreviation \\n\\n\\n\\n\\nexpress that x1 encodes\\na stack \\n\\n\\n\\n\\n .\\nLikewise, the formulas in the\\nabbreviation \\n\\n\\n\\n\\nexpress that x0 encodes a stack\\n\\n\\n\\n ,\\nand X1 encodes the stack \\n\\n\\n\\n .\\nThe abbreviation\\n EMPTY-STACK expresses that x0 encodes an empty stack.\\n\\n\\n\\n\\nWe have to prove that GNF Gand its simulation by R-AVG G'generate (almost) the same language. Obviously, R-AVG G'cannot generate the empty string. However,\\nfor all non-empty strings the following theorem holds.\\n\\n\\nTheorem  6.1   \\nStart nonterminal S of GNF Gderives string \\n\\n\\n\\n(\\n\\n\\n\\n\\n )\\nif, and only if,\\nstart nonterminal S of R-AVG G'derives string \\n\\n\\n\\nwith the empty stack.\\n\\n\\nProof.There are two cases to consider. First, S derives string \\n\\n\\n\\n in one step. Second, S derives string \\n\\n\\n\\nin more than one\\nstep. The lemma below is needed in the proof of the second case.\\n\\n\\nCase I\\nLet start nonterminal S derive string \\n\\n\\n\\nin one step.\\nGNF G contains a production\\n\\n\\n\\n\\niff R-AVG G'   contains a production \\n\\n\\n\\n\\nwith the equation\\n    EMPTY-STACK.\\n   So, S derives \\n\\n\\n\\nin a\\n   derivation of GNF G iff S derives \\n\\n\\n\\nwith an empty stack in the derivation of R-AVG G'.\\nCase II\\nInitial nonterminal S of GNF G derives\\n   string \\n\\n\\n\\n\\nin more than one step iff there is a\\n   left-most derivation \\n\\n\\n\\n\\n. GNF G contains production\\n\\n\\n\\n\\niff R-AVG G' contains production\\n\\n\\n\\n\\nwith the equation\\n    EMPTY-STACK. By the next lemma: \\n\\n\\n\\n\\niff \\n\\n\\n\\n\\nwith\\n   the empty stack.\\n   Hence S derives \\n\\n\\n\\nfor GNF G iff\\n   S derives \\n\\n\\n\\nwith empty stack for R-AVG G'.\\n\\n\\n\\n  Constructing an Honestly Parsable Attribute-Value\\nGrammar\\n\\n\\nIn this section we show how to add a binary counter to an\\nattribute-value grammar (AVG). This counter enforces the\\nHonest-Parsability Constraint (HPC) upon the AVG. To keep\\nthis section legible we sometimes use the\\nattribute-value matrices (AVMs) as descriptions.\\n In Section , we show how to create a counter for the AVG.\\n In Section  we show how to extend the syntactic rules and the lexicon\\nof the AVG.\\n\\n  Arithmetic by AVGs \\n\\nWe start with a little bit of arithmetic.\\n\\n  Natural numbers. \\n\\nThe AVMs below encode natural numbers in binary\\nnotation.  The sequences of attributes  0 and  1 in these\\nAVMs\\nencode natural numbers, from least- to most-significant bit.\\nThe attribute  V has value 1 (or 0) if, and only if,\\nit has a sister attribute  1 (or  0).\\n1.\\nThe AVMs \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\n        encode the natural numbers zero and one.\\n2.\\nThe AVMs \\n\\n\\n\\n\\n        and \\n\\n\\n\\n\\nencode\\n        natural numbers iff the AVM [F] encodes a natural number.\\n\\n  Syntactic rules that tests two numbers for equality. \\n\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] and [H] encode\\nnatural number x and y, respectively.\\nWe present one syntactic rule that derives from this\\nnonterminal A a nonterminal B with AVM\\n\\n\\n\\n\\nif x = y.\\n\\n\\n\\n\\nClearly, this simple test takes one step. A more sophisticated\\ntest, which also tests for inequality, would compare [F]and [G] bit-by-bit.  Such a test would take\\n\\n\\n\\n\\n\\n\\n\\n\\n derivation steps.\\n\\n\\n  Syntactic rules that multiply by two. \\n\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] encodes natural number x.\\nWe present one syntactic rule that derives from this\\nnonterminal A a nonterminal B with the AVM\\n\\n\\n\\n\\n ,\\nwhere [H] encodes natural number 2x.\\n\\n\\nThe number  N in [H] equals two times  N in [F]if, and only if, the least-significant bit of  N in [H]is 0, and the remaining bits form the same sequence as the\\nnumber  N in [F]. Multiplication by two takes one derivation\\nstep.\\n\\n\\n\\n\\n  Syntactic rules that increments by one. \\n\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] encodes natural number x.\\nWe present five syntactic rules that derive from this\\nnonterminal A a nonterminal C with AVM\\n\\n\\n\\n\\n ,\\nwhere [H] encodes natural number x+1.\\n\\n\\nThe increment of  N requires two additional pointers in the\\nAVM of A: attribute  P points to the next bit\\nthat has to be incremented; attribute  Q points to the\\nmost-significant bit of the (intermediate) result.\\nThese additional pointers are hidden from the\\nAVMs of the nonterminals A and C.\\n\\n\\n The five rules from Table  increment  N by one. Nonterminal A rewrites, in one or more steps, to nonterminal\\nC, potentially through a number of nonterminals B.\\n\\n\\n\\n\\n The first and fourth rule of Table  state that adding one to a zero bit sets\\nthis bit to one and ends the increment. The second and third rule state\\nthat adding one to a one bit sets this bit to zero and the increment\\ncontinues.  The fifth rule states that adding one to the\\nmost-significant bit\\nsets this bit to zero and yields a new most-significant one bit.\\nWe claim that \\n\\n\\n\\n\\n takes \\n\\n\\n\\n\\nderivation steps.\\n\\n\\nRules, similar to the ones above, can be given that decrement\\nthe attribute  N by one. We only have to take a little\\nextra care that the number 0 cannot be decremented.\\n\\n\\n  Syntactic rules that sum two numbers. \\n\\nIn this section we use the previous test and increment rules\\n(indicated by =).\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] and [H] encode\\nnatural number x and y, respectively.\\n We present syntactic rules (Table -) that derive from this\\nnonterminal A a nonterminal C with AVM\\n\\n\\n\\n\\n ,\\n where [F'] encodes the natural number x + y.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe increment of  N by  M is similar to the\\nincrement by one. Here, three additional pointers are required:\\nthe attributes  P and  Q point to the bits in  N and\\n M respectively that have to be summed next; attribute  R\\npoints to the most-significant bit of the (intermediate) result.\\nIn the addition two states\\nare distinguished. In the one state, the carry bit is zero, indicated\\nby nonterminal A'. In the other state, the carry bit is one,\\nindicated by nonterminal B.\\nWe claim that \\n\\n\\n\\n\\n takes \\n\\n\\n\\n\\n derivation steps.\\n\\n\\n  Syntactic rules that sum a sequence of numbers. \\n\\nIn this section we use the previous summation rules\\n(indicated by =).\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F'] encodes a list of numbers. To wit\\n\\n\\n\\nwhere [Gi] encodes natural number xi.\\n We present syntactic rules (Table ) that derive from this nonterminal A a nonterminal B with AVM\\n\\n\\n\\n\\n ,\\nwhere [F] encodes the natural number \\n\\n\\n\\n\\n .\\n\\n\\nThe summation requires an additional pointer in the\\nAVM [F']: attribute  P points to the next element\\nin the list that has to be summed.\\nWe claim that \\n\\n\\n\\n\\n takes \\n\\n\\n\\n\\nderivation steps.\\n\\n\\n\\n\\n\\n\\n  Creating a counter of logarithmic size\\n\\n\\nCreate an AVM of the following form:\\n\\n\\n\\nAttribute  COUNTER is used to distinguish the AVMs\\nthat encodes the counter from those in the\\noriginal attribute-value grammar. We will neglect the\\nattribute  COUNTER in the remainder of this section, because it is\\nnot essential here.\\nThe attributes  SIZE,  N,  M and  POLY\\nencode natural numbers.\\nThe attribute  SIZE records the size of the string that will be\\ngenerated.  The attribute  POLY records the maximum number of\\nderivation steps that is allowed for a string of size  SIZE.\\nThe attributes  N and  M are auxiliary numbers.\\n\\n\\nThe construction of the counter starts with an initiation-step.\\nThe further construction of the counter consists of cycles of two\\nphases. Each cycle starts in nonterminal A.\\n\\n  Initiation step and first phase. \\n\\nThe initiation-step sets the numbers  SIZE and  N to 0,\\nand the numbers  M and  POLY to 1.\\nIn the first phase of each cycle, the numbers  SIZE and  N\\nare incremented by 1.\\n\\n\\n\\n  The second phase of the cycle. \\n\\nIn this phase the numbers  N and  M are compared.\\nIf  N is twice  M, then (i) number\\n POLY is extended by k bits, (ii) number  M is doubled,\\nand (iii) number  N is set to 0.\\nIf  N is less than twice  M, nothing happens.\\n\\n\\nThe left rule of the second phase doubles the number  M in\\nthe second and the third equation.\\nThe test ``Is  N equal to 2 M?'' therefore reduces to one\\n(the first) equation.\\nThe fourth equation extend the number  POLY with k bits.\\nThe fifth and sixth equations set the number  N to 0.\\n\\n\\nThe right rule is always applicable. If the right rule is used where\\nthe left rule was applicable, then the number  N will never be\\nequal to \\n\\n\\n\\n\\nin the rest of the derivation. Thus  POLY will\\nnot be extended any more.\\n\\n\\n\\n\\nWe claim that the left rule appears \\n\\n\\n\\ntimes and the right\\nrule O(n) times in a derivation for input of size n.\\nObviously, the number  POLY is \\n\\n\\n\\n\\nwhen\\nthe number  SIZE is i.\\n\\n\\n\\n\\n  From AVG to HP-AVG\\n\\n\\nIn this section we show how to transform an AVG\\ninto an AVG that satisfies the HPC (HP-AVG).\\nSince all computation steps of the HP-AVG\\nonly require a linear amount of derivation steps,\\ntotal derivations of HP-AVGs have polynomial length.\\n\\n\\nWe can divide the attributes of the HP-AVG into two groups. The\\nattributes that encode the counters, and the attributes of the\\noriginal AVG. The former will be embedded under the attribute\\n COUNTER, the latter under the attribute  GRAMMAR.\\nIn the sequel, we mean by \\n\\n\\n\\n\\nthe formula \\n\\n\\n\\n embedded under the attribute  GRAMMAR, i.e., the formula\\nobtained from \\n\\n\\n\\nby substituting the variables xi by\\n\\n\\n\\n\\n .\\n\\n\\nThe HP-AVG is obtained from the AVG in three steps: change\\nthe start nonterminal, the lexicon and the syntactic rules.\\nFirst, the HP-AVG contains the rules of the previous section, which\\n construct the counter. The nonterminal S from Table  is the start nonterminal of the HP-AVG. For the nonterminal A the start\\n nonterminal of the AVG is taken. Nonterminal B from Table  is a fresh nonterminal, not occurring in the AVG.\\n\\n\\nSecond, the HP-AVG contains an extension of the lexicon of the AVG.\\nThe entries of the lexicon are extended in the following way. The size\\nof the lexical form is set to one, and the amount of derivation steps\\nis zero. Thus, if \\n\\n\\n\\n\\nis the lexicon of the AVG,\\nthen \\n\\n\\n\\n\\nis the lexicon of the HP-AVG, where\\n\\n\\n\\nThird, the HP-AVG contains extensions of the syntactic rules of the AVG.\\nThe syntactic rules are extended in the following way. The numbers\\n POLY and  SIZE of the daughter nonterminals are collected in\\nthe lists  PLIST and  SLIST. Both lists are summed.\\nThe number  SIZE of the mother nonterminal is equal to\\nthe sum of  SIZE's,\\nand the number  POLY of the mother nonterminal is one\\nmore than the sum of  POLY's.\\nThus, if \\n\\n\\n\\n\\n is a syntactic rule of the AVG, then \\n\\n\\n\\n\\n is a syntactic rule of the HP-AVG, where\\n\\n\\n\\nNow, a derivation for the HP-AVG starts with a nondeterministic\\nconstruction of a counter  SIZE with value n and a counter\\n POLY with value O(n[k]). Then, the derivation of the original\\nAVG is simulated, such that\\n(i)\\n the mother nonterminal produces a string of size n if, and only if\\n the daughter nonterminals together produce a string of size n, and\\n(ii)\\n the mother nonterminal makes n[k]+1 derivation steps if, and only if\\n the daughter nonterminals together make n[k] derivation steps.\\n\\nFootnotes\\n\\n  The author was supported in part by HCM grant\\n        ERB4050PL93-0516.\\n  The author was supported by the Foundation for language,\\n        speech and logic (TSL), which is funded by the Netherlands\\n        organization for scientific research (NWO)\\n\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwLZbyYc2teE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68eea8a4-b5e5-477d-ccb0-89c8111fd987"
      },
      "source": [
        "test_data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"\\n\\n  Introduction \\n\\nRecent years have seen a resurgence of interest in\\nprobabilistic techniques for automatic language analysis. In\\nparticular, there has arisen a distinct paradigm of processing on the basis of\\npre-analyzed data which has taken the name   Data-Oriented Parsing.\\n\\n\\n``Data Oriented Parsing (DOP) is a model where no abstract rules, but\\nlanguage experiences in the form of an analyzed corpus, constitute the\\n basis for language processing.'' \\n\\n\\nThere is not space here to present full justification for\\nadopting such an approach or to detail the advantages that it\\noffers. The main claim it makes is that effective language processing requires\\na consideration of both the structural and statistical aspects of\\nlanguage, whereas traditional competence grammars rely only on the\\nformer, and standard  statistical techniques such as n-gram models\\nonly on the latter.  DOP attempts to combine\\nthese two traditions and produce ``performance grammars'', which:\\n\\n\\n``... should not only contain information on\\nthe structural possibilities of the general language system, but also\\non details of actual language use in a language\\n community...'' \\n\\n\\nThis approach entails however that a corpus has first to be\\npre-analyzed (ie. hand-parsed), and the question immediately arises\\nas to the formalism to be used for this. There is no lack of\\ncompeting competence grammars available, but also no reason to\\nexpect that such grammars should be suited to a DOP approach,\\ndesigned as they were to characterize the nature of linguistic\\ncompetence rather than performance.\\n\\n\\nThe next section sets out some of the properties that we might\\nrequire from such a ``performance grammar'' and offers a formalism\\nwhich attempts to\\nsatisfy these requirements.\\n\\n\\n  A Formalism for DOP \\n\\nGiven that we are attempting to construct a formalism that will do\\njustice to both the statistical and structural aspects of language,\\nthe features that we would wish to maximize will include the\\nfollowing:\\n\\n\\n1.\\nThe formalism should be easy to use with probabilistic processing techniques,\\nideally having a close correspondence to a simple probabilistic model\\nsuch as a Markov process.\\n2.\\nThe formalism should be fine-grained, ie. responsive\\nto the behaviour of individual words (as n-gram models\\nare). This suggests a radically\\nlexicalist approach (cf. Karttunen, 1990) in which all rules are\\nencoded in the lexicon, there being no phrase structure rules which do\\nnot introduce lexical items.\\n3.\\nIt should be capable of capturing fully the linguistic intuitions of\\nlanguage users. In other words, using the formalism one should be able to\\ncharacterize the structural regularities of language with at least the\\nsophistication of modern competence grammars.\\n4.\\nAs it is to be used with real data, the formalism should be able to\\ncharacterize the wide range of syntactic structures found in actual\\nlanguage use, including those normally excluded by competence grammars\\nas belonging to the ``periphery'' of the language or as being\\n``ungrammatical''. Ideally every interpretable utterance should\\nhave one and only one analysis for any interpretation of it.\\nConsidering the first of these points, namely a close relation to a simple\\nprobabilistic model, a good place to\\nstart the search might be with a right-branching finite-state\\ngrammar. In this class of grammars every rule has the form\\nA \\n\\n\\n\\n\\na B (A,B \\n\\n\\n\\n {non-terminals}, a \\n\\n\\n\\n{terminals})\\nand all trees have the simple structure :\\n\\n\\nOr:\\nequivalent vertical alignment, henceforth to be used in this\\npaper, on the right)\\n\\n\\nIn probabilistic terms, a finite-state grammar corresponds to a first-order\\nMarkov process, where\\n given a\\nsequence of states Si, Sj,... drawn from a\\nfinite set of possible states\\n{S0,..,Sn} the probability of\\na particular state occurring depends solely on the\\nidentity of the previous state. In the finite-state grammar each word is\\nassociated with a transition between two\\ncategories, in the tree above `a' with the transition\\nA \\n\\n\\n\\n\\nB and so on.\\nTo calculate the probability that a string of words x1, x2,\\n  x3,... xn has the parse represented by the string of\\ncategory-states S1, S2, S3,...Sn, we simply take the\\nproduct\\nof the probability of each transition: ie. \\n\\n\\n\\n\\nIn addition to satisfying our first criterion, a\\nfinite-state grammar also fulfills the requirement that the\\nformalism be radically lexicalist, as by definition every rule\\nintroduces a lexical item.\\n\\n  Accounting for Linguistic Structure \\n\\nIf a finite-state grammar is chosen however, the third\\ncriterion, that of linguistic adequacy, seems to present an\\ninsurmountable stumbling block.  How can such a simple formalism, in\\nwhich syntax is reduced to a string of category-states, hope to\\ncapture even the basic hierarchical structure, the familiar ``tree\\nstructure'', of linguistic expressions?\\n\\n\\nIndeed, if the non-terminals are viewed as atomic\\ncategories then there is no way this can be done. If however, in line\\nwith most current theories, categories are taken to be bundles of\\nfeatures and crucially if one of these features has the value of a\\nstack of categories, then this hierarchical structure can indeed\\nbe represented.\\n\\n\\nUsing the notation A [B] to represent a state of basic category\\nA carrying a category B on its stack, the hierarchical\\nstructure of the sentence:\\n\\n\\n(1) The man gave the dog a bone.\\n\\n\\ncan be represented as:\\n\\n\\n\\t\\tThe \\t\\tS \\t\\t[ ] \\nman \\t\\tN \\t\\t[VP]  \\ngave \\t\\tVP \\t\\t[ ]  (1a)\\t\\tthe \\t\\tNP \\t\\t[NP]  \\ndog \\t\\tN \\t\\t[NP] \\na \\t\\tNP \\t\\t[ ] \\nbone \\t\\tN \\t\\t[ ]\\n\\n\\nIntuitively, syntactic links between\\nnon-adjacent words, impossible in a standard finite-state grammar, are\\nhere established by passing categories along on the stack ``through''\\nthe state of intervening words. That such a formalism can fully\\ncapture basic linguistic\\nstructures is confirmed by the proof in Aho (1968) that an   indexed grammar (ie. one where categories are supplemented with a\\nstack of unbounded length, as above), if\\nrestricted to right linear trees (also as above), is equivalent to a   context-free grammar.\\n\\n\\nA perusal of the state transitions associated with individual words in\\n(1a) reveals an obvious relationship to the ``types'' of categorial\\ngrammar. Using \\n\\n\\n\\nto represent a list of categories (possibly\\nnull), we arrive at the following transitions (with their\\ncorresponding categorial types alongside).\\n\\n\\nThe ditransitive verb `gave' is\\n\\n\\n  VP [\\n\\n\\n\\n  ]  \\n\\n\\n\\nNP [NP,\\n\\n\\n\\n ]   (VP/NP)/NP\\n\\n\\nDeterminers in complement position are both:\\n\\n\\n   NP [\\n\\n\\n\\n ] \\n\\n\\n\\n\\nN\\n  [\\n\\n\\n\\n ]   NP/N\\n\\n\\nDeterminer in subject position is\\n `type-raised' to: \\n\\n\\n  S [\\n\\n\\n\\n ] \\n\\n\\n\\n\\nN [VP,\\n\\n\\n\\n ]\\n  (S/VP)/N\\n\\n\\nThe common nouns are all:\\n\\n\\n  N [\\n\\n\\n\\n ] \\n\\n\\n\\n\\n\\n  N\\n\\n\\nIn fact as no intermediate constituents are formed in the analysis, an\\neven closer parallel is to a dependency syntax where only rightward pointing\\narrows are allowed, of which the formalism as presented above is a\\nnotational variant. This lack of intermediate constituents has the\\nadded benefit that no ``spurious ambiguities'' can arise.\\n\\n\\nKnowing now that the addition of a stack-valued feature suffices to\\ncapture the basic hierarchical structure of language, additional\\nfeatures can be used to deal with other syntactic relations. For\\nexample, following the example of GPSG,  unbounded dependencies can be\\ncaptured using ``slashed'' categories. If we represent a ``slashed''\\ncategory X  with the lower case x, and use the\\nnotation A(b) for a category A carrying a feature b,\\nthen the topicalized sentence:\\n\\n\\n(2) This bone the man gave the puppy.\\n\\n\\nwill have the analysis:\\n\\n\\n\\t\\tThis \\t\\tS \\t\\t[ ] \\nbone \\t\\tN \\t\\t[S(np)]  \\nthe \\t\\tS(np) \\t\\t[ ]  (2a)\\t\\tman \\t\\tN \\t\\t[VP(np)] \\ngave \\t\\tVP(np) \\t\\t[ ] \\nthe \\t\\tNP \\t\\t[ ] \\npuppy \\t\\tN \\t\\t[ ]\\n\\n\\nAlthough there is no space in this paper to go into greater detail,\\nfurther constructions involving unbounded dependency and complement\\ncontrol phenomena can be captured in similar ways.\\n\\n\\n  Coverage \\n\\nThe criterion that remains to be satisfied is that of width of\\ncoverage: can the formalism cope with the many ``peripheral''\\nstructures found in real written and spoken texts?  As it stands the\\nformalism is weakly equivalent to a context-free\\ngrammar and as such will have problems dealing with\\nphenomena like discontinuous constituents,\\nnon-constituent coordination and gapping. Fortunately if extensions\\nare made to the formalism, necessarily taking it outside weak\\nequivalence to a\\ncontext-free grammar, natural and general analyses\\npresent themselves for such constructions. Two of these will\\nnow be sketched.\\n\\n\\n  Discontinuous Constituents \\n\\nConsider the pair of sentences  (3) and (4),\\nidentical in interpretation, but the latter containing a discontinuous\\nnoun phrase and the former not:\\n\\n\\n(3) I saw a dog which had no nose yesterday.\\n\\n\\n(4)  I saw a dog yesterday which had no nose.\\n\\n\\nwhich have the respective analyses:\\n\\t\\tI \\t\\tS \\t\\t[ ] \\nsaw \\t\\tVP \\t\\t[ ]  \\na \\t\\tNP \\t\\t[NP(t)]\\t\\t`t' =    \\ndog \\t\\tN \\t\\t[NP(t)]\\t\\t`time adjunct'  (3a)\\t\\twhich \\t\\tS(rel) \\t\\t[NP(t)] \\t\\t `rel' =\\nhad \\t\\tVP \\t\\t[NP(t)] \\t\\t `relative' \\nno \\t\\tNP \\t\\t[NP(t)] \\nnose \\t\\tN \\t\\t[NP(t)] \\nyesterday \\t\\tNP(t) \\t\\t[ ]\\n\\n\\n\\t\\tI \\t\\tS \\t\\t[ ]  \\nsaw \\t\\tVP \\t\\t[ ]  \\na \\t\\tNP \\t\\t[NP(t)] \\ndog \\t\\tN \\t\\t[NP(t)]  (4a)\\t\\tyesterday \\t\\tNP(t) \\t\\t[S(rel)]   \\nwhich \\t\\tS(rel) \\t\\t[ ]  \\nhad \\t\\tVP \\t\\t[ ]  \\nno \\t\\tNP \\t\\t[ ]   \\nnose \\t\\tN \\t\\t[ ]\\n\\n\\nThe only transition in (4a) that differs from that of the corresponding word\\nin the\\n`core' variant (3a) is that of `dog' which has the respective\\ntransitions:\\n\\n\\n  N [NP(t)] \\n\\n\\n\\n\\nS(rel)\\n  [NP(t)]  (in 3a)\\n\\n\\n  N [NP(t)] \\n\\n\\n\\n\\nNP(t)\\n  [S(rel)]  (in 4a)\\n\\n\\nBoth nouns introduce a relative clause modifier S(rel), the\\ndifference being that in the discontinuous variant a\\ncategory has been taken off the stack at\\nthe same time as the modifier has been placed\\non the stack. It has been assumed so far that we are using a right-linear\\nindexed grammar, but such a rule is expressly disallowed in\\nan indexed grammar and so allowing transitions of this kind ends the\\nformalism`s weak equivalence to the context-free grammars.\\n\\n\\nOf course, having allowed such crossed dependencies, there is nothing\\nin the formalism itself that will disallow a similar analysis for a\\ndiscontinuity unacceptable in English such as:\\n\\n\\n(5) I saw a yesterday dog.\\n\\n\\nThis does not present a problem, however, as in DOP it is\\ninformation in the parsed\\ncorpus which determines the structures that are possible. There is no\\nneed to explicitly rule out (5), as the transition NP [\\n\\n\\n\\n ]\\n\\n\\n\\n\\n\\n  [N] will be vanishingly rare in any corpus of\\neven the most garbled speech, while the transition N [\\n\\n\\n\\n ]\\n\\n\\n\\n\\n\\n  [S(rel)] is commonly met with in both\\nwritten and spoken English.\\n\\n\\n  Non-Constituent Coordination \\n\\nThe analysis of standard coordination is shown in (6):\\n\\n\\n\\t\\tFido \\t\\tS \\t\\t[ ] \\ngnawed \\t\\tVP \\t\\t[ ]  \\na \\t\\tNP \\t\\t[VP(+)]  (6)\\t\\tbone \\t\\tN \\t\\t[VP(+)] \\nand \\t\\tVP(+) \\t\\t[ ] \\nbarked \\t\\tVP \\t\\t[ ]\\n\\n\\nInstead of a typical transition for `gnawed' of VP\\n\\n\\n\\n\\nNP, we have a transition introducing a coordinated\\nVP:   VP \\n\\n\\n\\n\\nNP [VP(+)]\\n\\n\\nIn general for any transition X  \\n\\n\\n\\n\\nY ,\\nwhere X is a category and Y a list of categories (possibly empty),\\nthere will be a transition introducing coordination:  \\nX  \\n\\n\\n\\n\\nY [X(+)]\\n\\n\\nNon-constituent coordinations such as (7) present serious problems for\\nphrase-structure approaches:\\n\\n\\n(7) Fido had a bone yesterday and biscuit today.\\n\\n\\nHowever if we generalize the schema already obtained for\\nstandard coordination by allowing X to be not only a single\\n category, but a list of categories, it is found to suffice for non-constituent coordination as well.\\n\\n\\n\\t\\tFido \\t\\tS \\t\\t[ ] \\nhad \\t\\tVP \\t\\t[ ]  \\na \\t\\tNP \\t\\t[NP(t)]  (7a)\\t\\tbone \\t\\tN \\t\\t[NP(t)] \\nyesterday \\t\\tNP(t) \\t\\t[N(+) [NP(t)]] \\nand \\t\\tN(+) \\t\\t[NP(t)] \\nbiscuit \\t\\tN \\t\\t[NP(t)] \\ntoday \\t\\tNP(t) \\t\\t[ ]\\n\\n\\nIn this analysis instead of a regular transition for `bone' of:\\n  N [NP(t)] \\n\\n\\n\\n\\nNP(t) [ ]\\n\\n\\nthere is instead a transition introducing coordination:\\n  N [NP(t)] \\n\\n\\n\\n\\nNP(t) [N(+) [NP(t)]]\\n\\n\\nAllowing categories on the stack to\\nthemselves have non-empty stacks moves the formalism\\none step further from being an indexed grammar. This is the final\\n incarnation of the formalism, being the   State-Transition Grammar of the title. \\n\\n\\nSimilar schemas are being investigated to characterize gapping\\nconstructions.\\n\\n\\n  Centre-Embedding \\n\\nIt should be noted that an indefinite amount of   centre-embedding can be described, but only at the\\nexpense of unlimited growth in the length of states:\\n\\n\\n\\t\\tThe \\t\\tS \\t\\t[ ] \\nfly \\t\\tN \\t\\t[VP] \\nthe \\t\\tS(np) \\t\\t[VP] \\ndog \\t\\tN \\t\\t[VP(np),VP] (8)\\t\\tthe \\t\\tS(np) \\t\\t[VP(np),VP] \\ncat \\t\\tN \\t\\t[VP(np),VP(np),VP] \\nscratched \\t\\tVP(np) \\t\\t[VP(np),VP] \\nswallowed \\t\\tVP(np) \\t\\t[VP] \\ndied \\t\\tVP \\t\\t[ ]\\n\\n\\nThis contrasts with unlimited right-recursion where there is no\\ngrowth in state length:\\n\\n\\n\\t\\tI \\t\\tS \\t\\t[ ] \\nsaw \\t\\tVP \\t\\t[ ] \\nthe \\t\\tNP \\t\\t[ ] \\ncat \\t\\tN \\t\\t[ ] (9)\\t\\tthat \\t\\tS(rel) \\t\\t[ ] \\nscratched \\t\\tVP \\t\\t[ ] \\nthe \\t\\tNP \\t\\t[ ] \\ndog \\t\\tN \\t\\t[ ] \\nthat \\t\\tS(rel) \\t\\t[ ]  \\t\\t... \\t\\t... \\t\\t \\nAs the model is to be trained from real data, transitions\\ninvolving long states as in (8) will have an ever smaller and eventually\\neffectively nil probability. Therefore, when tuned to any particular\\nlanguage corpus the resulting grammar will be effectively\\n finite-state. \\n\\n\\n\\n  Parsing \\n\\nAssuming that we now have a corpus parsed  with the state-transition\\ngrammar, how can this information be used to parse fresh text?\\n\\n\\nFirstly, for each word type in the corpus we can collect the\\ntransitions with which it occurs and calculate its\\nprobability distribution over\\nall possible transitions (an infinite number of which will be\\nzero).\\nTo make this concrete, there are five tokens of the word `dog' in the\\nexamples thus far, and so `dog' will have\\nthe transition probability distribution:\\nN [VP(np),VP] \\n\\n\\n\\nN [NP] \\n\\n\\n\\n  NP [ ]\\t\\t0.2 N [NP(t)] \\n\\n\\n\\n  S(rel) [NP(t)]\\t\\t0.2 N [NP(t)] \\n\\n\\n\\n  NP(t) [S(rel)]\\t\\t0.2 N [VP(np),VP] \\n\\n\\n\\n  S(np) [VP(np),VP]\\t\\t0.2 N [ ] \\n\\n\\n\\n  S(rel) [ ]\\t\\t0.2\\n\\n\\nTo find the most probable parse for a sentence, we simply find\\nthe path from word to word which maximizes the product of the state\\ntransitions (as we have a first order Markov process).\\n\\n\\nHowever this simple-minded approach, although easy\\nto implement, in other ways leaves much to be desired. The probability\\ndistributions are far too ``gappy'' and even if a huge amount of data\\nwere collected, the chances that they would provide the desired path\\nfor a sentence of any reasonable length are slim. The process of\\ngeneralizing or smoothing the transition probabilities is therefore\\nseen to be indispensable.\\n\\n  Smoothing Probability Distributions \\n\\nAlthough far from exhausting the possible methods for smoothing, the\\nfollowing three are those used in the implementation described at the\\nend of the paper.\\n\\n\\n1. Factor out elements on the stack\\nwhich are merely carried over from state to state (which was done\\nearlier in looking at the correspondence of state transitions to\\ncategorial types). The previous transitions for `dog'\\nthen become:\\n\\n\\naaaaaaN [\\n\\n\\n\\n ] \\n\\n\\n\\nS(rel)\\t\\tN [\\n\\n\\n\\n ] \\n\\n\\n\\n\\n  [ ]\\t\\t0.2 \\t\\tN [\\n\\n\\n\\n ] \\n\\n\\n\\n\\n  [S(rel)]\\t\\t0.2 \\t\\tN [\\n\\n\\n\\n ] \\n\\n\\n\\n  S(np) [\\n\\n\\n\\n ]\\t\\t0.2 \\t\\tN [\\n\\n\\n\\n ] \\n\\n\\n\\n  S(rel) [\\n\\n\\n\\n ]\\t\\t0.4\\n\\n\\n2. Factor out other features which are merely\\npassed from state to state. For instance in the\\nexample sentences, `the' has the generalized transitions:\\n\\n\\n\\t\\tS [\\n\\n\\n\\n ] \\n\\n\\n\\nN [VP,\\n\\n\\n\\n ] \\t\\tS(np) [\\n\\n\\n\\n ] \\n\\n\\n\\nN [VP(np),\\n\\n\\n\\n ]\\n\\n\\nwhich can be further generalized to the single transition:\\n\\n\\naaS(\\n\\n\\n\\n ) [\\n\\n\\n\\n ] \\n\\n\\n\\nN\\t\\tS(\\n\\n\\n\\n ) [\\n\\n\\n\\n ] \\n\\n\\n\\n  N  [VP(\\n\\n\\n\\n ),\\n\\n\\n\\n ]\\t\\t\\n\\n\\n\\n  = set of features\\n\\n\\n3. Establish word paradigms, ie. classes of words which occur with similar\\ntransitions. The probability distribution for individual words can\\nthen be smoothed by suitably blending in the paradigmatic distribution. These\\nparadigms  will correspond to\\na great extent to the word classes of rule-based grammars. The\\nadvantage would be retained however that the system is still\\nfine-grained enough to reflect the idiosyncratic patterns of\\nindividual words and could override this paradigmatic information if\\nsufficient data were available.\\n\\n\\nWords hitherto unknown to the system can be treated as being extreme\\nexamples of words lacking sufficient transition data and they might then\\nbe given a transition distribution blended from the open class word paradigms.\\n\\n\\n  Problems Arising from Smoothing \\n\\nAlthough essential for effective processing, the smoothing operations\\nmay give rise to new problems. For example, factoring out items on the\\nstack, as in (1), removes from the model the disinclination for long\\nstates inherent in the original corpus.  To recapture this discarded\\naspect of the language, it would be sufficient to\\nintroduce into the model a probabilistic penalty based on state\\nlength. This penalty may easily be calculated\\naccording to the lengths of states in the parsed corpus.\\n\\n\\nNot only would this allow the modelling of the restriction on\\ncentre-embedding, but it would also allow many other ``processing''\\nphenomena to be accurately characterized. Taking as an example\\n``heavy-NP shift'',  suppose that the corpus contained two\\ndistinct transitions for the word `threw', with the particle `out'\\nboth before and after the object.\\n\\n\\nthrew \\t\\tVP \\n\\n\\n\\nNP, X(out) \\t\\tprob: p1 \\nVP \\n\\n\\n\\nX(out), NP \\t\\tprob: p2\\n\\n\\nEven if p1 were considerably greater than p2, the cumulative negative effect of\\nthe longer states in (10) would eventually lead to the model giving\\nthe sentence with the shifted NP (11) a higher probability.\\n\\n\\n\\t\\tI \\t\\tS \\t\\t[ ] \\nthrew \\t\\tVP \\t\\t[ ]  \\nthe \\t\\tNP \\t\\t[X(out)]  \\nbacon \\t\\tN \\t\\t[X(out)]  (10)\\t\\tthat \\t\\tS(rel) \\t\\t[X(out)]  \\nFido \\t\\tS(np) \\t\\t[X(out)]  \\nhad \\t\\tVP(np) \\t\\t[X(out)]  \\nchewed \\t\\tVP(np) \\t\\t[X(out)]  \\nout \\t\\tX(out) \\t\\t[ ]\\n\\n\\n\\t\\tI \\t\\tS \\t\\t[ ] \\nthrew \\t\\tVP \\t\\t[ ]  \\nout \\t\\tX(out) \\t\\t[NP] \\nthe \\t\\tNP \\t\\t[ ]  (11)\\t\\tbacon \\t\\tN \\t\\t[ ]  \\nthat \\t\\tS(rel) \\t\\t[ ]  \\nFido \\t\\tS(np) \\t\\t[ ]  \\nhad \\t\\tVP(np) \\t\\t[ ]  \\nchewed \\t\\tVP(np) \\t\\t[ ]\\n\\n\\n  Capturing Lexical Preferences \\n\\nOne strength of\\nn-gram models is that they can capture a certain amount of lexical\\npreference information.\\nFor example, in a bigram model trained on sufficient data the\\nprobability of the bigram `dog barked' could be expected to be\\nsignificantly higher than `cat barked', and this slice of\\n``world knowledge'' is something our model lacks.\\n\\n\\nIt would not be difficult to make a small extension to the present\\nmodel to capture such information, namely by introducing an additional\\nfeature containing the ''lexical value'' of the head of a\\nphrase. Abandoning the shorthand `VP' and representing a\\nsubject explicitly as a ``slashed'' NP, a sentence with added lexical\\nhead features would appear as:\\n\\n\\n\\t\\tThe \\t\\tS \\t\\t[ ] \\ndog \\t\\tN(dog) \\t\\t[S(np(dog))]  \\nwhich \\t\\tS(rel,np(dog)) \\t\\t[S(np(dog))] (12)\\t\\tchased \\t\\tS(np(dog)) \\t\\t[S(np(dog))]  \\nthe \\t\\tNP(cat) \\t\\t[S(np(dog))]  \\ncat \\t\\tN(cat) \\t\\t[S(np(dog))]  \\nbarked \\t\\tS(np(dog)) \\t\\t[ ]\\n\\n\\nIn contrast to n-grams, where this sentence would cloud somewhat the\\n``world knowledge'', containing as it does the bigram `cat barked', the\\nadded structure of our model allows the lexical preference to be\\ncaptured no matter how far the head noun is from the head verb. From\\n(12) the world knowledge of the system would be reinforced by the two\\nstereotypical transitions:\\n\\n\\n`chased'    S(np(dog))  \\n\\n\\n\\n\\nNP(cat) \\n`barked'   S(np(dog))  \\n\\n\\n\\n\\n[ ]\\n\\n\\n\\n  Present Implementation \\n\\n16,000+ running words from section N of the Brown corpus (texts N01-N08)\\nwere hand-parsed using the state-transition grammar. The actual\\nformalism used was much fuller than the rather\\nschematic one given above, including many additional features such as case,\\ntense, person and number.\\nTransition probabilities were generalized in the ways\\ndiscussed in the previous section.\\n\\n  Results \\n\\n100 sentences of less than 15 words were chosen randomly from other\\ntexts in section N of the Brown corpus (N09-N14) and fed to the parser\\nwithout alteration.\\nUnknown words in the input, of which there were obviously many,  were\\nassigned to one of seven orthographic classes and given appropriate\\ntransitions calculated from the corpus.\\n\\n\\n\\n\\n27 were parsed correctly, ie. exactly the same as the hand parse or\\ndiffering in only relatively insignificant ways which the model could\\n not hope to know. \\n\\n\\n23 were parsed wrongly, ie. the analysis differed from the hand parse\\nin some non-trivial way.\\n\\n\\n50 were not parsed at all, ie. one or more of the transitions\\nnecessary to find a parse path was lacking, even after generalizing\\nthe transitions.\\n\\n\\n\\n\\n  Future Development \\n\\nAlthough the results at present are extremely modest, it should be\\nborne in mind both that the amount of data the system has to work on\\nis very small and that the smoothing of transition probabilities is\\nstill far from optimal. The present target is to achieve such a level\\nof performance that the corpus can be extended by\\nhand-correction of the parser output, rather than hand-parsing from\\nscratch.\\nNot only will this hopefully save a certain amount of\\ndrudgery, it should also help to minimize errors and\\nmaintain consistency.\\n\\n\\nA more distant goal is to ascertain whether the performance of the\\nmodel can improve after parsing new texts and processing the data\\ntherein even without hand-correction of the parses, and if so what\\nthe limits are to such ``self-improvement''.\\n\\n\\n\\n  References \\n\\nAHO A.V. 1968. Indexed Grammars.   Journal of the ACM, 15: 647-671.\\n\\n\\nBOD, RENS 1992. A Computational Model of\\nLanguage Performance: Data Oriented Parsing.   COLING-92.\\n\\n\\nKARTTUNEN L. 1990.\\n  Radical Lexicalism. In Baltin  Kroch (eds),     Alternative conceptions of phrase structure,\\n  Univ of Chicago Press, pp 43-65.\\n\\n\\nKRAUWER, STEVEN  DES TOMBES, LOUIS 1981. Transducers and\\nGrammars as Theories of Language. Theoretical Linguistics, 8,\\n173-202.\\n\\n\\nMILWARD, DAVID 1990. Coordination\\nin an Axiomatic Grammar. COLING-90.\\n\\n\\nMILWARD, DAVID 1994.\\nNon-constituent Coordination: Theory and Practice. COLING-94.\\n\\nFootnotes\\n\\n  This research was funded by a research\\n    studentship from the ESRC. My thanks also for discussion and\\ncomments to Matt Crocker, Chris Brew, David Milward and Anna Babarczy.\\n  Bod, 1992.\\n  ibid.\\n  ``VP'' is used here and\\n  henceforth as a shorthand for an S with a missing (ie. ``slashed'')\\n  subject.\\n  The unidirectionality of the formalism results\\n  in an automatic type-raising of all categories appearing before\\n  their heads.\\n  There is in general no\\n  upper limit to the  length of this list, eg. ``I\\n  gave Fido a biscuit yesterday in the house and Rover a bone today in\\n  his kennel.''\\n  Milward (1990)\\n  introduces a  formalism essentially identical to the\\n  one presented here, although viewed from a very different\\n  perspective. Milward (1994) shows how it handles a wide range\\n  of  non-constituent co-ordinations.\\n  This may be compared to the claim in Krauwer \\nDes Tombes (1981) that finite-state automata offer a more satisfactory\\ncharacterization of language than context-free grammars.\\n  Such as the system postulating that\\n  ``Jess''  was a surname, as against the\\n  hand-parser's guess of a masculine first name.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper presents a grammar formalism designed for use in data-oriented\\napproaches to language processing. It goes on to investigate\\nways in which a corpus pre-parsed with this\\nformalism may be processed to provide a probabilistic language model\\nfor use in the parsing of fresh texts.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nAlthough a universal feature theory does not exist, there is\\na general understanding of its objects. The objects of feature\\ntheories are abstract linguistic objects, e.g., an object ``sentence,''\\nan object ``masculine third person singular,'' an object ``verb,'' an\\nobject ``noun phrase.'' These abstract objects have properties like\\n``tense,'' ``number,'' ``predicate,'' ``subject.''\\nThe values of these properties\\nare either atomic, like ``present'' and ``singular,'' or abstract\\nobjects, like ``verb'' and ``noun-phrase.''\\nThe abstract objects are fully described by their properties and\\ntheir values. Multiple descriptions for the properties and values\\nof the abstract linguistic objects are presented in the literature.\\nExamples are:\\n\\n\\n1.\\nFeature graphs, which are labeled rooted directed acyclic graphs\\nG=(V,A), where\\nF is a collection of labels,\\na sink\\nin the graph represents an atomic value and the labeling function\\nis an injective function \\n\\n\\n\\n\\n .\\n2.\\nAttribute-value matrices,\\nwhich are matrices in which the\\nentries consist of an attribute and a value or\\na reentrance symbol. The values are either atomic or\\nattribute-value matrices.\\n\\n\\nFrom a computational point of view, all descriptions that are used in\\npractical problems are equivalent. Though there exist some theories\\n with a considerably higher expressive power . For this paper we adopt the feature graph description, which we will\\ndefine somewhat more formal in the next section.\\n Attribute Value Languages(AVL)  consist of sets of logical formulas that describe classes of feature graphs,\\nby expressing constraints on the type of paths that can exist within\\nthe graphs. To wit: In a sentence like ``a man walks'' the edges\\nlabeled with ``person'' that leave the nodes\\nlabeled ``a man'' and ``walks'' should both\\nend in a node labeled ``singular.'' Such a constraint is called\\na ``path equation'' in the attribute-value language.\\n\\n\\n A rewrite grammar  can be enriched with an AVL to construct an Attribute Value\\nGrammar(AVG), which consists of pairs of rewrite-rules and logical\\nformulas. The rewrite rule is applicable to a production (nonterminal)\\nonly if the logical formula that expresses the relation between\\nleft- and right-hand side of the rule evaluates to true.\\nThe recognition problem for attribute-value grammars can be\\nstated as: Given a grammar G and a string w does there\\nexist a derivation in G, that respects the constraints\\ngiven by its AVL, and that ends in w.\\nAs the intermediate productions correspond\\nto feature graphs this question can also be formulated as\\na question about the existence of a consistent sequence of\\nfeature graphs that results in a feature graph describing w.\\nFor the rewrite grammar, any formalism in the Chomsky hierarchy\\n(from regular to type 0) can be chosen. From a computational point\\nof view it is of course most desirable to restrict oneself to\\na formalism that on the one hand gives enough expressibility\\nto describe a large fragment of the (natural) language, and\\non the other hand is restrictive enough to preserve feasibility.\\nFor a discussion on the linguistic significance of such restrictions,\\n see . \\n\\n\\n Johnson  proved that attribute-value grammars that are as restrictive as being equipped with a rewrite grammar\\nthat is regular can already give rise to an undecidable recognition\\nproblem.\\nObviously, to be of any practical use, the\\nrewrite grammar or the attribute-value language must be more restrictive.\\nJohnson proposed to add the off-line parsability constraint,\\nwhich is respected if the rewrite grammar has no chain- or\\n\\n\\n\\n -rules. Then, the number of applications in a production\\nis linear and the size of the structure corresponding to the\\npartial productions is polynomial. Hence as by a modification of\\n Smolka's algorithm  consistency of intermediate steps can be checked in quadratic time, the complexity of\\nthe recognition problem can at most be (nondeterministic) polynomial\\n time. This observation was made in , which also has an \\n\\n\\n\\n\\n -hardness proof of the recognition problem.\\n\\n\\nWe further investigate the properties of these restricted AVGs (R-AVGs).\\nIn the next section, we give some more formal definitions and notations.\\n In Section  we show that the class of languages generated by an R-AVG (R-AVGL) includes\\nthe class of context free languages (CFL). It follows that any easily\\nparsable class of languages (like CFL) is a proper\\nsubset of R-AVGL, unless \\n\\n\\n\\n\\n .\\nLikewise, R-AVGL is a proper subset of the class of context\\nsensitive languages, unless \\n\\n\\n\\n\\n .\\n In Section  we propose a further refinement on the off-line parsability constraint,\\nwhich allows R-AVGs that respect this constraint to capture\\nprecisely complexity classes like \\n\\n\\n\\n\\nor \\n\\n\\n\\n\\n .\\nThat is, for any language L that has an \\n\\n\\n\\n\\n -parser, there exists\\nan R-AVG, say G, such that L=L(G). Though our refinement, the\\nhonest parsability constraint is probably not a property that\\ncan be decided for arbitrary R-AVGs,\\nwe show that R-AVGs can be equipped\\nwith restricting mechanisms that enforce this property.\\n The techniques that prove Theorem  and  Theorem  result from Johnson's work. Therefore, the proofs of these theorems are deferred\\nto the appendices.\\n\\n\\n  Definitions and Notation \\n  Attribute-Value Grammars \\n\\nThe definitions in this section are in the spirit\\n of , Section 3.2]  and , Sections 3-4]. Consider three sets of pairwise disjoint symbols.\\n\\nA, the finite set of constants, denoted (\\n\\n\\n )\\n\\nV, the countable set of variables, denoted (\\n\\n\\n )\\n\\nL, the finite set of attributes, also called features,\\ndenoted (\\n\\n\\n )\\n\\n\\n\\n\\nDefinition thedefctr:\\nAn f-edge from x to s is a triple (x,f,s) such that x is\\na variable, f is an attribute, and s is a constant or a variable.\\nA path, p, is a, possibly empty, sequence of f-edges\\n\\n\\n\\n\\n  in which the xi are\\nvariables and s is either a variable or a constant.\\nOften a path is denoted by the sequence of its edges' attributes,\\nin reversed order, e.g., \\n\\n\\n\\n\\n .\\nLet p be a path, ps denotes the path that starts from s,\\nwhere s is a constant only if p is the empty path. If the path\\nis nonempty, \\n\\n\\n\\n\\n ,\\nthen s is a variable.\\nFor paths\\nps and qt we write \\n\\n\\n\\n\\niff p and q start in sand t respectively and end in the same variable or constant.\\nThe expression \\n\\n\\n\\n\\nis called a path equation.\\nA feature graph is either a pair \\n\\n\\n\\n\\n ,\\nor a pair (x,E) where x is the root and E a finite set of\\nf-edges such that:\\n1.\\nif (y,f,s) and (y,f,t) are in E, then s=t;\\n2.\\nif (y,f,s) is in E, then there\\nis a path from x to y in E.\\n\\n\\nDefinition thedefctr:\\nAn attribute-value language \\n\\n\\n\\n\\nconsists of\\nsets of logical formulas that describe feature graphs,\\nby expressing constraints on the type of paths that can exist within\\nthe graphs.\\n\\nThe terms of an\\nattribute-value language \\n\\n\\nare the\\nconstants and the variables \\n\\n\\n .\\n\\nThe formulas of an attribute-value language \\n\\n\\n are path equations and Boolean combinations\\nof path equations. Thus all formulas are either \\n\\n\\n ,\\nwhere\\nps and qt are paths, or\\n\\n\\n ,\\n\\n\\n ,\\nor \\n\\n ,\\nwhere \\n\\nand\\n\\nare formulas.\\n\\n\\n\\n\\nAssume a finite set \\n\\n\\n\\n\\n(of lexical forms) and a finite set \\n\\n\\n\\n\\n (of categories).\\n\\n\\n\\n\\nwill play the role of the set of terminals and \\n\\n\\n\\n\\nwill play the\\nrole of the set of nonterminals in the productions.\\n\\n\\nDefinition thedefctr:\\nA constituent structure tree (CST) is a labeled\\ntree in which the internal nodes are labeled with elements of Cat and the\\nleaves are labeled with elements of Lex.\\n\\n\\nDefinition thedefctr:\\nLet T be a constituent structure tree and F be\\na set of formulas in an attribute-value language  \\n\\n\\n\\n\\n .\\nAn annotated constituent structure tree is a triple\\n\\n\\n\\n\\n ,\\nwhere h is a function that maps\\ninternal nodes in T onto variables in F.\\n\\n\\nDefinition thedefctr:\\nA lexicon is a finite subset of \\n\\n\\n\\n\\nA set of\\nsyntactic rules is a finite subset of\\n\\n\\n\\n\\n .\\nAn attribute-value grammar is a triple\\n\\n\\n\\n\\n>,\\nwhere lexicon is a lexicon, rules is a set of\\nsyntactic rules and start is an element of \\n\\n\\n\\n\\n .\\n\\n\\nDefinition thedefctr:\\n1.\\n , p .150]  A class \\n\\n\\n\\nof sets is recursively presentable iff there is an\\neffective enumeration \\n\\n\\n\\n\\nof deterministic Turing\\nmachines which halt on all their inputs, and such that \\n\\n\\n\\n\\n2.\\nWe say that a class of grammars \\n\\n\\n\\nis recursively\\npresentable iff the class of sets \\n\\n\\n\\n\\n is recursively presentable.\\n\\n\\n  Restricted Attribute-Value Grammars \\n\\nThe only formulas that are allowed in the attribute-value language\\nof restricted attribute-value grammars (R-AVGs)\\nare path-equations and conjunctions\\nof path-equations (i.e. disjunctions and negations are out). We\\nwill denote the attribute-value language of an R-AVG by\\n\\n\\n\\n\\nto make the distinction clear.\\nThe CST of an R-AVG is produced by a chain- and \\n\\n\\n\\n -rule free\\nregular grammar. The CST of an R-AVG can be either a left-branching\\nor a right-branching tree, since the grammar contains\\nat most one nonterminal in each rule.\\nDefinition thedefctr:\\nThe set of syntactic rules of a restricted attribute-value\\ngrammar is a subset of \\n\\n\\n\\n\\n>.\\nA restricted attribute-value grammar is a pair\\n\\n\\n\\n\\n>,\\nwhere rules is a set of syntactic rules and start is\\nan element of \\n\\n\\n\\n\\n .\\n\\n\\nDefinition thedefctr:\\nAn R-AVG \\n\\n\\n\\n\\n>\\ngenerates an annotated constituent structure\\ntree \\n\\n\\n\\n\\niff\\n1.\\nthe root node of T is start, and\\n2.\\nevery internal node of T is licensed by a syntactic rule, and\\n3.\\nthe set F is consistent, i.e., describes a feature graph.\\nLet \\n\\n\\n\\nstand for the formula \\n\\n\\n\\nin which all\\nvariable y is substituted for variable x.\\nAn internal node v of an annotated constituent structure tree is\\nlicensed by a syntactic rule \\n\\n\\n\\n\\n iff\\n1.\\nthe node v is labeled with category c0, \\n\\nh(v) = n0, and\\n2.\\nall daughters of v are leaves, which are labeled with\\n\\n\\n\\n\\n ,\\nand\\n3.\\n\\n\\n\\n\\nis in the set F.\\nAn internal node v of an annotated constituent structure tree is\\nlicensed by a syntactic rule \\n\\n\\n\\n\\n iff\\n1.\\nthe node v is labeled with category c0, \\n\\nh(v) = n0, and\\n2.\\none of v's daughters is an internal node, v1, which is\\nlabeled with category c1, and \\n\\nh(v1) = n1, and\\n3.\\nthe daughters of v that are leaves are labeled with\\n\\n\\n\\n\\n ,\\nand\\n4.\\n\\n\\n\\n\\nis in the set F.\\n\\n\\n\\n  Weak Generative Capacity\\n\\n\\n In , it is shown that the recognition problem for\\nR-AVGs is \\n\\n\\n\\n\\n -complete. This seems to indicate that although\\nthe mechanism for generating CSTs in R-AVGs is extremely simple, the\\ngenerative capacity of R-AVGs is different from the generative\\ncapacity of e.g., context free languages (CFLs), which have a polynomial\\n time parsing algorithm . Yet, a priori, there may exist CFLs that do not have an R-AVG.\\n\\n\\n    Let L be a context free language. There exists an\\nR-AVG G such that L=L(G).\\n\\n\\nProof.If L is a context free language, then there exists\\na context free grammar G' in Greibach normal form such that L=L(G').\\nFrom this grammar G', we can construct a pushdown store\\nM that accepts exactly the words in L(G')=L. Such a pushdown store\\nM is actually a finite state automaton M' with a stack S.\\nThe finite state automaton M' may be simulated by a\\nchain- and \\n\\n\\n\\n -rule free regular grammar.\\nFurthermore, we can construct an attribute-value language\\n\\n\\n\\n\\nthat simulates the stack S.\\nThus it should be clear that there exists an R-AVG G that\\nproduces word w iff \\n\\n\\n\\n\\n .\\n Details of this construction are deferred to Appendix . \\n\\n\\n\\n  The Honest Parsability Constraint and Consequences\\n\\n\\n According to Theorem , it is unlikely that the languages generated by R-AVGs can be limited to those languages with a\\npolynomial time recognition algorithm.\\n Trautwein  showed that all R-AVGs have nondeterministic polynomial time algorithms. Is it perhaps\\nthe case that any language that has a nondeterministic polynomial\\ntime recognition algorithm can be generated by an R-AVG. Does\\nthere exist a tight relation between time bounded machines and R-AVGs\\nas e.g., between LBAs and CSLs? The answer is that the off-line\\nparsability constraint that forces the R-AVG to have no chain-\\nor \\n\\n\\n\\n -rules\\nis just too restrictive to allow such a connection.\\nThe following trick to alleviate this problem has been observed earlier\\nin complexity theory. The off-line\\n parsability constraint(OLP)  relates the amount of ``work'' done by the grammar to produce a string\\nlinearly to the number of terminal symbols produced. It is therefore\\na sort of honesty constraint that is also demanded of functions\\nthat are used in e.g., cryptography. There the deal is, for each\\npolynomial amount of work done to compute the function at least\\none bit of output must be produced. In such a way, for polynomial\\ntime computable functions one can guarantee that the inverse of\\nthe function is computable in nondeterministic polynomial time.\\n\\n\\nAs a more liberal constraint on R-AVGs we propose an analogous\\nvariation on the OLP\\nDefinition thedefctr:\\nA grammar G satisfies the Honest Parsability\\nConstraint(HPC) iff there exists a polynomial p s.t. for each win L(G) there exists a derivation with at most \\n\\n\\n\\n\\n steps.\\n\\n\\nFrom Smolka's algorithm and Trautwein's observation it trivially\\nfollows that any attribute-value grammar that satisfies the\\nHPC (HP-AVG) has an \\n\\n\\n\\n\\nrecognition algorithm. The problem with the\\nHPC is of course that it is not a syntactic property of grammars.\\nThe question whether a given AVG satisfies the HPC (or the OLP for\\nthat matter) may well be undecidable.\\nNonetheless, we can produce a set of rules that,\\nwhen added to an attribute-value grammar enforces the\\nHPC. The newly produced language is then a subset of the old\\nproduced language with an \\n\\n\\n\\n\\nrecognition algorithm.\\nBecause of the fact that our addition may simulate any polynomial\\nrestriction, we regain the full class of AVG's that satisfy the HPC.\\nIn fact\\n\\n\\nTheorem  4.1   \\nThe class, P-AVGL, of languages produced by the HP-AVGs\\nis recursively presentable.\\n\\n\\nWe will give\\na detailed construction of such a set of rules in\\n Appendix . The existence of such a set of rules and the work of Johnson now gives the following theorem.\\n\\n\\n    For any language L that has an \\n\\n\\n\\n\\nrecognition\\nalgorithm, there exists a\\nrestricted attribute-value grammar G that respects\\nthe HPC and such that L=L(G).\\n\\n\\nProof.(Sketch)\\nLet M be the Turing machine that decides \\n\\n\\n\\n .\\nUse\\na variation of Johnson's construction of a Turing machine\\nto create an R-AVG\\nthat can produce any string w that is recognized by M. Add the\\nset of rules that guarantee that only strings that can be produced\\nwith a polynomial number of rules can be produced by the grammar.\\n\\n\\n\\n  Veer out the HPC \\n\\nInstead of creating a counter of logarithmic size as we do in\\n Appendix , it is quite straightforward to construct a counter of linear size (or exponential size if there is enough\\ntime). In fact, for well-behaved functions, the construction of a\\ncounter gives a method to enforce any desired time bound constraint on\\nthe recognition problem for attribute-value grammars.\\nFor instance, for nondeterministic exponential time we could define\\nthe Linear Dishonest Parsability Constraint (LDP) (allowing a linear\\nexponential number of steps) which would give.\\n\\n\\nTheorem  5.1   \\nThe class of languages generated by R-AVGs obeying the\\nLDP condition is exactly \\n\\n\\n\\n\\n .\\n\\n\\n  Acknowledgements \\n\\nWe are indebted to E. Aarts and W.C. Rounds for their valuable\\nsuggestions on an early presentation of this work.\\n\\nBibliography \\n\\nJ. Balczar, J. Daz, and J. Gabarr.\\nStructural Complexity I.\\nSpringer-Verlag, New York, 1988.\\n\\n\\nP. Blackburn and E. Spaan.\\nA modal perspective on the computational complexity of attribute\\n  value grammar.\\nJournal of Logic, Language and Information, 2(2):129-169,\\n  1993.\\n\\n\\nN. Chomsky.\\nThree models for the description of language.\\nIRE Transactions on Information Theory, 2(3):113-124, 1956.\\n\\n\\nJ. Earley.\\nAn efficient context-free parsing algorithm.\\nCommunications of the Association for Computing Machinery,\\n  13(2):94-102, February 1970.\\n\\n\\nJ. Hopcroft and J. Ullman.\\nIntroduction to Automata Theory, Languages, and Computation.\\nAddison Wesley, Reading, MA, 1979.\\n\\n\\nM. Johnson.\\nAttribute-Value Logic and the Theory of Grammar, volume 16 of\\n  CSLI Lecture Notes.\\nCSLI, Stanford, 1988.\\n\\n\\nC. Perrault.\\nOn the mathematical properties of linguistic theories.\\nComputational Linguistics, 10(3-4):165-176, 1984.\\n\\n\\nG. Smolka.\\nFeature-constraint logics for unification grammars.\\nJournal of Logic Programming, 12(1):51-87, 1992.\\n\\n\\nT. Sudkamp.\\nLanguages and Machines: An introduction to the Theory of\\n  Computer Science.\\nAddison Wesley, Reading, MA, 1988.\\n\\n\\nM. Trautwein.\\nAssessing complexity results in feature theories.\\nILLC Research Report and Technical Notes Series LP-95-01,\\n  University of Amsterdam, Amsterdam, 1995.\\nSubmitted to the CMP-LG archive.\\n\\n\\n\\n  Simulating a Context Free Grammar in GNF\\n\\n\\nA context free grammar (CFG) is a quadruple\\n\\n\\n\\n\\n ,\\nwhere N is a set of nonterminals,\\n\\n\\n\\nis a set of terminals, P is a set of productions,\\nand \\n\\n\\n\\nis the start nonterminal.\\nA CFG is in Greibach normalform (GNF) if, and only if, the\\nproductions are of one of the following\\nforms, where \\n\\n\\n\\n\\n and\\n\\n\\n\\n the empty string (c.f., ,  ): \\n\\n\\nGiven a GNF \\n\\n\\n\\n\\n ,\\nwe can construct a restricted attribute-value grammar (R-AVG)\\nG' that simulates grammar G. R-AVG G' consists\\nof the same set of nonterminals and terminals as GNF G.\\nThe productions of R-AVG G' are described by\\n Table . The only two attributes of R-AVG G' are  TOP and\\n REST. R-AVG G' contains |N| + 1 atomic values, one atomic\\nvalue for each nonterminal and the special atomic value $.\\nThe R-AVG G' uses the feature graph to\\nencode a push-down stack, similar to the encoding of a list.\\nThe stack will be used to store the\\nnonterminals that still have to be rewritten.\\n\\n\\nThe three syntactic abbreviations below are used to clarify the simulation.\\nWe use represent a stack by a Greek letter, or a string of symbols;\\nthe top of the stack is the leftmost symbol of the string.\\nLet x0 encode a stack \\n\\n\\n\\n ,\\nthen the formulas in\\nthe abbreviation \\n\\n\\n\\n\\nexpress that x1 encodes\\na stack \\n\\n\\n\\n\\n .\\nLikewise, the formulas in the\\nabbreviation \\n\\n\\n\\n\\nexpress that x0 encodes a stack\\n\\n\\n\\n ,\\nand X1 encodes the stack \\n\\n\\n\\n .\\nThe abbreviation\\n EMPTY-STACK expresses that x0 encodes an empty stack.\\n\\n\\n\\n\\nWe have to prove that GNF Gand its simulation by R-AVG G'generate (almost) the same language. Obviously, R-AVG G'cannot generate the empty string. However,\\nfor all non-empty strings the following theorem holds.\\n\\n\\nTheorem  6.1   \\nStart nonterminal S of GNF Gderives string \\n\\n\\n\\n(\\n\\n\\n\\n\\n )\\nif, and only if,\\nstart nonterminal S of R-AVG G'derives string \\n\\n\\n\\nwith the empty stack.\\n\\n\\nProof.There are two cases to consider. First, S derives string \\n\\n\\n\\n in one step. Second, S derives string \\n\\n\\n\\nin more than one\\nstep. The lemma below is needed in the proof of the second case.\\n\\n\\nCase I\\nLet start nonterminal S derive string \\n\\n\\n\\nin one step.\\nGNF G contains a production\\n\\n\\n\\n\\niff R-AVG G'   contains a production \\n\\n\\n\\n\\nwith the equation\\n    EMPTY-STACK.\\n   So, S derives \\n\\n\\n\\nin a\\n   derivation of GNF G iff S derives \\n\\n\\n\\nwith an empty stack in the derivation of R-AVG G'.\\nCase II\\nInitial nonterminal S of GNF G derives\\n   string \\n\\n\\n\\n\\nin more than one step iff there is a\\n   left-most derivation \\n\\n\\n\\n\\n. GNF G contains production\\n\\n\\n\\n\\niff R-AVG G' contains production\\n\\n\\n\\n\\nwith the equation\\n    EMPTY-STACK. By the next lemma: \\n\\n\\n\\n\\niff \\n\\n\\n\\n\\nwith\\n   the empty stack.\\n   Hence S derives \\n\\n\\n\\nfor GNF G iff\\n   S derives \\n\\n\\n\\nwith empty stack for R-AVG G'.\\n\\n\\n\\n  Constructing an Honestly Parsable Attribute-Value\\nGrammar\\n\\n\\nIn this section we show how to add a binary counter to an\\nattribute-value grammar (AVG). This counter enforces the\\nHonest-Parsability Constraint (HPC) upon the AVG. To keep\\nthis section legible we sometimes use the\\nattribute-value matrices (AVMs) as descriptions.\\n In Section , we show how to create a counter for the AVG.\\n In Section  we show how to extend the syntactic rules and the lexicon\\nof the AVG.\\n\\n  Arithmetic by AVGs \\n\\nWe start with a little bit of arithmetic.\\n\\n  Natural numbers. \\n\\nThe AVMs below encode natural numbers in binary\\nnotation.  The sequences of attributes  0 and  1 in these\\nAVMs\\nencode natural numbers, from least- to most-significant bit.\\nThe attribute  V has value 1 (or 0) if, and only if,\\nit has a sister attribute  1 (or  0).\\n1.\\nThe AVMs \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\n        encode the natural numbers zero and one.\\n2.\\nThe AVMs \\n\\n\\n\\n\\n        and \\n\\n\\n\\n\\nencode\\n        natural numbers iff the AVM [F] encodes a natural number.\\n\\n  Syntactic rules that tests two numbers for equality. \\n\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] and [H] encode\\nnatural number x and y, respectively.\\nWe present one syntactic rule that derives from this\\nnonterminal A a nonterminal B with AVM\\n\\n\\n\\n\\nif x = y.\\n\\n\\n\\n\\nClearly, this simple test takes one step. A more sophisticated\\ntest, which also tests for inequality, would compare [F]and [G] bit-by-bit.  Such a test would take\\n\\n\\n\\n\\n\\n\\n\\n\\n derivation steps.\\n\\n\\n  Syntactic rules that multiply by two. \\n\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] encodes natural number x.\\nWe present one syntactic rule that derives from this\\nnonterminal A a nonterminal B with the AVM\\n\\n\\n\\n\\n ,\\nwhere [H] encodes natural number 2x.\\n\\n\\nThe number  N in [H] equals two times  N in [F]if, and only if, the least-significant bit of  N in [H]is 0, and the remaining bits form the same sequence as the\\nnumber  N in [F]. Multiplication by two takes one derivation\\nstep.\\n\\n\\n\\n\\n  Syntactic rules that increments by one. \\n\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] encodes natural number x.\\nWe present five syntactic rules that derive from this\\nnonterminal A a nonterminal C with AVM\\n\\n\\n\\n\\n ,\\nwhere [H] encodes natural number x+1.\\n\\n\\nThe increment of  N requires two additional pointers in the\\nAVM of A: attribute  P points to the next bit\\nthat has to be incremented; attribute  Q points to the\\nmost-significant bit of the (intermediate) result.\\nThese additional pointers are hidden from the\\nAVMs of the nonterminals A and C.\\n\\n\\n The five rules from Table  increment  N by one. Nonterminal A rewrites, in one or more steps, to nonterminal\\nC, potentially through a number of nonterminals B.\\n\\n\\n\\n\\n The first and fourth rule of Table  state that adding one to a zero bit sets\\nthis bit to one and ends the increment. The second and third rule state\\nthat adding one to a one bit sets this bit to zero and the increment\\ncontinues.  The fifth rule states that adding one to the\\nmost-significant bit\\nsets this bit to zero and yields a new most-significant one bit.\\nWe claim that \\n\\n\\n\\n\\n takes \\n\\n\\n\\n\\nderivation steps.\\n\\n\\nRules, similar to the ones above, can be given that decrement\\nthe attribute  N by one. We only have to take a little\\nextra care that the number 0 cannot be decremented.\\n\\n\\n  Syntactic rules that sum two numbers. \\n\\nIn this section we use the previous test and increment rules\\n(indicated by =).\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F] and [H] encode\\nnatural number x and y, respectively.\\n We present syntactic rules (Table -) that derive from this\\nnonterminal A a nonterminal C with AVM\\n\\n\\n\\n\\n ,\\n where [F'] encodes the natural number x + y.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe increment of  N by  M is similar to the\\nincrement by one. Here, three additional pointers are required:\\nthe attributes  P and  Q point to the bits in  N and\\n M respectively that have to be summed next; attribute  R\\npoints to the most-significant bit of the (intermediate) result.\\nIn the addition two states\\nare distinguished. In the one state, the carry bit is zero, indicated\\nby nonterminal A'. In the other state, the carry bit is one,\\nindicated by nonterminal B.\\nWe claim that \\n\\n\\n\\n\\n takes \\n\\n\\n\\n\\n derivation steps.\\n\\n\\n  Syntactic rules that sum a sequence of numbers. \\n\\nIn this section we use the previous summation rules\\n(indicated by =).\\nAssume a nonterminal A with some AVM\\n\\n\\n\\n\\n ,\\nwhere [F'] encodes a list of numbers. To wit\\n\\n\\n\\nwhere [Gi] encodes natural number xi.\\n We present syntactic rules (Table ) that derive from this nonterminal A a nonterminal B with AVM\\n\\n\\n\\n\\n ,\\nwhere [F] encodes the natural number \\n\\n\\n\\n\\n .\\n\\n\\nThe summation requires an additional pointer in the\\nAVM [F']: attribute  P points to the next element\\nin the list that has to be summed.\\nWe claim that \\n\\n\\n\\n\\n takes \\n\\n\\n\\n\\nderivation steps.\\n\\n\\n\\n\\n\\n\\n  Creating a counter of logarithmic size\\n\\n\\nCreate an AVM of the following form:\\n\\n\\n\\nAttribute  COUNTER is used to distinguish the AVMs\\nthat encodes the counter from those in the\\noriginal attribute-value grammar. We will neglect the\\nattribute  COUNTER in the remainder of this section, because it is\\nnot essential here.\\nThe attributes  SIZE,  N,  M and  POLY\\nencode natural numbers.\\nThe attribute  SIZE records the size of the string that will be\\ngenerated.  The attribute  POLY records the maximum number of\\nderivation steps that is allowed for a string of size  SIZE.\\nThe attributes  N and  M are auxiliary numbers.\\n\\n\\nThe construction of the counter starts with an initiation-step.\\nThe further construction of the counter consists of cycles of two\\nphases. Each cycle starts in nonterminal A.\\n\\n  Initiation step and first phase. \\n\\nThe initiation-step sets the numbers  SIZE and  N to 0,\\nand the numbers  M and  POLY to 1.\\nIn the first phase of each cycle, the numbers  SIZE and  N\\nare incremented by 1.\\n\\n\\n\\n  The second phase of the cycle. \\n\\nIn this phase the numbers  N and  M are compared.\\nIf  N is twice  M, then (i) number\\n POLY is extended by k bits, (ii) number  M is doubled,\\nand (iii) number  N is set to 0.\\nIf  N is less than twice  M, nothing happens.\\n\\n\\nThe left rule of the second phase doubles the number  M in\\nthe second and the third equation.\\nThe test ``Is  N equal to 2 M?'' therefore reduces to one\\n(the first) equation.\\nThe fourth equation extend the number  POLY with k bits.\\nThe fifth and sixth equations set the number  N to 0.\\n\\n\\nThe right rule is always applicable. If the right rule is used where\\nthe left rule was applicable, then the number  N will never be\\nequal to \\n\\n\\n\\n\\nin the rest of the derivation. Thus  POLY will\\nnot be extended any more.\\n\\n\\n\\n\\nWe claim that the left rule appears \\n\\n\\n\\ntimes and the right\\nrule O(n) times in a derivation for input of size n.\\nObviously, the number  POLY is \\n\\n\\n\\n\\nwhen\\nthe number  SIZE is i.\\n\\n\\n\\n\\n  From AVG to HP-AVG\\n\\n\\nIn this section we show how to transform an AVG\\ninto an AVG that satisfies the HPC (HP-AVG).\\nSince all computation steps of the HP-AVG\\nonly require a linear amount of derivation steps,\\ntotal derivations of HP-AVGs have polynomial length.\\n\\n\\nWe can divide the attributes of the HP-AVG into two groups. The\\nattributes that encode the counters, and the attributes of the\\noriginal AVG. The former will be embedded under the attribute\\n COUNTER, the latter under the attribute  GRAMMAR.\\nIn the sequel, we mean by \\n\\n\\n\\n\\nthe formula \\n\\n\\n\\n embedded under the attribute  GRAMMAR, i.e., the formula\\nobtained from \\n\\n\\n\\nby substituting the variables xi by\\n\\n\\n\\n\\n .\\n\\n\\nThe HP-AVG is obtained from the AVG in three steps: change\\nthe start nonterminal, the lexicon and the syntactic rules.\\nFirst, the HP-AVG contains the rules of the previous section, which\\n construct the counter. The nonterminal S from Table  is the start nonterminal of the HP-AVG. For the nonterminal A the start\\n nonterminal of the AVG is taken. Nonterminal B from Table  is a fresh nonterminal, not occurring in the AVG.\\n\\n\\nSecond, the HP-AVG contains an extension of the lexicon of the AVG.\\nThe entries of the lexicon are extended in the following way. The size\\nof the lexical form is set to one, and the amount of derivation steps\\nis zero. Thus, if \\n\\n\\n\\n\\nis the lexicon of the AVG,\\nthen \\n\\n\\n\\n\\nis the lexicon of the HP-AVG, where\\n\\n\\n\\nThird, the HP-AVG contains extensions of the syntactic rules of the AVG.\\nThe syntactic rules are extended in the following way. The numbers\\n POLY and  SIZE of the daughter nonterminals are collected in\\nthe lists  PLIST and  SLIST. Both lists are summed.\\nThe number  SIZE of the mother nonterminal is equal to\\nthe sum of  SIZE's,\\nand the number  POLY of the mother nonterminal is one\\nmore than the sum of  POLY's.\\nThus, if \\n\\n\\n\\n\\n is a syntactic rule of the AVG, then \\n\\n\\n\\n\\n is a syntactic rule of the HP-AVG, where\\n\\n\\n\\nNow, a derivation for the HP-AVG starts with a nondeterministic\\nconstruction of a counter  SIZE with value n and a counter\\n POLY with value O(n[k]). Then, the derivation of the original\\nAVG is simulated, such that\\n(i)\\n the mother nonterminal produces a string of size n if, and only if\\n the daughter nonterminals together produce a string of size n, and\\n(ii)\\n the mother nonterminal makes n[k]+1 derivation steps if, and only if\\n the daughter nonterminals together make n[k] derivation steps.\\n\\nFootnotes\\n\\n  The author was supported in part by HCM grant\\n        ERB4050PL93-0516.\\n  The author was supported by the Foundation for language,\\n        speech and logic (TSL), which is funded by the Netherlands\\n        organization for scientific research (NWO)\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThe recognition problem for attribute-value grammars(AVGs) was shown\\nto be undecidable by Johnson in 1988. Therefore, the general\\nform of AVGs is of no practical use. In this paper we study\\na very restricted form of AVG, for which the recognition problem\\nis decidable (though still \\n\\n\\n\\n\\n -complete), the R-AVG. We show that\\nthe R-AVG formalism captures all of the context free languages and\\nmore, and introduce a variation on the so-called off-line\\nparsability constraint, the honest\\nparsability constraint, which lets different types of\\nR-AVG coincide precisely with well-known time complexity classes.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nRecently, there has been a growing interest in research on formalizing\\nfeature theory.\\nSome formalisms that appeared lately are the feature algebra of\\n , the modal logic of ,  the deterministic finite automata of , and  the first-order predicate logic of . These formalisms describe the use of feature theory in\\ncomputational linguistics. They are a source of interesting\\ntechnical research, and various complexity results have been achieved.\\nHowever, we argue that such formalisms offer little help to\\ncomputational linguists in practice.\\nThe grammatical theories used in computational linguistics do not\\nconsist of bare feature theories.\\nThe feature theories that are used in computational linguistics are\\ncontained in unification grammars. These unification grammars\\nconsist of constituent structure components, and feature theories.\\nWe claim that the complexity results from the formalisms do no\\nlonger hold when a feature theory\\nand a constituent structure component are combined into\\na unification grammar.\\n\\n\\nIn this paper, we will focus on the complexity results that are obtained\\nfrom formalizing feature theories.\\nWe will prove that these complexity results do not hold if we consider\\nunification grammars that use these feature\\ntheories in addition to a constituent structure component.\\nFirst we will show, that the complexity of\\na unification grammar theory may be higher than the\\ncomplexity of its feature theory and constituent structure components.\\nSecond we will explain, that the complexity of a unification grammar\\nmay be lower than the complexity of the formalized feature theory.\\n\\n\\nBoth proofs put the complexity results that have been achieved in a\\ndifferent perspective. The first proof\\nimplies that the complexity of a feature\\ntheory does not provide an upper bound for the complexity\\nof grammars using that feature theory. The second proof\\nimplies that the  complexity of a feature\\ntheory might not provide a lower bound for the complexity\\nof grammars using that feature theory.\\nTherefore, we argue that if one is interested in the complexity of\\nunification grammars that are used in grammars, one should look at the\\ncomplexity of these unification grammars as a whole.\\nNo insight in the complexity of a unification grammar is gained by\\nlooking only at the complexity of its components in isolation.\\n\\n\\nThe outline of this paper is as follows. The next section contains\\nthe preliminaries on complexity theory and feature theory. In\\n Section , we introduce a simple feature theory: a feature theory with only reentrance.\\n In Section , we present a unification grammar that uses this simple feature theory.\\nWe show that the recognition problem of this grammar is harder than\\nthe unification problem of the feature theory and the recognition\\nproblem of the constituent structure component.\\n In Section , we explain why the recognition problem of a unification grammar might be of lower\\ncomplexity than the unification problem of the feature theory.\\n In Section , we present our conclusions.\\n\\n\\n  Preliminaries \\n  Complexity Theory. \\n\\nIn complexity theory one tries to determine the complexity of\\nproblems. The complexity is measured by the amount of time and\\nspace needed to solve a problem. Usually, one considers decision\\nproblems: problems that are answered  `Yes' or `No'.\\nOften we are interested in the distinction between tractable\\nand intractable problems. A problem is tractable if\\nits solution requires an amount of steps that is polynomial in the\\nsize of the input: we say that the problem requires\\npolynomial time. Likewise, we speak of linear time, etcetera.\\nThe tractable problems are also called `P problems'.\\nThe intractable problems are called `NP-hard problems'. The easiest\\nintractable problems are the `NP-complete problems'. It is\\nunknown whether NP-complete problems have polynomial time solutions.\\nHowever we know, that solutions for NP-complete problems can\\nbe guessed and checked in polynomial time. It is strongly\\nbelieved that the class of P problems and the class of\\nNP-complete problems are different, although this is yet\\nunproven.\\n\\n\\nThere is a direct manner to determine the upper bound\\ncomplexity of a problem,\\nif there is an algorithm that solves the problem: determine the\\ncomplexity of that algorithm.\\nAn indirect way to determine the lower bound\\ncomplexity of a problem is the reduction.\\nA reduction from some problem A to some problem B maps\\ninstances of problem A onto instances of problem B.\\n\\n\\nThe reductions that we will consider are known as polynomial time,\\nmany-one reductions. These many-one reductions are subject to\\ntwo conditions:\\n(1) the reductions are easy to compute, and\\n(2) the reductions preserve the answers.\\nA reduction from A to B is easy to compute, if the mapping takes\\npolynomial time.\\nA reduction preserves answers if the answer to the instance of A is\\nthe same as the answer to the instance of B. That is, the answer\\nto the instance of A is `Yes' if, and only if,\\nthe answer to the instance of B is also `Yes'.\\n\\n\\nA reduction is an elegant way to classify a problem as intractable.\\nSuppose problem B is a problem with unknown complexity.\\nLet there be a reduction f from an NP-hard problem A to problem B.\\nFurthermore, let f conform to the two conditions above.\\nBy an indirect proof, it follows from this reduction that\\nB is at least as hard as A.\\nHence B is also an NP-hard problem. If we also prove that we can guess\\na solution for B and check that guessed solution in polynomial time,\\nthen B is an NP-complete problem.\\n\\n\\nA well-known NP-complete problem is  SATISFIABILITY (SAT).\\n\\n\\nDefinition  2.1   \\n SATISFIABILITY \\n INSTANCE:\\n A formula \\n\\n\\n\\n ,\\nfrom propositional logic, in conjunctive\\n normalform.  \\n QUESTION:\\n Is there an assignment of truth-values to the propositional\\n variables of \\n\\n\\n\\n ,\\nsuch that \\n\\n\\n\\nis true?\\n\\n\\nThe instances of  SATISFIABILITY are formulas in conjunctive\\nnormalform, i.e., the formulas are conjunctions of\\nclauses. The clauses are disjunctions of literals, and the literals are\\npositive and negative occurrences of propositional variables.\\nWe call formula \\n\\n\\n\\na satisfiable formula\\nif an assignment exists that makes formula \\n\\n\\n\\ntrue.\\n\\n\\nAn assignment assigns either the value true or\\nthe value false to each propositional variable.\\nGiven such an assignment, we can determine the truth-value of a\\nformula.\\nThe formula \\n\\n\\n\\n\\n is true if, and only if, each clause, \\n\\n\\n\\n ,\\nis true.\\nA clause \\n\\n\\n\\n\\n is true if, and only if, at least one literal, li, is true.\\nA positive (negative) literal, li = pj (\\n\\n\\n\\n\\n ),\\nis true if, and only if, the variable pj is\\nassigned the value true (false).\\n\\n  Feature theory. \\n\\nAlthough there is no such thing as a universal feature theory, there\\nis a general understanding of its abstract objects.\\nThese abstract objects describe the internal information or\\nproperties of words and phrases.\\nProperties that these abstract objects typically have\\nare the case, the gender, the number, and the tense of\\nwords and phrases.\\n\\n\\nThe properties of abstract objects can be combined to form new\\nabstract objects. This operation is called unification.\\nThe unification of abstract objects combines all the properties of\\nthese abstract objects, provided that the properties are\\nnot contradictory.\\n\\n\\nAll kinds of additions to these rudiments of feature theory have been\\npresented in the literature. We will not discuss them here, but\\n refer to Section , in which we introduce a feature theory that serves our purposes.\\n\\n\\n\\n\\n  A simple feature theory\\n\\n\\nIn this section we will present a simple\\nfeature theory. The feature theory contains reentrance, but\\nno negation or disjunction.\\nAlthough this feature theory is simple, it contains\\nmany aspects from other feature theories.\\n In addition, Section  shows that combining this simple feature theory with a simple\\nconstituent structure component results in a\\ndifficult unification grammar.\\n\\n\\nIn the first part of this section, we will\\nformalize the notion of a feature theory.\\nIn the second part of this section, we will\\npresent an algorithm that solves\\nthe unification problem in an amount of time that is quadratic\\nin the size of its input.\\nThis part should convince the reader that the feature theory\\nis indeed simple.\\n\\n  The feature theory formally. \\n\\nAlthough a universal feature theory does not exist,\\nthere is a general understanding of its objects.\\nThe object of feature theories are abstract linguistic objects,\\ne.g., an object `sentence', an object `masculine third person singular',\\nan object `verb', an object `noun phrase'. These\\nabstract objects have properties, like, tense, number, predicate,\\nsubject. The values of these properties are either atomic, like,\\npresent and singular, or abstract objects, like, verb and noun\\nphrase.\\n\\n\\nThe abstract objects can be represented as rooted graphs\\n(`feature-graphs'). The nodes of these graphs stand for abstract\\nobjects, and the edges represent the properties.\\nMore formally, a feature-graph is either a pair \\n\\n\\n\\n\\n ,\\nwhere a is an\\natomic value and \\n\\n\\n\\nis the empty set, or a pair (x, E),\\nwhere x is a root node, and E is a finite, possibly empty set of\\nedges such that (1) for each property and all nodes there is at\\nmost one edge that represents the property departing from the node,\\nand (2) if there\\nis an edge in E from node y to node z, then there is a path\\nin E leading from node x to node y.\\n\\n\\nAs an example consider the following abstract objects and\\nsimplified feature-graph.\\n\\n\\nExample(s)\\n\\nSentence: A man walks \\n        This abstract object has property  TENSE with value\\n        present, property  SUBJECT with value Noun phrase:\\n        A man, and property  PREDICATE with value Verb: walks.\\n\\nNoun phrase: A man \\n        has property  NUMBER with value singular.\\n\\nVerb: walks \\n        also has property  NUMBER with value singular.\\n\\n\\n\\n\\nThe abstract objects are fully described by their properties and their\\nvalues.\\nMultiple descriptions for the properties and values of the abstract\\nlinguistic objects are presented in the literature.\\nA formal description language for these\\nproperties and values of the abstract linguistic objects is\\na sublanguage of predicate logic with equality, FL,\\n introduced by . \\n\\n\\nAssume three pair-wise disjunct sets of symbols: the set of constants\\nA, the set of variables V, and the set of attributes L.\\nThe attributes (denoted by f, g, h or capitalized strings)\\ncorrespond to the properties of\\nthe abstract objects,\\nthe variables (denoted by x, y, z) correspond to the abstract\\nobjects, and\\nthe constants (denoted by a, b, c or italicized strings)\\ncorrespond to the atomic values.\\nLet s, t denote variables or constants, and let\\na path (denoted by p, q) be a finite, possible empty sequence of\\nattributes.\\n\\n\\nDefinition  3.1   \\nThe terms of the description language FL are the elements\\nfrom V and A.\\nThe formulas of the description language (FL-formulas) are\\nequations, and conjunctions:\\n\\n\\n\\nif \\n\\n\\n\\n\\nare formulas, p, q are paths, and s, t are\\nterms.  The formulas of the following form are called\\nprimitive formulas:\\n\\n\\n\\nThe description language FL is interpreted as a special\\n algebra in . However for our purposes it suffices to interpret the formulas\\nas feature graphs. The formula\\n\\n\\n\\n\\nis interpreted as: the terms s and t denote the\\nsame node in the feature-graph.\\nThe formula \\n\\n\\n\\n\\nis interpreted as:\\nthere is an edge with label ffrom the node denoted by s to the node denoted by tin the feature-graph.\\n\\n\\n As an example, consider the feature-graph given in Figure . The following formula describes the feature-graph,\\nprovided that the proper sets A, V and L are given.\\n\\n\\n\\nAnother familiar, intuitive description is the attribute-value matrix\\nnotation. An attribute-value matrix (AVM) is a set of attribute-value pairs.\\nThe values of the attribute-value pairs are boxlabels, and atomic\\nvalues or AVMs, where equal boxlabels denote equal values. The\\nelements of an AVM are written below one another. The total\\nset is written between squared brackets.\\n\\n\\n For instance, the feature-graph given in Figure  could be represented by the following attribute-value matrix.\\nThe box-labels \\n\\n\\n\\nare used to denote that the two attributes\\n NUMBER have the same value.\\n\\n\\n\\nThe AVM notation is intuitive because AVMs strongly resemble\\nfeature-graphs. We can view the opening brackets and the\\natomic values of an AVM as nodes. The outermost bracket is the\\nroot-node. The attributes of the AVM can be view as edges with the\\nattribute as their label. The box-labels identify nodes in the\\nfeature-graph.\\n\\n\\nIn this paper we will use both the AVMs and the FL-formulas as a\\ndescription language. Because AVMs can be transformed in linear time\\n into formulas , Section 6] the use of different notations should cause no confusion.\\n\\n  Unification in FL. \\n\\nLet A and B be abstract linguistic objects, or feature-graphs,\\nthat are described by the FL-formulas \\n\\n\\n\\nand \\n\\n\\n\\n ,\\nrespectively.  The unification of A and B is described by\\nFL-formula \\n\\n\\n\\n\\n if and only if \\n\\n\\n\\n\\ndescribes a\\nfeature-graph.\\nIn the final part of this section\\nwe will present an efficient algorithm that\\ndetermines whether an FL-formula describes a feature-graph.\\nHence we can view the algorithm as a unification algorithm.\\n\\n  Unification in AVM. \\n\\nLet A and B be abstract linguistic objects, or feature-graphs,\\nthat are described by the AVMs [F] and [G],\\nrespectively.  The unification of A and B is denoted by\\n\\n\\n\\n\\n .\\nThe algorithm of the final part of this section can be used\\nto compute the AVM \\n\\n\\n\\n\\nefficiently, in the following way.\\n\\n\\nFirst, there is a linear time algorithm that\\ntransforms AVMs into FL formulas.\\nSecond, the algorithm of the final part of this section can\\neasily be modified\\nsuch that it also outputs the feature-graph that is described by\\nan FL-formula. Since the modified algorithm will remain\\nefficient, the feature-graph will be small.\\nFinally, there is a trivial, linear time, algorithm that transforms\\nfeature-graphs into AVMs.\\n\\n\\n  This feature theory is simple. \\n\\nIn the remainder of this section we will show that the feature theory\\nis simple. We will provide an algorithm, called  FEATUREGRAPHSAT, that\\ndetermines whether a formula of the description language describes a\\nfeature-graph.\\nThe algorithm is a slight modification of the constraint-solving\\n algorithm in , Section 5]. \\n\\n\\nThe algorithm  FEATUREGRAPHSAT can be used to\\ndetermine whether two abstract objects can be unified: if the formulas\\n\\n\\n\\nand \\n\\n\\n\\ndescribe abstract objects, then \\n\\n\\n\\n\\ndescribes their unification if, and only if, the unification\\nexists. So we may say that the algorithm solves the unification\\nproblem.\\n\\n\\nThe algorithm  FEATUREGRAPHSAT below\\ndetermines syntactically whether a formula is\\nsatisfiable in some feature algebra. Because there is a 1-1\\ncorrespondence between\\nsatisfiable formulas and feature-graphs, the algorithm determines\\nwhether a formula describes a feature-graph.\\nThe algorithm first transforms any formula by means of syntactic\\nsimplification rules into a normal form. Then\\nthis normal form is checked syntactically in order to see\\nwhether the formula is satisfiable.\\n\\n\\nThe correctness and the complexity of the algorithm  FEATUREGRAPHSAT\\n follow from , Section 5]. The function  TRANSFORM, the procedure  SIMPLIFY, the clash-freeness test and\\nthe acyclicity test can all be computed in an amount of time that is\\nquadratic in the size of the formula \\n\\n\\n\\n .\\nHence the algorithm  FEATUREGRAPHSAT takes quadratic time,\\nand thus shows that the feature theory is indeed simple.\\n\\n\\n ALGORITHM FEATUREGRAPHSAT  INPUT: \\t\\tFormula \\n\\n\\n\\nfrom the                description language.  OUTPUT: \\t\\t1) `Yes' if \\n\\n\\n\\ndescribes an acyclic                feature-graph, or         2) `No' otherwise.Begin AlgorithmEach \\n\\n\\n\\nis of the form \\n\\n\\n\\n ,\\nwhere p, q are paths,s,t are terms.  TRANSFORM \\n\\n\\n\\ninto a set of primitive formulas:                   \\n\\n\\n\\n .\\n SIMPLIFY the set P, yielding set S, until no further                simplification is possible. If set S is clash-free and acyclic, then\\t\\tExit with answer `Yes', else \\t\\tExit with answer `No'. End Algorithm\\n\\n\\n FUNCTION TRANSFORM         INPUT: \\t\\tFormula \\n\\n\\n\\nfrom the                description language.  OUTPUT: \\t\\tA set of primitive formulas \\n\\n\\n\\n .\\nBegin Function \\n\\n\\n\\n ,\\nwhere Step 0. \\t\\t\\n\\n\\n\\nStep 1. \\t\\t\\n\\n\\n\\n, where y is a        fresh variable Step 2. \\t\\t\\n\\n\\n\\n, where         yi (\\n\\n\\n\\n )\\nare fresh variables, and y is a variable        introduced in step 1. End Function\\n\\n\\nIn the procedure  SIMPLIFY we will use the following\\nnotations. We use [x/s]P to denote the set that is obtained from\\nP by replacing every occurrence of variable x by term s, and\\n\\n\\n\\n\\nto denote the set \\n\\n\\n\\n\\n ,\\nprovided that \\n\\n\\n\\n\\n .\\n\\n\\n  PROCEDURE SIMPLIFY (c.f., )        INPUT: \\t\\tSet of primitive formulas P. OUTPUT: \\t\\tSimplified set of primitive formulas S. Begin ProcedureDo while one of the following four simplification rules isapplicable    1. \\t\\t \\n\\n\\n\\nif x occurs in P and \\n\\n\\n\\n   2. \\t\\t\\n\\n\\n\\n   3. \\t\\t\\n\\n\\n\\n   4. \\t\\t\\n\\n\\n\\n   End while    Exit with the simplified form of set P, S. End Procedure\\n\\n\\nLemma  3.1   \\nA simplified set of primitive formulas S is clash-free if\\n1.\\nS contains no formula \\n\\n\\n\\n\\n ,\\nand\\n2.\\nS contains no formula \\n\\n\\n\\n\\nsuch that \\n\\n\\n\\n .\\n\\n\\n ProofFrom , Proposition 5.4]. \\n\\n\\nLemma  3.2   \\nA simplified set of primitive formulas S is acyclic if, and only\\nif, S does not contain a sequence of formulas \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\n(\\n\\n\\n\\n\\n ).\\n\\n\\nProofBy induction on the length of a cycle.\\n\\n\\n\\n\\n\\n  No upper bound\\n\\n\\nAn novice in complexity theory might expect\\nthat a problem is not harder\\nthan the problem's hardest component. However,\\ncombining problems may yield a problem that is harder than each of the\\nproblems when considered separately.\\nFor instance,\\n  combines context-free grammars with a  simple feature theory similar to the one in Section . Of course, both the\\nsatisfiability problem of this feature theory and\\nthe universal recognition problem of context-free grammars are\\ndecidable.\\nNevertheless, Johnson shows that the universal recognition problem\\nof the combination is undecidable in general.\\nJohnson also proves that this problem is decidable under the restriction that\\nthe context-free grammar does not contain detours. This restriction\\nis called the `Off-line Parsability Constraint'.\\n\\n\\nFrom Johnson's work, we see that combining problems may change\\nthe complexity from decidable to undecidable.\\nWe claim that combining problems may change also\\nthe complexity from tractable to intractable.\\nHence, even when we confine ourselves to decidable problems,\\nthe complexity of the recognition problem of a unification grammar that\\nuses some feature theory may be higher than\\nthe complexity of the\\nsatisfiability problem of that feature theory.\\nThe claim shows that even under the Off-line Parsability Constraint\\nthe complexity of the feature theory still does not provide an upper\\nbound on the complexity of the unification grammar.\\n\\n\\nIn the next section we will present a fixed regular grammar.\\nThen we combine this regular grammar with the feature theory from\\n Section  into a unification grammar. The recognition problem of this unification grammar is decidable,\\nbecause the regular grammar does not contain detours.\\nFinally, we will prove by a reduction from  SATISFIABILITY\\nthat the recognition problem of this unification grammar is NP-hard,\\nwhich proves the claim by example.\\n\\n  A fixed regular grammar\\n\\n\\nThe regular language that we want to recognize is\\n\\n\\n\\n\\n .\\nThe rules of a regular grammar G' that generates this language are\\n given in Table . \\n\\n\\n     The regular grammar in Table     generates the language \\n\\n\\n\\n\\n .\\n\\n\\nMany other regular grammars could be given for the same language.\\nHowever, the one presented, as will be seen later, is sufficient for our\\npurposes here: that is, the reduction from  SATISFIABILITY.\\nObviously, the recognition problem of fixed regular grammar\\ntakes linear time.\\n\\n\\n  Combining a regular grammar and a feature\\ntheory\\n\\n\\nIn this section, we will present the unification grammar G, which is\\na combination of the regular grammar from the previous section and\\n the feature theory from Section . There are multiple formalisms for unification grammars.\\nMost of these formalisms distinguish two components: a constituent\\nstructure and a feature graph. The two components are related\\nby a mapping from the nodes in the constituent structure to the nodes\\nin the feature graph.\\n\\n\\n Table  contains the grammar rules of unification grammar G.\\n The notation for the grammar is similar to .  The rules of Section  are annotated with formulas taken  from the feature theory given in Section . The set of attributes is \\n\\n\\n\\n\\nthe set of atomic values is\\n\\n\\n\\n\\n .\\nThe linear rewrite rules describe how constituents are formed.\\nThe formulas indicate how nodes of the feature-graphs\\nare related to the non-terminals of the rewrite rules.\\n\\n\\n The second rule in the first line of Table  will be used to explain the notation.\\nThe non-terminal on the left-hand side of the rewrite rule is related\\nto the node denoted by variable x0.\\nThe leftmost non-terminal on the right-hand side of the rewrite rule\\nis related to the node denoted by variable x1.\\nThe first conjunct of the formula states that the values of the\\nattributes  ASSIGN is the same for the nodes related to the\\nnon-terminals S and T.\\nThe second conjunct requires that the attribute  ASSIGN\\nof the node related to the non-terminal S has also the same\\nvalue as the\\nattribute  NEW of node related to the non-terminal T.\\nWe will clarify the use of the grammar by means\\nof an example.\\n\\n\\nExample(s)We will show the potential derivation of the string\\n\\n\\n\\n\\n .\\n On the left of the figures  and  the constituent structure trees are given. The non-terminals\\nare related to nodes in the feature-graphs by undirected arcs.\\n We present the first steps (figure ) and the `final'  result (figure ) of the potential derivation. The reader should check that the feature-graph indeed conforms to the\\nformulas of the applied rules.\\n\\n\\n The potential feature-graph in figure  shows that the rightmost\\nnode should have two different atomic values, indicated by + or -.\\nHence this potential feature graph is not valid.  Consequently, the\\nderivation given above fails, and the string\\n\\n\\n\\n\\ncannot be generated.\\n\\n\\n The following fact results from fact  and the previous example,\\nwhich showed that \\n\\n\\n\\n\\n cannot be generated by G.\\n\\n\\nFact  4.2   \\nThe language recognized by the unification grammar Gis a proper subset of the regular language\\n\\n\\n\\n\\n .\\n\\n\\nThe following fact will be useful in the proof of\\n Lemma . The fact states that if S derives  \\n\\n\\n\\nin\\nd steps\\n(\\n\\n\\n\\n\\n ), then there are two intermediate stages.\\nFirst, S derives\\n\\n\\n\\n\\nin a steps.\\nThis T derives \\n\\n\\n\\nin b steps.\\nFinally, this A derives \\n\\n\\n\\n\\n in csteps.\\n\\n\\n    If \\n\\n\\n\\n\\n ,\\nwhere\\n\\n\\n\\n\\n ,\\nand\\n\\n\\n\\n\\n ,\\nthen\\nthere is a \\n\\n\\n\\n\\n\\n\\n\\n\\nsuch that\\n\\n\\n\\n\\n(d = a + b + c)and the feature structure\\n\\n\\n\\n\\n is associated with T,\\nwhere \\n\\n\\n\\n\\nif l = p,\\nand   \\n\\n\\n\\n\\nif \\n\\n\\n\\n\\n .\\n\\n\\n  The reduction from SAT. \\n\\nIn the previous section we combined the regular grammar from\\n Section  and the feature theory from  Section  into a unification grammar G. Both the recognition problem of this regular grammar,\\nand the\\nsatisfiability problem of this feature\\ntheory take polynomial time.\\nHowever, we will prove that the recognition problem of\\nthe unification grammar G is NP-hard.\\nThus the complexity of the\\nfeature theory does not provide an upper bound on the complexity of\\nthe  grammar that used this feature theory.\\n\\n\\nFirst, we will give the reduction from the NP-complete problem SAT\\nto the recognition problem\\nof G. Then we will show that this reduction is\\ncomputable in polynomial time\\nand answer preserving.\\nThus we have proven that the recognition problem of\\nthe unification grammar G is NP-hard.\\n\\n\\nThe reduction from SAT to the recognition problem of Gmaps propositional logical formulas onto strings.\\nWe assume, without loss of generality, that the indices\\nof the propositional logical variables are in binary representation.\\nThis reduction, f, is defined by the following four equations:\\n\\n\\n\\n    The reduction f maps formula \\n\\n\\n\\nonto string\\n\\n\\n\\n\\n ,\\nwhere\\n\\n\\n\\n\\n ,\\nand\\nv[i]j is a string of the form \\n\\n\\n\\n\\n .\\n\\n\\n    The reduction f is computable in linear time.\\n\\n\\nProofBy induction on the construction of SAT formulas.\\n\\n\\n    Let \\n\\n\\n\\nbe a propositional logical formula in conjunctive\\nnormalform, and f the reduction stated above.\\nFormula \\n\\n\\n\\n\\nis a\\nsatisfiable formula if, and only if, string \\n\\n\\n\\n\\nis in the\\nlanguage generated by G.\\n\\n\\nProofThe proof of this lemma is split in two subproofs. First, we will prove\\nthat if \\n\\n\\n\\nis satisfiable, then w is in the language generated\\nby G. Second, we will prove that if\\n\\n\\n\\n\\nis in the language generated by G,\\nthen \\n\\n\\n\\nis satisfiable.\\n\\n  Only if: \\n\\n let \\n\\n\\n\\nbe a satisfiable formula.\\nThen there is an assignment g such that\\n\\n(1) if g assigns a truth-value to one occurrence of a variable,\\nthen g assigns that truth-value to all occurrences of that variable\\nin the formula. In other words, g is consistent.\\n\\n(2) g assigns truth to the formula. That is, in each clause,\\ng assigns truth to some literal.\\n\\n\\nWe have to show that \\n\\n\\n\\n\\nis generated by G. According\\n to Fact  \\n\\n\\n\\n .\\nThis string w is\\ngenerated by G if, and only if, the string \\n\\n\\n\\n\\nis\\nderived by S. Moreover, \\n\\n\\n\\n\\nif and\\nonly if \\n\\n\\n\\n\\n .\\n By Fact , each derivation  \\n\\n\\n\\n ,\\nhas the\\nfollowing intermediate steps:\\n\\n\\n\\nLet us assume that\\n\\n\\n\\n\\n ,\\nonly if the\\nassignment g assigns truth to the k-th literal in the i-th\\nclause of \\n\\n\\n\\n .\\nThis k-th literal in the i-th clause, is either\\n\\n\\n\\n\\nor \\n\\n\\n\\n\\n .\\nIn the first\\ncase g assigns truth-value true to variable \\n\\n\\n\\n\\n ,\\nin\\nthe second case g assigns truth-value false to variable\\n\\n\\n\\n\\n .\\nBy induction on the number of substrings wi, we will prove that\\nunder the above made\\nassumption S derives \\n\\n\\n\\n\\n .\\n\\n\\nOne substring wm:\\nLet S0 = S derive wm S (\\n\\n\\n\\n\\n ),\\nwhere k depends on the assignment g:\\n\\n\\n\\nThe non-terminal S derives the empty string in one step.\\nThus the feature structure associated with S is\\n\\n\\n\\n\\n .\\nThe feature structure associated with T is the unification of\\n\\n\\n\\n\\nand\\nthe feature structure associated with S:\\n\\n\\n\\nwhere \\n\\n\\n\\n\\nif \\n\\n\\n\\n\\n ,\\nand \\n\\n\\n\\n\\nif \\n\\n\\n\\n\\n .\\nThe feature structure associated with S0 is\\n\\n\\n\\nNone of the unifications fails, and thus S derives wm.\\n\\n\\nMore than one substring wi:\\nLet S0 = S derive wi S (\\n\\n\\n\\n\\n ):\\n\\n\\n\\nBy the induction hypothesis, we assume that S derives \\n\\n\\n\\n\\nMoreover, the feature structure associated with S is\\n\\n\\n\\n\\n= \\n\\n\\n\\n ,\\nwhere \\n\\n\\n\\nis a feature structure of the form \\n\\n\\n\\n\\nor a unification of such feature\\nstructures.\\nThe feature structure associated with T is the unification of\\n\\n\\n\\n\\nand\\nthe feature structure associated with S:\\n\\n\\n\\nIn the case that v[i]k is a prefix of wi the feature\\n structure () is associated with S0. In the other cases, there is an intermediate step\\n\\n\\n\\n and feature structure () is associated with F, where \\n\\n\\n\\nis the unification of\\n\\n\\n\\n\\nand \\n\\n\\n\\n .\\n\\n\\n\\n\\n\\n In all cases the unification in () fails only if  \\n\\n\\n\\n contains \\n\\n\\n\\n\\n ,\\nand\\n\\n\\n\\n\\nfails. But, \\n\\n\\n\\n\\n fails only if g assigns\\nboth truth-value true and truth-value false to variable\\n\\n\\n\\n\\n .\\nHence\\n\\n\\n\\n\\n would fail only if g would be inconsistent, which g is not.\\nHence there is a derivation for string \\n\\n\\n\\n\\n if \\n\\n\\n\\nis satisfiable.\\n\\n  If: \\n\\n suppose that \\n\\n\\n\\n\\nis in the language\\ngenerated by G.\\n By fact  \\n\\n\\n\\n ,\\nwhere\\n\\n\\n\\n\\n .\\nWe will prove that for all i, there is a k such that\\n\\n1) \\n\\n\\n\\n2) the feature structure associated with the non-terminal S        that derives w contains \\n\\n\\nwhere\\n\\n\\nif \\n\\n\\n ,\\nand\\n\\n\\nif \\n\\n\\n .\\n\\n3) the feature structure associated with the non-terminal S        that derives w does not contain both\\n\\n\\nand\\n\\n\\nThen the feature structure associated with the non-terminal Sthat derives w encodes a consistent assignment for \\n\\nthat\\nmakes every clause of \\n\\ntrue.\\n\\n\\n\\n\\nObviously, \\n\\n\\n\\n\\nif, and only if, \\n\\n\\n\\n\\n .\\n Hence 1) and 2) follow from fact . Because S derives w, the feature structure associated with S does not contain\\ncontradicting information: 3) follows.\\nThis completes the second subproof.\\n\\n\\nThe previous lemma proves that the reduction f from SAT\\n to the recognition problem of the unification grammar Gis answer preserving. Lemma  proves that this reduction f is computable in polynomial time.\\nHence these two lemmas together prove that the recognition\\nproblem of the unification grammar G is NP-hard.\\n  show that the complexity result of the recognition problem for unification grammars\\nthat combine a regular grammar and\\n the feature theory from Section  is strengthened. An additional NP upper bound is proven\\nfor an arbitrary string and grammar,\\nwhich results in an\\nNP-complete recognition problem.\\n\\n\\nLemma  4.8   \\nLet w be any string and G be any\\nunification grammar that combines a regular grammar and\\n the feature theory from Section . Then the recognition problem for w and G is NP-complete.\\n\\n\\nProofAn NP-hard lower bound is proven above. An NP upper bound\\nis proven when we can guess a solution, and check\\nthat solution in polynomial time.\\nThe NP upper bound is proven as follows.\\n\\n\\nGiven a string w and a grammar G, we can guess a sequence of\\nO(|w|) rules that encode the derivation for w. The guessed\\nrules describe a constituent structure tree and a set of formulas.\\nFirst, we must check that the constituent structure tree described\\nby the rules has yield w. Second, we have\\nto check that the set of formulas describes some feature-graph.\\n\\n\\nThe first check is trivial. The second check is performed by the\\n algorithm  FEATUREGRAPHSAT from Section . Clearly, both checks only take polynomial time.\\n\\n\\n\\n\\n\\n  On lower bounds\\n\\n\\nThe previous section shows the complexity of a feature\\ntheory does not provide an upper bound for the complexity\\nof a unification grammar that uses this feature theory.\\nThe question that arises is whether the complexity of a feature\\ntheory provides a lower bound for the complexity\\nof such a unification grammar.\\n\\n\\nIn general, it seems that the complexity of the combination of\\ntwo problems is at least as hard as the complexity of these\\ntwo problems in isolation. So one would be tempted to answer\\nthe question\\nabove in the affirmative. However, if a problem A contains\\ninformation about solutions for a problem B, and vice versa,\\nthen the combination of A and B may have lower complexity than Aand B in isolation.  For instance, let problem A be the complement\\nof problem B. Then the combinations `A or B' and `A and B'\\nhave the trivial solutions `always answer yes' and `always answer no',\\nrespectively.\\n\\n\\nTo be more specific, in the case of unification grammars,\\nthere seem to be easy reductions from the unification problem\\nof a feature theory to the recognition problem of arbitrary\\nunification grammars that use this feature theory.\\nIn some specific situations, however, these reductions do not exist.\\nBelow, we will present some examples of situations in which the\\nfeature theory does not provide a lower bound for the\\nrecognition problem.\\n\\n\\nExample(s)\\n\\nThe feature theory does not provide a lower bound if\\n        the complexity of the recognition problem of the\\n        grammar component provides a lower bound for the complexity of\\n        the recognition problem of the unification grammar.\\n        Consider for instance the class of grammars that generate\\n        a finite language.\\n        The combination of a feature theory with a grammar from this\\n        class yields a unification grammar that generates a finite\\n        language. Obviously, the recognition problem of this\\n        unification grammar\\n        does not depend on the unification problem of the feature\\n        theory. Hence the lower bound complexity of this class of\\n        unification grammars is not provided by the complexity of the\\n        feature theory.\\n\\nThe feature theory does not provide a lower bound if the\\n        unification grammar uses only a fragment of the feature theory.\\n        This happens when the unification grammar formalism\\n        restricts the unification. For instance, the unification grammar\\n        formalism may demand that feature structures are unified at the\\n        outermost attributes. This demand implies that the size of the\\n        feature structures that appear in the fixed unification grammar is\\n        bounded. Consequently, there have to be feature structures in\\n        the feature theory that cannot be encoded by the unification\\n        grammar.\\nOne may object that the obligatory\\n        unification at the outermost attribute should be incorporated\\n        in the formalization of the feature theory. Thus reducing\\n        the complexity of the unification problem of the feature theory.\\n        However, there is no predefined way to construct unification\\n        grammars from a feature theory and a grammar component. So,\\n        there may be many blurred restrictions on the unification.\\n        These blurred restrictions are the cause that the formalization\\n        of the feature theory\\n        may be too expressive and that the unification grammar uses\\n        only a fragment of the feature theory.\\n\\n\\n\\n\\nThe two examples show that not in all\\nsituations the complexity of the unification problem of the\\nfeature theory provides a lower bound for the complexity of\\nthe recognition problem of the unification grammar. In some special\\ncases the complexity of the unification grammar may be lower than\\nthe complexity of the feature theory.\\nHence care has to be taken for\\ndrawing overhasty conclusions about the lower bound complexity of\\nthe unification grammar from the complexity of the feature theory.\\n\\n\\n  Conclusions\\n\\n\\nIn this paper, we have assessed the complexity results of\\nformalizations that intend to describe feature theories in\\ncomputational linguistics.\\nThese formalizations do not take the constituent structure\\ncomponent of unification grammars into account. As a result,\\nthe complexity of the unification problem of\\nfeature theories does not provide an upper bound, and need not provide\\na lower bound, for the complexity of the recognition problem of\\nunification grammars using these theories.\\n\\n\\nThus the complexity results that have been achieved in\\nthe formalisms of feature theories\\nare not immediately relevant for unification\\ngrammars used in computational linguistics.\\nComplexity analyses will only contribute to computational linguistics\\nif the analyzed formalizations are connected closely with actual\\nunification grammars.\\nTherefore, we argue for formalisms that describe\\nunification grammars as a whole instead of bare feature theories.\\n\\nBibliography \\n\\nFranz Baader, Hans-Jrgen Brckert, Bernhard Nebel, Werner Nutt, and Gert\\n  Smolka.\\nOn the expressivity of feature logics with negation, functional\\n  uncertainty, and sort equations.\\nJournal of Logic, Language and Information, 2(1):1-18, 1993.\\n\\n\\nPatrick Blackburn and Edith Spaan.\\nA modal perspective on the computational complexity of attribute\\n  value grammar.\\nJournal of Logic, Language and Information, 2(2):129-169,\\n  1993.\\n\\n\\nMark Johnson.\\nAttribute-Value Logic and the Theory of Grammar, volume 16 of\\n  CSLI Lecture Notes.\\nCSLI, Stanford, 1988.\\n\\n\\nRobert T. Kasper and William C. Rounds.\\nThe logic of unification in grammar.\\nLinguistics and Philosophy, 13:35-58, 1990.\\n\\n\\nGert Smolka.\\nFeature-constraint logics for unification grammars.\\nJournal of Logic Programming, 12(1):51-87, 1992.\\n\\n\\nLeen Torenvliet and Marten Trautwein.\\nFeatures that count.\\nPresented at CLIN V (Fifth Computational Linguistics in the\\n  Netherlands Meeting), December 1994.\\n\\nFootnotes\\n\\n  This research was supported by the\\n        Linguistic Research Foundation, which is funded by the Netherlands\\n        organisation for scientific research, NWO\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nIn this paper, we assess the complexity results of\\nformalisms that describe the\\nfeature theories used in computational linguistics.\\nWe show that from these complexity results\\nno immediate conclusions can be drawn about the complexity of\\nthe recognition\\nproblem of unification grammars using these feature\\ntheories.\\nOn the one hand, the complexity of feature\\ntheories does not provide an upper bound for the complexity\\nof such unification grammars.\\nOn the other hand, the complexity of feature\\ntheories need not provide a lower bound.\\nTherefore, we argue for formalisms that describe\\nactual unification grammars instead of feature theories.\\nThus the complexity results of these formalisms\\njudge upon the hardness of unification\\ngrammars in computational linguistics.\\n\\n'],\n",
              " ['\\n\\n  Introduction \\n\\nCompositionality is defined as the property that the\\nmeaning of a whole is a function of the meaning of its parts (cf. e.g.\\n , pp.24-25). (A slightly less general definition,  e.g. , postulates the existence of a homomorphism from syntax to semantics). However,\\nwe can prove a theorem stating that any semantics can be encoded as a\\ncompositional semantics, which means that, essentially, the standard\\ndefinition of compositionality is formally vacuous.\\nThus, although intuitively clear, the definition is not restrictive\\nenough. We illustrate the power of the theorem by showing how\\nto assign compositional semantics to idioms and to a very\\ncounterintuitive semantics of coordination (Section 4).\\n\\n\\nGiven a class of functions F, we say that the\\ncompositional semantics is systematic  if the\\nmeaning function belongs to the class F. We show that when\\ncompositional semantics is required to be systematic\\nwe can distinguish between grammars with\\ncompositional and non-compositional semantics;\\nwe present an example of a simple grammar for which there is no\\n\"systematic\" compositional semantics (Section 3).\\n\\n\\nAs a result, we believe that the paper clarifies the\\nconcept of compositionality and opens a possibility of making systematic\\ncomparisons of different systems of grammars and\\nnatural language understanding programs.\\nFurthermore, the concept of systematicity that we introduce in\\nthe paper might be useful in extracting the formal meaning behind\\nvarious versions of compositionality as a philosophical principle,\\n cf.. But in this paper we restrict ourselves to the mathematics of\\ncompositionality.\\n\\n\\nCompositional semantics is usually defined  as  a functional\\ndependence of the meaning of an expression on the meanings of its  parts.\\nOne   of  the  first  natural  questions  we  might want to ask is whether a\\nset of natural language expressions, i.e. a language,\\ncan have some compositional semantics. This question has\\nbeen answered positively by\\n . However his result says nothing about what kinds of things should be assigned e.g. to nouns, where,\\nobviously,\\nwe would like nouns to be mapped into sets of entities, or something like\\nthat. That is, we want semantics to encode some basic\\nintuitions, e.g. that\\nnouns denote sets of entities, and verbs denote relations between entities,\\nand so on; in other words, we would like to have\\na compositional semantics that agrees with intuitions.\\nMore formally, the questions is whether after deciding what\\nsentences and\\ntheir parts mean, we can find a function that would compose the meaning  of a\\nwhole from the meanings of its parts.\\n\\n\\nThe answer to this question is somewhat disturbing. It turns out  that\\nwhatever we decide that some language expressions should mean, it is always\\npossible to produce a function that would give compositional semantics\\nto it (see below for a more\\nprecise formulation of this fact). The upshot is that compositionality, as\\ncommonly defined, is not a strong constraint on a semantic theory.\\n\\n\\n  Proving the existence of compositional semantics \\n\\nLet S be  any collection of expressions (intuitively, sentences and their\\nparts). Assume that elements of S (e.g. s.t)\\nare composed from other elements of\\nS (that is, s and t)  by concatenation (\".\").\\nWe do not assume that concatenation is associative, that is\\n\\n(a.(b.c)) = ((a.b).c). Intuitively, this means that we assign\\nsemantics to parse trees,  and not to strings of words.\\n\\n\\nLet M be a set  of meanings, and let for any \\n\\n\\n\\n ,\\n\\n\\n\\n\\nbe the meaning of s. We want to show that there\\nis  a compositional semantics for S which agrees with the function massociating s with m(s).\\n\\n\\nSince elements of Mcan be of any type, we do not automatically have\\n\\n\\n\\n\\n ,\\nwhere \\n\\n\\n\\nis some\\noperation on the meanings. To\\nget that kind of homomorphism we have to perform a type raising operation\\nthat would map elements of Sinto functions  and  then the functions into the\\nrequired meanings.\\nNote that such a type raising operation is quite common\\nboth in mathematics  (e.g. 1 being a function equal to 1 for all\\nvalues) and in mathematical linguistics.\\nThe  meaning function \\n\\n\\n\\nthat we want to define will provide\\ncompositional semantics for S by mapping it into\\na set of functions in such a way that \\n\\n\\n\\n\\n ,\\nfor all elements s.t of S.\\n\\n\\nSecondly, we want that the original semantics be\\neasily decoded from  \\n\\n\\n\\n .\\nThere is more than one way of\\ndoing this.\\nOne can trivially extend the language S by adding\\nto   it an \"end of expression\" character \\n\\n\\n\\n ,\\nwhich may appear only as the\\nlast element of any expression. The purpose of it is to encode the function\\nm(x) in the following way:\\n\\n\\n\\n\\n ,\\nfor all s in S. Intuitively, the\\ncharacter \\n\\n\\n\\nis like the period at the end of a sentence, or the pause\\nmarking the end of an utterance. In effect, we will be treating all\\nsentences as idioms, or garden path sentences, where the meanings are\\nclear only once the sentence is completed (Theorem 2).\\nBut, as we are going to show now, the original\\nsemantics can be encoded in a different way, without extending the original\\nlanguage, e.g. by assuring \\n\\n\\n\\n\\n ,\\nfor all s in S (Theorem 1).\\n\\n\\nTo make the notation simple, we have assumed that there is only one\\nway of composing elements  of S, by concatenation,\\nbut  all our arguments work for languages with many operators as well.\\nWe show an example of how such operators can be handled in Section 4.\\n\\n\\n THEOREM 1.\\nLet M be an arbitrary set.\\nLet A be an arbitrary alphabet. Let \".\" be a binary operation,\\nand let S be the set closure of A under \".\". Let \\n\\n\\n\\n\\nbe an arbitrary function.\\n\\n\\nThen there is a set of functions M[*] and a unique\\nmap \\n\\n\\n\\n\\nsuch that for all s, \\n\\n\\n\\n\\n COROLLARY 2.  Theorem 1 is also true when\\nthe binary operation \".\" is partial.\\n PRELIMINARIES TO THE PROOF: THE SOLUTION LEMMA \\nOur results will be proved in set theory with the\\nanti-foundation axiom.\\nThis set theory, ZFA,  is equiconsistent with\\nthe standard system of ZFC, thus the theorem does not assume\\nanything more than what is needed for \"standard mathematical\\npractice\". Furthermore, ZFA is better suited as foundations for\\nsemantics of natural language  than ZFC\\n (). \\n\\n\\nWe need only one (but fundamental) theorem of ZFA:\\n the solution lemma ( and  ), which says any (well-formed) collection of equations that define sets\\nhas a unique solution.\\nFor the reader who is not familiar with set theory, the meaning of the\\nsolution lemma can be explained as follows:\\nWe have a universe of sets V, and\\na set of variables \\n\\n\\n\\n\\n ,\\nwhich may be infinite (countable or uncountable). We can form equations\\nof the form\\n\\n\\n\\nwhere \\n\\n\\n\\n\\nis a set expression involving the variables and\\nelements of V, for instance, if \\n\\n\\n\\nand\\n\\n\\n\\n ,\\nwe can write the following equations:\\n\\n\\n\\nWe say that such a set is well-formed if each variable appear only\\nonce on the left, and each left hand side is a variable.\\nThe solution lemma says that any set of such equations (finite or\\ninfinite) has a unique\\nsolution. That is, there is a unique\\ncollection of sets that satisfy them.\\n\\n\\n\\n PROOF OF THEOREM 1 AND COROLLARY 2 \\n\\n\\nProof of Theorem 1.\\nIt is enough to ensure that for all \\n\\n\\n\\n\\nClearly,\\n\\n\\n\\nis a function, because it is a collection of pairs.\\nThe proof is complete once we check that\\nfor  s\\'s and   t\\'s in S we have\\n(i)  \\n\\n\\n\\n\\n, and\\n(ii) \\n\\n\\n\\n\\n.\\nUsing the above equality we check (i): If \\n\\n\\n\\n ,\\nthen\\n\\n\\n\\n\\n .\\nSimilarly for (ii).\\n\\n\\nIt remains to show that using the solution lemma we can make the\\nabove equation true for all \\n\\n\\n\\n .\\nWe begin by\\nintroducing a set variable\\nXs for every  \\n\\n\\n\\n ,\\nand observing that\\n\\n\\n\\nis a well-formed set equation for any \\n\\n\\n\\n .\\n(The pair [ a , b ] is set theoretically defined as\\n\\n\\n\\n\\n .\\nHence the solution\\nlemma applies, and there are unique sets \\n\\n\\n\\nthat satisfy\\neach equation. But each such \\n\\n\\n\\nis a collection of pairs, i.e.\\na function. Furthermore, since each \\n\\n\\n\\nis unique, and S is\\na set, the mapping \\n\\n\\n\\nassociating \\n\\n\\n\\nwith each \\n\\n\\n\\nis\\na function. This completes the proof of Theorem 1. \\n\\n\\n\\nProof of Corollary 2.\\nIt is enough to observe that we can add an extra condition in the\\nmain equation of Theorem 1, and the proof still works:\\n\\n\\n\\n\\n NOTE. Notice that we can view using the solution lemma in the\\nabove proofs as\\nan extreme example of defining a function by cases. To see it more\\nclearly, one can make the main equation of the proof of Theorem 1\\nexplicit.\\nLet  \\n\\n\\n\\n\\n enumerate  S.\\nWe can\\ncreate a big table specifying meaning values for all strings and  their\\ncombinations.  Then the  conditions on the meaning functions \\n\\n\\n\\n can be written as the set of equations below\\n\\n\\n\\n>\\n\\n\\nIn ordinary mathematics, this would correspond to saying that if\\nx is 1 then f(x)=32,\\nif x is 2 then \\n\\nf(x)=14732,\\nif x is 3 then f(x)=1, and so on. Clearly, such a process\\ndefines the\\nfunction f, but, intuitively, it is not a definition we would\\ncare much for. Before showing that requiring a better description\\nof an f than as a set of pairs makes sense, we want to observe that\\nthe encoding of the original meaning function\\ncan be uniform in the following sense: \\n\\n\\n PROPOSITION 3.\\nIn addition to the assumptions of Theorem 1, let \\n\\n\\n\\n\\n ,\\nand\\nlet \\n\\n\\n\\nbe the language obtained by the mapping\\n\\n\\n\\n\\n ,\\nfor all \\n\\n\\n\\n .\\nThen there is a set of functions M[*] and a unique\\nmap \\n\\n\\n\\n\\nsuch that for all s, \\n\\n\\n\\n\\nProof. As in the proof of\\nCorollary 2, we can change the set of equations to\\n\\n\\n\\nTo finish the construction of \\n\\n\\n\\n ,\\nwe make sure that the equation\\n\\n\\n\\n\\n holds. Formally, this requires adding the pair \\n\\n\\n\\n\\ninto\\nthe graph of \\n\\n\\n\\nthat was obtained from the solution lemma.\\nAlso, we have to extend the domain of function \\n\\n\\n\\nto include\\n\\n\\n\\n .\\nThis is easily done by adding to the already constructed\\npart of \\n\\n\\n\\nthe set of pairs\\n\\n\\n\\n\\n .\\nThe proof is complete once we check that for  s\\'s  in S we have\\n\\n\\n\\n\\n ,\\nand that\\n\\n\\n\\n\\n (because \\n\\n\\n\\n\\n ,\\nand, according to the equation,\\n\\n\\n\\n\\n ). \\n\\n\\n\\nNote that, as in Corollary 2,\\nif a certain string does not belong to the language, we can assume that\\nthe corresponding value in this table is undefined; thus \\n\\n\\n\\nis not\\nnecessarily defined for all possible concatenations of strings of\\nS.\\n\\n\\nIn view of the above theorems, any semantics is equivalent\\nto a compositional\\nsemantics, and hence it would be meaningless to keep the definition of\\ncompositionality as the existence of a homomorphism from syntax to semantics\\nwithout imposing some conditions on this homomorphism.\\nNotice that requiring\\nthe computability of the meaning function will not do.\\nIn mathematics, where semantics obviously is\\ncompositional, we can talk about noncomputable functions, and it\\nis usually clear what we postulate about them. Also, we have the\\nfollowing proposition. \\n\\n\\n PROPOSITION 4.\\nIf the set of expressions S and the original meaning\\nfunction m(x) are computable, then so is the meaning\\nfunction \\n\\n\\n\\n .\\n\\n\\nProof. One can easily check that\\nthe table defining the meaning functions \\n\\n\\n\\n\\n in the note above is effectively\\ncomputable from the functions m(x) and \\n\\n\\n\\n\\n .\\nHence\\nso is the function \\n\\n\\n\\n .\\n\\n\\n\\n NOTE. What does it mean that the table is effectively\\ncomputable from the functions m(x) and \\n\\n\\n\\n\\n ?\\nIt means that given a Turing machine, T1, that prints all\\nelements of S, and another Turing machine, T2, that takes\\nan element s on the output tape of T1 as input and produces\\nas output m(s), we can construct a third Turing machine, T3, that\\nproduces the successive elements of the table, i.e. enumerates all the\\nequations (e.g. for any pair [m, n] gives the nth value pair of the\\nmth equation).\\nBut these equations define our function \\n\\n\\n\\n .\\nHence\\n\\n\\n\\nis effectively computable.\\nAlso, notice that the proposition holds true for generalized\\n computability, in the sense of . \\n\\n\\n  Systematic semantics. I. Examples \\n\\n, pp.27, talks about compositionality, postulating that the meaning of a whole should\\nbe a \"systematic\" function of the\\nmeanings of the parts. He does not define the word\\n\"systematic\" except as being an antonym to \"idiosyncratic\".\\nWhat it could mean is that we want to avoid such meaning functions\\nas the ones defined in\\nthe proof of Theorem 1 and the subsequent propositions.\\nWe suggest a simple way of doing it --\\nby requiring that the meaning function belong to a certain class.\\nBy Proposition 4 this does not work if we merely postulate that\\nthe function be computable. However it does work for\\nsmaller classes of functions as the following examples show.\\n A SIMPLE GRAMMAR WITHOUT A SYSTEMATIC SEMANTICS \\nIf meanings have to be expressed using certain natural,\\nbut restricted, sets\\nof operations, it may turn out that even simple grammars do not have a\\ncompositional semantics.\\nConsider two grammars of numerals in base 10: \\n\\n\\nGrammar ND \\n    N \\n\\n\\n\\n\\nN D \\n    N \\n\\n\\n\\n\\nD  \\n    D \\n\\n\\n\\n\\n0 \\n\\n\\n\\n1 \\n\\n\\n\\n2 \\n\\n\\n\\n3 \\n\\n\\n\\n4 \\n\\n\\n\\n5 \\n\\n\\n\\n 6 \\n\\n\\n\\n7 \\n\\n\\n\\n8 \\n\\n\\n\\n9 \\n\\n\\nGrammar DN   \\n    N \\n\\n\\n\\n\\nD N \\n    N \\n\\n\\n\\n\\nD \\n    D \\n\\n\\n\\n\\n0 \\n\\n\\n\\n1 \\n\\n\\n\\n2 \\n\\n\\n\\n3 \\n\\n\\n\\n4 \\n\\n\\n\\n5 \\n\\n\\n\\n 6 \\n\\n\\n\\n7 \\n\\n\\n\\n8 \\n\\n\\n\\n9 \\n\\n\\n PROPOSITION 5.\\nFor the grammar ND, the meaning of any numeral can be expressed in the model\\n\\n\\n\\n\\nas\\n\\n\\n\\nthat is, a polynomial\\nin two variables with coefficients in natural numbers.\\n\\n\\n\\nOn the other hand, for the grammar DN,\\nwe can prove that no such a polynomial exists:\\n\\n\\n THEOREM 6.\\nThere is no polynomial p in two variables x, y such that\\n\\n\\n\\nand such that the value of \\n\\n\\n\\n\\nis the number expressed by the string  D  N in base 10.\\n\\n\\nProof.\\nWe are looking for\\n\\n\\n\\nwhere the function pmust be a polynomial in these two variables.\\nIf such a polynomial exists, it would have to be equal to\\n\\n\\n\\nfor \\n\\n\\n\\nin the interval 0..9, and to\\n\\n\\n\\n\\n for \\n\\n\\n\\nin 10..99, and to \\n\\n\\n\\n\\nfor\\n\\n\\n\\nin 100..999, and\\nso on. Let the degree of this polynomial be less\\nthan n , for\\nsome n. Let us consider the interval\\n\\n10[n] .. 10[(n+1)]- 1. On this interval the polynomial\\nwould have to be equal identically\\nto \\n\\n\\n\\n\\n .\\nNow, if two polynomials of degrees less than n agree on\\nn different values, they must be identical. Hence,\\n\\n\\n\\n\\n .\\nBut this\\nwould give wrong values for other intervals, e.g 10..99.\\nContradiction. \\n\\n\\n\\nBut notice that there is a\\ncompositional semantics  for the grammar DN that\\ndoes not agree with intuitions:\\n\\n\\n\\n\\n ,\\nwhich\\ncorresponds to reading the number backwards. And there are\\nmany other semantics\\ncorresponding to all possible polynomials in\\n\\n\\n\\nand \\n\\n\\n\\n .\\nAlso observe that (a) if we specify enough values of the meaning function we\\ncan exclude any particular polynomial; (b) if we do not restrict the degree\\nof the polynomial, we can write one that would give any values we want on a\\nfinite number of words in the grammar.\\nThe moral is that not only it is natural to restrict meaning functions to,\\nsay, polynomials, but to further restrict them.\\nE.g. if we restrict the meaning functions to polynomials of degree 1,\\nthen by specifying only three values  of the meaning function we can (a)\\nhave a unique compositional semantics for the first grammar; and\\n(b) show that\\nthere is no compositional semantics for the second grammar\\n(directly from the\\nproof of the above theorem).\\n\\n\\n  Some linguistic examples \\n\\nIn this section we want to explain how the theorems we proved in\\nSection 2\\napply to typical linguistic examples. In the process\\nwe also explain how languages with operators can be handled by our\\nmethod. We begin by discussing the simple case of idioms.\\n\\n\\n IDIOMS.\\nIntuitively, the meaning of \"high seas\" is not compositional,\\nbecause \"high\" refers to length or distance, and not to open\\nspaces; moreover one could even argue that although \"seas\" is\\nplural, \"high seas\" is semantically singular, for it means\\nan \"open sea\". (However, the precise semantics of the expression\\nis not important at this\\npoint). We want to show how we can assign compositional semantics\\nto such non-compositional examples.\\n\\n\\nLet the language S consist of \\n\\nwall, seas,  high,\\nhigh.wall, and high.seas. The equations we need to ensure\\nthe compositionality of semantics have the familiar form:\\n\\n\\n\\n>\\n\\n\\nNotice, that we could add building and other words\\nto the language and easily\\nextend this set of equations. The intuition we associate with\\ncompositionality would be captured by the uniformity of\\nthe meanings of high.X as \\n\\nhigh(m(X)), where X ranges over\\n\\nwall, building, ....\\nHowever the formal expression\\nof this intuition as the principle of compositionality does not work,\\nwhich can be seen by noticing that the meaning of high.seas is\\na composition of the meaning functions for high and seas.\\nWhat is happening has to do with the fact that, by\\ndefinition, functions defined by cases are as good as any others.\\nAnd what we have done is to have defined the meaning of high by\\ncases.\\n\\n\\n COORDINATION.\\nWe now turn our attention to a slightly more complicated example.\\nConsider disjunction and conjunction, +and \\n\\n\\n\\n .\\nWe plan to prove the following results:\\n\\n\\n PROPOSITION 7. Let + and \\n\\n\\n\\ndenote \"or\" and \"and\".\\nThen: \\n  (A). It is possible to assign compositionally the \"natural\"\\nsemantics of \\n\\n\\n\\n to expressions of type \\n\\n\\n\\nand preserve the original\\nmeanings of a+b and \\n\\n\\n\\n .\\n  (B). (A) is not possible if the meaning functions have to be Boolean\\npolynomials. \\nProof.\\nTo keep things as simple as possible, consider language S consisting of\\n\\n\\n\\n ,\\n\\n\\n\\n ,\\nb, c, and a. To apply directly the\\nsolution lemma we should represent the operators in their prefix form;\\ne.g. a+b becomes +.a.b. Then our language S becomes\\n\\n\\n\\n\\n ,\\n\\n\\n\\n ,\\n+, \\n\\n\\n\\n ,\\nb, c, and a. (And for the sake\\nof completeness we can add to it \\n\\n\\n\\n ,\\nand b.c).\\nAs before we write our equations (this time using the version\\nwith $):\\n\\n\\n\\n>\\n\\n\\nIt can be easily checked that as before \\n\\n\\n\\n\\nfor all\\n\\n\\n\\n\\n .\\nThe meaning m(a) of a is arbitrary, but we\\nwould typically identify it with its logical value (true or false).\\nAlso, notice that without loss of generality those  a, b, c(and hence the language S)\\ncan be \"expanded\" to sets of variables\\n\\nai , bj , ck, resulting in a slight change in the equations,\\nbut not changing the content of the theorem. Then,\\nfor all pairs of variables disjunction and conjunction would behave as\\nusual; however for a combination of a variable with a\\nBoolean formula they could behave arbitrarily.\\nThis proves the first part.\\n\\n\\nWe now prove the second part, i.e. that if the meaning functions are\\nrestricted to Boolean polynomials, it is impossible to assign\\ncompositional semantics given by the \"natural\" semantics of\\n\\n\\n\\n to expressions of the type \\n\\n\\n\\nand preserve the original\\nmeanings of the connectives. (The \"natural\" semantics is of\\ncourse the conjunction of the Boolean value of a + b with the\\nBoolean value of c).\\n\\n\\nThe proof consists in observing that\\n\\n\\n\\n\\ncannot be obtained as a Boolean polynomial\\n(function) \\n\\n\\n\\n\\n .\\nTo see it, it is enough to\\nconstruct a truth table showing the values of \\n\\n\\n\\n\\nand\\n\\n\\n\\nand observing that there cannot be a functional\\ndependence of the former on the latter and the value of a(compare the values for the triples \\n\\n[ a=1 , b =0 , c=1 ] and\\n\\n[ a=1 , b =1 , c=0 ]).\\n\\n\\n\\n  Systematic semantics. II. Discussion \\n\\nAbove, we have argued that the existence of a homomorphism from\\nsyntax to semantics does not restrict the grammar, but if we put\\nsome constraints on such a homomorphism, they actually might restrict\\ngrammars of languages. We have called such\\nhomomorphisms (F-)systematic. However the nature of systematicity\\nseems to be very much an open problem. In this section we discuss\\nsome of the most obvious issues, and propose some research\\npossibilities in this area.\\n\\n\\nThe first\\nnatural question that arises is: What should be this class F? We have\\nshown that for a grammar of numbers, and a grammar of two Boolean\\nconnectives the natural classes are polynomials. Clearly, this cannot\\nalways be the case. For instance, it seems natural to map verbs\\ninto predicates and nouns into their arguments. But we know that\\nif we want to provide compositional semantics for more than the\\nsimplest case of subject-verb-object construction we need other\\nmechanism, e.g. type raising. On the other hand, unrestricted type\\nraising leads to the results we have just discussed. We arrive then\\nat the following variant of the natural question: How should we restrict\\ntype raising? (So that we can account e.g. for ellipsis, but at the\\nsame time constrain the grammars).\\n\\n\\nMany grammatical constructions express meanings that go beyond\\nexpressing predicate-arguments assignments. For example, \"the X-er,\\nthe Y-er\" construction\\n (, ), as in \"the more you dive, the better you swim\", expresses a proportional dependence.\\nOther constructions can express speech acts, various kind of conflict,\\netc., hence creating rich sets of meanings. The next question we\\ncan ask is whether we can express constraints on the syntax-semantics\\ninterface by saying that the homomorphism should be simple,\\nrelative to the the class of\\nmeaning functions. A mathematical analogy would be\\nto say that we are given\\nmany different functions, e.g. \\n\\nsin(x), cos(x), e[x], ...,  but\\nonly simple ways of composing them, e.g. only by the +.\\n\\n\\nIn essence, when we say that a function is F-systematic, we view\\nF as a measure of complexity and/or expressive power.\\nHence the natural association with polynomials. But there are other\\nmeasures of complexity.\\n  discusses grammars and languages in terms of Kolmogoroff complexity, suggesting that more compact grammars\\nare better even if they overgeneralize (e.g. by approximating\\na finite language by an infinite one).\\nWe believe that his work is relevant for systematicity, but\\nwe do not have any formal argument supporting that claim, except\\nthe observation that polynomials are more compact than functions\\ndefined by cases. So perhaps this might be a beginning of a formal\\nconnection.\\n\\n\\nOne of the referees has pointed out two other ideas. First,\\nthere could be other more natural notions of systematicity, in cases\\nwhen meanings are specified\\nby means of constraint solving, as is implicit in unification-based\\nformalisms, and even in Theorem 1, where meanings are extracted as\\nsolutions to equations. The second idea,\\nthe differences between natural and formal languages notwithstanding,\\nis a programming language semantics\\nconcept which actually restricts the ability of a semantics to be\\ncompositional. This concept is \"full abstraction\", i.e. the\\nequivalence between the operational and denotational semantics,\\n (cf. ), which can be viewed as a general constraint on compositionality.\\n\\n\\nA radically different approach to the interaction of syntax and semantics\\n has been presented in  and . Language is modeled there as a set of constructions (cf. also\\n  and ). In that model there is no separate syntax, since constructions encode both\\nform and meaning. Each construction\\nexplicitly defines the meaning function taking the meanings of\\nits subconstructions as arguments.\\nIntuitively, in that model we assume that\\neach word sense requires a separate semantic description;\\nthe same is true about each\\n idiom, open idiom (), and a phrasal construction. This means that we make each\\nsemantic function as complicated as linguistically\\nnecessary, but their mode of combination is restricted.\\n(Continuing the above mathematical analogy, we would say that\\nthe only mode of combination is substitution for an argument).\\nIn the construction-based model semantics is \"compositional\"\\nand \"systematic\" (with respect to the set of all these\\nmeaning functions), but there is no homomorphism from syntax\\nto semantics, because there is no syntax to speak of.\\nIt is \"compositional\", because the meaning of a construction is\\na function of the meanings of its parts and their mode of\\ncombination. (Note that such a function\\nis different for different constructions, and each construction\\ndefines its own mode of combination). And it is systematic, because\\nthe modes of combination are not arbitrary, as they have to be\\nlinguistically justified.\\nBut since only few formal aspects\\nof constructions have been worked out, we can only speak of that model\\nas of yet another possibility.\\n\\n\\nThe last point we want to make is that while it is true that the\\nhomomorphism condition is too weak (in general) to count as\\nsystematicity, the semanticists (e.g. Montague)\\nhave not been using arbitrary homomorphisms. Thus a careful\\nexamination of their work should lead to some characterization\\nof \"good\" homomorphisms; and this seems to be another interesting\\navenue of research. (As suggested by one of the referees, a technique\\nfrom universal algebra might also prove helpful,\\nwhere one first gives a class\\nof algebras and then specifies meanings as homomorphisms\\nfrom the initial algebra of the class).\\n\\n\\n  Conclusions \\n\\nIn this paper we have shown the formal vacuity of the compositionality\\nprinciple. That is, we have shown that the property that the meaning\\nof the whole is a function of the meanings of its parts does not put any\\nmaterial constraints on syntax or semantics.\\nTheorem 1 (and its corollaries) explain formally why\\nthe postulate of a\\nhomomorphism between syntax and semantics is not restrictive\\nenough: the syntactic operator of concatenation\\n\".\" can be mapped into functional\\ncomposition operating on functions that encode arbitrary semantics\\nof an arbitrary language.\\nAs we have seen in the examples, the\\npresence of other operators does not change the result, because\\nthey can be treated as yet another letter of the alphabet, and\\none can still produce a homomorphism between syntax and semantics.\\n\\n\\nThe problem of the vacuity of compositional semantics\\narises, because in the formal definition of compositionality\\nmeaning functions can be completely arbitrary. Therefore we have\\nproposed that the meaning functions should be systematic, i.e.,\\nnon-arbitrary.\\nWe have shown that this notion makes sense formally; that is,\\nwe presented examples\\nof semantic classes of functions, for which there are grammars with\\nmeaning functions in that class, as well as we have shown that there\\nare grammars that cannot have a meaning function in that class.\\nAs we noted in the previous section, both\\nthe formal and the linguistic nature of systematicity\\nremains an open problem, but with a few promising avenues of research.\\n\\n\\nIn this paper we have restricted ourselves to the mathematics of\\ncompositionality, and many important issues have not been discussed.\\nFor instance, the main result is relevant for\\ntheories of grammar and for the thesis about\\nthe reduction of syntax to lexical meanings (cf. e.g.\\n T. Wasow on pp.204-205 in ). Also, systematicity of semantics should give us a handle in\\nconstraining the power of the semantic as well as the syntactic\\n components of a grammar (cf. ). Furthermore, our results have implications for\\ncomputational linguistics (they are briefly discussed in\\n ). \\n\\n\\nFinally, the reader should note that one of the more bizarre\\nconsequences of Theorem 1 is that we do not have to start building\\ncompositional semantics for natural language beginning\\nwith assigning of the meanings to words. We can do equally well by\\nassigning meanings to phonems  or even LETTERS, assuring\\nthat, for any sentence, the intuitive meaning we associate with\\nit would be a function of the meanings of\\nthe letters from which that sentence is composed. But then\\nthe cabalists had always known it.\\n\\n\\n ACKNOWLEDGEMENTS. I would like to thank Alexis Manaster\\nRamer for our many\\ndiscussions of compositionality, and the referees for their\\ninsightful comments.\\n\\nBibliography \\n\\nP. Aczel.\\nLectures on Non-Well-Founded Sets.\\nCSLI Lecture Notes, Stanford, CA, 1987.\\n\\n\\nJon Barwise and John Etchemendy.\\nThe Liar.\\nOxford University Press, New York, NY, 1987.\\n\\n\\nJon Barwise.\\nAdmissible Sets and Structures.\\nSpringer, New York, NY, 1975.\\n\\n\\nL. Bloomfield.\\nLanguage.\\nThe University of Chicago Press, Chicago, IL, 1933.\\n\\n\\nCharles J. Fillmore, Paul Kay, and Mary Catherine O\\'Connor.\\nRegularity and idiomaticity in grammatical constructions.\\nLanguage, 64(3):501-538, 1988.\\n\\n\\nC.A. Gunter.\\nSemantics of Programming Languages.\\nMIT Press, Cambridge, MA, 1993.\\n\\n\\nGraeme Hirst.\\nSemantic interpretation and the resolution of ambiguity.\\nCambridge University Press, Cambridge, Great Britain, 1987.\\n\\n\\nD. Jurafsky.\\nAn On-line Computational Model of Sentence Interpretation.\\nPhD thesis, University of California, Berkeley, 1992.\\nReport No. UCB/CSD 92/676.\\n\\n\\nEdward L. Keenan and Leonard M. Faltz.\\nBoolean Semantics for Natural Language.\\nD Reidel, Dordrecht, Holland, 1985.\\n\\n\\nAlexis Manaster-Ramer and Wlodek Zadrozny.\\nSystematic semantics.\\nin preparation, 1994.\\n\\n\\nBarbara H. Partee, Alice ter Meulen, and Robert E. Wall.\\nMathematical Methods in Lingusitics.\\nKluwer, Dordrecht, The Netherlands, 1990.\\n\\n\\nBarbara H. Partee.\\nThe logic of semantics.\\nIn Fred Lanman and Frank Veltman, editors, Varieties of Formal\\n  Semantics, pages 281-312. Foris, Dordrecht, Holland, 1982.\\n\\n\\nWalter J. Savitch.\\nWhy it might pay to assume that languages are infinite.\\nAnnals of Mathematics and Artificial Intelligence,\\n  8(1,2):17-26, 1993.\\n\\n\\nP. Sells.\\nLectures on Contemporary Syntactic Theories.\\nCSLI Lecture Notes (3), Stanford, CA, 1985.\\n\\n\\nJohan van Benthem.\\nThe logic of semantics.\\nIn Fred Lanman and Frank Veltman, editors, Varieties of Formal\\n  Semantics, pages 55-80. Foris, Dordrecht, Holland, 1982.\\n\\n\\nWlodek Zadrozny and Alexis Manaster-Ramer.\\nThe significance of constructions.\\nsubmitted to Computational Linguistics, 1994.\\n\\n\\nWlodek Zadrozny.\\nOn compositional semantics.\\nProc. Coling\\'92, pages 260-266, 1992.\\n\\nFootnotes\\n\\n  Linguistics and Philosophy(17):329-342, 1994\\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nWe prove a theorem stating that any semantics can be encoded as a\\ncompositional semantics, which means that, essentially, the standard\\ndefinition of compositionality is formally vacuous. We then show that when\\ncompositional semantics is required to be \"systematic\"\\n(that is, the meaning\\nfunction cannot be arbitrary, but must belong to some class),\\nit is possible to\\ndistinguish between compositional and non-compositional semantics.\\nAs a result, we believe that the paper clarifies the\\nconcept of compositionality and opens a possibility of making systematic\\nformal comparisons of different systems of grammars.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nA number of researchers have shown that there is organisation in\\n          discourse above the level of the individual utterance (5,\\n          8, 9, 10), The current exploratory study uses control as a\\n          parameter for identifying these higher level structures. We then\\n          go on to address how conversational participants co-ordinate\\n          moves between these higher level units, in particular looking at\\n          the ways they use to signal the beginning and end of such high\\n          level units.\\n\\n\\nPrevious research has identified three means by which speakers\\n          signal information about discourse structure to listeners: Cue\\n          words and phrases (5, 10); Intonation (7); Pronominalisation\\n          (6, 2). In the cue words approach, Reichman\\n          (10) has claimed that phrases like ``because'', ``so'', and ``but''\\n          offer explicit information to listeners about how the speaker's\\n          current contribution to the discourse relates to what has gone\\n          previously. For example a speaker might use the expression ``so''\\n          to signal that s/he is about to conclude what s/he has just said.\\n          Grosz and Sidner (5) relate the use of such phrases to changes in\\n          attentional state. An example would be that ``and'' or ``but'' signal\\n          to the listener that a new topic and set of referents is being\\n          introduced whereas ``anyway'' and ``in any case'' indicate a return\\n          to a previous topic and referent set. A second indirect way of\\n          signalling discourse structure is intonation. Hirschberg and\\n          Pierrehumbert (7) showed that intonational contour is closely\\n          related to discourse segmentation with new topics being signalled\\n          by changes in intonational contour. A final more indirect cue to\\n          discourse structure is the speaker's choice of referring\\n          expressions and grammatical structure. A number of researchers\\n          (4, 2, 6, 10) have given accounts of how these relate to the\\n          continuing, retaining or shifting of focus.\\n\\n\\nThe above approaches have concentrated on particular surface\\n          linguistic phenomena and then investigated what a putative cue\\n          serves to signal in a number of dialogues. The problem with this\\n          approach is that the cue may only be an infrequent indicator of a\\n          particular type of shift. If we want to construct a general\\n          theory of discourse than we want to know about the whole range\\n          of cues serving this function. This study therefore takes a\\n          different approach. We begin by identifying all shifts of control\\n          in the dialogue and then look at how each shift was signalled by\\n          the speakers. A second problem with previous research is that the\\n          criteria for identifying discourse structure are not always made\\n          explicit. In this study explicit criteria are given: we then go\\n          on to analyse the relation between cues and this structure.\\n\\n\\n  The data \\n\\nThe data were recordings of telephone conversations between\\n          clients and an expert concerning problems with software. The tape\\n          recordings from four dialogues were then transcribed and the\\n          analysis conducted on the typewritten transcripts rather than the\\n          raw recordings. There was a total of 450 turns in the dialogues.\\n\\n\\n2.1 Criteria for classifying utterance types. Each utterance in the\\ndialogue was classified into one of four categories: (a)\\n          Assertions - declarative utterances which were used to state\\n          facts. Yes or no answers to questions were also classified as\\n          assertions on the grounds that they were supplying the listener\\n          with factual information; (b) Commands - utterances which were\\n          intended to instigate action in their audience. These included\\n          various utterances which did not have imperative form, (e.g.\\n          ``What I would do if I were you is to relink X'') but were intended\\n          to induce some action; (c) Questions - utterances which were\\n          intended to elicit information from the audience. These included\\n          utterances which did not have interrogative form. e.g. ``So my\\n          question is....'' They also included paraphrases, in which the\\n          speaker reformulated or repeated part or all of what had just\\n          been said. Paraphrases were classified as questions on the\\n          grounds that the effect was to induce the listener to confirm or\\n          deny what had just been stated; (d) Prompts - These were\\n          utterances which did not express propositional content. Examples\\n          of prompts were things like ``Yes'' and ``Uhu''.\\n\\n\\n2.2 Allocation of control in the dialogues.           We devised several\\n          rules to determine the location of control in the dialogues. Each\\n          of these rules related control to utterance type: (a) For\\n          questions, the speaker was defined as being in control unless the\\n          question directly followed a question or command by the other\\n          conversant. The reason for this is that questions uttered\\n          following questions or commands are normally attempts to clarify\\n          the preceding utterance and as such are elicited by the previous\\n          speaker's utterance rather than directing the conversation in\\n          their own right. (b) For assertions, the speaker was defined as\\n          being in control unless the assertion was made in response to a\\n          question, for the same reasons as those given for questions; an\\n          assertion which is a response to a question could not be said to\\n          be controlling the discourse; (c) For commands, the speaker was\\n          defined as controlling the conversation. Indirect commands (i.e.\\n          utterances which did not have imperative form but served to\\n          elicit some actions) were also classified in this way; (d) For\\n          prompts, the listener was defined as controlling the\\n          conversation, as the speaker was clearly abdicating his/her turn.\\n          In cases where a turn consisted of several utterances, the\\n          control rules were only applied to the final utterance.\\n\\n\\nWe applied the control rules and found that control did not\\n          alternate from speaker to speaker on a turn by turn basis, but\\n          that there were long sequences of turns in which control remained\\n          with one speaker. This seemed to suggest that the dialogues were\\n          organised above the level of individual turns into phases where\\n          control was located with one speaker. The mean number of turns in\\n          each phase was 8.03.\\n\\n\\n  Mechanisms for switching control \\n\\nWe then went on to analyse how control was exchanged between\\n          participants at the boundaries of these phases. We first examined\\n          the last utterance of each phase on the grounds that one\\n          mechanism for indicating the end of a phase would be for the\\n          speaker controlling the phase to give some cue that he (both\\n          participants in the dialogues were always male) no longer wished\\n          to control the discourse. There was a total of 56 shifts of\\n          control over the 4 dialogues and we identified 3 main classes of\\n          cues used to signal control shifts These were prompts,\\n          repetitions and summaries. We also looked at when no signal was\\n          given (interruptions).\\n\\n\\n3.1 Prompts. On 21 of the 56 shifts (38%), the utterance\\n          immediately prior to the control shift was a prompt. We might\\n          therefore explain these shifts as resulting from the person in\\n          control explicitly indicating that he had nothing more to say.\\n\\n\\n(In the following examples a line indicates a control\\n          shift)\\n\\n\\nExample 1 - Prompt  Dialogue C -\\n\\n\\n\\n3.2 Repetitions and summaries. On a further 15 occasions (27%),\\n          we found that the person in control of the dialogue signalled\\n          that they had no new information to offer. They did this either\\n          by repeating what had just been said (6 occasions), or by giving\\n          a summary of what they had said in the preceding utterances of\\n          the phase (9 occasions). We defined a repetition as an assertion\\n          which expresses part or all of the propositional content of a\\n          previous assertion but which contains no new information. A\\n          summary consisted of concise reference to the entire set of\\n          information given about the client's problem or the solution\\n          plan.\\n\\n\\nExample 2 - Repetition. Dialogue C -\\n\\n\\n\\nHalf the repetitions were accompanied by cue words. These were\\n          ``and'', ``well'' and ``so'', which prefixed the assertion.\\n\\n\\nExample 3 - Summary Dialogue B -\\n\\n\\n\\nWhat are the linguistic characteristics of summaries? Reichman\\n          (10) suggests that ``so'' might be a summary cue on the part of the\\n          speaker but we found only one example of this, although there\\n          were 3 instances of ``and'', one ``now'' one ``but'' and one ``so''.\\nIn\\n          our dialogues the summaries seemed to be characterised by the\\n          concise reference to objects or entities which had earlier been\\n          described in detail, e.g. (a) ``Now, I'm wondering how the two are\\n          related'' in which ``the two'' refers to the two error messages\\n          which it had taken several utterances to describe previously. The\\n          other characteristic of summaries is that they contrast strongly\\n          with the extremely concrete descriptions elsewhere in the\\n          dialogues, e.g. ``err the system program standard call file\\n          doesn't complete this means that the file does not have a tail\\n          record'' followed by ``And I've no clue at all how to get out of\\n          the situation''. Example 3 also illustrates this change from\\n          specific (1, 3, 5) to general (7).          How then do repetitions\\nand summaries operate as cues?  In\\n          summarising, the speaker is indicating a natural breakpoint in\\n          the dialogue and they also indicate that they have nothing more\\n          to add at that stage. Repetitions seem to work in a similar way:\\n          the fact that a speaker reiterates indicates that he has nothing\\n          more to say on a topic.\\n\\n\\n3.3 Interruptions. In the previous cases, the person controlling\\n          the dialogue gave a signal that control might be exchanged. There\\n          were 20 further occasions (36% of shifts) on which no such\\n          indication is given. We therefore went on to analyse the\\n          conditions in which such interruptions occurred. These seem to\\n          fall into 3 categories: (a) vital facts; (b) responses to vital\\n          facts; (c) clarifications.\\n\\n\\n3.3.1 Vital facts. On a total of 6 occasions (11% of shifts) the\\n          client interrupted to contradict the speaker or to supply what\\n          seemed to be relevant information that he believed the expert did\\n          not know.\\n\\n\\nExample 4  Dialogue C -\\n\\n\\n\\n          Two of these 6 interjections were to supply extra information and\\n          one was marked with the cue ``as well''. The other four were to\\n          contradict what had just been said and two had explicit markers\\n          ``though'' and ``well actually'': the remaining two being direct\\n          denials.\\n\\n\\n3.3.2 Reversions of control following vital facts. The next\\n          class of interruptions occur after the client has made some\\n          interjection to supply a missing fact or when the client has\\n          blocked a plan or rejected an explanation that the expert has\\n          produced. There were 8 such occasions (14% of shifts).\\n\\n\\nThe interruption in the previous example illustrates the\\n          reversion of control to the expert after the client has supplied\\n          information which he (the client) believes to be highly relevant\\n          to the expert. In the following example, the client is already in\\n          control.\\n\\n\\nExample 5  Dialogue B -\\n\\n\\n\\n3.3.3 Clarifications. Participants can also interrupt to clarify\\n          what has just been said. This happened on 6 occasions (11%) of\\n          shifts.\\n\\n\\nExample 6  Dialogue C -\\n\\n\\n\\n          On two occasions clarifications were prefixed by ``now'' and twice\\n          by ``so''.  On the final two occasions there was no such marker,\\n          and a direct question was used.\\n\\n\\n3.3.4 An explanation of interruptions. We have just described the\\n          circumstances in which interruptions occur, but can we now\\n          explain why they occur? We suggest the following two principles\\n          might account for interruptions: these principles concern: (a)\\n          the information upon which the participants are basing their\\n          plans, and (b) the plans themselves.\\n(A). Information quality:\\nBoth expert and client must believe\\nthat the information that the\\nexpert has about the problem is true and that this information is\\n          sufficient to solve the problem. This can be expressed by the\\n          following two rules which concern the truth of the information\\n          and the ambiguity of the information: (A1) if the listener\\n          believes a fact P and believes that fact to be relevant and\\n          either believes that the speaker believes not P or that the\\n          speaker does not know P then interrupt; (A2) If the listener\\n          believes that the speaker's assertion is relevant but ambiguous\\n          then interrupt.\\n(B). Plan quality:\\nBoth expert and client must\\n          believe that the plan that the expert has generated is adequate\\n          to solve the problem and it must be comprehensible to the client.\\n          The two rules which express this principle concern the\\n          effectiveness of the plan and the ambiguity of the plan: (B1) If\\n          the listener believes P and either believes that P presents an\\n          obstacle to the proposed plan or believes that part of the\\n          proposed plan has already been satisfied, then interrupt; (B2) If\\n          the listener believes that an assertion about the proposed plan\\n          is ambiguous, then interrupt.\\n\\n\\nIn this framework, interruptions\\n          can be seen as strategies produced by either conversational\\n          participant when they perceive that a either principle is not\\n          being adhered to.\\n\\n\\n3.4 Cue reliability. We also investigated whether there were\\n          occasions when prompts, repetitions and summaries failed to\\n          elicit the control shifts we predicted. We considered two\\n          possible types of failure: either the speaker could give a cue\\n          and continue or the speaker could give a cue and the listener\\n          fail to respond. We found no instances of the first case;\\n          although speakers did produce phrases like ``OK'' and then\\n          continue, the ``OK'' was always part of the same intonational\\n          contour as that further information and there was no break\\n          between the two, suggesting the phrase was a prefix and not a\\n          cue. We did, however, find instances of the second case: twice\\n          following prompts and once following a summary, there was a long\\n          pause, indicating that the speaker was not ready to respond. We\\n          conducted a similar analysis for those cue words that have been\\n          identified in the literature. Only 21 of the 35 repetitions,\\n          summaries and interruptions had cue words associated with them\\n          and there were also 19 instances of the cue words ``now'', ``and'',\\n          ``so'', ``but'' and ``well'' occurring without a control shift.\\n\\n\\n  Control cues and global control \\n\\nThe analysis so far has been concerned with control shifts where\\n          shifts were identified from a series of rules which related\\n          utterance type and control. Examination of the dialogues\\n          indicated that there seemed to be different types of control\\n          shifts: after some shifts there seemed to be a change of topic,\\n          whereas for others the topic remained the same. We next went on\\n          to examine the relationship between topic shift and the different\\n          types of cues and interruptions described earlier. To do this it\\n          was necessary first to classify control shifts according to\\n          whether they resulted in shifts of topic.\\n\\n\\n4.1 Identifying topic shifts. We identified topic shifts in the\\n          following way: Five judges were presented with the four dialogues\\n          and in each of the dialogues we had marked where control shifts\\n          occurred. The judges were asked to state for each control shift\\n          whether it was accompanied by a topic shift. All five judges\\n          agreed on 24 of the 56 shifts, and 4 agreed for another 22 of the\\n          shifts. Where there was disagreement, the majority judgment was\\n          taken.\\n\\n\\n4.2 Topic shift and type of control shift.  Analysing each type of\\n          control shift, it is clear that there are differences between the\\n          cues used for the topic shift and the no shift cases. For\\n          interruptions, 90% occur within topic, i.e. they do not result in\\n          topic shifts. The pattern is not as obvious for prompts and\\n          repetitions/summaries, with 57% of prompts occurring within topic\\n          and 67% of repetitions/summaries occurring within topic. This\\n          suggests that change of topic is a carefully negotiated process.\\n          The controlling participant signals that he is ready to close the\\n          topic by producing either a prompt or a repetition/summary and\\n          this may or may not be accepted by the other participant. What is\\n          apparent is that it is highly unusual for a participant to seize\\n          control and change topic by interruption. It seems that on the\\n          majority of occasions (63%) participants wait for the strongest\\n          possible cue (the prompt) before changing topic.\\n\\n\\n4.3 Other relations between topic and control. We also looked at\\n          more general aspects of control within and between topics. We\\n          investigated the number of utterances for which each participant\\n          was in control and found that there seemed to be organisation in\\n          the dialogues above the level of topic. We found that each\\n          dialogue could be divided into two parts separated by a topic\\n          shift which we labelled the central shift. The two parts of the\\n          dialogue were very different in terms of who controlled and\\n          initiated each topic. Before the central shift, the client had\\n          control for more turns per topic and after it, the expert had\\n          control for more turns per topic. The respective numbers of turns\\n          client and expert are in control before and after the central\\n          shift are :Before 11-7,22-8,12-6,21-6; After 12-33,16-23,2-11,0-5\\nfor the four dialogues.\\n          With the exception of the first topic in Dialogues 1 and 4, the\\n          client has control of more turns in every topic before the\\n          central shift, whereas after it, the expert has control for more\\n          turns in every topic. In addition we looked at who initiated each\\n          topic, i.e. who produced the first utterance of each topic. We\\n          found that in each dialogue, the client initiates all the topics\\n          before the central shift, whereas the expert initiates the later\\n          ones. We also discovered a close relationship between topic\\n          initiation and topic dominance. In 19 of the 21 topics, the\\n          person who initiated the topic also had control of more turns. As\\n          we might expect, the point at which the expert begins to have\\n          control over more turns per topic is also the point at which the\\n          expert begins to initiate new topics.\\n\\n\\n  Conclusions \\n\\nThe main result of this exploratory study is the finding that\\n          control is a useful parameter for identifying discourse\\n          structure. Using this parameter we identified three levels of\\n          structure in the dialogues: (a) control phases; (b) topic; and\\n          (c) global organisation. For the control phases, we found that\\n          three types of utterances (prompts, repetitions and summaries)\\n          were consistently used to signal control shifts. For the low\\n          level structures we identified, (i.e. control phases), cue words\\n          and phrases were not as reliable in predicting shifts.  This\\n          result challenges the claims of recent discourse theories (5, 10)\\n          which argue for a the close relation between cue words and\\n          discourse structure. We also examined how utterance type related\\n          to topic shift and found that few interruptions introduced a new\\n          topic. Finally there was evidence for high level structures in\\n          these dialogues as evidenced by topic initiation and control,\\n          with early topics being initiated and dominated by the client and\\n          the opposite being true for the later parts.\\n\\n\\nAnother focus of current research has been the modelling of\\n          speaker and listener goals (1, 3) but there has been little\\n          research on real dialogues investigating how goals are\\n          communicated and inferred. This study identifies surface\\n          linguistic phenomena which reflect the fact that participants are\\n          continuously monitoring their goals. When plans are perceived as\\n          succeeding, participants use explicit cues such as prompts,\\n          repetitions and summaries to signal their readiness to move to\\n          the next stage of the plan. In other cases, where participants\\n          perceive obstacles to their goals being achieved, they resort to\\n          interruptions and we have tried to make explicit the rules by\\n          which they do this.\\n\\n\\nIn addition our methodology is different from other studies\\n          because we have attempted to provide an explanation for whole\\n          dialogues rather than fragments of dialogues, and used explicit\\n          criteria in a bottom-up manner to identify discourse structures.\\n          The number of dialogues was small and taken from a single problem\\n          domain. It seems likely therefore that some of our findings (e.g\\n          the central shift) will be specific to the diagnostic dialogues\\n          we studied. Further research applying the same techniques to a\\n          broader set of data should establish the generality of the\\n          control rules suggested here.\\n\\nBibliography \\n\\n`=1000\\n\\n\\nAllen, J.F. and Perrault, C.R. (1980).\\nAnalyzing intentions in utterances.\\nArtificial Intelligence, 15, 143-178.\\n\\n\\nBrennan, S. E., Friedman, M. W., and Pollard, C. (1987)\\nA centering approach to pronouns.\\nIn Proceedings of the 25th Annual\\n          Meeting of the Association for Computational Linguistics.\\n\\n\\nCohen, P. R. and Levesque, H. J. (1985)\\nSpeech acts and rationality.\\nIn Proceedings of the 23th Annual Meeting of the\\n          Association for Computational Linguistics.\\n\\n\\nGrosz, B. J., Joshi, A. K., Weinstein, S. (1986)\\nTowards a computational theory of discourse interpretation.\\nDraft.\\n\\n\\nGrosz, B. J., and Sidner, C. L. (1986)\\nAttentions, intentions and the structure of discourse.\\nComputational Linguistics, 12, 175 - 204.\\n\\n\\nGuindon, R., Sladky, P., Brunner, H., and Conner, J. (1986).\\nThe structure of user-adviser dialogues: Is there method in their\\n          madness?\\nIn Proceedings of the 24th Annual Meeting of the\\n          Association for Computational Linguistics.\\n\\n\\nHirschberg, J. and Pierrehumbert, J. B. (1986)\\nThe intonational structuring of discourse.\\nIn Proceedings of the 24th Annual\\n          Meeting of the Association for Computational Linguistics.\\n\\n\\nLevin, J. A. and Moore, J. A. (1977)\\nDialogue games: metacommunication structures for natural language\\ninteraction.\\nCognitive Science, 4, 395 - 421.\\n\\n\\nPolanyi, L. and Scha, R. (1983).\\nConnectedness in Sentence, Discourse and Text. Tilburg\\n          University, Tilburg, 141-178.\\n\\n\\nReichman, R. (1985)\\nGetting computers to talk like you and\\n          me. Cambridge, M.A.: MIT Press.\\n\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe conducted an empirical analysis into the relation between\\n          control and discourse structure. We applied control criteria to\\n          four dialogues and identified 3 levels of discourse structure. We\\n          investigated the mechanism for changing control between these\\n          structures and found that utterance type and not cue words\\n          predicted shifts of control. Participants used certain types of\\n          signals when discourse goals were proceeding successfully but\\n          resorted to interruptions when they were not.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nConversation between two people has a number of characteristics that have yet\\nto be modeled adequately in human-computer dialogue.  Conversation is \\nBIDIRECTIONAL; there is a two way flow of information between participants.\\nInformation is exchanged by  MIXED-INITIATIVE. Each participant will, on\\noccasion, take the conversational lead.  Conversational partners not only\\nrespond to what others say, but feel free to volunteer information that\\n is not requested and sometimes ask questions of their own. As  INITIATIVE passes back and forth between the discourse participants,\\nwe say that  CONTROL over the conversation gets transferred from one\\ndiscourse participant to another.\\n\\n\\nWhy should we, as computational linguists, be interested in factors that\\ncontribute to the interactivity of a discourse?  There are both theoretical and\\npractical motivations. First, we wish to extend formal accounts of single\\nutterances produced by single speakers to explain multi-participant,\\n multi-utterance discourses,.  Previous studies of the discourse structure of multi-participant dialogues have often factored out\\nthe role of  MIXED-INITIATIVE, by allocating control to one\\n participant,, or by assuming a passive  listener,.  Since conversation is a collaborative  process,, models of conversation can provide the basis for  extending planning theories,.  When the situation requires the negotiation of a collaborative plan, these theories must account for the\\ninteracting beliefs and intentions of multiple participants.\\n\\n\\nFrom a practical perspective, there is ample evidence that limited\\nmixed-initiative has contributed to lack of system usability. Many\\nresearchers have noted that the absence of mixed-initiative gives rise\\nto two problems with expert systems: They don't allow users to\\nparticipate in the reasoning process, or to ask the questions they\\n want answered,,.  In addition, question answering systems often fail to take account of the system's\\nrole as a conversational partner.  For example, fragmentary utterances\\nmay be interpreted with respect to the previous user input, but what\\nusers say is often in reaction to the system's previous\\n response,. \\n\\n\\nIn this paper we focus on interactive discourse.  We model\\nmixed-initiative using an utterance type classification and a set of\\nrules for transfer of control between discourse participants that were\\n proposed by Whittaker and Stenton. We evaluate the generality of this analysis by applying the control rules to 4 sets of\\ndialogues, including both advisory dialogues (ADs) and task-oriented\\ndialogues (TODs).  We analysed both financial and support ADs. The\\nfinancial ADs are from the radio talk show ``Harry Gross: Speaking of\\n Your Money''. The support ADs resulted from a client phoning an expert to help them diagnose and repair various\\n software faults. The TODs are about the construction of a plastic water pump in both\\n telephone and keyboard modality. \\n\\n\\nThe application of the control rules to these dialogues lets us derive\\ndomain-independent discourse segments with each segment being\\ncontrolled by one or other discourse participant. We propose that\\ncontrol segments correspond to different subgoals in the evolving\\ndiscourse plan. In addition, we argue that various linguistic devices\\nare necessary for conversational participants to coordinate their\\ncontributions to the dialogue and agree on their mutual beliefs with\\nrespect to a evolving plan, for example, to agree that a particular\\nsubgoal has been achieved. A final phenomenon concerns shifts of\\ncontrol and the devices used to achieve this.  Control shifts occur\\nbecause it is unusual for a single participant to be responsible for\\ncoordinating the achievement of the whole discourse plan.  When a\\ndifferent participant assumes control of a discourse subgoal then a\\ncontrol shift occurs and the participants must have mechanisms for\\nachieving this.  The control framework distinguishes instances in\\nwhich a control shift is negotiated by the participants and instances\\nwhere one participant seizes control.\\n\\n\\nThis paper has two objectives:\\n\\nTo explore the phenomenon of control in relation to  ATTENTIONAL\\n STATE ,,.  We predict shifts of attentional state when shifts in control are negotiated and agreed by all\\nparticipants, but not when control is seized by one participant without the\\nacceptance of the others. This should be reflected in different distribution of\\nanaphora in the two cases.\\n\\nTo test predictions about the distribution of control in\\ndifferent types of dialogues.  Because the TOD's embody the\\n master-slave assumption, and control is allocated to the expert, our expectation is that control should be located exclusively\\nwith one participant in the TODs in contrast with the ADs.\\n\\n\\n\\n\\n  Rules for the Allocation and Transfer of Control \\n\\nWe use the framework for the allocation and transfer of control of Whittaker\\n and Stenton.  The analysis is based on a classification of  utterances into 4 types. These are: \\n\\nUTTERANCE TYPES\\n\\n ASSERTIONS: Declarative utterances used to state facts.\\n        Yes and No in response\\n to a question were classified as assertions on the basis that they are\\n supplying information.\\n\\n COMMANDS: Utterances intended to instigate action.\\n        Generally imperative form, but\\n       could be indirect such as My suggestion would be that you do .....\\n\\n QUESTIONS: Utterances which are intended to elicit information,\\n          including indirect forms such as I was wondering whether I\\nshould ....\\n\\n PROMPTS:\\n        Utterances which did not express propositional content,\\n              such as Yeah, Okay, Uh-huh ....\\n\\n\\n\\n\\n\\n\\nNote that prompts are in direct contrast to the other options that a\\nparticipant has\\navailable at any point in the discourse.  By indicating that the speaker does\\nnot\\nwant the floor, prompts function on a number of levels, including the\\nexpression\\n of understanding or agreement. \\n\\n\\nThe rules for the allocation of control are based on the utterance type\\nclassification and allow a dialogue to be divided into segments that correspond\\nto\\nwhich speaker is the controller of the segment.\\n\\n\\n\\nCONTROL RULES  \\n\\n\\n\\n\\nThe definition of controller can be seen to correspond to the intuitions behind\\nthe term  INITIATING CONVERSATIONAL PARTICIPANT (ICP), who is defined as\\n the initiator of a given discourse segment. The  OTHER CONVERSATIONAL PARTICIPANT(S), OCP, may speak some utterances in a segment,\\nbut the  DISCOURSE SEGMENT PURPOSE, must be the purpose of the ICP.  The\\ncontrol rules place a segment boundary whenever the roles of the participants\\n(ICP or OCP) change.  For example:\\n\\n\\n\\nWhittaker and Stenton also performed a post-hoc analysis of the segment\\nboundaries that are defined by the control rules. The\\nboundaries fell into one of three types:\\n\\n\\n\\nCONTROL SHIFT TYPES\\n\\n ABDICATION: Okay, go on.\\n\\n REPETITION/SUMMARY: That would be my recommendation\\nand that will ensure that you get a logically integral set of files.\\n\\n INTERRUPTION: It is something new though um.\\n\\n\\n\\n\\n\\n\\n  ABDICATIONS correspond to those cases where the controller produces a prompt as the last utterance of the segment.  The class\\n REPETITION/SUMMARY corresponds to the controller producing a\\nredundant utterance. The utterance is either an exact repetition\\nof previous propositional content, or a summary that realizes a\\nproposition, P, which could have been inferred from what came\\nbefore.  Thus orderly control shifts occur when the controller\\nexplicitly indicates that s/he wishes to relinquish control. What\\nunifies  ABDICATIONS and  REPETITION/SUMMARIES is that the\\ncontroller supplies no new propositional content.  The remaining\\nclass,  INTERRUPTIONS, characterize shifts occurring when the\\nnoncontroller displays initiative by seizing control.  This class is\\nmore general than other definitions of Interruptions. It properly\\ncontains cross-speaker interruptions that involve topic shift, similar\\n to the true-interruptions of Grosz and Sidner, as well as  clarification subdialogues,. \\n\\n\\nThis classification suggests that the transfer of control is often a\\ncollaborative phenomenon.  Since a noncontroller(OCP), has the option\\nof seizing control at any juncture in discourse, it would seem that\\ncontrollers(ICPs), are in control because the noncontroller allows it.\\nThese observations address problems raised by Grosz and\\nSidner, namely how ICPs signal and OCPs recognize segment\\nboundaries.  The claim is that shifts of control often do not occur\\nuntil the controller indicates the end of a discourse segment by\\nabdicating or producing a repetition/summary.\\n\\n\\n    Control Segmentation and Anaphora\\n\\n\\nTo determine the relationship between the derived control segments and\\n ATTENTIONAL STATE we looked at the distribution of anaphora with\\nrespect to the control segments in the ADs.  All data were analysed\\nstatistically by \\n\\n\\n\\nand all differences cited are significant at\\nthe 0.05 level. We looked at all anaphors (excluding first and second\\nperson), and grouped them into 4 classes.\\n\\n\\n\\nClasses of Anaphors\\n\\n 3RD PERSON: it, they, them, their, she, he, her, him, his\\n\\n ONE/SOME, one of them, one of those, a new one, that\\none, the other one, some\\n\\n DEICTIC: Noun phrases, e.g. this, that, this NP, that\\nNP, those NP, these NP\\n\\n EVENT: Verb Phrases, Sentences, Segments, e.g. this, that, it\\n\\n\\n\\n\\n\\n\\nThe class  DEICTIC refers to deictic references to material introduced\\nby noun phrases, whereas the class  EVENT refers to material introduced\\nclausally.\\n\\n    Hierarchical Relationships\\n\\n\\nThe first phenomenon we noted was that the anaphora distribution indicated that\\n some segments are hierarchically related to others. This was especially apparent in\\ncases where one discourse participant interrupted briefly, then immediately\\npassed control back to the other.\\n\\n\\n\\nThe following example illustrates the same point.\\n\\n\\n\\nThe control segments as defined would treat both of these cases as\\ncomposed of 3 different segments. But this ignores the fact that\\nutterances (1) and (5) have closely related propositional content in\\nthe first example, and that the plural pronoun straddles the central\\nsubsegment with the same referents being picked out by they and\\ntheir in the second example. Thus we allowed for hierarchical\\nsegments by treating the interruptions of 2-4 as subsegments, and\\nutterances 1 and 5 as related parts of the parent segments. All\\ninterruptions were treated as embeddings in this way.  However the\\nrelationship of the segment after the interruption to the segment\\nbefore must be determined on independent grounds such as topic or\\nintentional structure.\\n\\n\\n  Distribution \\n\\nOnce we extended the control framework to allow for the embedding of\\ninterrupts, we coded every anaphor with respect to whether its\\nantecedent lay outside or within the current segment.  These are\\nlabelled X (cross segment boundary antecedent) NX (no cross segment\\n boundary), in Figure . In addition we break these down as to which type of control shift occurred at the previous segment\\nboundary.\\n\\n\\nWe also looked at the distribution of anaphora in the Support ADs and found\\nsimilar results.\\n\\n\\nFor both dialogues, the distribution of anaphors varies according to which\\ntype of control shift occurred at the previous segment boundary. When we look\\nat the different types of anaphora, we find that third person and one anaphors\\ncross boundaries extremely rarely, but the event anaphors and the deictic\\npronouns demonstrate a different pattern.  What does this mean?\\n\\n\\nThe fact that anaphora is more likely to cross segment boundaries following\\ninterruptions than for summaries or abdications is consistent with the control\\nprinciples.  With both summaries and abdications the speaker gives an explicit\\nsignal\\nthat s/he wishes to relinquish control.  In contrast, interruptions are the\\nunprompted attempts of the listener to seize control, often having to do with\\nsome\\n`problem' with the controller's utterance. Therefore, interruptions are much\\nmore\\nlikely to be within topic.\\n\\n\\nBut why should deixis and event anaphors behave differently from the other\\nanaphors?  Deixis serves to pick out objects that cannot be selected by the use\\nof standard anaphora, i.e.  we should expect the referents for deixis to be\\noutside immediate focus and hence more likely to be outside the current\\n segment.  The picture is more complex for event anaphora, which seems to serve a number of different functions in the dialogue. It is used to\\ntalk about the past events that lead up to the current situation, I did\\nTHAT in order to move the place.  It is also used to refer to sets of\\npropositions of the preceding discourse, Now THAT'S a little\\n background (cf ). The most prevalent use, however, was to refer to\\nfuture events or actions, THAT would be the move that I would make - but\\nyou have to\\ndo IT the same day.\\n\\n\\n\\nSince the task in the ADs is to develop a plan, speakers use event anaphora as\\nconcise references to the plans they have just negotiated and to discuss the\\nstatus and quality of plans that have been suggested.  Thus the frequent\\ncross-speaker references to future events and actions correspond to phases of\\n plan negotiation. More importantly these references are closely related to the control structure.  The example above illustrates the clustering\\nof event anaphora at segment boundaries.  One discourse participant uses an\\nanaphor to summarize a plan, but when the other participant evaluates this plan\\nthere may be a control shift and any reference to the plan will necessarily\\ncross a control boundary.  The distribution of event anaphora bears this out,\\nsince 23/25 references to future actions are within 2 utterances of a segment\\nboundary (See the example above). More significantly every instance of event\\nanaphora crossing a segment boundary occurs when the speaker is talking about\\nfuture events or actions.\\n\\n\\nWe also looked at the TODs for instances of anaphora being used to describe a\\nfuture act in the way that we observed in the ADs.  However, over the 938 turns\\nin the TODs, there were only 18 instances of event anaphora, because in the\\nmain there were few occasions when it was necessary to talk about the plan.\\nThe financial ADs had 45 event anaphors in 474 utterances.\\n\\n\\n\\n  Control and Collaborative Plans \\n\\nTo explore the relationship of control to planning, we compare the TODs\\nwith both types of ADs (financial and support). We would expect these\\ndialogues to differ in terms of initiative.  In the ADs, the objective is to\\ndevelop\\na collaborative plan through a series of conversational exchanges.  Both\\ndiscourse participants believe that the expert has knowledge about the domain,\\nbut\\nonly has partial information about the situation. They also believe that the\\nadvisee\\nmust contribute both the problem description and also constraints as to how the\\nproblem\\ncan be solved.  This information must be exchanged, so that the mutual beliefs\\nnecessary to develop the collaborative plan are established in the\\n conversation. The situation is different in the TODs.  Both participants here believe at the outset that the expert has sufficient\\ninformation about\\nthe situation and complete and correct knowledge about how to execute the Task.\\nSince\\nthe apprentice has no need to assert information to change the expert's beliefs\\nor to\\nask questions to verify the expert's beliefs or to issue commands, we should\\nnot expect\\nthe apprentice to have control.  S/he is merely present to execute the actions\\nindicated by the knowledgeable participant.\\n\\n\\nThe differences in the beliefs and knowledge states of the participants can\\nbe interpreted in the terms of the collaborative planning principles of\\n Whittaker and Stenton.  We generalize the principles of  INFORMATION QUALITY and  PLAN QUALITY, which predict when an\\ninterrupt should occur.\\n\\n\\n\\n INFORMATION QUALITY: The listener must believe that the\\ninformation that the speaker has provided is true, unambiguous and relevant\\nto the mutual goal. This corresponds to the two rules: (A1)  TRUTH: If\\nthe listener believes a fact P and believes that fact to be relevant and\\neither believes that the speaker believes not P or that the speaker does not\\nknow P then interrupt; (A2) AMBIGUITY: If the listener believes that the\\nspeaker's assertion is relevant but ambiguous then interrupt.\\n\\n PLAN QUALITY: The listener must believe that the action proposed\\n by the speaker is a part of an adequate plan to achieve the mutual goal and\\nthe action must also be comprehensible to the listener.  The two rules to\\nexpress this are: (B1) EFFECTIVENESS: If the listener believes P and\\neither believes that P presents an obstacle to the proposed plan or believes\\nthat P is part of the proposed plan that has already been satisfied, then\\ninterrupt; (B2)  AMBIGUITY: If the listener believes that an assertion\\nabout the proposed plan is ambiguous, then interrupt.\\n\\n\\n\\n\\nThese principles indirectly provide\\na means to ensure mutual belief.  Since a participant must interrupt\\nif any condition for an interrupt holds, then lack of interruption\\nsignals that there is no discrepancy in mutual beliefs.  If there is\\nsuch a discrepancy, the interruption is a necessary contribution to a\\ncollaborative plan, not a distraction from the joint activity.\\n\\n\\nWe compare ADs to TODs with respect to how often control is exchanged by\\n calculating the average number of turns between control shifts.  We also investigate whether control is shared equally between participants and what percentage\\nof control shifts are represented by abdications, interrupts, and summaries\\n for each dialogue type. See Figure . \\n\\n\\nThree things are striking about this data.  As we predicted, the\\ndistribution of control between expert and client is completely\\ndifferent in the ADs and the TODs. The expert has control for around\\n90% of utterances in the TODs whereas control is shared almost\\nequally in the ADs.  Secondly, contrary to our expectations, we did\\nfind some instances of shifts in the TODs.  Thirdly, the distribution\\nof interruptions and summaries differs across dialogue types.  How can\\nthe collaborative planning principles highlight the differences we\\nobserve?\\n\\n\\nThere seem to be two reasons why shifts occur in the TODs.  First, many\\ninterruptions in the TODs result from the apprentice seizing control just\\nto indicate that there is a temporary problem and that plan execution\\nshould be delayed.\\n\\n\\n\\nSecond, control was exchanged when the execution of the task started to go\\nawry.\\n\\n\\n\\nThe problem with the physical situation indicates to the apprentice that\\nthe relevant beliefs are no longer shared.  The Instructor is not in\\npossession of critical information such as the current state of the\\napprentice's pump.  This necessitates an information exchange to\\nresynchronize mutual beliefs, so that the rest of the plan may be\\nsuccessfully executed. However, since control is explicitly allocated to\\nthe instructor in TODs, there is no reason for that participant to believe\\nthat the other has any contribution to make. Thus there are fewer attempts\\nby the instructor to coordinate activity, such as by using summaries to\\nsynchronize mutual beliefs.  Therefore, if the apprentice needs to make a\\ncontribution, s/he must do so via interruption, explaining why there are\\n many more interruptions in these dialogues. In addition, the majority of Interruptions (73%) are initiated by apprentices, in contrast to the\\nADs in which only 29% are produced by the Clients.\\n\\n\\nSummaries are more frequent in ADs.  In the ADs both participants believe\\nthat a plan cannot be constructed without contributions from both of them.\\nAbdications and summaries are devices which allow these contributions to be\\ncoordinated and participants use these devices to explicitly set up\\nopportunities for one another to make a contribution, and to ensure mutual\\nbeliefs. The increased frequency of summaries in the ADs may result from\\nthe fact that the participants start with discrepant mutual beliefs about\\nthe situation and that establishing and maintaining mutual beliefs is a key\\npart of the ADs.\\n\\n\\n    Discussion\\n\\n\\nIt has often been stated that discourse is an inherently collaborative\\nprocess and that this is manifested in certain phenomena, e.g. the use\\n of anaphora and cue words ,, by which the speaker makes aspects of the discourse structure explicit.  We found\\nshifts of attentional state when shifts in control are negotiated and\\nagreed by all participants, but not when control is seized by one\\nparticipant without the acceptance of the others. This was reflected\\nin different distribution of anaphora in the two cases.  Furthermore\\nwe found that not all types of anaphora behaved in the same way.\\nEvent anaphora clustered at segment boundaries when it was used to\\nrefer to preceding segments and was more likely to cross segment\\nboundaries because of its function in talking about the proposed plan.\\nWe also found that control was distributed and exchanged differently\\nin the ADs and TODs. These results provide support for the control\\nrules.\\n\\n\\nIn our analysis we argued for hierarchical organization of the control\\nsegments on the basis of specific examples of interruptions. We also\\nbelieve that there are other levels of structure in discourse that are\\nnot captured by the control rules, e.g. control shifts do not always\\ncorrespond with task boundaries.  There can be topic shifts without\\nchange of initiation, change of control without a topic\\n shift. The relationship of cue words, intonational  contour and the use of modal subordination to the segments derived from the control rules is a topic for future\\nresearch.\\n\\n\\nA more controversial question concerns rhetorical relations and the extent\\n to which these are detected and used by listeners.  Hobbs has applied  COHERENCE RELATIONS to face-to-face conversation in which\\n mixed-initiative is displayed by participants,.  One category of rhetorical relation he describes is that of  ELABORATION,\\nin which a speaker repeats the propositional content of a previous\\nutterance.  Hobbs has some difficulties determining the function of this\\nrepetition, but we maintain that the function follows from the more general\\nprinciples of the control rules: speakers signal that they wish to shift\\ncontrol by supplying no new propositional content.  Abdications, repetitions\\nand summaries all add no new information and function to signal to the\\nlistener that the speaker has nothing further to say right now. The listener\\ncertainly must recognize this fact.\\n\\n\\nSummaries appear to have an additional function of synchronization, by\\nallowing both participants to agree on what propositions are mutually\\nbelieved at that point in the discussion. Thus this work highlights aspects\\nof collaboration in discourse, but should be formally integrated with\\n research on collaborative planning,, particularly with respect to the relation between control shifts and the coordination of\\nplans.\\n\\n\\n  Acknowledgements \\n\\nWe would like to thank Aravind Joshi for his support, comments and criticisms.\\nDiscussions of joint action with Phil Cohen and the members of CSLI's DIA\\nworking group have influenced the first author. We are also indebted to Susan\\nBrennan, Herb Clark, Julia Hirschberg, Jerry Hobbs, Libby Levison, Kathy\\nMcKeown, Ellen Prince, Penni Sibun, Candy Sidner, Martha Pollack, Phil Stenton,\\nand Bonnie Webber for their insightful comments and criticisms on drafts of\\nthis paper.\\n\\nBibliography \\n\\nSusan E. Brennan, Marilyn Walker Friedman, and Carl J. Pollard.\\nA centering approach to pronouns.\\nIn Proc. 25th Annual Meeting of the ACL, Stanford, pages\\n  155-162, 1987.\\n\\n\\nPhillip R. Cohen, Hector J. Levesque, Jose H. T. Nunes, and Sharon L. Oviatt.\\nTask oriented dialogue as a consequence of joint activity.\\nIn Pacific Rim Conference on Artificial Intelligence, 1990.\\n\\n\\nPhillip R. Cohen.\\nThe pragmatics of referring and the modality of communication.\\nComputational Linguistics, 10:97-146, 1984.\\n\\n\\nRobin Cohen.\\nAnalyzing the structure of argumentative discourse.\\nComputational Linguistics, 13:11-24, 1987.\\n\\n\\nPhillip R. Cohen, C. Raymond Perrault, and James F. Allen 1982.\\nBeyond question answering.\\nIn Wendy Lehnert and Martin Ringle, editors, Strategies for\\n  Natural Language Processing, pages 245-274. Lawrence Erlbaum Ass. Inc,\\n  Hillsdale, N.J., 1982.\\n\\n\\nPhilip R. Cohen and C. Raymond Perrault.\\nElements of a plan-based theory of speech acts.\\nIn Barbara J. Grosz, Karen Sparck Jones, and Bonnie Lynn Webber,\\n  editors, Readings in Natural Language Processing, pages 423-440.\\n  Morgan Kauffman, Los Altos, Ca., 1986.\\n\\n\\nHerbert H. Clark and Deanna Wilkes-Gibbs.\\nReferring as a collaborative process.\\nCognition, 22:1-39, 1986.\\n\\n\\nDavid M. Frohlich and Paul Luff.\\nConversational resources for situated action.\\nIn Proc. Annual Meeting of the Computer Human Interaction of the\\n  ACM, 1989.\\n\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.\\nTowards a computational theory of discourse interpretation.\\nUnpublished Manuscript, 1986.\\n\\n\\nBarbara J. Grosz.\\nThe representation and use of focus in dialogue understanding.\\nTechnical Report 151, SRI International, 333 Ravenswood Ave, Menlo\\n  Park, Ca. 94025, 1977.\\n\\n\\nBarbara J. Grosz and Candace L. Sidner.\\nAttentions, intentions and the structure of discourse.\\nComputational Linguistics, 12:175-204, 1986.\\n\\n\\nBarbara J. Grosz and Candace L. Sidner.\\nPlans for discourse.\\nIn Cohen, Morgan and Pollack, eds. Intentions in Communication,\\n  MIT Press, 1990.\\n\\n\\nJerry R. Hobbs and Michael H. Agar.\\nThe coherence of incoherent discourse.\\nTechnical Report CSLI-85-38, Center for the Study of Language and\\n  Information, Ventura Hall, Stanford University, Stanford, CA 94305, 1985.\\n\\n\\nJulia Hirschberg and Diane Litman.\\nNow lets talk about now: Identifying cue phrases intonationally.\\nIn Proc. 25th Annual Meeting of the ACL, Stanford, pages\\n  163-171, Stanford University, Stanford, Ca., 1987.\\n\\n\\nJerry R. Hobbs.\\nCoherence and coreference.\\nCognitive Science, 3:67-90, 1979.\\n\\n\\nAravind K. Joshi.\\nMutual beliefs in question-answer systems.\\nIn Neil V. Smith, editor, Mutual Knowledge, pages 181-199.\\n  Academic Press, New York, New York, 1982.\\n\\n\\nAlison Kidd.\\nThe consultative role of an expert system.\\nIn P. Johnson and S. Cook, editors, People and Computers:\\n  Designing the Interface. Cambridge University Press, Cambridge, U.K., 1985.\\n\\n\\nDiane Litman and James Allen.\\nRecognizing and relating discourse intentions and task-oriented\\n  plans.\\nIn Cohen, Morgan and Pollack, eds. Intentions in Communication,\\n  MIT Press, 1990.\\n\\n\\nKathleen R. McKeown.\\nDiscourse strategies for generating natural language text.\\nArtificial Intelligence, 27(1):1-42, September 1985.\\n\\n\\nR.S. Nickerson.\\nOn converational interaction with computers.\\nIn SiegFried Treu, editor, User-Oriented Design of Interactive\\n  Graphics Systems, pages 101-65. Elsevier Science, 1976.\\n\\n\\nSharon L. Oviatt and Philip R. Cohen.\\nThe effects of interaction on spoken discourse.\\nIn Proc. 27th Annual Meeting of the Association of Computational\\n  Linguistics, pages 126-134, 1989.\\n\\n\\nJanet Pierrehumbert and Julia Hirschberg.\\nThe meaning of intonational contours in the interpretation of\\n  discourse.\\nIn Cohen, Morgan and Pollack, eds. Intentions in Communication,\\n  MIT Press, 1990.\\n\\n\\nMartha Pollack, Julia Hirschberg, and Bonnie Webber.\\nUser participation in the reasoning process of expert systems.\\nIn AAAI82, 1982.\\n\\n\\nMartha Pollack.\\nInferring domain plans in question answering.\\nTechnical Report 403, SRI International - Artificial Intelligence\\n  Center, 1986.\\nUniversity of Pennsylvania Dissertation.\\n\\n\\nCraige Roberts.\\nModal Subordination and Anaphora.\\nPhD thesis, Linguistics Dept, University of Massachusetts, Amherst,\\n  1986.\\nPublished by Garland Press.\\n\\n\\nEmanuel A. Schegloff.\\nDiscourse as an interactional achievement: Some uses of 'uh huh' and\\n  other things that come between sentences.\\nIn D. Tannen, editor, Analyzing Discourse: Text and Talk, pages\\n  71-93. Georgetown University Press, 1982.\\n\\n\\nCandace L. Sidner.\\nToward a computational theory of definite anaphora comprehension in\\n  English.\\nTechnical Report AI-TR-537, MIT, 1979.\\n\\n\\nCandace Sidner.\\nWhat the speaker means: the recognition of speakers plans in\\n  discourse.\\nInternational Journal of Computers and Mathematics, 9:71-82,\\n  1983.\\n\\n\\nHarvey Sacks, Emmanuel Schegloff, and Gail Jefferson.\\nA simplest systematics for the organization of turn-taking in\\n  conversation.\\nLanguage, 50:325-345, 1974.\\n\\n\\nMarilyn A. Walker.\\nEvaluating discourse processing algorithms.\\nIn Proc. 27th Annual Meeting of the Association of Computational\\n  Linguistics, pages 251-261, 1989.\\n\\n\\nBonnie Lynn Webber.\\nTwo steps closer to event reference.\\nTechnical Report MS-CIS-86-74, Linc Lab 42, Department of Computer\\n  and Information Science, University of Pennsylvania, 1986.\\n\\n\\nBonnie Lynn Webber.\\nDiscourse deixis: Reference to discourse segments.\\nIn Proc. 26th Annual Meeting of the ACL, Association of\\n  Computational Linguistics, pages 113-123, 1988.\\n\\n\\nSteve Whittaker and Phil Stenton.\\nCues and control in expert client dialogues.\\nIn Proc. 26th Annual Meeting of the ACL, Association of\\n  Computational Linguistics, pages 123-130, 1988.\\n\\nFootnotes\\n\\n  10 randomly selected dialogues (474 turns) from\\na corpus that was collected and transcribed by Martha Pollack and\\n Julia Hirschberg,. \\n  4 dialogues (450 turns) from tapes made at\\n one of Hewlett-Packard's customer response centers.  See . \\n  5 keyboard (224 turns) and 5\\ntelephone dialogues (714 turns), which were collected in an experiment\\nby Phil Cohen to explore the relationship between modality,\\n interactivity and use of referring expressions. \\n  The theory of centering, which is\\npart of\\nattentional state, depends on discourse participants' recognizing the beginning\\nand\\n end of a discourse segment,. \\n  The relationship between utterance level\\nmeaning and discourse intentions rests on a theory of joint commitment or\\n shared plans, \\n  Our abdication category was called prompt\\n by . \\n  Similar phenomena\\nhas been noted by many researchers in discourse\\n including,,,. \\n  We\\nexcluded turns in dialogue openings and closings.\\n  The higher percentage\\nof Interruptions in the keyboard TODs in comparison with the telephone TODs\\nparallels Oviatt and Cohen's analysis, showing that participants exploit\\nthe wider bandwidth of the interactive spoken channel to break tasks down\\n into subtasks,. \\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nConversation between two people is usually of  MIXED-INITIATIVE,\\nwith  CONTROL over the conversation being transferred from one\\nperson to another.  We apply a set of rules for the transfer of\\ncontrol to 4 sets of dialogues consisting of a total of 1862 turns.\\nThe application of the control rules lets us derive domain-independent\\ndiscourse structures. The derived structures indicate that initiative\\nplays a role in the structuring of discourse.  In order to explore the\\nrelationship of control and initiative to discourse processes like\\ncentering, we analyze the distribution of four different classes of\\nanaphora for two data sets. This distribution indicates that some\\ncontrol segments are hierarchically related to others.  The analysis\\nsuggests that discourse participants often mutually agree to a change\\nof topic. We also compared initiative in Task Oriented and Advice\\nGiving dialogues and found that both allocation of control and the\\nmanner in which control is transferred is radically different for the\\ntwo dialogue types. These differences can be explained in terms of\\ncollaborative planning principles.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nParsing a natural language sentence can be viewed as making a sequence\\nof disambiguation decisions: determining the part-of-speech of the\\nwords, choosing between possible constituent structures, and selecting\\nlabels for the constituents.  Traditionally, disambiguation problems\\nin parsing have been addressed by enumerating possibilities and\\nexplicitly declaring knowledge which might aid the disambiguation\\nprocess.  However, these approaches have proved too brittle for most\\ninteresting natural language problems.\\n\\n\\nThis work addresses the problem of automatically discovering the\\ndisambiguation criteria for all of the decisions made during the\\nparsing process, given the set of possible features which can act as\\ndisambiguators.  The candidate disambiguators are the words in the\\nsentence, relationships among the words, and relationships among\\nconstituents already constructed in the parsing process.\\n\\n\\nSince most natural language rules are not absolute, the disambiguation\\ncriteria discovered in this work are never applied deterministically.\\nInstead, all decisions are pursued non-deterministically according to\\nthe probability of each choice.  These probabilities are estimated\\nusing statistical decision tree models.  The probability of a complete\\nparse tree (T) of a sentence (S) is the product of each decision\\n(di) conditioned on all previous decisions:\\n\\n\\n\\nEach decision sequence constructs a unique parse, and the parser\\nselects the parse whose decision sequence yields the highest\\ncumulative probability.  By combining a stack decoder search with a\\nbreadth-first algorithm with probabilistic pruning, it is possible to\\nidentify the highest-probability parse for any sentence using a\\nreasonable amount of memory and time.\\n\\n\\nThe claim of this work is that statistics from a large corpus of\\nparsed sentences combined with information-theoretic classification\\nand training algorithms can produce an accurate natural language\\nparser without the aid of a complicated knowledge base or grammar.\\nThis claim is justified by constructing a parser, called SPATTER\\n(Statistical PATTErn Recognizer), based on very limited linguistic\\ninformation, and comparing its performance to a state-of-the-art\\ngrammar-based parser on a common task.  It remains to be shown that an\\naccurate broad-coverage parser can improve the performance of a text\\nprocessing application.  This will be the subject of future\\nexperiments.\\n\\n\\nOne of the important points of this work is that statistical models of\\nnatural language should not be restricted to simple,\\ncontext-insensitive models.  In a problem like parsing, where\\nlong-distance lexical information is crucial to disambiguate\\ninterpretations accurately, local models like probabilistic\\ncontext-free grammars are inadequate.  This work illustrates that\\nexisting decision-tree technology can be used to construct and\\nestimate models which selectively choose elements of the context which\\ncontribute to disambiguation decisions, and which have few enough\\nparameters to be trained using existing resources.\\n\\n\\nI begin by describing decision-tree modeling, showing that\\ndecision-tree models are equivalent to interpolated n-gram models.\\nThen I briefly describe the training and parsing procedures used in\\nSPATTER.  Finally, I present some results of experiments comparing\\nSPATTER with a grammarian's rule-based statistical parser, along with\\nmore recent results showing SPATTER applied to the Wall Street Journal\\ndomain.\\n\\n\\n  Decision-Tree Modeling \\n\\nMuch of the work in this paper depends on replacing human\\ndecision-making skills with automatic decision-making algorithms.  The\\ndecisions under consideration involve identifying constituents and\\nconstituent labels in natural language sentences.  Grammarians, the\\nhuman decision-makers in parsing, solve this problem by enumerating\\nthe features of a sentence which affect the disambiguation decisions\\nand indicating which parse to select based on the feature values.  The\\ngrammarian is accomplishing two critical tasks: identifying the\\nfeatures which are relevant to each decision, and deciding which\\nchoice to select based on the values of the relevant features.\\n\\n\\nDecision-tree classification algorithms account for both of these\\ntasks, and they also accomplish a third task which grammarians\\nclassically find difficult.  By assigning a probability distribution\\nto the possible choices, decision trees provide a ranking\\nsystem which not only specifies the order of preference for the\\npossible choices, but also gives a measure of the relative likelihood\\nthat each choice is the one which should be selected.\\n\\n  What is a Decision Tree? \\n\\nA decision tree is a decision-making device which assigns a\\nprobability to each of the possible choices based on the context of\\nthe decision: P(f|h), where f is an element of the future\\nvocabulary (the set of choices) and h is a history (the\\ncontext of the decision).  This probability P(f|h) is determined by\\nasking a sequence of questions \\n\\n\\n\\n\\nabout the context,\\nwhere the ith question asked is uniquely determined by the answers\\nto the i-1 previous questions.\\n\\n\\nFor instance, consider the part-of-speech tagging problem.  The first\\nquestion a decision tree might ask is:\\n1.\\nWhat is the word being tagged?\\nIf the answer is the, then the decision tree needs to ask no\\nmore questions; it is clear that the decision tree should assign the\\ntag \\n\\n\\n\\n\\nwith probability 1.  If, instead, the answer\\nto question 1 is bear, the decision tree might next ask the\\nquestion:\\n2.\\nWhat is the tag of the previous word?\\nIf the answer to question 2 is determiner, the decision tree\\nmight stop asking questions and assign the tag \\n\\n\\n\\n\\nwith\\nvery high probability, and the tag \\n\\n\\n\\n\\nwith much lower\\nprobability.  However, if the answer to question 2 is noun, the\\ndecision tree would need to ask still more questions to get a good\\nestimate of the probability of the tagging decision.  The decision\\n tree described in this paragraph is shown in Figure . \\n\\n\\nEach question asked by the decision tree is represented by a tree\\nnode (an oval in the figure) and the possible answers to this\\nquestion are associated with branches emanating from the node.  Each\\nnode defines a probability distribution on the space of possible\\ndecisions.  A node at which the decision tree stops asking questions\\nis a leaf node.  The leaf nodes represent the unique states in\\nthe decision-making problem, i.e. all contexts which lead to the same\\nleaf node have the same probability distribution for the decision.\\n\\n\\n  Decision Trees vs. n-grams \\n\\nA decision-tree model is not really very different from an\\ninterpolated n-gram model.  In fact, they are equivalent in\\nrepresentational power.  The main differences between the two modeling\\ntechniques are how the models are parameterized and how the parameters\\nare estimated.\\n\\n  Model Parameterization \\n\\nFirst, let's be very clear on what we mean by an n-gram model.\\nUsually, an n-gram model refers to a Markov process where the\\nprobability of a particular token being generating is dependent on the\\nvalues of the previous n-1 tokens generated by the same process.  By\\nthis definition, an n-gram model has |W|[n] parameters, where\\n|W| is the number of unique tokens generated by the process.\\n\\n\\nHowever, here let's define an n-gram model more loosely as a model\\nwhich defines a probability distribution on a random variable given\\nthe values of n-1 random variables,\\n\\n\\n\\n\\nThere is no assumption in the\\ndefinition that any of the random variables F or Hi range over\\nthe same vocabulary.  The number of parameters in this n-gram\\nmodel is \\n\\n\\n\\n\\nUsing this definition, an n-gram model can be represented by a\\ndecision-tree model with n-1 questions.  For instance, the\\npart-of-speech tagging model \\n\\nP(ti|witi-1ti-2) can be\\ninterpreted as a 4-gram model, where H1 is the variable\\ndenoting the word being tagged, H2 is the variable denoting the tag\\nof the previous word, and H3 is the variable denoting the tag of\\nthe word two words back.  Hence, this 4-gram tagging model is the\\nsame as a decision-tree model which always asks the sequence of 3\\nquestions:\\n1.\\nWhat is the word being tagged?\\n2.\\nWhat is the tag of the previous word?\\n3.\\nWhat is the tag of the word two words back?\\n\\n\\nBut can a decision-tree model be represented by an n-gram model?\\nNo, but it can be represented by an interpolated n-gram\\nmodel.  The proof of this assertion is given in the next section.\\n\\n\\n  Model Estimation \\n\\nThe standard approach to estimating an n-gram model is a two step\\nprocess.  The first step is to count the number of occurrences of each\\nn-gram from a training corpus.  This process determines the\\nempirical distribution,\\n\\n\\n\\nThe second step is smoothing the empirical distribution using a\\nseparate, held-out corpus .  This step improves the empirical\\ndistribution by finding statistically unreliable parameter estimates\\nand adjusting them based on more reliable information.\\n\\n\\nA commonly-used technique for smoothing is deleted interpolation.\\nDeleted interpolation estimates a model\\n\\n\\n\\n\\nby using a linear combination\\nof empirical models \\n\\n\\n\\n\\nwhere m [ nand \\n\\nki-1 [ ki [ n for all \\n\\n\\n\\nFor example, a model\\n\\n\\n\\n\\nmight be interpolated as follows:\\n\\n\\n\\nwhere \\n\\n\\n\\n\\nfor all histories \\n\\nh1h2h3. The\\noptimal values for the \\n\\n\\n\\nfunctions can be estimated using\\n the forward-backward algorithm . \\n\\n\\nA decision-tree model can be represented by an interpolated n-gram\\nmodel as follows.  A leaf node in a decision tree can be represented\\nby the sequence of question answers, or history values, which leads\\nthe decision tree to that leaf.  Thus, a leaf node defines a\\nprobability distribution based on values of those questions:\\n\\n\\n\\n\\nwhere m [ n and\\n\\nki-1 [ ki [ n, and where hki is the answer to one of the\\n questions asked on the path from the root to the leaf. But this is the same as one of the terms in the interpolated n-gram model.  So, a decision tree can be defined as\\nan interpolated n-gram model where the \\n\\n\\n\\nfunction is\\ndefined as:\\n\\n\\n\\n\\n  Decision-Tree Algorithms \\n\\nThe point of showing the equivalence between n-gram models and\\ndecision-tree models is to make clear that the power of decision-tree\\nmodels is not in their expressiveness, but instead in how they can be\\nautomatically acquired for very large modeling problems.  As ngrows, the parameter space for an n-gram model grows\\nexponentially, and it quickly becomes computationally infeasible to\\nestimate the smoothed model using deleted interpolation.  Also, as ngrows large, the likelihood that the deleted interpolation process\\nwill converge to an optimal or even near-optimal parameter setting\\nbecomes vanishingly small.\\n\\n\\nOn the other hand, the decision-tree learning algorithm increases the\\nsize of a model only as the training data allows.  Thus, it can\\nconsider very large history spaces, i.e. n-gram models with very\\nlarge n. Regardless of the value of n, the number of parameters in\\nthe resulting model will remain relatively constant, depending mostly\\non the number of training examples.\\n\\n\\nThe leaf distributions in decision trees are empirical estimates,\\ni.e. relative-frequency counts from the training data.  Unfortunately,\\nthey assign probability zero to events which can possibly occur.\\nTherefore, just as it is necessary to smooth empirical n-gram\\nmodels, it is also necessary to smooth empirical decision-tree models.\\n\\n\\nThe decision-tree learning algorithms used in this work were developed\\nover the past 15 years by the IBM Speech Recognition group\\n .  The growing algorithm is an adaptation of the CART  algorithm in .  For detailed descriptions and discussions of the decision-tree algorithms used in this work, see\\n . \\n\\n\\nAn important point which has been omitted from this discussion of\\ndecision trees is the fact that only binary questions are used in\\nthese decision trees.  A question which has k values is decomposed\\ninto a sequence of binary questions using a classification tree on\\nthose k values.  For example, a question about a word is represented\\nas 30 binary questions.  These 30 questions are determined by growing\\na classification tree on the word vocabulary as described in\\n .  The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined\\nsuch that it is possible to identify each word by asking all 30\\nquestions.  For more discussion of the use of binary decision-tree\\n questions, see . \\n\\n\\n\\n  SPATTER Parsing \\n\\nThe SPATTER parsing algorithm is based on interpreting parsing as a\\nstatistical pattern recognition process.  A parse tree for a sentence\\nis constructed by starting with the sentence's words as leaves of a\\ntree structure, and labeling and extending nodes these nodes until a\\nsingle-rooted, labeled tree is constructed.  This pattern recognition\\nprocess is driven by the decision-tree models described in the\\nprevious section.\\n\\n  SPATTER Representation \\n\\nA parse tree can be viewed as an n-ary branching tree, with each\\nnode in a tree labeled by either a non-terminal label or a\\npart-of-speech label.  If a parse tree is interpreted as a geometric\\npattern, a constituent is no more than a set of edges which meet at\\nthe same tree node.  For instance, the noun phrase, ``a brown cow,''\\nconsists of an edge extending to the right from ``a,'' an edge\\nextending to the left from ``cow,'' and an edge extending straight up\\nfrom ``brown''.\\n\\n\\nIn SPATTER, a parse tree is encoded in terms of four elementary\\ncomponents, or features: words, tags, labels, and extensions.\\nEach feature has a fixed vocabulary, with each element of a given\\nfeature vocabulary having a unique representation.  The word feature\\ncan take on any value of any word.  The tag feature can take on any\\nvalue in the part-of-speech tag set.  The label feature can take on\\nany value in the non-terminal set.  The extension can take on any of\\nthe following five values:\\nright\\n- the node is the first child of a constituent;\\nleft\\n- the node is the last child of a constituent;\\nup\\n- the node is neither the first nor the last child of a\\nconstituent;\\nunary\\n- the node is a child of a unary constituent;\\nroot\\n- the node is the root of the tree.\\n\\n\\nFor an n word sentence, a parse tree has n leaf nodes, where the\\nword feature value of the ith leaf node is the ith word in the\\nsentence.  The word feature value of the internal nodes is intended to\\ncontain the lexical head of the node's constituent.  A deterministic\\nlookup table based on the label of the internal node and the labels of\\nthe children is used to approximate this linguistic notion.\\n\\n\\nThe SPATTER representation of the sentence\\n(S (N Each_DD1 code_NN1\\n      (Tn used_VVN\\n          (P by_II (N the_AT PC_NN1))))\\n   (V is_VBZ listed_VVN))\\n is shown in Figure .  The nodes are constructed bottom-up from left-to-right, with the constraint that no constituent\\nnode is constructed until all of its children have been constructed.\\nThe order in which the nodes of the example sentence are constructed\\nis indicated in the figure.\\n\\n\\n  Training SPATTER's models \\n\\nSPATTER consists of three main decision-tree models: a part-of-speech\\ntagging model, a node-extension model, and a node-labeling model.\\n\\n\\nEach of these decision-tree models are grown using the following\\nquestions, where X is one of word, tag, label, or extension, and Yis either left and right:\\n\\nWhat is the X at the current node?\\n\\nWhat is the X at the node to the Y?\\n\\nWhat is the X at the node two nodes to the Y?\\n\\nWhat is the X at the current node's first child from the Y?\\n\\nWhat is the X at the current node's second child from the Y?\\n\\n\\nFor each of the nodes listed above, the decision tree could also ask\\nabout the number of children and span of the node.  For the tagging\\nmodel, the values of the previous two words and their tags are also\\nasked, since they might differ from the head words of the previous two\\nconstituents.\\n\\n\\nThe training algorithm proceeds as follows.  The training corpus is\\ndivided into two sets, approximately 90% for tree growing and 10%\\nfor tree smoothing.  For each parsed sentence in the tree growing\\ncorpus, the correct state sequence is traversed.  Each state\\ntransition from si to si+1 is an event; the history is made up\\nof the answers to all of the questions at state si and the future\\nis the value of the action taken from state si to state si+1.Each event is used as a training example for the decision-tree growing\\nprocess for the appropriate feature's tree (e.g. each tagging event is\\nused for growing the tagging tree, etc.).  After the decision trees\\nare grown, they are smoothed using the tree smoothing corpus using a\\nvariation of the deleted interpolation algorithm described in\\n . \\n\\n\\n  Parsing with SPATTER \\n\\nThe parsing procedure is a search for the highest probability parse\\ntree.  The probability of a parse is just the product of the\\nprobability of each of the actions made in constructing the parse,\\naccording to the decision-tree models.\\n\\n\\nBecause of the size of the search space, (roughly \\n\\nO(|T|[n|N|n]),where |T| is the number of part-of-speech tags, n is the number of\\nwords in the sentence, and |N| is the number of non-terminal\\nlabels), it is not possible to compute the probability of every parse.\\nHowever, the specific search algorithm used is not very important, so\\nlong as there are no search errors.  A search error occurs when the\\nthe highest probability parse found by the parser is not the highest\\nprobability parse in the space of all parses.\\n\\n\\nSPATTER's search procedure uses a two phase approach to identify the\\nhighest probability parse of a sentence.  First, the parser uses a\\nstack decoding algorithm to quickly find a complete parse for the\\nsentence.  Once the stack decoder has found a complete parse of\\nreasonable probability (]10[-5]), it switches to a breadth-first\\nmode to pursue all of the partial parses which have not been explored\\nby the stack decoder.  In this second mode, it can safely discard any\\npartial parse which has a probability lower than the probability of\\nthe highest probability completed parse.  Using these two search\\nmodes, SPATTER guarantees that it will find the highest probability\\nparse.  The only limitation of this search technique is that, for\\nsentences which are modeled poorly, the search might exhaust the\\navailable memory before completing both phases.  However, these search\\nerrors conveniently occur on sentences which SPATTER is likely to get\\nwrong anyway, so there isn't much performance lossed due to the search\\nerrors.  Experimentally, the search algorithm guarantees the highest\\nprobability parse is found for over 96% of the sentences parsed.\\n\\n\\n\\n  Experiment Results \\n\\nIn the absence of an NL system, SPATTER can be evaluated by comparing\\nits top-ranking parse with the treebank analysis for each test\\nsentence.  The parser was applied to two different domains, IBM\\nComputer Manuals and the Wall Street Journal.\\n\\n  IBM Computer Manuals \\n\\nThe first experiment uses the IBM Computer Manuals domain, which\\nconsists of sentences extracted from IBM computer manuals.  The\\ntraining and test sentences were annotated by the University of\\nLancaster.  The Lancaster treebank uses 195 part-of-speech tags and 19\\nnon-terminal labels.  This treebank is described in great detail in\\n . \\n\\n\\nThe main reason for applying SPATTER to this domain is that IBM had\\nspent the previous ten years developing a rule-based,\\nunification-style probabilistic context-free grammar for parsing this\\ndomain.  The purpose of the experiment was to estimate SPATTER's\\nability to learn the syntax for this domain directly from a treebank,\\ninstead of depending on the interpretive expertise of a grammarian.\\n\\n\\nThe parser was trained on the first 30,800 sentences from the\\nLancaster treebank.  The test set included 1,473 new sentences, whose\\nlengths range from 3 to 30 words, with a mean length of 13.7 words.\\nThese sentences are the same test sentences used in the experiments\\n reported for IBM's parser in .  In , IBM's parser was evaluated using the 0-crossing-brackets measure,\\nwhich represents the percentage of sentences for which none of the\\nconstituents in the parser's parse violates the constituent boundaries\\nof any constituent in the correct parse.  After over ten years of\\ngrammar development, the IBM parser achieved a 0-crossing-brackets\\nscore of 69%.  On this same test set, SPATTER scored 76%.\\n\\n\\n  Wall Street Journal \\n\\nThe experiment is intended to illustrate SPATTER's ability to\\naccurately parse a highly-ambiguous, large-vocabulary domain.  These\\nexperiments use the Wall Street Journal domain, as annotated in the\\nPenn Treebank, version 2.  The Penn Treebank uses 46 part-of-speech\\n tags and 27 non-terminal labels. \\n\\n\\nThe WSJ portion of the Penn Treebank is divided into 25 sections,\\nnumbered 00 - 24.  In these experiments, SPATTER was trained on\\nsections 02 - 21, which contains approximately 40,000 sentences.  The\\ntest results reported here are from section 00, which contains 1920\\n sentences.  Sections 01, 22, 23, and 24 will be used as test data in future experiments.\\n\\n\\nThe Penn Treebank is already tokenized and sentence detected by human\\nannotators, and thus the test results reported here reflect this.\\nSPATTER parses word sequences, not tag sequences.  Furthermore,\\nSPATTER does not simply pre-tag the sentences and use only the\\nbest tag sequence in parsing.  Instead, it uses a probabilistic model\\nto assign tags to the words, and considers all possible tag sequences\\naccording to the probability they are assigned by the model.  No\\ninformation about the legal tags for a word are extracted from the\\ntest corpus.  In fact, no information other than the words is used\\nfrom the test corpus.\\n\\n\\nFor the sake of efficiency, only the sentences of 40 words or fewer\\n are included in these experiments.  For this test set, SPATTER takes on average 12 seconds per sentence on an SGI R4400\\nwith 160 megabytes of RAM.\\n\\n\\nTo evaluate SPATTER's performance on this domain, I am using the\\n PARSEVAL measures, as defined in : \\nPrecision\\n\\n\\n\\nRecall\\n\\n\\n\\nCrossing Brackets\\nno. of constituents which violate constituent\\nboundaries with a constituent in the treebank parse.\\n\\n\\nThe precision and recall measures do not consider constituent labels\\nin their evaluation of a parse, since the treebank label set will not\\nnecessarily coincide with the labels used by a given grammar.  Since\\nSPATTER uses the same syntactic label set as the Penn Treebank, it\\nmakes sense to report labelled precision and labelled recall.  These\\nmeasures are computed by considering a constituent to be correct if\\nand only if it's label matches the label in the treebank.\\n\\n\\n Table  shows the results of SPATTER evaluated against the Penn Treebank on the Wall Street Journal section 00.\\n\\n\\n Figures , , and   illustrate the performance of SPATTER as a function of sentence length.  SPATTER's performance degrades slowly\\nfor sentences up to around 28 words, and performs more poorly and more\\n erratically as sentences get longer.  Figure  indicates the frequency of each sentence length in the test corpus.\\n\\n\\n\\n  Conclusion \\n\\nRegardless of what techniques are used for parsing disambiguation, one\\nthing is clear: if a particular piece of information is necessary for\\nsolving a disambiguation problem, it must be made available to the\\ndisambiguation mechanism.  The words in the sentence are clearly\\nnecessary to make parsing decisions, and in some cases long-distance\\nstructural information is also needed.  Statistical models for parsing\\nneed to consider many more features of a sentence than can be managed\\nby n-gram modeling techniques and many more examples than a human\\ncan keep track of.  The SPATTER parser illustrates how large amounts\\nof contextual information can be incorporated into a statistical model\\nfor parsing by applying decision-tree learning algorithms to a large\\nannotated corpus.\\n\\nBibliography \\n\\nL. R. Bahl, P. F. Brown, P. V. deSouza, and R. L. Mercer.\\n1989.\\nA tree-based statistical language model for natural language\\nspeech recognition.\\nIEEE Transactions on Acoustics, Speech, and Signal\\nProcessing, Vol. 36, No. 7, pages 1001-1008.\\n\\n\\nL. E. Baum.\\n1972.\\nAn inequality and associated maximization technique in\\nstatistical estimation of probabilistic functions of markov processes.\\nInequalities, Vol. 3, pages 1-8.\\n\\n\\nE. Black and et al.\\n1991.\\nA procedure for quantitatively comparing the syntactic\\ncoverage of english grammars.\\nProceedings of the February 1991 DARPA Speech and\\nNatural Language Workshop, pages 306-311.\\n\\n\\nE. Black, R. Garside, and G. Leech.\\n1993.\\nStatistically-driven computer grammars of english: the\\nibm/lancaster approach.\\nRodopi, Atlanta, Georgia.\\n\\n\\nL. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone.\\n1984.\\nClassification and Regression Trees.\\nWadsworth and Brooks, Pacific Grove, California.\\n\\n\\nP. F. Brown, V. Della Pietra, P. V. deSouza, J. C. Lai, and\\nR. L. Mercer.\\n1992.\\n``Class-based n-gram models of natural language.''\\nComputational Linguistics, 18(4), pages 467-479.\\n\\n\\nD. M. Magerman.\\n1994.\\nNatural Language Parsing as Statistical Pattern Recognition.\\nDoctoral dissertation.\\nStanford University, Stanford, California.\\n\\nFootnotes\\n\\n  This work was sponsored by the Advanced Research\\nProjects Agency, contract DABT63-94-C-0062.  It does not reflect the\\nposition or the policy of the U.S. Government, and no official\\nendorsement should be inferred.  Thanks to the members of the IBM\\nSpeech Recognition Group for their significant contributions to this\\nwork.\\n  Note\\nthat in a decision tree, the leaf distribution is not affected by the\\norder in which questions are asked.  Asking about h1 followed by\\nh2 yields the same future distribution as asking about h2followed by h1.\\n  This treebank also contains\\ncoreference information, predicate-argument relations, and trace\\ninformation indicating movement; however, none of this additional\\ninformation was used in these parsing experiments.\\n  For an independent research project on\\ncoreference, sections 00 and 01 have been annotated with detailed\\ncoreference information.  A portion of these sections is being used as\\na development test set.  Training SPATTER on them would improve\\nparsing accuracy significantly and skew these experiments in favor of\\nparsing-based approaches to coreference.  Thus, these two sections\\nhave been excluded from the training set and reserved as test\\nsentences.\\n  SPATTER returns a complete\\nparse for all sentences of fewer then 50 words in the test set, but\\nthe sentences of 41 - 50 words required much more computation than the\\nshorter sentences, and so they have been excluded.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nSyntactic natural language parsers have shown themselves to be\\ninadequate for processing highly-ambiguous large-vocabulary text, as\\nis evidenced by their poor performance on domains like the Wall Street\\nJournal, and by the movement away from parsing-based approaches to\\ntext-processing in general.  In this paper, I describe SPATTER, a\\nstatistical parser based on decision-tree learning techniques which\\nconstructs a complete parse for every sentence and achieves accuracy\\nrates far better than any published result.  This work is based on the\\nfollowing premises: (1) grammars are too complex and detailed to\\ndevelop manually for most interesting domains; (2) parsing models must\\nrely heavily on lexical and contextual information to analyze\\nsentences accurately; and (3) existing n-gram modeling techniques\\nare inadequate for parsing models.  In experiments comparing SPATTER\\nwith IBM's computer manuals parser, SPATTER significantly outperforms\\nthe grammar-based parser.  Evaluating SPATTER against the Penn\\nTreebank Wall Street Journal corpus using the PARSEVAL measures,\\nSPATTER achieves 86% precision, 86% recall, and 1.3 crossing\\nbrackets per sentence for sentences of 40 words or less, and 91%\\nprecision, 90% recall, and 0.5 crossing brackets for sentences\\nbetween 10 and 20 words in length.\\n\\n\"],\n",
              " ['\\n\\n  Introduction \\n\\nIn their most common implementations, logic grammars resort to list\\nrepresentations of the strings being analyzed or synthesized.\\nThis list-based implementation results in several deficiencies in logic\\ngrammars, while other deficiencies are inherited from Prolog.  Datalog\\ngrammars were born in order to address all these deficiencies, namely: an\\ninfinite Herbrand Universe, non-termination, unnecessary recomputation,\\nstructure creation on the heap, bottleneck for multi-threaded execution due to\\nthe use of (sequential)\\nlist data structures, and inability to work directly on files.\\n\\n\\nIn Datalog grammars, a given CF grammar is automatically translated into an\\n assertional representation, first proposed by Robert Kowalski,\\nwhich is largely equivalent to the list-based one but which ensures, under\\nappropriate evaluation mechanisms such as OLDT resolution, that the\\ntermination and complexity properties of the original CF-grammar are\\npreserved.  We have moreover shown that,\\nin restricted but useful cases, this can be achieved even in the\\npresence of extra arguments.\\n\\n\\nCoordination has long been a  difficult problem both in linguistics and in\\nlanguage processing. The difficulty lies in that any two constituents\\ncan be coordinated (even of different kind), and in that often some substring\\nthat is explicit in one of the conjuncts is missing in the other. For instance,\\nWood\\'s example:\\n\\n\\nJohn drove the car through and completely demolished a window.\\n\\n\\n exhibits\\na missing object ( \\'\\'a window\") in the first conjunct, and a\\nmissing subject (\\'\\'John\") in the second. Moreover, in representing these\\ncoordinated sentences, say in some logical form, we must take care of not\\nrequantifying \"a window\" when we reconstitute its meaning at the missing\\npoint: the window driven through must be equated with the demolished one.\\n\\n\\nWhile humans have in general no trouble reconstituting these missing elements\\nand attaching the right semantics to them, it is a challenge to efficiently\\nspell out for a machine the regularities found in coordination\\nphenomena.\\n\\n\\nIn this article we show how we can extend the\\nincremental evaluation implementation of Datalog grammars in order to\\nautomatically extend a grammar which has no rules for coordination with a\\nmeta-grammatical treatment which allows us to parse coordinated sentences.\\n\\n\\nOur treatment of coordination incorporates an adaptation of\\nrecent work on ellipsis which\\nresorts to the idea of parallel structures\\n (,,, but unlike these approaches which stress semantic parallelism, we use both syntactic and\\nsemantic parallelism.\\n\\n\\n  Background \\n  Assertional Representation \\n\\nIn DLGs, a call to analyze \"the martian disappeared\",  for\\ninstance, compiles into:\\n\\n\\n\\n while\\nlexical rules compile into forms that  use these representations\\naccordingly, e.g.:\\n\\n\\n\\n Other grammar rules translate just as in Definite Clause Grammars,\\nthe standard Prolog grammar formalism.\\n\\n\\n  Incremental evaluation \\n\\nIn order to increase efficiency, one possible implementation for for DLGs\\nexploits the incremental Datalog\\ntechnique of generating and maintaining data bottom-up. Using the well-known\\nsemi-naive evaluation algorithm, we begin with the set of axioms and obtain the\\ntheorems of the\\nfirst \"layer\" by applying the derivation rules; then we take these theorems\\nas new starting point, to derive the theorems of the second layer , and so\\non. Generally to derive the theorems of the next \"layer\", at least one\\ntheorem produced at the previous stage must be used. This process\\nterminates when no more new theorems can be generated.\\n\\n\\n  Coordination \\n\\nEarly work on coordination proposed meta-grammatical treatments (e.g.\\n ,), in which the appearance of a coordinating word, or conjunction (e.g. \\'\\'and\", \\'\\'or\", \\'\\'but\") is treated as a demon. When a\\nconjunction\\nappears in a\\nsentence of the form\\n\\n\\n A X conj Y B\\n\\n\\n a process is triggered in which backing up is done in the parse\\nhistory in order\\nto parse Y parallel to X, and B is parsed by merger with the state interrupted\\nby the conjunction.\\n\\n\\nThus, in Wood\\'s example we would have:\\n\\n\\n\\nThe reconstructed phrase should then be A X B and A Y B, with the warning\\nalready made re. requantification.\\n\\n\\nWe next modify this treatment and express it through\\nDLG constraints to be intertwined with the incremental evaluation of a\\nDLG grammar.  We shall then discuss more recent views on parsing parallel\\nstructures, and extend our treatment by adapting some of these ideas into our\\nDLG framework.\\n\\n\\n\\n  Treating coordination through DLGs plus constraints \\n\\nOur idea for a Datalog treatment of coordination is also, as in the work\\nreviewed in the last section, based on the\\nassumption that a string containing a conjunction contains around that\\nconjunction two\\nconstituents which are being coordinated. But instead of identifying\\nfour substrings A, B, X and Y, we simply assume that there are two\\ncoordinating constituents, V and W, surrounding the conjunction,\\nwhich must in general be of the same category and have parallel parses. Thus\\nany missing\\nelements in either V or W can be reconstructed from the other. We also adopt\\nthe heuristics that closer scoped coordinations will be attempted before\\nlarger scoped ones. Thus in Wood\\'s example, \\'\\'vp conj vp\" is tried before\\n\\'\\'sent conj sent\".\\n\\n\\nThus in that example, \\'\\'John\" would parse as the subject noun phrase of a\\n sentence with\\na complex verb phrase. Therefore we have\\n\\n\\n\\nBecause the conjunction is reached before the first verb phrase is finished\\nparsing (\"through\" analyses as a preposition introducing a prepositional\\nphrase- i.e., expecting a noun phrase to follow), the unfulfilled expectation\\nof a noun phrase is postponed until it can be equated with a noun phrase in\\nW.\\n\\n\\nNotice that what we mean by V and W having parallel parses is not that they\\nmust necessarily\\nfollow the  same structure to the last details, but that their\\nstructures must complement each other so that missing constituents in one may\\nbe\\nreconstructed from the other. We further assume, for the purposes of this\\narticle, that they both must\\nhave the same root (in this case, a verb phrase root),  although this\\nassumption is not necessary in general.\\n\\n\\nAnother thing to notice is that, whereas in the first analysis of Wood\\'s\\nexample we end up with two conjoined sentences, in the analysis just\\nproposed we end up with a sentence having a verb phrase which decomposes\\ninto two conjoined verb phrases. Linguistically speaking, it is\\narguable whether one analysis is preferable over\\nthe other one. But computationally speaking, the second analysis allows us\\nto apply our meta-grammatical treatment of coordination to sentences for which\\nthe first analysis would fail. An example is\\n\\n\\nJean mange une pomme rouge et une verte.\\n\\n\\nThis sentence cannot be split into A X conj B Y to reconstitute an unreduced\\nstructure following the first analysis. On the other hand, using the second\\nanalysis, we can postulate\\n\\n\\nV = une pomme rouge, W = une verte\\n\\n\\nand require that W follow a structure parallel to that of V. This then allows\\nus to reconstitute the missing noun in W.\\n\\n\\nWe now describe our proposed extension of incrementally implemented Datalog\\ngrammars in an intuitive manner, using the above example. We assume a simple\\nFrench grammar with rules such as\\n\\n\\n\\nOur grammar includes no rules for coordination (but does, of course,\\nrecognize conjunctions as such).\\n\\n\\nLet us recall that, in a Datalog grammar, our input string would be\\nrepresented as:\\n\\n\\n\\nThe idea is simply to check, at every step of the incremental derivation\\nof the theorems, whether a theorem conj(N,M) has been derived. As soon as\\none is, a constraint is added to the effect that, in some subsequent step\\nof the incremental derivation of theorems, a constituent of category Cat must\\nbe found between some point Z and the point N, such\\nthat the same category stretches between M and some later point P; and that\\nfinding them implies that the string between Z and P must also have category\\nCat.\\n\\n\\nThis constraint can be noted:\\n\\n\\n\\nAs soon as one of these predictions is fulfilled (e.g. when we have found a\\nnoun\\nphrase \\'\\'une pomme rouge\" between Z=2 and N=5), we can further specify the\\nother prediction to follow the same structure as that of the found noun\\nphrase, which will allow us to reconstruct any missing elements.\\n\\n\\n Notice that backtracking can occur. For instance, the machine will first\\npostulate that the conjoined categories must be \\'\\'adjective\", and that Z=4\\n(this\\nwould be a good guess for the sentence: \\'\\'Jean mange une pomme rouge et\\nverte\").\\nBut in our sample sentence, this first try will fail to find an adjective\\nstarting at point M=6, so backtracking would undo the bindings and suspend the\\nconstraint until\\nother suitable candidates for \\'\\'Cat\" and \\'\\'Z\" are derived.\\n\\n\\nWe next present a step-by-step follow up for the example given. Sequences of\\ntheorems derived are noted T1, T2, etc.; whereas sets of constraints are noted\\nC1,C2,etc.\\n\\n\\n\\n\\n tries Z=4 and fails. So the\\nconstraint suspends until something else of the form Cat(Z,5) appears.\\n\\n\\n\\n tries Z=2, and uses top-down prediction to find a\\n(possibly incomplete) noun phrase stretching from point 6 to some\\npoint P, e.g. through the rule:\\n\\n\\n\\n succeeds with  substitutions X=7, Y=7,P=8\\n\\n\\n\\nNotice that, at the point in which the constraint succeeds with  substitutions\\nX=7, Y=7, if the grammar included arguments for semantic representation,\\nthe semantic representations for the two nouns would be unified, given that\\none of them is missing (as shown by the fact that its starting point, 7, is the\\nsame as its ending point). We shall later give a full example involving\\nsemantic representations.\\n\\n\\n  Related work on ellipsis \\n\\nA notion that is central to recent work on ellipsis, and which has been\\npresent in embryonic form, as we have seen, even in the early work on\\ncoordination, is that of parallelism as a key element in the determination of\\n implicit meanings. Asher  defines parallelism as \\n\\n\\na pairing of constituents ... and their parts, such that each pair contains\\ntwo semantically and structurally similar objects\\n\\n\\n describes  an elliptical construction as\\none involving two phases\\n(usually clauses) that are parallel in structure in some sense.\\n\\n\\n, following , also postulates the necessity,\\nwithin a feature-structure setting, of combining elements which exhibit\\na degree of syntactic-semantic parallelism in order to determine the\\nway in which some kinds of anaphora are resolved, and argue that the use\\nof default unification (or priority union) improves on\\nPrst\\'s operation for\\ncombining the parallel structures.\\n\\n\\n Although the analysis of  precedes that of , the latter may be easier to follow, so we shall discuss it first.\\n\\n\\n Intuitively, default unification  takes two feature structures, one of which (called the TARGET) is identified as \\'\\'strict\",\\nwhile the other one (called the SOURCE) is \\'\\'defeasible\", and combines\\nthe information in both such that the information in the strict structure\\ntakes priority over that in the defeasible structure. For instance,\\nthe combination of the feature structures shown below for sentences 1a and\\n1b\\n\\n\\n\\n results in the priority union:\\n\\n\\n\\nThus, the implicit constituent in the second sentence is reconstituted from the\\nfirst by using a generally applicable procedure on the representations of the\\nparallel structures.\\n\\n\\n postulated a similar analysis, but it was based on \\n\\n\\n\\n -calculus semantic representations, and used higher order unification.\\n\\n\\nFor instance, in their example:\\nDan likes golf, and George does too.\\n\\n\\n they identify the antecedent or source as the complete structure\\n(\\'\\'Dan likes\\ngolf\"), whereas the target clause (\\'\\'George does too\") is either missing,\\nor contains only vestiges of, material found overtly in the source.\\n\\n\\nTheir analysis of such structures consists of:\\n\\n\\n a) determining the parallel structure of source and target;\\n\\n\\n b) determining which are parallel elements in source and target\\n(e.g.,\\n\\'\\'Dan\" and \\'\\'George\" are parallel elements in the example);\\n\\n\\n  c) using Huet\\'s higher-order unification algorithm  for finding\\na property P such that \\n\\nP(s1,...,sn)=S,\\n\\n\\n where s1 through sn are the\\ninterpretations of the parallel elements of the\\nsource, and s is the interpretation of the source itself. Only solutions\\nwhich do not contain a primary occurrence of the parallel elements are\\nconsidered (occurrences are primary if they arise directly from the parallel\\nelements, as opposed to those arising for instance from a pronoun).\\n\\n\\n In the example,\\n\\n\\n\\n is solved by equating P with\\n\\n\\n\\n x. likes(x,golf)\\n\\n\\n given that the other possible solution,\\n\\n\\n\\n x. likes(dan,golf) contains\\na primary occurrence of the parallel element, \\'\\'dan\", and must therefore\\nbe discarded.\\n\\n\\n d) applying the property on the representation of the target, e.g.\\n\\n\\nP(george)= [\\n\\n\\n\\n x.likes(x,golf)] george = likes(george,golf)\\n\\n\\n e) conjoining the meanings of the source and of the target thus\\ncompleted,\\ne.g.:\\n\\n\\n\\n  Both   and   provide ambiguous readings of discourses such as\\n\\n\\n\\n can be provided, unlike previous analyses,\\n without having to postulate ambiguity in the source (this is achieved in\\n  by allowing for priority union to either preserve or not preserve structure-sharing information in the source, and in\\n   by the distinction between primary and secondary occurrences\\nof parallel elements). Another notable point    in both\\nthese approaches is that they address the issue of semantic parallelism, which\\nin most previous approaches was understressed in favor of syntactic\\nparallelism.\\n\\n\\nHowever, both methods share the following limitations:\\n\\n\\n a) neither method formulates exactly how parallelism is to be\\ndetermined- it\\nis just postulated as a prerequisite to the resolution of ellipsis (although\\n  speculates on possible ways of formulating this, leaving it for future\\nwork)\\n\\n\\n b) both approaches stress semantic parallelism, while pointing out\\nthat\\nthis is not sufficient in all cases\\n\\n\\nBy examining ellipsis in the context of coordinated structures, which are\\nparallel by definition, and by using extended DLGs, we provide a method in\\nwhich parallel structures are detected and resolved through syntactic and\\nsemantic criteria, and which can be applied to either grammars using different\\nsemantic representations- feature structure, \\n\\n\\n\\n -calculus, or other. We\\n exemplify using a logic based semantics along the lines of . \\n\\n  Our semantico-syntactic treatment of parallelism \\n\\n Let us now consider the  string\\n\\n\\n John drove the car through and demolished a window\\n0    1     2   3   4       5    6         7 8      9\\n\\n\\n where we\\nhave indicated the connections as numbers in between the words.\\n\\n\\nWe  use the following grammar:\\n\\n\\n\\n T1 would contain the \\'D\\' connections for this sentence, and T2\\nadds:\\n\\n\\n{name(john,0,1), verb2(X,Y,Z, drove_through(X,Y,Z),1,2),\\n  det(Y,R,Sc,the(Y,R,Sc),2,3), noun(Y,car(Y),3,4),\\n  prep(through,4,5), conj(and,5,6), verb1(X1,Y1,\\n  demolished(X1,Y1),6,7), det(W,R1,Sc1,a(W,R1,Sc1),7,8),\\n  noun(V,window(V),8,9)}\\n\\n\\n At this point, a constraint to find points P and Q such that\\nCat(...,P,5)\\nis parallel to Cat(...,6,Q) is generated (upon whose finding, something\\nof the form\\n Cat(..., P,Q) will be added to the set of theorems ), and this constraint\\n suspends until the following new theorems have been derived:\\n\\n\\n\\n We can now postulate Cat= vp and use top-down prediction to derive\\na\\n(possibly incomplete) vp ending at point 5. When trying rule\\n\\n\\n\\n and after development of the pp, we can adapt the analysis of\\n  and identify: \\n\\n\\n\\n Now we can build an abstract NP by abstracting over the Scope\\nargument of the source, and postulating an empty surface string (i.e., by\\nequating the start\\nand end points of the string):\\n\\n\\n\\n We now unify the abstracted NP with the target NP to obtain the\\nresolved\\ntarget NP:\\n\\n\\n\\n which in turn completes the target vp:\\n\\n\\n\\n The constraint now reads:\\n\\n\\n\\n Now we\\nneed to conjoin the parallel structures. This is done by what we\\ncall c-unification: unify the\\nparts in the parallel terms which are unifiable, and conjoin  those that\\nare not(i.e., the parallel elements), with the exception of the last two\\narguments, which are generating from the two pairs of last arguments P1-P2 and\\n P2+1-P3 of the parallel structures, as P1 and P3 . We obtain:\\n\\n\\n\\nAfter this theorem\\'s addition, the sent rule can apply to derive\\n\\n\\n\\n\\n  Discussion \\n\\nThe previous section shows that when we introduce syntactic as well as\\nsemantic parallelism, this can help determine which are the parallel\\nstructures automatically, by incremental application of a Datalog grammar\\nconstraint on coordination coupled with top-down prediction to complete\\nany missing structures through an analysis of parallelism that is inspired in\\n that of  but complements it in various ways. Syntactic criteria on the determination of parallelism that can be found in the\\nliterature can also, of course, be added to complement this initial proposal.\\n\\n\\nSeveral observations are in order. In the first place, we must note that a\\nsimple conjoining of the representations obtained for the parallel\\n structures as proposed in  may not, as  the example of the previous section shows, suffice. Since these structures are quite dissimilar,\\nwe must conjoin only the parallel elements. We  postulate that the parallel\\nelements will be represented by those subterms which are not unifiable.\\n\\n\\nSecondly, our notion of   abstraction, which relies on converting into a\\nvariable those parts of a semantic representation which are contributed by the\\nconstituent that contains it, can be adapted to suit\\nother semantic representations, provided that we can identify for them\\nwhich part of the semantic representation each rule for a constituent\\ncontributes to the overall representation. This is not an unreasonable\\nexpectation for compositionally defined semantics.\\n\\n\\nIn the third place, we should note that our analysis allows for the source\\nclause to not necessarily be the first one- again as the example we just\\nexamined shows, we can have structures in which the incomplete substructure\\ndoes not  antecede  the complete one. Thus our analysis can handle more\\ncases than those in previous related work.\\n\\n\\nNote that some special cases allow to use unification\\nbetween isomorphical objects to obtain the proper quantification.\\nBy slightly modifying the grammar as\\n\\n\\n\\n we can handle directly phrases like:\\n\\n\\n\\n Clearly this\\nworks only for a class of particular constraints exhibiting strong isomorphism\\nin the constructed meaning.\\nFor instance, noun groups of the form np1 and np2 and np3do have this property.\\n\\n\\nWe must note, however, that in some cases we will need to complement our\\nanalysis with a further phase which we shall call \\'\\'reshaping\". Take for\\ninstance the sentence \\'\\'Each man and each woman ate an apple\". Since both\\nparallel structures are complete, we do not need to perform abstraction\\nand c-unification, but we do need to reshape the result of the analysis\\nthrough distribution, thus converting\\n\\n\\n\\n into\\n\\n\\n\\n Reshaping operations have been used in , and are useful in particular to decide on appropriate quantifier scopings where coordination is\\ninvolved. It would be interesting to study how to adapt these operations\\nto the present work.\\n\\n\\n Another interesting observation is that the results in  concerning the use of the distinction between primary and secondary occurrences\\nof parallel elements in order to provide ambiguous readings of discourses\\n such as\\n\\'\\'Jessie likes her brother. So does Hannah.\" could in principle be transferred\\ninto our approach as well.\\n\\n\\n Let us also note that, as observed in , the notion of compositional\\nsemantics of the two clauses (on which the related previous work, and ours\\nto some extent, is based) is not enough in some cases. For instance, consider:\\n\\n\\n\\nIn this sentence, the conclusion which holds if Fred drinks BUT SAM DOES NOT,\\ndoes not hold if both Fred and Sam drink. The implicit information that\\nthe first conclusion holds only if the premiss of the second sentence does not\\nhold must be inferred. Using our approach, we could use the re-shaping\\nphase to deal with cases such as this one, in which the presence of\\nwords such as \\'\\'too\" would trigger the generation of the full reading.\\nA sentence of the form\\n\\n\\n\\n would generate a representation such as\\n\\n\\n\\n which after reshaping would become:\\n\\n\\n\\nFinally, let us point out that, unlike most current efforts on programming\\n with constraints,   the constraints we propose in this paper\\ndo not limit themselves to pruning the search\\nspace, but actively contribute to finding a solution. In this sense they\\n relate more to database work such as  than to the literature in either constraint logic programming or constraint logic grammars.\\n\\n\\n  Acknowledgement \\n\\nThis research was supported by NSERC Research grants 31-611024 and OGP0107411,\\nand by NSERC, CSS and SFU PRG Infrastructure and Equipment grant given to the\\nLogic and Functional Programming Laboratory at SFU, in whose facilities\\npart of this work was developed. We are also grateful to the Centre for Systems\\nScience, LCCR and the School of Computing Sciences at Simon Fraser University\\nfor the use of their facilities. Paul Tarau also thanks for support from the\\nFESR of the Universit de Moncton.\\n\\nBibliography \\n\\nN. Asher.\\nReference to Abstract Objects in Discourse, volume 50 of   Studies in Linguistics and Philosophy.\\nKluwer, 1992.\\n\\n\\nJ. H. R. Calder.\\nAn Interpretation of Paradigmatic Morphology.\\nPhD thesis, University of Edinburgh, 1990.\\n\\n\\nV. Dahl and M. McCord.\\nTreating coordination in logic grammars.\\nAmerican Journal of Computational Linguistics, 9:69-91, 1983.\\n\\n\\nV. Dahl, P. Tarau, and Y. N. Huang.\\nDatalog grammars.\\nIn Proc. 1994 Joint Conference on Declarative Programming,\\n  Peniscola, Spain, September 1994.\\n\\n\\nM. Darlymple, S. Shieber, and F. Pereira.\\nEllipsis and higher-order unification.\\nLinguistics and Philosophy, 14(4):399-452, 1991.\\n\\n\\nC. Grover, C. Brew, S. Manandhar, and M. Moens.\\nPriority union and generalization in discourse grammars.\\nIn Proc. 32nd ACL Conference, New Mexico, 1994.\\n\\n\\nJ. Hodas.\\nSpecifying Filler-Gap Dependency Parsers in a Linear\\n  Logic Programming Language.\\nIn K. Apt, editor, Proc. 1992 Joint International Conference and\\n  Symposium on Logic Programming, pages 622-636. MIT Press, 1992.\\n\\n\\nG. Huet.\\nA unification algorithm for typed lambda-calculus.\\nTheoretical Computer Science, 1:27-57, 1975.\\n\\n\\nH. Prust.\\nOn Discourse Structuring, Verb Phrase Anaphora and Gapping.\\nPhD thesis, Universiteit van Amsterdam, 1992.\\n\\n\\nD. Srivastava and R. Ramakrishnan.\\nPushing Constraint Selections.\\nThe Journal of Logic Programming, 16:361-414, 1993.\\n\\n\\nP. Tarau.\\nBinProlog 3.30 User Guide.\\nTechnical Report 95-1, Dpartement d\\'Informatique, Universit\\n  de Moncton, Feb. 1995.\\nftp://clement.info.umoncton.ca/BinProlog.\\n\\n\\nW. Woods.\\nAn experimental parsing system for transition network grammars.\\nIn R. Rustin, editor, Natural Language Processing, pages\\n  145-149. Algorithmic Press, New York, 1973.\\n\\nFootnotes\\n\\n  This means, in case of Prolog\\'s\\nusual execution strategy that something like linear implication\\n  should be used instead of asserting to the dynamic database.\\nEven more appropriate for this purpose is BinProlog\\'s\\n linear assumption operator assumel/1 , which ensures that\\nthe assumed facts scope will range over the `continuation\\' i.e. it will\\nbe true in all future computations belonging the same\\nresolution branch.\\n  We shall\\ndescribe below the constraints that this addition must satisfy.\\n  Of course, for other\\ntypes of coordination we use the appropriate connective: \\'\\'but\", \\'\\'or\",...\\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\n In  we studied a new type of DCGs, Datalog grammars, which are inspired on database theory. Their efficiency\\nwas shown to be better than that of their DCG counterparts under\\n(terminating) OLDT-resolution.  In this article we motivate a variant of\\n Datalog grammars\\nwhich allows us a meta-grammatical treatment of coordination. This treatment\\nimproves in some respects over previous work on coordination in logic\\ngrammars, although more research is needed for testing it in other respects.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nThe experiments reported in the current article continue a line of\\nresearch  in the field of part-of-speech tagging using self-organizing\\nmodels that was presented at the previous (9th) Scandinavian Conference\\non Computational Linguistics. Then, the well-established HMM-based Xerox\\n tagger, see , was  compared with some less known taggers, namely a neural-network tagger described in\\n[], and a Bayesian tagger presented  in\\n . The Xerox tagger performs lexical generalizations by clustering words based  on their distributional patterns, while the\\nlatter two utilize the morphological information present in Swedish by\\ngeneralizing over word suffixes.\\n\\n\\nThis time, another HMM-based approach is compared with a novel\\nreductionistic statistical tagger inspired by the successful Constraint\\n Grammar  system,    []. \\n\\n\\nThe performed experiments do not only serve to evaluate the two taggers,\\nbut also shed some new light on the Teleman corpus as an evaluation\\ndomain for part-of-speech taggers compared to other, English, corpora.\\n\\n\\n The paper is organized as follows: Section  discusses the  Teleman corpus and the tagsets used. Section  describes the  HMM-based tagger  and Section  the reductionistic statistical one. The vital issue of handling sparse data is addressed in\\n Section  and the experimental results are presented in  Section . \\n\\n\\n    The Teleman Corpus\\n\\n\\n The Teleman corpus  is a corpus of contemporary Swedish, representing a mixture of different text genres like\\ninformation brochures on military service and medical care, novels, etc.\\nIt comprises 85,408 words (tokens; here, words is a collective\\ndenotation of proper words, numbers, and punctuation). There are 14,191\\ndifferent words (types); the most frequent one is ``.'', which occurs\\n4,662 times; the most frequent proper word is ``och'' (and), which occurs 2,217 times. 8,458 of the words occur exactly\\nonce, which is 60% of the types but only 10% of the tokens.\\n\\n\\nFor the experiments, we used two different tagsets. First, we used the\\noriginal tagset, consisting of 258 tags. Each of the 14,191 word types\\ncan have between one and 15 of the 258 tags (the highly ambiguous word\\n``fr'' (for, stern, lead, too, ...) has the maximum\\nnumber of tags). We then used a reduced tagset, consisting of 19 tags,\\nwhich represent  common syntactic categories and punctuation. This\\ntagset is identical  to that used in the publications mentioned above.\\nEach of the word types then has between one and 7 tags (``fr'' and ``i'' have the maximum number of tags).\\n\\n  Comparison with an English Corpus \\n\\nSince 10% of the words in the Teleman corpus occur only once, we expect\\n from the Good-Turing formula  that 10% of the words in new text be unknown, which is a very high percentage. Other publications\\ntypically report 5%. Since most of the work in this area is on English\\ncorpora, we compared the Teleman corpus with an English corpus, namely\\n the Susanne corpus , which is a re-annotated part of the  Brown Corpus , comprising different text genres. The  relevant facts are summarized and compared in Table . The major difference (apart from corpus size and tagsets used) is the\\npercentage of words that occur exactly once: 10% for Teleman vs. 4%\\nfor Susanne. According to the Good-Turing formula, this percentage is\\nidentical to the expected percentage of unknown words. Actual counts by\\ndividing the corpora into training and test parts yield around 14% and\\n7%, respective. This indicates that unseen Swedish text will have\\nsubstantially more unknown words than unseen English, which is most\\nlikely due to the higher degree of morphological variation in Swedish.\\n\\n\\nA further difficulty with the Swedish corpus is the higher degree of\\nambiguity. In the Teleman corpus, each word in the running text has  in\\naverage 2.38 tags for the small tagset, and 3.69 for the large tagset.\\nThese numbers are 2.07 and 2.61 for the Susanne corpus, despite the fact\\n that the tagsets for the Susanne corpus are larger than those for the\\nTeleman corpus.  Thus, there is much more work for the tagger to do in\\nthe Teleman corpus. Some more numbers: in the running text,\\n54.5%/64.2% of the words in the Teleman corpus are ambiguous, and only\\n44.3%/48.9% in the Susanne corpus (small/large tagset, resp.; see\\n Table  for further details). \\n\\n\\n\\n\\n\\n\\nTags in the Susanne corpus with\\nindices are counted as separate tags.\\n\\n\\n\\n\\n\\n\\nUnknown words are words that\\noccur only in the test set, but not in the training set.\\n\\n\\n\\n\\n\\n\\nThe remaining 9,823 words of the\\nSusanne corpus were not used in the experiments.\\n\\n\\n\\n    The HMM Approach\\n\\n\\nA Hidden Markov Model (HMM) consists of a set of states, a set of output\\nsymbols and a set of transitions. For each state and each symbol, the\\nprobability that this symbol is emitted by that state is given. Also, a\\nprobability is associated with each transition between states (see\\n  for a good introduction). The transition probability, and thus the probability of the following state, depends only on the\\nprevious state for first order HMMs, or on k previous states for HMMs\\nof kth order. HMM approaches to part-of-speech tagging make the\\nwell-known assumption that the current category or part-of-speech of a\\nword depends only on the previous (n-1) categories (Markov\\nassumption), thus they assume that natural language is a Markov process\\nof order (n-1), which of course is not true, but a successful\\napproximation. n = 3 is chosen in most of the cases, resulting in a\\ntrigram model (i.e., always working with a window of size 3), since it\\nyields the best compromise between size of corpora needed for training\\nand tagging accuracy. Furthermore, the current word (symbol) depends\\nonly on the current category (state). Thus, instead of calculating and\\nmaximizing \\n\\n\\n\\n\\n ,\\nwith Ti tags and\\nWi words, which is impossible in all practical cases, one calculates\\nand maximizes\\n\\n\\n\\n\\n\\nto find the best sequence of tags for a given sequence of words.\\n\\n\\nThe parameters of an HMM can be estimated directly from a pretagged\\ncorpus via maximum-likelihood estimation (MLE). But MLE sets a lot of\\nthe transition probabilities to zero, and if one of the multiplied\\n probabilities in () is zero, the product becomes zero, leaving no means to distinguish between different products that contain\\na zero probability. This results in poor estimates for the probabilities\\nof new sequences of words. This problem is addressed in Section\\n . \\n\\n\\nAnother way of estimating the parameters of an HMM is to use an untagged\\ncorpus, a lexicon with parts-of-speech lists for the words and the\\n Baum-Welch algorithm . This approach has the advantage of avoiding the tedious work of manually annotating a corpus, but it\\nrequires a sophisticated choice of initial biases, and generally, the\\nperformance is worse than that achieved with annotated corpora.\\n\\n\\nWhen using an HMM for tagging, the system gets a string of words and has\\nto find the most probable sequence of tags that could have produced\\nthe string of words. This is done with a dynamic programming method, the\\n Viterbi algorithm . The algorithm finds the most probable sequence of states in time linear\\nin the length of the input string.\\n\\n\\n    The Reductionistic Statistical Approach\\n\\n\\nAlthough not yet fully realized, the basic philosophy behind the\\nreductionistic statistical approach is to give it the same expressive\\npower as the Constraint Grammar system.\\n\\n    Constraint Grammar\\n\\n\\nThe Constraint Grammar system performs remarkably well;\\n[Voutilainen  Heikkil 1994] report 99.7% recall, or 0.3%\\nerror rate,  which is ten times smaller than that of the best\\nstatistical taggers. These impressive results are achieved by:\\n1.\\nUtilizing a number of different information\\nsources, and not only the stereotyped lexical statistics and n-gram tag\\nstatistics that have become the de facto standard in statistical\\npart-of-speech tagging. 2.\\n  Not fully resolving all  ambiguities when this would jeopardize the recall. Property     means that the system trades precision for recall, which makes it ideal as a preprocessor for natural language systems\\nperforming deeper analysis.\\n\\n\\nThe Constraint Grammar system works as follows:\\nFirst, the input string is assigned all possible tags from the lexicon, or\\nrather, from the morphological analyzer.\\nThen, tags are removed iteratively by repeatedly applying a set of rules,\\nor constraints, to the tagged string.\\nWhen no more tags are removed by the last iteration, the process terminates,\\nand morphological disambiguation is concluded.\\nThen a set of syntactic tags are assigned to the tagged input\\nstring and a similar process is performed for syntactic disambiguation.\\nThis method is often referred to as reductionistic tagging.\\n\\n\\nThe rules are sort-of formulated as finite state automata [Tapanainen,\\npersonal communication], which allows very fast processing.\\n\\n\\nEach rule applies to a current word with a set of candidate tags.\\nThe structure of a rule is typically:\\n``In the following context, discard the following tags.''\\nor\\n``In the following context, commit to the following tag.''\\nWe will call discarding or committing to tags the rule action.\\nA typical rule context is:\\n``There is a word to the left that is unambiguously tagged with the\\nfollowing tag, and there are no intervening words tagged with such\\nand such tags.''\\n\\n\\n    The New Approach\\n\\n\\nThe structure of the Constraint Grammar rules readily allows their contexts\\nto be viewed as the conditionings of conditional probabilities,\\nand the actions have an obvious interpretation as the corresponding\\nprobabilities.\\n\\n\\nEach context type can be seen as a separate information source, and we will\\ncombine information sources \\n\\n\\n\\n\\nby multiplying the scaled\\nprobabilities:\\n\\n\\n\\nThis formula can be established by Bayesian inversion, then performing the\\nindependence assumptions, and renewed  Bayesian inversion:\\n\\n\\n\\nIn standard statistical part-of-speech tagging there are only two\\ninformation sources -- the lexical probabilities and the tags assigned\\nto neighbouring words. We thus have:\\n\\n\\n\\nThe context will in general not be fully disambiguated. Rather than\\nemploying dynamic programming over the lattice of remaining candidate\\ntags, the new approach uses the weighted average over the remaining\\ncandidate tags  to estimate the probabilities:\\n\\n\\n\\nIt is assumed that \\n\\n\\n\\n\\nconstitutes a partition of\\nthe context C, i.e., that \\n\\n\\n\\n\\nand that \\n\\n\\n\\n\\nfor \\n\\n\\n\\n .\\nIn particular, trigram probabilities are\\ncombined as follows:\\n\\n\\n\\nHere T denotes a candidate tag of the current word, Tl denotes a\\ncandidate tag of the immediate left neighbour, and Tr denotes a\\ncandidate tag of the  immediate right neighbour. C is the set of\\nordered pairs (Tl,Tr) drawn from the set  of candidate tags of the\\nimmediate neighbours. \\n\\n\\n\\n\\nis the symmetric trigram\\nprobability.\\n\\n\\nThe tagger is reductionistic since it repeatedly removes low-probability\\ncandidate tags.  The probabilities are then recalculated, and the\\nprocess terminates when  the probabilities have stabilized and no more\\ntags can be removed without  jeopardizing the recall; candidate tags are\\nonly removed if their probabilities are below some threshold value.\\n\\n\\n\\n    Sparse Data\\n\\n\\nHandling sparse data consists of two different tasks:\\n1.\\nEstimating the probabilities of events that do not occur in the\\ntraining data.\\n2.\\nImproving the estimates of conditional probabilities where the\\n        number of observations under this conditioning is small.\\nCoping with unknown words, i.e., words not encountered in the training\\nset, is an archetypical example of the former task. Estimating\\nprobability distributions conditional on small contexts is an example of\\nthe latter task. We will examine several approaches to these tasks.\\n\\n\\nFor the HMM, it is necessary to avoid zero probabilities. The most\\nstraight-forward strategy is employing the expected-likelihood estimate\\n(ELE), which simply adds 0.5 to each frequency count and then constructs\\n a maximum-likelihood estimate (MLE), (see e.g. ). The MLE of the probability is the relative frequency r. Another possibility is\\n the Good-Turing method , where each frequency f is replaced by \\n\\n\\n\\n\\n ,\\nwhere Nf denotes the\\nfrequency of frequency f.  Alternatively, one can use linear\\ninterpolation of the probabilities obtained by MLE,\\n\\n\\n\\n\\n[] let the  \\n\\n\\n\\nvalues\\ndependent on the context, which improves the tagging accuracy. This is\\nrelated to the idea of successive abstraction presented in\\n Section . To achieve improved estimates of lexical  probabilities, words can be clustered together, see []. \\n\\n\\nThere are several ways to handle unknown words. These include:\\n1.\\nMaking every tag a possible tag for that word with equal probability\\nand finding the most probable tag solely based on context\\n        probabilities. The results can be slightly improved by trying only\\n        open-class tags for unknown words.\\n2.\\nAs an extension to case 1, choosing different but again constant\\n        probabilities for each possible tag. This constitutes an a priori\\n        distribution for unknown words, reflecting for example that most of\\n        the unknown words are nouns. The probabilities could be obtained\\n        from a separate training part, or from the distribution of words\\n        that occur only once in the training corpus. These words reflect\\n        the distribution of unknown\\n         words according to the formula presented in . 3.\\n Exploiting word-form information as proposed in .         Here, the probability distributions are determined from the last\\n        n characters of the word, and the remaining number of syllables.\\n        This method has been proven successful for Swedish text.\\n4.\\nUtilizing orthographical cues such as capitalization.\\n\\n    Successive Abstraction\\n\\n\\nAssume that we want to estimate the probability \\n\\n\\n\\n\\nof the\\nevent  E given a context C from the number of times NE it occurs\\nin N = |C| trials, but that this data is sparse. Assume further that\\nthere is abundant  data in a more general context \\n\\n\\n\\n\\nthat we\\nwant to use to  get a better estimate of \\n\\n\\n\\n\\n .\\n\\n\\nIf there is an obvious linear order  \\n\\n\\n\\n\\nof the various  generalizations Ck of C,\\nwe can build the estimates of \\n\\n\\n\\n\\non the relative frequency\\n\\n\\n\\n\\nof event E in context Ck and the previous estimates\\nof \\n\\n\\n\\n\\n .\\nWe call this method linear successive\\nabstraction. A simple example is estimating the probability  \\n\\n\\n\\n\\nof a tag T given \\n\\n\\n\\n\\n ,\\nthe\\nlast j+1 letters of the word. In this case, the estimate will be based\\non the relative frequencies  \\n\\n\\n\\n\\n Previous experiments  indicate that the following is a suitable formula:\\n\\\\hat{P}(E \\\\mid C) = \\\\frac{\\\\sqrt{N} \\\\: r(E \\\\mid C) + \\\\hat{P}(E \\\\mid\\nC')}{\\\\sqrt{N} + 1}\\n\\\\end{eqnarray} -->\\nThis formula simply up-weights the relative frequency r by a factor\\n\\n\\n\\n ,\\nthe square root of the size of context C, which is the\\nactive ingredient of the standard deviation of r.\\n\\n\\nIf there is only a partial order of the various generalizations, the\\nscheme is  still viable. For example, consider generalizing symmetric\\ntrigram statistics, i.e., statistics  of the form \\n\\n\\n\\n\\n .\\nHere, both Tl and Tr are one-step generalizations of the context\\nTl,Tr, and both have in turn the common  generalization \\n\\n\\n\\n .\\nWe\\n modify Equation  accordingly:\\n\\n\\n\\nand\\n\\n\\n\\nWe call this partial successive abstraction.\\n\\n\\n\\n    Experiments\\n\\n\\nFor the experiments, both corpora were divided into three sets, one\\nlarge set and two small sets. We used three different divisions into\\ntraining and testing sets. First, all three sets were used for both\\ntraining and testing. In the second and third case, training and test\\nsets were disjoint, the large set and one of the small sets were used\\nfor training, the remaining small set was used for testing. As a\\nbaseline to indicate what is gained by taking the context into account,\\nwe performed an additional set of experiments that used lexical\\nprobabilities only, and ignored the context.\\n\\n  HMM Approach \\n\\nThe experiments of this section were performed with a trigram tagger as\\n described in Section . Zero frequencies were avoided by using expected-likelihood estimation. Unknown words were handled by a\\n mixture of methods 2 and 3 listed in Section : If the suffix of 4 characters (3 characters for the Susanne corpus) of the unknown\\nwords was found in the lexicon, the tag distribution for that suffix was\\nused. Otherwise we used the distribution of tags for words that occurred\\nonly once in the training corpus.\\n\\n\\nAs opposed to trigram tagging, lexical tagging ignores context\\nprobabilities and is based solely on lexical probabilities. Each word is\\nassigned its most frequent tag from the training corpus. Unknown words\\nwere assigned the most frequent tag of words that occurred exactly once\\nin the training corpus. The most frequent tags for single occurrence\\nwords are for the Teleman corpus NNSS (indefinite noun-noun\\ncompound) and noun (large and small tagset, resp.), for the\\nSusanne corpus NN2 (plural common noun) and NN (common noun;\\nagain large and small tagset resp.).\\n\\n\\nTagging speed was generally between 1000 and 2000 words per second on a\\nSparcServer 1000; most of this variation was due to variations in the\\nnumber of unknown words.\\n\\n\\nThe results for the Teleman corpus are shown in\\n Table  and the results for the Susanne  corpus in Table . \\n\\n\\nWhat immediately attracts attention is the remarkably low performance of\\nthe trigram approach for the Teleman corpus. Already the baseline\\nobtained by lexical tagging is below 80% for new text, usual results\\nare around 90%. Normal results can be obtained only for known words or\\nwhen using the small tagset, the latter being in fact a very simple\\ntask,  since the algorithm has to choose from only 19 tags.  For the\\nlarge tagset, trigram tagging achieves only 83% accuracy.  This low\\nfigure is due to the unusually high number of unknown words and  the\\nlarger degree of ambiguity compared to English corpora, as is discussed\\n in Section . Using a large Swedish lexicon or morphological  analyzer should improve the results significantly.\\n\\n\\nAnother interesting result is that accuracy increases when the size of\\nthe tagset increases for the cases where known text is tagged and\\ncontext probabilities are taken into account. This means that the\\nadditional information about the context in the larger tagset is very\\nhelpful for disambiguation, but only when disambiguating known text.\\nThis could arise from the fact that a large number (\\n\\n\\n\\n )\\nof the\\ntrigrams that occur in the training text occur exactly once. And most of\\nthe possible trigrams do not occur at all (generally more than 90%,\\ndepending on the size of the tagset). Now, the trigram approach has a\\ndistinct bias to those trigrams that occurred once over those that never\\noccurred. These happen to be the right ones for known text but not\\nnecessarily for new text, thus the positive effect of a larger tagset\\nvanishes for fresh text.\\n\\n\\nThe results for the Susanne corpus are similar to those reported in\\nother publications for (other) English corpora.\\n\\n\\n  Reductionistic Approach \\n\\n The reductionistic statistical tagger described in Section  was tested on the same data as the HMM tagger.  The information sources\\nemployed in the experiments were lexical statistics and contextual\\ninformation, which consisted of symmetric trigram statistics. Unknown\\nwords were handled by creating a decision tree of the four last letters\\nfrom words with three or less occurrences.  Each node in the tree was\\nassociated with a probability distribution (over the  tagset) extracted\\nfrom these words, and the probabilities were smoothened through linear\\n successive abstraction, see Section . \\n\\n\\nThere were two cut-off values for contexts: Firstly, any context with\\nless than 10 observations was discarded. Secondly, any context where the\\nprobability distributions did not differ substantially from the\\nunconditional one was also discarded. Only the remaining ones were used\\nfor disambiguation.  Due to the computational model employed, omitted\\ncontexts are equivalent to backing off to whatever the current\\nprobability distribution is. The distributions conditional on contexts\\nare however susceptible to the problem of sparse data. This was handled\\nusing partial successive abstraction as described in\\n Section . \\n\\n\\n The results are shown in Tables   and . They clearly indicate that: \\n\\nThe employed treatment of unknown words is quite effective.\\n\\nUsing contextual information, i.e., trigrams, improves tagging\\n        accuracy.\\n\\nThe performance is on pair with the HMM tagger and comparable to\\n        state-of-the-art statistical part-of-speech taggers.\\n\\nTeleman is a considerably tougher nut to crack than Susanne.\\n\\n\\nThe results using the Susanne corpus are similar to those reported for\\n the  Lancaster-Oslo-Bergen (LOB) corpus in , where a statistical n-best-path approach was employed to trade precision for\\nrecall.\\n\\n\\nThe tagging speed was typically a couple of hundred words per second on\\na SparcServer 1000, but varied with the size of the tagset and the\\namount of remaining ambiguity.\\n\\n\\n\\n    Conclusions\\n\\n\\nThe experiments with the HMM approach show that it is much harder to\\nprocess the Swedish than the English corpus. Although the two corpora\\nare not fully comparable because of the differences in size and tagsets\\nused, they reveal a strong tendency. The difficulty in processing is\\nmostly due to the rather large number of unknown words in the Swedish\\ncorpus and the higher degree of ambiguity despite having smaller\\ntagsets. These effects mainly arise from the higher morphological\\nvariation of Swedish which calls for additional strategies to be\\napplied. These could be the use of a large corpus-independent lexicon\\nand a separate morphological analysis.\\n\\n\\nIt is reassuring to see that the reductionistic tagger performs as well\\nas the HMM tagger, indicating that the new framework is as powerful as\\nthe conventional one when using strictly conventional information\\nsources. The new framework also enables using the same sort of\\ninformation as the highly successful Constraint Grammar approach, and\\nthe hope is that the addition of further information sources can advance\\nstate-of-the-art performance of statistical taggers.\\n\\n\\nViewed as an extension of the Constraint Grammar approach, the new\\nscheme allows making decisions on the basis of not fully disambiguated\\nportions of the input string. The absolute value of the probability of\\neach tag can be used as a quantitative measure of when to remove a\\nparticular candidate tag and when to leave in the ambiguity.  This\\nprovides a tool to control the tradeoff between recall (accuracy) and\\nprecision (remaining ambiguity).\\n\\n\\n  Acknowledgements \\n\\nWe wish to thank Bjrn Gambck for providing information on previous\\nwork with the Teleman corpus.\\n\\nBibliography \\n\\nL. E. Baum.\\n``An inequality and associated maximization technique in statistical\\n  estimation for probabilistic functions of Markov processes'',\\nInequalities III, pp. 1-8, 1972.\\n\\n\\nP. F. Brown, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer and\\n  P. S. Roossin.\\n``Class-based n-gram models of natural language'',\\nComputational Linguistics 18(4) pp. 467-479, 1992.\\n\\n\\nDouglass Cutting.\\n``A Practical Part-of-Speech Tagger'',\\nin Procs. 9th Scandinavian Conference on Computational Linguistics,\\npp. 65-70, Stockholm University, 1994.\\n\\n\\nDouglass R. Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun.\\n``A Practical Part-of-Speech Tagger''.\\nin Procs. 3rd Conference on Applied Natural Language Processing,\\npp. 133-140, ACL, 1992.\\n\\n\\nMartin Eineborg and Bjrn Gambck.\\n``Tagging Experiments Using Neural Networks'',\\nin Procs. 9th Scandinavian Conference on Computational Linguistics,\\npp. 71-82, Stockholm University, 1994.\\n\\n\\nN. W. Francis and H. Kucera.\\nFrequency Analysis of English Usage,\\nHoughton Mifflin, Boston, 1982.\\n\\n\\nW. A. Gale and K. W. Church.\\n``Poor Estimates of Context are Worse than None'',\\nin Proc. of the Speech and Natural Language Workshop,\\npp. 283-287, Morgan Kaufmann, 1990.\\n\\n\\nI. J. Good.\\n``The population frequencies of species and the estimation of population\\nparameters'',\\nBiometrika 40, pp. 237-264, 1953.\\n\\n\\nFred Karlsson, Atro Voutilainen, Juha Heikkil and Arto Anttila (eds).\\nConstraint Grammar. A Language-Independent System for Parsing\\nUnrestricted Text,\\nMouton de Gruyter, Berlin / New York, 1995.\\n\\n\\nCarl G. de Marcken.\\n``Parsing the LOB Corpus'',\\nin Procs. 28th Annual Meeting of the Association for Computational\\nLinguistics, pp. 243-251, ACL 1990.\\n\\n\\nL. R. Rabiner.\\n``A tutorial on hidden Markov models and selected applications in speech\\nrecognition'',\\nin Proceedings of the IEEE 77(2), pp. 257-285, 1989.\\n\\n\\nGeoffrey Sampson.\\nEnglish for the Computer,\\nOxford University Press, Oxford, 1995.\\n\\n\\nChrister Samuelsson.\\n``Morphological Tagging Based Entirely on Bayesian Inference'',\\nin Procs. 9th Scandinavian Conference on Computational Linguistics,\\npp. 225-238, Stockholm University, 1994.\\n\\n\\nUlf Teleman.\\nManual fr grammatisk beskrivning av talad och skriven svenska,\\n(in Swedish), Studentlitteratur, Lund, Sweden 1974.\\n\\n\\nA. Viterbi.\\n``Error bounds for convolutional codes and an asymptotically optimum\\ndecoding algorithm'',\\nin IEEE Transactions on Information Theory, pp. 260-269, 1967.\\n\\n\\nAtro Voutilainen and Juha Heikkil.\\n``An English constraint grammar (ENGCG): a surface-syntactic parser of\\nEnglish'',\\nin Procs. 14th International Conference on English Language\\nResearch on Computerized Corpora, pp. 189-199, Zrich, 1994.\\n\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nExperiments were carried out comparing the Swedish Teleman and the\\nEnglish Susanne corpora using an HMM-based and a novel reductionistic\\nstatistical part-of-speech tagger. They indicate that tagging the\\nTeleman corpus is the more difficult task, and that the performance of\\nthe two different taggers is comparable.\\n\\n'],\n",
              " ['\\n\\n  Introduction\\n\\n\\nOne of the more significant developments in generative linguistics over\\nthe last decade has been the development of constraint-based formalisms--grammar formalisms that define languages\\nnot in terms of the derivations of the strings in the language, but\\nrather in terms \\nof well-formedness conditions on the structures analyzing their syntax.\\nBecause traditional notions \\nof language complexity are generally defined in terms of rewriting mechanisms,\\ncomplexity of the languages licensed by these formalisms can be\\ndifficult to determine.\\n\\n\\nA particular example, one that will be a focus of this paper, is\\nGovernment and Binding Theory.  While this is often modeled as a\\nspecific range of Transformational Grammars, the connection between\\nthe underlying grammar mechanism and the language a given GB theory\\nlicenses is quite weak.  In an extreme view, one can take the underlying\\nmechanism simply to generate the set of all finite trees (labeled with\\n some alphabet of symbols)  while the grammatical theory is actually \\nembodied in a set of principles that filter out the ill-formed analyses.\\nAs a result, it has been difficult to establish language\\ncomplexity results for GB theories, even at the level of the\\n recursive ,  or context-sensitive  languages. \\n\\n\\nThat language complexity results for GB should be difficult to come by\\nis hardly surprising.  The development of GB coincided with the\\nabandonment, by GB theorists, of the presumption that the traditional\\nlanguage complexity classes would provide any useful characterization of\\nthe human languages.  This followed, at least in part, from the recognition of\\nthe fact that\\nthe structural properties that characterize natural languages as a class may\\nwell not be those that can be distinguished by existing language complexity\\nclasses. \\nThere was a realization that the theory needed to be driven\\nby the regularities identifiable in natural languages, rather than\\nthose suggested by abstract mechanisms.  Berwick characterized this\\napproach as aiming to ``discover the properties of natural languages\\n first, and then characterize them formally.\\'\\' , pg. 100] \\n\\n\\nBut formal language theory still has much to offer to generative\\nlinguistics.  Language complexity provides one of the most useful\\nmeasures with which to compare languages and language formalisms. \\nWe have an array of results establishing the\\nboundaries of these classes, and, while many of the results do not seem\\nimmediately germane to natural languages,\\neven seemingly artificial diagnostics\\n(like the copy language \\n\\n\\n\\n\\n )\\ncan provide the basis\\nfor useful classification results (such as Shieber\\'s argument for the\\n non-context-freeness of Swiss-German ). More importantly, characterization results for language\\ncomplexity classes tend to be in terms of the structure\\nof languages, and the structure of natural language, while hazy, is\\nsomething that can be studied more or less directly.  Thus there is a\\nrealistic expectation of finding empirical evidence falsifying a given\\nhypothesis.  (Although such evidence may well be difficult to find, as\\nwitnessed by the history of less successful attempts to establish results such\\n as Shieber\\'s ,.) Further,\\nlanguage complexity classes characterize, along one dimension, the\\ntypes\\nof resources necessary to parse or recognize a language.  Results of\\nthis type for the class of human languages, then, make specific\\npredictions about the nature of the human language faculty, predictions\\nthat, at least in principle, can both inform and be informed by progress\\nin uncovering the physical nature of that faculty.\\n\\n\\nIn this paper we discuss a flexible and quite powerful approach to\\nestablishing language complexity results for formalisms based on systems\\n of constraints on trees.  In Section  we introduce a logical language, \\n\\n\\n\\n ,\\ncapable\\nof encoding such constraints lucidly.  The key merit of such an encoding\\nis the fact that sets of trees are definable in \\n\\n\\n\\nif and only if\\nthey are strongly context-free. Thus definability in \\n\\n\\n\\n characterizes the strongly context-free languages.  This is our primary\\n result, and we develop it in Section . \\n\\n\\nWe have used this technique to establish both inclusion and exclusion\\nresults for a variety of linguistic principles within the GB\\n framework .  In the remainder of the paper we demonstrate some of these.  In\\n Section  we sketch a proof of the non-definability of free-indexation, a mechanism that is nearly ubiquitous in GB theories. \\nThe consequence of this result is that languages that are licensed by\\ntheories that necessarily employ free-indexation are outside of the\\nclass of CFLs.  Despite the unavailability of free-indexation, we are\\nable to capture a mostly standard GB account of English within \\n\\n\\n\\n .\\nThus we are able to show that the language licensed by this particular\\n GB theory is strongly context-free. In Section  we illustrate some of the issues \\ninvolved in establishing this result, particularly in light of the\\nnon-definability of free-indexation.  We\\nclose, finally, with some\\nspeculation on the possible significance of these results for generative\\nlinguistics.\\n\\n\\n\\n\\nL[2]K,P\\n\\n\\nThe idea of employing mathematical logic to provide a precise formalization of\\nGB theories is a natural one.  This has been done, for instance, by\\n Johnson    and Stabler  using first-order logic (or the Horn-clause  fragment of first-order logic) and by Kracht  using a fragment of dynamic logic.  What distinguishes the formalization we discuss\\nis the fact that it is\\ncarried out in a language which can only define strongly context-free sets.\\nThe fact that the formalization is possible, then, establishes a relatively \\nstrong language complexity result for the theory we capture.\\n\\n\\nWe have, then, two conflicting criteria for our language.  It must be\\nexpressive enough to capture the relationships that define the trees licensed\\nby the theory, but it must be restricted sufficiently to be no more\\nexpressive than Context-Free Grammars.\\nIn keeping with the first of these our language is intended to support, as\\ntransparently as possible, \\nthe kinds of reasoning about trees typical of linguistic applications.\\nIt includes binary predicates for  the usual structural relationships between\\nthe nodes in the trees--parent (immediate domination), \\ndomination (reflexive), proper domination (irreflexive), left-of (linear\\nprecedence) and equality.  In addition, it includes an arbitrary array\\nof monadic predicate constants--constants naming specific subsets of\\nthe nodes in the tree.  These can be thought of as atomic labels.\\nThe formula \\n\\n\\n\\n ,\\nfor instance, is true at\\nevery node labeled N.  It includes, also, a similar array of\\nindividual constants--constants naming specific individuals in the\\ntree--although these prove to be of limited usefulness.  There are two\\nsorts of variables as well--those that range over nodes in the tree and those\\nthat range over arbitrary subsets of those nodes (thus this is is monadic\\nsecond-order language).  Crucially, though, this is all the language includes.\\nBy restricting ourselves to this language we restrict ourselves to working with\\nproperties that can be expressed in terms of these basic predicates.\\n\\n\\nTo be precise, the actual language we use in a given situation depends on the\\nsets of constants in use in that context.  We are concerned then with a\\nfamily of languages, parameterized by the sets of individual and set\\nconstants they employ.\\n\\n\\n\\nWe use infix notation for the fixed predicate constants\\n\\n\\n\\n ,\\n\\n\\n\\n ,\\n\\n\\n\\n ,\\n\\n\\n\\n ,\\nand \\n\\n\\n\\n .\\nWe use lower-case for\\nindividual variables and constants, and upper-case for set variables and\\npredicate constants.  Further, we will say X(x) to assert that the individual\\nassigned to the variable x is included in the set assigned to the\\nvariable X.  So, for instance,\\n\\n\\n\\nasserts that the set assigned to X includes every node dominated by the node\\nassigned to x.\\n\\n\\nTruth, for these languages, is defined relative to a specific class of models.\\nThe basic models are just ordinary structures interpreting the individual\\nand predicate constants.\\n\\n\\n\\nIf the domain of \\n\\n\\n\\nis empty (i.e., the model is for a language\\n\\n\\n\\n\\n )\\nwe will generally omit it.  Models for\\n\\n\\n\\n\\n ,\\nthen, are tuples \\n\\n\\n\\n\\n .\\n\\n\\nThe intended class of these models are, in essence, labeled tree domains.\\nA tree domain is the set of node addresses generated by giving\\nthe address \\n\\n\\n\\nto the root and giving the children of the node\\nat address w addresses (in order, left to right)\\n\\n\\n\\n\\n ,\\nwhere the centered dot denotes\\n concatenation. Tree domains, then, are particular subsets of \\n\\n\\n\\n .\\n(\\n\\n\\n\\nis the\\nset of natural numbers.)\\n\\n\\n\\n>\\n\\n\\nEvery tree domain has a natural interpretation as a model for\\n\\n\\n\\n\\n(which interprets only the fixed predicate symbols.)\\n\\n\\n\\n>\\n\\n\\nThe structures of interest to us are just those models that are the natural\\ninterpretation of a tree domain, augmented with interpretations of\\n additional individual and predicate constants. \\n\\n\\nIn general, satisfaction is relative to an assignment mapping each\\nindividual variable into a member of \\n\\n\\n\\nand each predicate variable\\ninto a subset of \\n\\n\\n\\n .\\nWe use\\n\\n\\n\\nto denote that a model M satisfies a formula \\n\\n\\n\\nwith an\\nassignment s.  The notation\\n\\n\\n\\nasserts that M models \\n\\n\\n\\nwith any assignment.  When \\n\\n\\n\\nis a\\nsentence (has no unquantified variables) we will usually use this form.\\n\\n\\nProper domination is a defined predicate:\\n\\n\\n  Definability in L[2]K,P\\n\\n\\nWe are interested in the subsets of the class of intended\\nmodels which are definable\\nin \\n\\n\\n\\nusing any sets K and P.  \\nIf  \\n\\n\\n\\nis a set of\\nsentences in a language \\n\\n\\n\\n ,\\nwe will use the notation \\n\\n\\n\\n\\nto denote\\nthe set of trees, i.e., \\nintended models, that satisfy all of the sentences in \\n\\n\\n\\n .\\nWe are\\ninterested, then, in the sets of trees that are \\n\\n\\n\\n\\nfor some such\\n\\n\\n\\n .\\nIn developing our definitions we can use individual and monadic\\npredicates freely (since K and P can always be taken to\\nbe the sets that actually occur in our definitions) and we can quantify over\\nindividuals and sets of individuals.  We will also use\\nnon-monadic predicates and even higher-order predicates,\\ne.g., properties of subsets, but only those that can be explicitly defined, that is, those which can be eliminated by a simple\\nsyntactic replacement of the predicate by its definition.\\n\\n\\nThis use of explicitly defined predicates is crucial to the transparency of\\ndefinitions in \\n\\n\\n\\n .\\nWe might, for instance, define a simplified version of\\ngovernment in three steps:\\n\\n\\n\\nin words, x governs y iff it c-commands y and no barrier\\nintervenes between them.  It \\nc-commands y iff neither x nor y dominates the other and every\\nbranching node that properly dominates x also properly dominates y.\\n\\n\\n\\n\\nis just a monadic predicate; it is within the language of\\n\\n\\n\\n(for suitable P) and its definition is simply a biconditional \\n\\n\\n\\n formula. \\nIn contrast, C-Command and Governs are non-monadic and do\\nnot occur in \\n\\n\\n\\n .\\nTheir definitions, however, are ultimately in\\nterms of monadic predicates and the fixed predicates (parent,\\netc.) only. One can replace each of their occurrences in a formula\\nwith the right hand side of their definitions and eventually derive a\\nformula that is in \\n\\n\\n\\n .\\nWe will reserve the use of \\n\\n\\n\\n(in\\ncontrast to \\n\\n\\n\\n )\\nfor explicit definitions of non-monadic predicates.\\n\\n\\nDefinitions can also use predicates expressing properties of sets and relations\\nbetween sets, as long as those properties can be explicitly defined.  The \\nsubset relation, for instance can be defined:\\n\\n\\n\\nWe can also\\ncapture the stronger notion of one set being partitioned by a collection\\nof others:\\n\\n\\n\\nHere \\n\\n\\n\\nis a some sequence of set variables and \\n\\n\\n\\n\\nis shorthand for the disjunction \\n\\n\\n\\n\\nfor all Xi in \\n\\n\\n\\n ,\\netc.  There is a distinct\\ninstance of \\n\\n\\n\\n\\nfor each sequence \\n\\n\\n\\n ,\\nalthough we can ignore\\ndistinctions between sequences of the same length.\\nFinally, we note that finiteness is a definable property of subsets in\\nour intended models.  This follows from the fact that these models are\\nlinearly ordered by the  lexicographic order relation: \\n\\n\\n\\nand that every non-empty subset of such a model has a least element with\\nrespect to that order.  \\nA set of nodes, then, is finite iff\\neach of its non-empty subsets has an upper-bound with respect to lexicographic\\norder as well.\\n\\n\\n\\nThese three second-order relations will play a role in the next section.\\n\\n\\n\\n  Characterizing the Local Sets\\n\\n\\nWe can now give an example of a class of sets of trees that is definable in\\n\\n\\n\\n --the local sets (i.e., the sets of derivation trees generated by\\nContext-Free Grammars).  The idea behind the definition is simple.  Given an\\narbitrary Context-Free Grammar, we can treat its terminal and\\nnon-terminal symbols as monadic predicate constants.  The productions of the\\ngrammar, then, relate the label of a node to the number and labels of its\\nchildren.  If the set of productions for a non-terminal A, for instance, is\\n\\n\\n\\nwe can translate this as\\n\\n\\n\\nwhere\\n\\n\\n\\nWe can collect such translations of all the productions of the grammar together\\nwith sentences requiring nodes labeled with terminal symbols to have no\\nchildren, requiring the root to be labeled with the start symbol, requiring\\nthe sets of nodes labeled with the terminal and non-terminal symbols to\\npartition the set of all nodes in the tree, and requiring that set of nodes to\\nbe finite.  It is easy to show that the models of this set of sentences\\n are all and only the derivation trees of the grammar.  In this way we get the first half of our characterization of the local sets.\\n\\n\\n\\nIt is, perhaps, not surprising that we can define the local sets with \\n\\n\\n\\n .\\nThis is superficially quite a powerful language, allowing, as it does, a\\ncertain amount of second-order quantification.  It is maybe more remarkable\\nthat, modulo a projection, the only sets of finite trees (with\\nbounded branching)\\nthat are definable in \\n\\n\\n\\nare the local sets.  \\n\\n\\n\\nThe proof hinges on the fact that one can translate formulae in \\n\\n\\n\\ninto the\\nlanguage of SnS--the monadic second-order theory of multiple successor\\nfunctions.  This is the monadic second-order theory of the structure\\n\\n\\n\\na generalization of the\\nnatural numbers with successor and less-than.  The universe, Tn, is the\\ncomplete n-branching tree domain.  The relation \\n\\n\\n\\nis domination,\\n\\n\\n\\nis lexicographic order, and the functions \\n\\n\\n\\nare the\\nsuccessor functions, each taking nodes into their \\n\\n\\n\\nchild (\\n\\n\\n\\n\\n).  Rabin  showed that SnS is decidable for any  \\n\\n\\n\\nOne way of understanding his proof is via the observation that \\nsatisfying assignments for\\na formula \\n\\n\\n\\n\\n ,\\n with free variables among  \\n\\n\\n\\ncan be understood as trees\\nlabeled with (subsets of) \\nthe variables in \\n\\n\\n\\n .\\nA node is in the set assigned to\\nXi in \\n\\n\\n\\niff it is labeled with Xi.  Rabin showed that, for any\\n\\n\\n\\n\\nin the language of SnS, the set of trees encoding the\\nsatisfying assignments for \\n\\n\\n\\n\\nin \\n\\n\\n\\nis accepted by a\\nparticular type of finite-state automaton on infinite trees.  We say that the\\nset is Rabin recognizable.  He goes on to show that emptiness of these\\nsets is decidable.  It follows that satisfiability of these\\nformulae, and hence the theory SnS, is decidable.\\n\\n\\nFor us, the key point is the fact that the sets encoding satisfying assignments\\nare Rabin recognizable.  It is not difficult to exhibit a syntactic\\ntransformation which, given any \\n\\n\\n\\n\\nin \\n\\n\\n\\n ,\\nproduces a\\nformula \\n\\n\\n\\n\\nin the language of SnS,\\nwhere XU is a new variable and\\n\\n\\n\\nis a sequence of new variables (one for each of the finitely many\\npredicates in P that occur in \\n\\n\\n\\n )\\nsuch that,\\n\\n\\n\\niff\\n\\n\\n\\nthat is, the set AU and \\nthe sequences of sets \\n\\n\\n\\nand \\n\\n\\n\\nform\\na satisfying assignment for \\n\\n\\n\\nin \\n\\n\\n\\niff the structure consisting of\\nthe universe AU along with the natural interpretation of \\n\\n\\n\\n ,\\n\\n\\n\\n ,\\nand \\n\\n\\n\\non AU, and the sets \\n\\n\\n\\n ,\\nsatisfies \\n\\n\\n\\nwith the assignment taking \\n\\n\\n\\n into \\n\\n\\n\\n .\\nIt follows that a set of trees is definable in \\n\\n\\n\\niff\\nthey are Rabin recognizable.\\n\\n\\nIf we restrict our attention to sets of finite trees, we can take Rabin\\'s\\nautomata\\n to be ordinary finite-state automata over finite trees , that is, \\nthe sets of finite trees that are definable in \\n\\n\\n\\nare simply recognizable.  One can think of these automata as traversing the tree, top\\ndown, assigning states to the children of a node on the basis of a transition\\nfunction that depends on the state of the node, its label, and the position of\\nthe child among its siblings.  A tree is accepted if it can be labeled by the\\nautomaton in such a way that the root is labeled with a start state and the\\nset of states labeling the leaves is one of a set of accepting sets of\\nstates. Every set of trees that is accepted in this way is the projection of a\\n local set.   To see this, suppose that \\n\\n\\n\\nis a tree accepted by a tree automaton.  Then there is some assignment\\nof states to the nodes in \\n\\n\\n\\nthat witnesses this fact.  \\n\\n\\n\\n\\nSuppose, for\\ninstance, \\n\\n\\n\\n is the tree of Figure , labeled as shown. Consider the tree \\n\\n\\n\\nin which each node is labeled with a pair consisting\\nof the label from \\n\\n\\n\\nand the state assigned to that node.  It is easy to show that, given a\\nrecognizable set of trees, one can construct a CFG to generate the\\ncorresponding set of trees labeled with pairs as in \\n\\n\\n\\n .\\nIn the example,\\nfor instance, this would include, among others, the productions\\n\\n\\n\\nThe original set of trees is then the first projection of the set generated by\\nthe CFG.\\n\\n\\nTogether, these two theorems give us our primary result.\\n\\n\\n\\n  Non-Definability of Free Indexation\\n\\n\\nThis characterization provides a powerful tool for establishing strong\\ncontext-freeness of  classes of languages that are\\ndefined by constraints on the structure of the trees analyzing the strings in\\nthe language.  If one can show that the\\nconstraints defining such a set, or perhaps that any constraints in the class\\nemployed by a given formalism, can be defined within\\n\\n\\n\\nthen the corresponding language or class of languages is\\nstrongly context-free.  Much of the value of standard language complexity\\nclasses, on the other hand, comes from results that allow one to show that a\\ngiven language or class of languages is not included in a particular complexity\\nclass.  Such\\nresults are available here as well, in the form of non-definability results for\\n\\n\\n\\n .\\nOne relatively easy way of establishing such results is by employing\\n the contrapositive of Theorem .  If one can show that a given predicate, when added to \\n\\n\\n\\nallows definition of known\\nnon-CF languages, then clearly that predicate properly extends the power of the\\nlanguage and cannot be definable.  In this way, one can show that the predicate\\n\\n\\n\\n\\nwhich holds between two nodes iff the yields of the\\nsubtrees rooted at those nodes are labeled identically wrt P is not definable\\nin \\n\\n\\n\\n ,\\nfor if it were one could define the copy language \\n\\n\\n\\n\\n .\\n\\n\\nIn this section we will explore an approach that is more difficult but is\\none of the most  general--reduction from the monadic second-order theory of\\nthe grid--and will use it to demonstrate non-definability of\\nfree-indexation--a mechanism which shows up in a number of modules of GB.\\n\\n\\nThe grid is the structure\\n\\n\\n\\n\\nwhere\\n\\n\\n\\nThis is the structure of the (discrete) first quadrant.  Note the similarity to\\n\\n\\n\\n ,\\nthe structure of two successor functions.  The key distinction is the\\nfact that G satisfies the property \\n\\n\\n\\nthat is, the horizontal successor of the vertical successor of a point is the\\nsame as the vertical successor of its horizontal successor.  Let \\n\\n\\n\\nbe\\n the monadic second-order theory of G.  Lewis  showed that this theory is undecidable by showing how one could define the set of\\nterminating computations of an arbitrary Turing machine within it.\\n\\n\\nNow, the monadic second-order theory of any of our intended structures is\\ndecidable (by reduction to SnS), as is the monadic second-order theory of any\\nof our intended structures augmented with any predicate that is definable in\\n\\n\\n\\n(since we can reduce this to the theory of the original structure via\\nthat definition).  Our approach to showing that a predicate is not\\ndefinable in \\n\\n\\n\\nis to show that the theory of one of our structures\\naugmented with that predicate is not decidable.  In particular, we will show\\nthat the theory of such a structure includes an undecidable fragment of the\\nmonadic second-order theory of the grid.\\n\\n\\nOur focus, in this section, is the mechanism known as free-indexation.\\nIn the Government and Binding Theory framework this is the mechanism that\\nis generally assumed\\nto mediate issues like agreement, co-reference of nominals, and identification\\nof moved elements with their traces.  In its most general form this operates by\\nassigning indices to the nodes of the tree randomly and then filtering out\\nthose assignments that do not meet various constraints on agreement,\\nco-reference, etc.  In essence, the indexation is an equivalence relation, one\\nthat distinguishes  \\nunboundedly many equivalence classes among the nodes of the tree.  That is,\\neach value of the index identifies an equivalence class and there is no a\\npriori bound on its maximum value.  Free-indexation views constraints on the\\nindexation as a filter that admits only those equivalence relations that meet\\nspecific conditions on the relationships between the individuals in these\\nclasses.\\n\\n\\nTo see that we cannot define such equivalence relations in \\n\\n\\n\\n ,\\nconsider the\\nclass of structures\\n\\n\\n\\nwhere T2 is the complete binary-branching tree domain, \\n\\n\\n\\n ,\\n\\n\\n\\n ,\\nand\\n\\n\\n\\nare the natural interpretations of parent, domination, and left-of on\\nthat domain, and \\n\\n\\n\\nis any arbitrary equivalence relation.   Let S2S+CI\\nbe the monadic second-order theory of this class of structures.\\nOur claim is that\\n this is an undecidable theory. \\n\\n\\n\\nLewis\\'s proof of the non-decidability of \\n\\n\\n\\nis based on a construction\\nthat takes any given Turning Machine M into a formula \\n\\n\\n\\n\\nsuch that \\n\\n\\n\\n\\niff M halts (when\\nstarted, say, on the empty tape).  \\nThe idea behind our proof of the non-decidability of S2S+CI is that there is a\\nnatural correspondence between points in T2 and those in \\n\\n\\n\\nthat is\\ninduced by interpreting node addresses in T2 as paths (non-decreasing in\\nboth x and y) from the origin in \\n\\n\\n\\n .\\nOf course, in general, there\\nwill be many points in T2 that correspond to the same point in \\n\\n\\n\\n ,\\nbut\\nwe can restrict the interpretation of \\n\\n\\n\\nin such a way that all points in\\nT2 that correspond to the same point in \\n\\n\\n\\nwill be co-indexed.  We\\nthen restrict the interpretation of the variables in \\n\\n\\n\\nin such a\\nway that it does not break the classes of \\n\\n\\n\\n .\\nIn more typically\\nlinguistic terms, we require \\nco-indexed nodes to agree on the features in \\n\\n\\n\\n .\\n\\n\\nThe formula\\n\\n\\n\\n\\nof Lewis\\' proof\\ninvolves only the constant \\n\\n\\n\\n ,\\nthe successor functions\\n\\n\\n\\nand \\n\\n\\n\\n ,\\nsome set of (bound) individual variables, the (free)\\nmonadic predicate variables in \\n\\n\\n\\n ,\\nand the logical connectives.\\n\\n\\nLet\\n\\n\\n\\nThen \\n\\n\\n\\nis true only at the root,\\n\\n\\n\\n\\nis true iff y is the leftmost child of x and\\n\\n\\n\\n\\nis true iff y is the rightmost child of x.  These\\ntranslations are sufficient for us to translate \\n\\n\\n\\n\\ninto a\\nformula \\n\\n\\n\\n\\nthat, when combined with an axiom\\n\\n\\n\\n\\n constraining the interpretation of \\n\\n\\n\\nand \\n\\n\\n\\nas sketched\\nabove, will be satisfiable by a model in the class \\n\\n\\n\\n\\niff\\n\\n\\n\\n\\nis satisfied by G.  That is:\\n\\n\\n\\niff\\n\\n\\n\\nThis in turn implies that\\n\\n\\n\\nDecidability of S2S+CI, then, would imply decidability of the halting problem.\\n\\n\\nIt remains only to define \\n\\n\\n\\n\\n .\\nLet\\n  ALIGN=\"right\" \\n\\n\\n\\nwhere\\n\\n\\n\\nThis requires that\\nevery node is co-indexed with itself,\\nthat the left children of co-indexed nodes are\\nco-indexed as are the right children of co-indexed nodes,\\nand that the left child of the right child and right child of the left child of\\nco-indexed nodes are co-indexed.  Finally all co-indexed nodes are\\nforced, by \\n\\n\\n\\n\\n ,\\nto agree on all predicates in \\n\\n\\n\\n .\\nThat this is sufficient to carry the reduction of the halting\\nproblem to membership in S2S+CI depends on the fact that\\n\\n\\n\\n\\nforces all points in T2 equivalent in the sense that\\nthey correspond to the same\\npoint in G as sketched above, to agree on the predicates in \\n\\n\\n\\n .\\nThus we (roughly) can take the quotient with respect to\\nthis equivalence without affecting satisfiability of\\n\\n\\n\\n\\n .\\nThe resulting structure is isomorphic to G and\\nsatisfies \\n\\n\\n\\n\\niff G satisfies \\n\\n\\n\\n\\n .\\n The proof is carried out in detail in . \\n\\n\\nThe non-definability of free-indexation is a significant obstacle to\\ncapturing GB accounts of language in \\n\\n\\n\\n .\\nAs it turns out,  other constraints employed in GB theories are not generally\\ndifficult to define.  Our ability to capture these accounts,\\nthen, depends directly on the degree to which they necessarily employ\\nfree-indexation.  \\nThe common practice, in GB, is to simply assume co-indexation\\nalmost whenever there is a \\nneed to identify components of the tree in some way.  Unfortunately, we\\ncannot capture directly accounts that are defined in these terms.\\nRather, we are compelled to restate them\\nwithout reference to indices.  On the other hand, it is not at all\\nclear that accounts that appeal to free-indexation actually require so\\ngeneral a mechanism.  On the contrary, it seems that indices are frequently\\nonly a conceptually simple way of encoding more complicated, but less\\ngeneral relationships.\\nThere has been a tendency, in the more recent GB literature, to avoid\\nfree-indexation in favor of these more specific relationships.  Chomsky, \\nfor instance, comments:\\nA theoretical apparatus that takes indices seriously as entities...\\nis questionable on more general grounds. Indices are basically the\\nexpression of a relationship, not entities in their own right.  They\\nshould be replaceable without loss by a structural account of the\\n relation they annotate. , pg. 49, note 52] \\n\\n\\nThis quote comes in the context of a suggestion for a re-interpretation\\nof the standard account of Binding Theory in a manner that avoids use\\n of indices.    Rizzi, in , motivated by an examination of a wide variety of extraction phenomena,\\noffers a re-interpretation of the Empty Category Principle and the\\ntheory of chains that restricts the role of indices to a relatively\\nsmall class of movements.\\nAs we will see in the next section,\\nRizzi\\'s theory provides us with the\\nfoundation we need to capture a largely complete GB account of English\\nin \\n\\n\\n\\n .\\nWe thus establish that this account licenses a strongly\\ncontext-free language.  It seems noteworthy that GB theorists have been led,\\nby purely linguistic considerations, to precisely the kind of re-interpretation\\nof the theory we require in order to establish our language-theoretic\\nresults. \\n\\n\\n  Defining Chains\\n\\n\\nWe turn now to an example that is particularly relevant to the issue of\\ncapturing a Government and Binding Theory account of English in \\n\\n\\n\\n ,\\nand in\\nparticular capturing it without use of \\nindices.  This is our definition of chains--the core notion in\\ncontemporary GB accounts of movement.  Our exposition is intended to be\\naccessible without prior familiarity with GB, although possibly\\nmysterious in some of its details.  It will necessarily be\\nsomewhat meager both in the details of the definition and in the details of the\\nunderlying theory.  A more complete treatment can be found \\n in . \\n\\n\\n\\n  Identifying Antecedents of Traces \\n\\nGovernment and Binding Theory analyzes sentences with four distinct\\nsyntactic representations which are related by the general transformation\\nmove-\\n\\n\\n\\n .\\nThese are D-Structure--corresponding\\nto the deep-structure of earlier transformational theories, S-Structure--roughly corresponding to the surface-structure of those\\ntheories, Phonetic Form--the actual phonetic structure of the sentence,\\nand Logical Form--a more or less direct representation of the sentence\\'s\\nsemantic content.  The principles embodying a GB theory of\\nlanguage are collected into modules which apply at various levels of\\nthis analysis. \\nThe principles\\nwe capture include basic X-bar Theory, Theta Theory, the Case Filter, \\nBinding Theory, Control Theory and various constraints on movement, in\\nparticular the Empty Category Principle.\\nIn this section we focus on the Empty Category Principle and the\\ndefinition of chains.\\n\\n\\nAs we noted in the introduction, we prefer to regard GB theories as a\\nset of constraints on structures rather than a mechanism for\\nconstructing them.  We take this a step further by assuming that those\\nconstraints apply to a single tree which includes S-Structure and\\n D-Structure as submodels, rather than having some constraints apply to one structure, others to the other, and others still to the relationship\\nbetween them.  In this view, D-Structure and move-\\n\\n\\n\\nare best\\nunderstood as perspicuous means of stating constraints which are\\nobscured in a single-level representation (see, for instance,\\n Koster  and Brody ). One argument against such a view is that in some cases (such as\\nhead-raising) chains formed by one movement can be disrupted by\\nsubsequent movement.  Indeed, representational accounts, such as ours,\\nfrequently appeal to a notion of reconstruction--effectively\\nderivation in reverse--to resolve such difficulties.  In fact, at\\nleast if one can employ indices to identify the elements of chains,\\nthere is no need for such a retreat.  Even limiting oneself to the\\nlanguage of L[2]K,P, if one restricts attention to languages, like\\nEnglish, in which head-movement is strictly limited, it is possible to\\nget a purely declarative (and reasonably clear) account of the issues\\nusually treated by reconstruction.  Details of such an account are\\n given in . \\n\\n\\n\\n\\n Figure  gives the S-Structure of a more or less typical GB analysis of the\\nsentence:\\nWhom do you think Alice will invite.\\n\\n\\n\\n\\n In the D-Structure (Figure ) the element carrying the inflection is positioned between the subject and the predicate and Whom is in its standard position as the object of invite.\\nMove-\\n\\n\\n\\ntransforms this structure by cutting out the subtrees rooted at\\nIj and Ni, leaving phonetically empty traces (tj and\\nti), and re-attaching them a higher positions in the tree.\\nIn the case of Whom the movement occurs in two steps, with\\ntraces being left at each intermediate position.  The original position\\nof the moved element is referred to as the base position, and its\\nfinal resting place is the target position.  The moved element is\\nidentified with its traces by co-indexation.  Together, an element and\\nthe traces co-indexed with it form a chain.  Chains can be broken\\nup into a sequence of links each consisting of a trace and its\\nantecedent--the next higher element of the chain.\\n\\n\\nThe fundamental issue we must address in defining chains within \\n\\n\\n\\n is how to identify the antecedent of a trace without reference to\\nindices.  Our key idea is that, if we can limit the portion of the tree\\nin which an antecedent can occur, then we can possibly bound the number\\nof potential antecedents a trace may have.  Such a bound would suffice\\nsince, while we cannot capture indexations with an unbounded range of\\nindex, we can capture any indexation in which there is a constant bound on \\nthe total number of distinct indices.\\n\\n\\n In the standard GB account of movement, that of Barriers , there are two principles that tend to bound the length of links.  The\\nfirst is n-subjacency, which, roughly, limits the number of\\nphrasal boundaries that a link can cross.  This is exactly the kind of\\nconstraint we need.  Unfortunately it is responsible only for weak\\neffects; there are many sentences that violate n-subjacency that\\nare only of degraded acceptability rather than outright ungrammatical.\\nThe second principle that might do is the Empty Category\\nPrinciple.  This puts specific constraints on the structural\\nrelationship between a trace and its antecedent.  Indices, however, play a\\nsignificant role in Chomsky\\'s formulation of this principle.\\n\\n\\nThere is a formulation of ECP, due to Rizzi and\\n based on his notion of Relativized Minimality , in which the role of indexation is largely eliminated.  In Rizzi\\'s theory,\\nthis is a conjunctive principle with two components, a Formal Licensing\\nrequirement and an Identification requirement:\\nECP (Rizzi):\\n\\nA non-pronominal empty category must be properly head-governed.\\n(Formal Licensing)\\n\\nOperators must be identified with their variables. (Identification)\\n\\n\\nWe are interested in the identification requirement, which, incidently,\\nis responsible for most of the effects attributed to ECP in the Barriers\\naccount.  This constraint requires every trace (variable) to be identified with\\nits target (operator).  This can be done in one of two ways,  either by \\na particular class of index, the referential indices,\\nor by a sequence of antecedent-government\\nlinks.\\nIn the latter case the role of indices in identifying chains can be\\ntaken over by the antecedent-government relation.\\n\\n\\nTo a first approximation, government is simply a relation between an\\nelement and those elements occurring in a specifically limited region of\\nthe tree dominated by the phrase in which that element (the governor)\\noccurs.  Its definition\\n has three components.  First, for the class of government\\nrelations we are considering here, the governor must c-command the\\nelements it governs, that is, those elements must be dominated by a sibling \\nof the governor.  Second, there must be no intervening barrier.  For\\nRizzi, the notion of barrier is much weaker than it is in the Barriers\\naccount.  Here, this constraint simply forbids the government relation\\nfrom crossing certain phrasal boundaries (in particular specifiers,\\nadjuncts and complements of nouns or prepositions).  The final component\\nof the government relation requires a governor to be the minimal\\npotential governor of the elements it governs, that is, no potential\\ngovernor can fall properly between a governor and the elements it\\ngoverns.  There are a range of types of government relations that fall under\\nthis general category.  In Rizzi\\'s theory \\nonly potential governors of the same type count for the minimality\\nrequirement.  (This is the relativized aspect of his theory.)  For\\nantecedent-government there is an additional requirement that the\\ngovernor be co-indexed with the trace.\\n\\n\\n\\nAs we will see, we can drop the co-indexation\\nrequirement on the grounds that, when it\\nexists, the antecedent-governor is unique.\\n\\n\\n\\n\\n As an example of these relationships, consider, in Figure , the trace in the specifier of the lower I, that is, the trace of Who falling immediately under the I.  The elements\\nc-commanding this trace include the (empty) C, the ti\\nin the specifier of C, the V, etc.  This is a Wh-Trace\\nwhich means that, by the principles of Binding Theory,\\nits antecedent must fall in a non-argument\\nposition.  In the example, the non-argument positions c-commanding the\\ntrace are just the specifiers of the Cs.  By minimality, no\\npotential antecedent of the trace beyond the closest specifier of\\nC can govern it.  Thus the only possible antecedent-governor\\nof the trace in question is the trace in the specifier of\\nthe lower C, which is, in fact, its antecedent.\\n\\n\\n\\n\\nIn contrast, if we fill that position with a moved adverbial, as in the\\n example of Figure , there is a problem.  The element why cannot be the antecedent of the trace in the specifier of the\\nlower I, but it blocks government by all other potential\\nantecedents.  Thus the trace ti cannot be identified with its\\nantecedent, and the sentence is ruled ungrammatical on the grounds that\\nit violates ECP.\\n\\n\\nIn this way, minimality suffices to pick out the unique antecedent of\\ntraces in chains that are identified by antecedent-government.  But\\nunder Rizzi\\'s criteria chains can also be identified by\\nreferential indices.  These are just indices assigned to elements that\\nreceive what are termed referential Theta roles.  Again to a\\nfirst approximation, we can take these simply to be elements that are\\nthe objects of verbs.  \\n\\n\\n\\n\\n In Figure  Who is extracted from the embedded subject.\\nIf we return to our original example, in which we\\nextract from the object, we find that filling the specifier of the lower\\n C with a moved adverbial (Figure ) has a less dramatic effect.  While antecedent government of the trace in the\\ncomplement of the lower V is  blocked, that trace can now be\\nidentified with its target by the referential index they share.  The\\nfact that this example is not judged to be as bad as the example from\\n Figure  is attributed, then, to the fact that it is only a 1-subjacency violation rather than an ECP violation.\\n\\n\\nIn general, we could be forced to resort to a mechanism equivalent to\\nindexation in order to distinguish such referential chains.  It turns out,\\nhowever, that in English, at least, chains of this type do not overlap.\\n Manzini , in fact, argues for an account of A-movement (movements, like these we have been considering, to non-argument positions)\\nwhich implies that no more than two such chains--one \\nreferential and one non-referential--may ever overlap.\\nThus, we need to identify only a single referential antecedent in any\\nsingle context.\\n\\n\\n  Defining Antecedent-Government, Links, and Chains \\n\\nRelativized Minimality theory distinguishes a number of distinct\\nvarieties of antecedent-government, one for each class of movement.  \\nWe look at one representative case\\n-antecedent-government.  This is defined, in \\n\\n\\n\\nas follows:\\n\\n\\n\\nIn words, this says simply that x is an -antecedent-governor of y iff\\nx is in a non-argument () position, it c-commands y, no barrier\\nintervenes between x and y, and no non-argument specifier falls between xand y.  The actual definitions of \\n\\n\\n\\n ,\\n\\n\\n\\n ,\\n\\n\\n\\n\\n ,\\n\\n\\n\\n ,\\nand \\n\\n\\n\\n\\nis unimportant\\nhere.   The predicate \\n\\n\\n\\nis used to check the compatibility of the\\nfeatures of the trace with those of its antecedent.\\n\\n\\nUsing this, we can define the link relation. \\n\\n\\n\\nThis is just antecedent-government with certain additional configurational\\nrequirements.  We can extend the notion of\\nlinks based on Rizzi\\'s antecedent-government to include antecedents and\\ntraces that Rizzi  identifies with a referential index\\n(which we refer to as -referential links), and links formed by\\nrightward movement.  This gives us five distinct link relations.  As they\\nare mutually exclusive, we can take their disjunction to form a single link\\nrelation which must be satisfied by every trace and its antecedent.\\n\\n\\n\\nThe idea, now, is to define chains as any set of nodes that are\\nlinearly ordered by \\n\\n\\n\\n .\\nBefore we can do this, though, we have one\\nmore issue to resolve.  The problem is that, while we can identify a unique\\nantecedent for each trace, nothing assures us that there will be a unique trace\\nfor each antecedent, that is, nothing prevents us from identifying the same\\nnode as the antecedent of more than one trace.\\n\\n\\n\\n\\nAs an example, we might license\\n the tree in Figure .  This is the conflation of two sentences: \\nWhoi has ti told you Alice invited him.\\nWhoi has Alice told you ti ti invited him.\\nIn the first we have extracted Who from the subject of the matrix\\nclause and in the second we have extracted it from the subject of the embedded\\nclause.  We can find a link relation between Who and the trace in the\\nspecifier of the matrix I and a link relation between Who and the\\ntrace in the specifier of the embedded C, but clearly it cannot have moved\\nfrom both positions. \\n\\n\\nWe rule out such structures by requiring that chains not only be linearly\\nordered by \\n\\n\\n\\n ,\\nbut that they are also closed under the link relation,\\nthat is,  every chain includes every node that is related by \\n\\n\\n\\nto any\\n node in the chain.  Trees like the the one in Figure  are ruled out on the grounds that any chain that contains either of the traces in\\nquestion must include both of them, and will therefore not be linearly ordered.\\n\\n\\nFormalizing this, we get:\\n\\n\\n\\n  Defining the ECP \\n\\nWe can now capture Rizzi\\'s version of the Empty Category Principle:\\n\\n\\nLicensing\\n\\n\\n\\nIdentification\\n\\n\\n\\nNote, in particular, that in our definition the identification requirement\\nis reduced simply to a requirement that every trace is a member of some\\nwell-formed chain.  As we admit the notion of trivial chains--chains\\nwith a single element, formed by zero movements--we can generalize this to a\\nglobal requirement that every element of the tree is a member of a (possibly\\ntrivial) well-formed chain.\\n\\n\\nIdentification (Generalized)\\n\\n\\n\\nRecall that identification is the component of Rizzi\\'s definition  that\\naccounts for most of the effects attributed to ECP in the Barrier\\'s account of\\nmovement.  Thus we have reduced a variety of effects to a single simple global\\nprinciple.  Of course we have paid for this with a complex\\ndefinition of chains, but much of this complexity lies in the definition\\nof antecedent-government and Rizzi argues, on\\nlinguistic grounds, for essentially this definition in\\nany case.  It is satisfying that we can recover its added complexity in the\\nform of a greatly simplified ECP.\\n\\n\\n  Limits of the Definition \\n\\nThe fact that we can exhibit a definition in \\n\\n\\n\\nof the class of trees\\nlicensed by a specific GB account of English provides a strong complexity\\nresult for that class of trees--it is strongly context-free.  We don\\'t, on the\\nother hand, expect this formalization to work for GB theories in general, and,\\nin particular we don\\'t expect it to work for a GB account of Universal Grammar.\\n\\n\\n\\n\\nA more or less typical account of head-raising in Dutch, for instance, is given\\n in Figure .  This is the type of movement presumed to be responsible for the cross-serial dependencies that form the basis of Shieber\\'s\\n claim that Swiss-German is non-context-free .  Bresnan, et  al.,  have pointed out that analyses such as these form a non-recognizable set.  Consequently, it cannot be possible to capture this\\naccount within \\n\\n\\n\\n ,\\nand, in fact, the definition we give fails to license\\nthese structures.  Examining why this is the case provides some insight into\\nthe kinds of natural properties of linguistic structures that correspond to\\nincreased language-theoretic complexity.\\n\\n\\nIn order to rule out the possibility of ``forking\\'\\' chains--of some nodes\\nparticipating in the licensing of multiple gaps--we have required chains to be\\nmaximal in the sense that they include every node that is related by link to\\nany node in the chain.  Consequently, we can license overlapping chains only if\\nthey are distinguished in some way.  The account works for English because we\\ncan \\nclassify chains in English into a bounded set of types in such a way that no\\ntwo chains of the same type ever cross.  (This fact depends to a great\\nextent on the minimality requirement in the antecedent-government relation.)\\nThis property can be stated as a principle:\\nThe number of chains which overlap at any single position in the\\ntree is bounded by a constant.\\nOur approach to chains will work for any account of language that satisfies\\nthis principle.  Once again, the linguistics literature provides arguments that\\nsuch bounds exist, at least in some cases.  As we have already noted, Manzini\\'s\\n Locality Theory  implies that no more than two  A-chains ever overlap.  Stabler  makes the stronger claim that such bounds exist for all linguistically relevant relationships in all\\nlanguages.  \\n\\n\\nLeaving aside the possibility that it may be possible to account for\\ncross-serial dependencies in Dutch in other ways, we can note that accounts\\n employing structures such as the one in Figure  fail to meet the bound on overlapping chains.  This is despite the fact that, if one orders\\nthe movements bottom-up, each movement meets the strictest conceivable locality\\nconstraint--each head moves to the closest possible position (often stated as\\nthe Head Movement Constraint).  The problem is that, even if the\\nmovements are ordered in this way, each movement carries the target\\npositions of the prior movements along with it.  Thus, in the final structure\\nall chains of \\nhead-movement overlap.  Given that the number of heads participating in these\\nstructures is arbitrary, there can be no a priori bound on the number of\\noverlapping chains.  Note that in the example the two helpen chains\\n(\\n\\n\\n\\n\\nand \\n\\n\\n\\n\\n )\\nare indistinguishable.  Any attempt to\\nform a chain including any of these nodes will be required to include all four\\nand the result will not be linearly ordered.\\n\\n\\n\\n  Conclusion\\n\\n\\nIn this paper we have \\nintroduced a kind of descriptive complexity result for the\\nstrongly Context-Free Languages--a language is strongly context-free iff\\nthe set of trees analyzing the syntax of its strings is definable in \\n\\n\\n\\n (modulo a projection).  Using this result we have sketched a couple of language\\ncomplexity results relevant to GB, namely, that free-indexation cannot, in\\ngeneral, be \\nenforced by CFGs, and that a specific GB account of English licenses\\na strongly context-free language.  The first of these results is not likely to\\ncome as a surprise to the GB community.  The appropriateness of free-indexation\\nas a fundamental component in linguistic theories has been questioned in the\\nmore recent GB literature on purely linguistic (rather than complexity\\ntheoretic) grounds.\\n\\n\\nThe second result is more surprising.  We don\\'t expect it to extend to\\nthe whole range of human languages, that is, to any theory of Universal\\n Grammar.  Shieber  and  Miller  (to cite two examples) give fairly strong evidence that there are \\nconstructions that occur in human languages that are beyond the CFLs, and hence\\nnot possible to capture in \\n\\n\\n\\n .\\nAs expected, our definitions fail for these\\nconstructions.  The fact that the\\ndefinition works for English is a consequence of the fact that, in the account\\nof English we capture, it is \\npossible to classify chains into finitely many categories in such a way that\\nno two \\nchains from a given category ever overlap. \\n  GB-style analyses of the constructions studied by Shieber and by\\nMiller include positions in which an unbounded number of chains can overlap.\\nOur definition is unable to identify any well-formed\\nchains including these positions; indeed, there is unlikely to be any way to\\ndistinguish these chains without the \\nequivalent of unbounded indices.\\n\\n\\nAs it stands, this result speaks only of the particular account of English we\\ncapture.  The fact that this is context-free says nothing about the nature of\\nhuman language faculty, since the principle it depends upon is unlikely to be a\\nprinciple of Universal Grammar.  It does, however, raise the prospect of wider\\nresults.  Extensions of our descriptive\\ncomplexity result to larger \\nlanguage complexity classes\\ncould provide formal restrictions on the principles employed by GB theories\\nthat would be sufficient to provide non-trivial generative capacity results for\\nthose theories\\nwithout losing the ability to capture the full range of human language.  With\\nsuch extended characterizations\\none might establish upper bounds on the complexity of human language in\\ngeneral. \\nThe possibility that such results might be obtainable is suggested by the fact\\nthat we find numerous cases in which the issues arising in our studies for\\ndefinability reasons, and ultimately for language complexity reasons, have\\nparallels that arise in the GB literature \\nmotivated by more purely linguistic\\nconcerns.  This suggests that the regularities of human languages that are the\\nfocus of the linguistic studies are perhaps reflections of properties of the\\nhuman language faculty that can be characterized, at least to some extent, by\\nlanguage complexity classes.\\n\\nBibliography \\n\\nRobert C. Berwick.\\nStrong generative capacity, weak generative capacity, and modern\\n  linguistic theories.\\nComputational Linguistics, 10:189-202, 1984.\\n\\n\\nJoan Bresnan, Ronald M. Kaplan, Stanley Peters, and Annie Zaenen.\\nCross-serial dependencies in Dutch.\\nLinguistic Inquiry, 13:613-635, 1982.\\n\\n\\nMichael Brody.\\n\\n\\n\\n -theory and arguments.\\nLinguistic Inquiry, 24(1):1-23, 1993.\\n\\n\\nRobert C. Berwick and Amy S. Weinberg.\\nThe Grammatical Basis of Linguistic Performance.\\nMIT Press, 1984.\\n\\n\\nNoam Chomsky.\\nBarriers.\\nMIT Press, 1986.\\n\\n\\nNoam Chomsky.\\nA minimalist program for linguistic theory.\\nIn The View from Building 20, pages 1-52. MIT Press, 1993.\\n\\n\\nJohn Doner.\\nTree acceptors and some of their applications.\\nJournal of Computer and System Sciences, 4:406-451, 1970.\\n\\n\\nFerenc Gcseg and Magnus Steinby.\\nTree Automata.\\nAkadmiai Kiad, Budapest, 1984.\\n\\n\\nMark Johnson.\\nThe use of knowledge of language.\\nJournal of Psycholinguistic Research, 18(1):105-128, 1989.\\n\\n\\nJan Koster.\\nDomains and Dynasties.\\nForis Publications, Dordrecht, Holland, 1987.\\n\\n\\nMarcus Kracht.\\nSyntactic codes and grammar refinement.\\nJournal of Logic, Language, and Information, to appear.\\n\\n\\nSteven G. Lapointe.\\nRecursiveness and deletion.\\nLinguistic Analysis, 3(3):227-265, 1977.\\n\\n\\nHarry R. Lewis.\\nUnsolvable Classes of Quantificational Formulas.\\nAddison-Wesley, 1979.\\n\\n\\nMaria Rita Manzini.\\nLocality: A Theory and Some of Its Empirical Consequences.\\nMIT Press, Cambridge, Ma, 1992.\\n\\n\\nPhilip H. Miller.\\nScandinavian extraction phenomena revisited: Weak and strong\\n  generative capacity.\\nLinguistics and Philosophy, 14:101-113, 1991.\\n\\n\\nGeoffrey K. Pullum and Gerald Gazdar.\\nNatural language and context-free languages.\\nLinguistics and Philosophy, 4:471-504, 1982.\\n\\n\\nGeoffery K. Pullum.\\nOn two recent attempts to show that English is not a CFL.\\nComputational Linguistics, 10:182-188, 1984.\\n\\n\\nMichael O. Rabin.\\nDecidability of second-order theories and automata on infinite trees.\\nTransactions of the American Mathematical Society, 141:1-35,\\n  July 1969.\\n\\n\\nLuigi Rizzi.\\nRelativized Minimality.\\nMIT Press, 1990.\\n\\n\\nJames Rogers.\\nStudies in the Logic of Trees with Applications to Grammar\\n  Formalisms.\\nPh.D. dissertation, Univ. of Delaware, 1994.\\n\\n\\nStuart M. Shieber.\\nEvidence against the context-freeness of natural language.\\nLinguistics and Philosophy, 8:333-343, 1985.\\n\\n\\nEdward P. Stabler, Jr.\\nThe Logical Approach to Syntax.\\nBradford, 1992.\\n\\n\\nEdward P. Stabler.\\nThe finite connectivity of linguistic structure.\\nIn C. Clifton, L. Frazier, and K. Rayner, editors, Perspectives\\n  on Sentence Processing, chapter 13, pages 303-336. Lawrence Erlbaum,\\n  Hillsdale, NJ, 1994.\\n\\n\\nJ. W. Thatcher.\\nCharacterizing derivation trees of context-free grammars through a\\n  generalization of finite automata theory.\\nJournal of Computer and System Sciences, 1:317-322, 1967.\\n\\nFootnotes\\n\\n  To Appear:\\nSpecifying Syntactic Structures (papers from the Logic, Structures, and Syntax\\nworkshop, Amsterdam, Sept. 1994)\\n  Or, following\\na strictly derivational approach, the set of all structures consisting of a\\ntriple of finite trees along with a representation of PF.\\n  We will usually dispense with the dot and denote\\nconcatenation by juxtaposition.\\n  A partial\\n axiomatization of this class of models is given in . \\n  A more complete\\n proof is given in . \\n  We\\nwill assume, for simplicity, that only set variables occur free.  Since\\nindividual variables can be re-interpreted as variables ranging over\\nsingleton sets, this is without loss of generality.\\n  This\\n proof is evidently originally due to Thatcher .  In addition,  Theorem  is implicit in the proof of a related theorem due to  Doner . \\n  Since the property of being an\\nequivalence relation--being reflexive, symmetric, and transitive--is definable\\nin \\n\\n\\n\\n ,\\nour result is one way of showing that \\n\\n\\n\\naugmented with a\\nsingle arbitrary binary relation has a non-decidable monadic second-order\\ntheory.\\n  While we don\\'t treat Logical Form,\\nthere is no reason this cannot be incorporated into our structures in\\nmuch the same way.\\n  It is\\n interesting that Johnson, in  initially defines all four levels of structure, but then, through a series of standard\\nprogram transformations, optimizes away everything except PF and LF.\\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nWe introduce  \\n\\n\\n\\n ,\\na monadic second-order language for reasoning\\nabout trees which characterizes the strongly Context-Free Languages in the\\nsense that a set of finite trees is definable in \\n\\n\\n\\niff it is (modulo a\\nprojection) a Local Set--the set of derivation trees generated by a CFG.\\nThis provides a flexible approach to establishing language-theoretic complexity\\nresults for formalisms that are based on systems of well-formedness constraints\\non trees.  We demonstrate this technique by sketching two such results for\\nGovernment and Binding Theory.  First, we show that free-indexation,\\nthe mechanism assumed to mediate a variety of \\nagreement and binding relationships in GB, is not definable in \\n\\n\\n\\nand\\ntherefore not enforcible by CFGs.  Second, we show how, in spite of this\\nlimitation, a reasonably complete GB account of English can be defined in\\n\\n\\n\\n .\\nConsequently, the language licensed by that account is strongly context-free.\\nWe illustrate some of the issues involved in establishing this result by\\nlooking at the definition, in \\n\\n\\n\\n ,\\nof chains.  The limitations of this\\ndefinition provide some insight into the types of natural linguistic principles\\nthat correspond to higher levels of language complexity.  We close with some\\nspeculation on the possible significance of these results for generative\\nlinguistics.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nWe give an algorithm which is at the heart of a type\\ndiagnosis system for a higher-order concurrent\\nconstraint language, viz. the \\n\\n\\n\\ncalculus\\n  which is the underlying  operational model of the programming language Oz . The algorithm decides satisfiability of\\nconstraints\\ncontaining equations \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\n ,\\nand constraints \\n\\n\\n\\nover infinite constructor\\ntrees with free variables.\\nThe algorithm is given fully in terms of constraint simplification.\\nOne the one hand, this gives credit to the close\\nrelationship between type inference and constraint solving\\n(e.g.,\\n ,, and many others).\\nOn the other hand it establishes yet another correspondence\\nbetween unification problems arising from polymorphic type inference\\nand unification based grammar formalisms:\\nThe most prominent one is the equivalence of\\n type checking polymorphic recursion ,  with semi-unification ,  both of which are\\nundecidable in general. To avoid this undecidability,\\nwe chose a weaker instance relation to give semantics to\\n\\n\\n\\n .\\nFor example, we allow \\n\\n\\n\\nas an instance\\nof \\n\\n\\n\\neven if \\n\\n\\n\\n .\\nOn the type\\nside, this type of constraints maintains some of the polymorphic\\nflavour, but  abandons full parametric\\n polymorphism . \\n\\n\\nWe start out from the set of\\ninfinite constructor trees with holes (free variables).\\nWe give a semantics which interprets the tree assigned\\nto a variable dually: As itself\\nand the set of its ``weak'' instances.\\nOur algorithm terminates, and can be shown to\\nbe correct and complete under this semantics.\\nThe decidability problem for our constraints turned out\\nto be equivalent to weak subsumption over feature graphs\\n solved by Drre  for\\nfeature graphs with feature (but no arity) constraints.\\n\\n\\nHowever, only half of Drre's two-step\\nsolution is a constraint solving\\nalgorithm. The second step relies on the\\nequivalence of non-deterministic and deterministic\\nfinite state automata. In contrast, our algorithm\\ndecides satisfiability in a completely incremental\\nmanner and is thus amenable to be integrated\\n in an concurrent constraint language like Oz   or AKL . \\n\\n\\nThe extension of our algorithm towards\\nfeature trees is easily possible\\n (see ). This allows to do type diagnosis for\\n records  and objects. An entirely set-based semantics allows\\nto naturally extend the algorithm\\nto a full-fledged type diagnosis system,\\ncovering - among other aspects - sorts, disjunctive types,\\nand recursive data type declarations\\n . \\n\\n  Type Diagnosis. \\n\\nAs an illustrating example for the form of type diagnosis\\nwe have in mind, consider the following \\n\\n\\n\\nprogram:\\n\\n\\n\\nThis program declares four variables x,y,z, and p. It defines\\na relational abstraction p, which states that its two arguments\\nu and v are related through the equation \\n\\n\\n\\n\\n  . Furthermore, it states the equality \\n\\n\\n\\n\\nand\\napplies p to yy.\\nThis application \\n\\n\\n\\n\\nreduces to a copy of\\nthe abstraction p with the\\nactual arguments yy replaced for the formal ones uv:\\n\\n\\n\\nObserve how the abstraction p is  defined by\\nreference to the global variable x, while\\nthe value of x is defined through an application\\nof p: \\n\\n\\n\\n\\n .\\nSuch a cycle is specific to the \\n\\n\\n\\ncalculus\\nsince no other language offers\\nexplicit declaration of logic variables global to an abstraction\\n(be it logic, functional, or concurrent\\n languages, e.g., Prolog, ML ,  or Pict ). \\n\\n\\nThe types of the variables involved are described by\\n the following constraint. For ease of reading, we slightly abuse notation and pick\\nthe type variables identical to the\\ncorresponding object variables:\\n\\n\\n\\n\\n\\n\\n\\nis the relational type of p,\\nand the application gives rise to the constraint\\n\\n\\n\\n\\n ,\\nwhich says that y is constrained by both formal arguments\\nof the procedure p.\\nThe subconstraint\\n\\n\\n\\n\\n reflects the cyclic dependency between x and p. It says\\nthat y be in the set of instances of\\nv which depends through \\n\\n\\n\\n\\n on x, and at the same time that x should be exactly\\n\\n\\n\\n .\\n\\n\\nType diagnosis along this line\\n is discussed in depth in . \\n\\n\\n  Related Work. \\n\\nApart from the already mentioned work,\\nrelated work includes investigations about membership\\n constraints (e.g., ), type analysis for untyped languages (Soft Typing)\\n ,,, constraint-based program analysis\\n   and the derivation of recursive sets from programs . For proofs and a detailed discussion of related work\\n see . \\n\\n\\n  Plan of the Paper. \\n\\n This paper is structured as follows. In the Section  below we present our constraints along with their semantics and\\n give necessary notation. Section  gives a simple algorithm which is correct but non-terminating.\\n Section  gives the rules of the full algorithm.  Section  concludes and gives a brief outlook. \\n\\n\\n\\n    Constraints and Semantics\\n\\n\\nWe assume a signature \\n\\n\\n\\nof function\\nsymbols with at least two elements ranged over by\\n\\nf,g,h,a,b,c and an infinite\\nset of base variables \\n\\n\\n\\nranged over by\\n\\n\\n\\n\\n .\\nIf V is a further set of variables then\\n\\n\\n\\nstands for the set of all finite or infinite trees\\nover signature \\n\\n\\n\\nand variables V. Trees of \\n\\n\\n\\n are always ranged over by s and t. The set of variables\\noccurring in a tree t is denoted by \\n\\n\\n\\n .\\nSequences of variables are written as \\n\\n\\n\\n ,\\nor \\n\\n\\n\\n .\\n\\n\\nWe build constraints over a set of\\nconstraint variables ranged over by x, y,\\nz, u, v, w. Constraint variables must contain at least\\nbase variables. The syntax of\\nour constraints \\n\\n\\n\\n ,\\n\\n\\n\\nis as follows:\\n\\n\\n\\nAs atomic constraints we consider equations\\n\\n\\n\\n\\nor \\n\\n\\n\\n\\nand \\nconstraints \\n\\n\\n\\n .\\nConstraints are atomic\\nconstraints closed under conjunction. First-order\\nformulae build over constraints \\n\\n\\n\\nare denoted by\\n\\n\\n\\n .\\nWe define \\n\\n\\n\\nto be the least binary\\nrelation on \\n\\n\\n\\nsuch that \\n\\n\\n\\nis associative\\nand commutative. For convenience, we shall use the\\nfollowing notation:\\n\\n\\n\\nAs semantic structures we pick tree-structures\\nwhich we also call \\n\\n\\n\\nfor some set V. The domain of a\\ntree-structure \\n\\n\\n\\nis the set of trees \\n\\n\\n\\n .\\nIts interpretation is defined by\\n\\n\\n\\n\\n .\\nWe define the application\\n\\n\\n\\nof f to a sequences of sets of trees \\n\\n\\n\\n elementwise, \\n\\n\\n\\n\\n .\\nGiven a tree \\n\\n\\n\\n\\n ,\\nthe set \\n\\n\\n\\n\\n of weak instances of s is defined as the greatest fixed point of:\\n\\n\\n\\nNotice that this definition implies\\n\\n\\n\\n\\n ,\\neven if \\n\\n\\n\\n .\\nLet V1, V2 be two sets whose elements we call\\nvariables. A V1-V2-substitution \\n\\n\\n\\nis a\\nmapping from V1 to \\n\\n\\n\\n .\\nBy homomorphic extension, every\\nsubstitution can be extended to a mapping from \\n\\n\\n\\n to \\n\\n\\n\\n .\\nThe set of strong instances of s\\nis defined by\\n\\n\\n\\n\\n. Note that \\n\\n\\n\\n\\n ,\\nand that\\n\\n\\n\\n\\nif \\n\\n\\n\\n .\\nUsing \\n\\n\\n\\n\\ninstead of \\n\\n\\n\\n\\nwould\\nmake satisfiability of our constraints equivalent to\\nsemi-unification and undecidable\\n ,. \\n\\n\\nLet \\n\\n\\n\\nbe a V1-V2-substitution,\\n\\n\\n\\n\\n ,\\nand \\n\\n\\n\\n constraints such that \\n\\n\\n\\n\\n ,\\n\\n\\n\\n\\n .\\n Then we define:\\n\\n\\n\\nA V1-V2-solution of \\n\\n\\n\\nis a V1-V2-substitution\\nsatisfying \\n\\n\\n\\n\\n .\\nA constraint \\n\\n\\n\\nis called\\nsatisfiable, if there exists a V1-V2-solution for\\n\\n\\n\\n .\\nThe notion of \\n\\n\\n\\n\\nextends to arbitrary first-order\\nformulae \\n\\n\\n\\nin the usual way.\\nWe say that a formula \\n\\n\\n\\nis valid, if \\n\\n\\n\\n\\nholds for all V1-V2-substitutions \\n\\n\\n\\nwith\\n\\n\\n\\n\\n .\\nIn symbols, \\n\\n\\n\\n\\n .\\n\\n\\nOur setting is a conservative extension of\\nthe usual rational unification problem. This means\\nthat free variables in the semantic domain do not affect\\nequality constraints. A constraint \\n\\n\\n\\nis satisfiable in the\\ntree-model \\n\\n\\n\\n ,\\nif there\\nexists a \\n\\n\\n\\n -V-solution of \\n\\n\\n\\n .\\nThe trees of\\n\\n\\n\\n\\nare called ground trees.\\n\\n\\n\\nThe statement would be wrong for \\n\\n\\n\\n 's\\ncontaining weak subsumption constraints. For instance,\\nconsider the following \\n\\n\\n\\nwith \\n\\n\\n\\n :\\n\\n\\n\\nThis \\n\\n\\n\\nis not satisfiable in the model of ground trees,\\nsince the set \\n\\n\\n\\n\\nis a singleton for\\nall ground trees t, whereas any V1-V2-solution \\n\\n\\n\\nof\\n\\n\\n\\nhas to satisfy\\n\\n\\n\\n\\n.  However, there exists a \\n\\n\\n\\n -\\n\\n\\n\\n -solution\\n\\n\\n\\nof \\n\\n\\n\\n ,\\nwhere \\n\\n\\n\\nis an singleton:\\n\\n\\n\\n\\n  Weak Subsumption vs. Sets of Weak Instances. \\n\\nIn the remainder of this section we\\ncompare our sets of weak instance with Drre's\\nnotion of weak subsumption.\\nLet us consider constructor\\ntrees as special feature trees with integer-valued features,\\na distinguished feature\\n label (e.g., ,), and a distinguished feature arity.\\nGiven feature constraints x[f]y saying that xhas direct subtree y at feature f, the equation\\n\\n\\n\\n\\ncan be considered\\n equivalent to: \\n\\n\\n\\nLet us write \\n\\n\\n\\n\\nto say that\\nthe tree s has some direct subtree at f.\\nA simulation between \\n\\n\\n\\nand \\n\\n\\n\\nis a relation\\n\\n\\n\\n\\nsatisfying:\\nIf \\n\\n\\n\\n\\nthen\\n\\n\\nNow, the weak subsumption preorder \\n\\n\\n\\n\\nis defined by:\\n\\n\\n\\nWe have the following lemma:\\n\\n\\n\\nA similar statement can be derived for the set of strong\\ninstances\\nand a strong subsumption preorder\\n following . The difference between \\n\\n\\n\\n\\nand Drre's\\nnotion of weak subsumption is that he does not require\\nArity Simulation, while we naturally do\\nsince we  start\\nfrom constructor trees. For type checking, constructor\\ntrees seem more natural: For illustration note that\\nthe arity of a procedure is essential type information.\\n\\n\\n\\n    A Non-terminating Solution\\n\\n\\nIn order to solve our constraints one could come up\\nwith the system\\n given in Figure . Besides the three usual unification rules for\\nrational trees, the only additional rule is (Descend).\\nThis algorithm is correct and very likely to be complete\\nin that for an unsatisfiable constraint \\n\\n\\n\\nthere is a derivation\\nfrom \\n\\n\\n\\nto \\n\\n\\n\\n .\\nHowever, this intuitive algorithm loops due to the introduction\\nof new variables.\\n\\n\\n\\nNote that some form of descending is necessary in order to derive the clash\\nfrom the inconsistent constraint\\n\\n\\n\\n\\n    Algorithm\\n\\n\\nTo consider trees with free variables as set of instances\\nmeans that we need to compute intersections of such sets\\nand to decide their emptiness.\\nWhen we simplify \\n\\n\\n\\n\\n in a context \\n\\n\\n\\n ,\\nwe have to compute the intersection of the\\nsets of instances of y and z.\\nIn order to avoid the introduction of new variables we\\nadd a new class of variables to represent such intersections,\\nand one new constraint.\\nIntersection variables are defined as nonempty\\nfinite subsets of base variables. In order\\ncapture the intended semantics, we\\nwrite \\n\\n\\n\\n\\ninstead of\\n\\n\\n\\n\\n .\\nThe equality \\n\\n\\n\\non intersection variables is the equality\\non powersets, which  satisfies:\\n\\n\\n\\nWe call an x a component of y, if\\n\\n\\n\\n\\nfor some z.\\nThe set of components of a variable x is denoted by\\n\\n\\n\\n\\n .\\nNote that \\n\\n\\n\\n  Acknowledgements. \\n\\nWe would like to thank Ralf Treinen for pointing\\nus to Drre's paper and the anonymous referees\\nfor useful remarks.\\nThe research reported in this paper has been supported by the\\nBundesminister fr Bildung, Wissenschaft, Forschung und\\nTechnologie (FTZ-ITW-9105), the Esprit Project ACCLAIM (PE 7195),\\nthe Esprit Working Group CCL (EP 6028), and a fellowship of the\\nGraduiertenkolleg 'Kognition' at the Universitt des Saarlandes\\nof the first author.\\n\\n\\n\\n    Outlook\\n\\n\\nWe have presented an algorithm for deciding\\nsatisfiability of constraints over infinite\\nconstructor trees with holes.\\nOur motivation to solve such constraints\\ngrew out of a type inference problem. Formally,\\nthe problem is equivalent to type checking a weak form\\nof polymorphic recursion. Type checking\\npolymorphic recursion is equivalent to\\nsemi-unification and to subsumption of feature graphs.\\nAll three are undecidable\\n ,,. We establish a similar correspondence between\\na type inference problem and\\nweak subsumption of feature graphs:\\nThe latter has been investigated by Drre\\nlooking for a logical treatment\\nof coordination phenomena in unification based\\n grammar formalisms . Our starting point from the constraint language Oz however\\nlead us to an incremental algorithm, in contrast\\nto the automata based solution of Drre.\\n\\nBibliography \\n\\nA. Aiken and E. Wimmers.\\nType Inclusion Constraints and Type Inference.\\nIn ACM Conference on Functional Programming and Computer\\n  Architecture, pages 31-41, Copenhagen, Denmark, June 1993.\\n\\n\\nRolf Backofen.\\nExpressivity and Decidability of First-order Languages over\\n  Feature Trees.\\nDoctoral Dissertation. Universitt des Saarlandes, Technische\\n  Fakultt, 66041 Saarbrcken, Germany, 1994.\\n\\n\\nR. Cartwright and M. Fagan.\\nSoft Typing.\\nIn ACM Conference on Programming Language Design and\\n  Implementation, pages 278-292, June 1991.\\n\\n\\nJochen Drre.\\nFeature-Logic with Weak Subsumption Constraints.\\n In Rupp et al. , chapter 7, pages   187-203.\\n\\n\\nJochen Drre and William C. Rounds.\\nOn Subsumption and Semiunification in Feature Algebras.\\nIn IEEE Symposium on Logic in Computer Science, pages 300-310,\\n  1990.\\n\\n\\nT. Frhwirth, E. Shapiro, M. Y. Vardi, and E. Yardeni.\\nLogic Programs as Types for Logic Programs.\\nIn IEEE Symposium on Logic in Computer Science, pages 300-309,\\n  1991.\\n\\n\\nF. Henglein.\\nType Inference and Semi-Unification.\\nIn ACM Conference on LISP and Functional Programming, pages\\n  184-197, January 1988.\\n\\n\\nRobert Harper, Dave MacQueen, and Robin Milner.\\nStandard ML.\\nTechnical Report ECS-LFCS-86-2, Department of Computer Science,\\n  University of Edinburgh, 1986.\\n\\n\\nSverker Janson and Seif Haridi.\\nProgramming Paradigms of the Andorra Kernel Language.\\nIn International Logic Programming Symposium, pages 167-186,\\n  1991.\\n\\n\\nDexter Kozen, Jens Palsberg, and Michael I. Schwartzbach.\\nEfficient Inference of Partial Types.\\nJournal of Computer and System Sciences, 49(2):306-324, 1994.\\nhas also appared in Proc. FOCS, pp.363-371.\\n\\n\\nA. Kfoury, J. Tiuryn, and P. Urzyczyn.\\nThe Undecidability of the Semi-Unification Problem.\\nIn ACM Symposium on Theory of Computation, pages 468-476, May\\n  1990.\\n\\n\\nA. J. Kfoury, J. Tiuryn, and Urzycyn.\\nType Recursion in the Presence of Polymorphic Recursion.\\nACM Transactions on Programming Languages and Systems, pages\\n  290-311, 1993.\\n\\n\\nMartin Mller and Joachim Niehren.\\nA Type is a Type is a Type.\\nResearch report, DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbrcken,\\n  Germany, 1995.\\nIn Preparation.\\n\\n\\nMartin Mller.\\nType Diagnosis for a Higher-Order Concurrent Constraint\\n  Language.\\nDoctoral Dissertation. Universitt des Saarlandes, Technische\\n  Fakultt, 66041 Saarbrcken, Germany, 1996.\\nIn Preparation.\\n\\n\\nAlan Mycroft.\\nPolymorphic Type Schemes and Recursive Definitions.\\nIn International Symposium on Programming, number 167 in\\n  Lecture Notes in Computer Science, 1984.\\n\\n\\nJoachim Niehren and Andreas Podelski.\\nFeature Automata and Recognizable Sets of Feature Trees.\\nIn Tapsoft, pages 356-375, April 1993.\\n\\n\\nJoachim Niehren, Andreas Podelski, and Ralf Treinen.\\nEquational and Membership Constraints for Infinite Trees.\\nIn Claude Kirchner, editor, Proceedings of the RTA '93, pages\\n  106-120, 1993.\\n\\n\\nBenjamin C. Pierce and David N. Turner.\\nPICT User Manual.\\nTechnical Report to appear, LFCS, Edinburgh, 1995.\\nVersion 3.5g.\\n\\n\\nC. J. Rupp, M. A. Rosner, and R. L. Johnson, editors.\\nConstraints, Languages, and Computation.\\nAcademic Press, 1994.\\n\\n\\nGert Smolka.\\nA Foundation for Concurrent Constraint Programming.\\nIn Jean-Pierre Jouannaud, editor, Constraints in Computational\\n  Logics, volume 845 of Lecture Notes in Computer Science, pages 50-72,\\n  Mnchen, Germany, 7-9 September 1994.\\n\\n\\nGert Smolka and Ralf Treinen.\\nRecords for Logic Programming.\\nIn International Conference and Symposium on Logic Programming,\\n  pages 240-254, November 1992.\\nHas appeared in the Journal of Logic Programming, April 1994.\\n\\n\\nGert Smolka and Ralf Treinen, editors.\\nDFKI Oz Documentation Series.\\nStuhlsatzenhausweg 3, D-66123 Saarbrcken, Germany, 1994.\\nDocumentation and System available through anonymous ftp from\\n  ps-ftp.dfki.uni-sb.de or through WWW from http://ps-www.dfki.uni-sb.de.\\n\\n\\nMitchell Wand.\\nA Simple Algorithm and Proof for Type Inference.\\nFundamenta Informaticae, 10:115-122, 1987.\\n\\n\\nAndrew K. Wright and Robert Cartwright.\\nA Practical Soft Type System for Scheme.\\nTechnical Report 93-218, Rice University, December 1993.\\n\\nFootnotes\\n\\n   Note that\\n\\n\\n\\n\\nis different from a\\n        named \\n\\n\\n\\nabstraction \\n\\n\\n\\n\\nbecause it is relational rather than functional,\\n   and also different to the Prolog program\\n!-- MATH: $p(u,v)\\\\ \\\\mbox{:-}\\\\ v=cons(x\\\\: u).$ -->\\n\\n\\n\\n ,\\n   because Prolog does not allow\\n   variables to be global wrt. a predicate\\n   but rather existentially quantifies x.\\n  The formal account of the derivation of type constraints from programs\\n   will be given in  . \\n  This simpler encoding of constructor trees not using arity\\n  constraints has been suggested by one of the referees.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nWe introduce constraints\\nnecessary for type checking a higher-order concurrent\\nconstraint language,\\nand solve them with an incremental algorithm.\\nOur constraint system extends rational unification\\nby constraints \\n\\n\\n\\n saying that ``x has at least the structure of y'',\\nmodelled by a weak instance relation between trees.\\nThis notion of instance has been carefully chosen\\nto be weaker than the usual one which\\nrenders semi-unification undecidable.\\nSemi-unification has more than once served to link\\nunification problems arising from type inference\\nand those considered in computational linguistics.\\nJust as polymorphic recursion corresponds to subsumption\\nthrough the semi-unification problem, our\\ntype constraint problem corresponds to weak\\nsubsumption of feature graphs in linguistics.\\nThe decidability problem for for feature\\n graphs has been settled by Drre . \\nIn contrast to Drre's, our algorithm is fully incremental\\nand does not refer to finite state automata.\\nOur algorithm also is a lot more flexible. It allows\\na number of extensions (records, sorts, disjunctive types,\\ntype declarations, and others)\\nwhich make it suitable for type inference\\nof a full-fledged programming language.\\nKeywords: type inference, weak subsumption,\\nunification, constraints, constraint programming\\n\\n\"],\n",
              " [\"\\n\\n  \\nDiscourse Processing Architecture\\n\\n\\nI will assume in this paper that a discourse is a sequence of\\nutterances produced (spoken or written) by one or more discourse\\nparticipants. Utterances are tokens of sentences or sentence\\nfragments with which the speakers communicate certain information, and\\nit is done in a context.  Utterance interpretation depends on\\nthe context, and utterance meaning updates the context.\\n\\n\\nA specification of the complex interdependencies involved in utterance\\ninterpretation is greatly facilitated if it is couched in a discourse\\nprocessing architecture that is both logically coherent and as closely\\nas possible an approximation of the human cognitive\\n architecture for discourse processing. What are the major modules of the architecture, and what\\ntypes of inferences do they support?  I claim that the most\\nfundamental separation is between the spaces of possibilities\\nand preferences.\\n\\n  Separating Combinatorics and Preferences \\n\\nThere is an assumption in computational linguistics that combinatorics\\nshould take precedence over preferences.  The wisdom is to maximize\\nthe combinatoric space of utterance interpretation and to keep a firm\\nline between this space and the other, preferential, space of\\ninterpretation.  Preferences are affected by computationally expensive\\nopen-ended commonsense inferences.  Combinatorics determine all and\\nonly possible interpretations, and preferences prioritize the\\n possibilities. Seen from another point of view, combinatorics are indefeasible -- that is, never overridden by\\ncommonsense plausibility, whereas preferences are defeasible --\\nthat is, can be overridden by commonsense plausibility. I will\\nhenceforth assume that the grammar subsystem consists only of\\n indefeasible possibilities, hence  monotonic, whereas the pragmatics subsystem consists mostly (or possibly entirely) of defeasible\\n preferences. \\n\\n\\nAn example of indefeasible rules of grammar in English is the\\nSubject-Verb-Object constituent order. The sentence Coffee drinks\\nSally uttered in a normal intonation cannot mean ``Sally drinks\\ncoffee'' despite the commonsense support.  An example of defeasible\\npreferences is the interpretation of the pronoun he in discourse\\n``John hit Bill. He was severely injured.'' The combinatoric\\nrule of pronoun interpretation would say that both John and Bill are\\npossible referents of he, while the preferential rule would say\\nthat Bill is preferred here because it is more plausible that the one\\nwho is hit gets injured rather than vice versa. Crucially, this\\npreference is overridden in certain contexts.  For instance, if Bill\\nis an indestructible cyborg, the preferred semantic value of he\\nwould shift to John.\\n\\n\\nThe inferential properties of the grammar\\n subsystem as a space of possibilities are well-illustrated in the so-called unification-based grammatical\\n formalisms (UBG). A UBG system consists of context-free phrase structure constraints and\\nunification constraints. Maxwell and Kaplan (1993) describe how the\\nconstraint interactions can be made efficient by exploiting the\\nfollowing properties of a UBG system: (1) monotonicity -- no\\ndeduction is ever retracted when new constraints are added, (2) independence -- no new constraints can be deduced when two systems\\nare conjoined, (3) conciseness -- the size of the system is a\\npolynomial function of the input that it was derived from, and (4)\\norder invariance -- sets of constraints can be processed in any\\n order without changing the final result. \\n\\n\\nThe inferential properties of the pragmatics\\n subsystem are much less understood. Its general features can be characterized as those of preferential\\nreasoning, a topic more studied in AI than in linguistics.  The\\npragmatics subsystem contains sets of preference rules that, in\\ncertain combinations, could lead to conflicting preferences. This\\nfundamental indeterminacy leads to the properties opposite from those\\nof the grammar subsystem: (1) nonmonotonicity -- preferences\\ncan be canceled when overriding preferences are added, (2) dependence -- new preferences may result when two pragmatic\\nsubsystems are conjoined, (3) explosion -- the system size is\\npossibly an exponential (or worse) function of the input that it was\\nderived from, and (4) order variance -- changing the order in\\nwhich sets of preferences are processed may also change the final\\nresult.  The key to a discourse processing architecture is to preserve\\nthe above computational properties of the grammar subsystem while\\nstriving for a maximal control of the preference interactions in the\\n pragmatics subsystem. \\n\\n\\nExisting logical semantic theories employing dynamic interpretation\\nrules (e.g., Kamp, 1981; Heim, 1982; Groenendijk and Stokhof, 1991;\\nKamp and Reyle, 1993) formalize the basic context dependence of\\nindefeasible semantics. While these theories predict the possible dynamic interpretations of utterances, they are not\\nconcerned with how to compute the relative preferences among them.\\nLascarides and Asher (1993) extend the Discourse Representation\\n Theory (DRT) (Kamp, 1981) with the interaction of defeasible rules for integrating a new utterance\\ncontent into the discourse information state. The input to their\\ndefeasible reasoning is a fully interpreted DR\\n Structure (DRS), with all the NPs already interpreted. The pragmatics subsystem I am concerned\\nwith here also includes the defeasible rules for NP interpretation and\\nconstituent attachments needed for DRS construction. The input to\\npragmatics in the present proposal is a much less specified logical\\nform, and pragmatics kicks in during DRS construction.\\n\\n\\n  \\nThe Processing Architecture\\n\\n\\n The discourse processing architecture that I will assume in the background of the remainder of  this paper is this. \\n\\n\\n\\n Let discourse be a sequence of utterances,\\n\\n\\n\\n\\n .\\nWe say that utterance utti defines a transition relation between the input context Ci-1 and the\\noutput context Ci. Context C is a multicomponent data\\n structure (see section ).  The transition takes place as follows:\\n\\n\\n\\n Let grammar G consist of rules of syntax and\\nsemantics that assign each utterance utti the initial logical\\n form  \\n\\n\\n\\n .\\n\\n\\n\\n\\nrepresents a disjunctive set of underspecified\\n formulas containing unresolved references, unscoped quantifiers, and vague\\nrelations. \\n\\n\\n\\nis the weakest formula that packages a family of formulas that covers the entire range of possible\\n interpretations of utti (see section ). \\n\\n\\n\\n Let pragmatics P consist of rules for specifying and\\ndisambiguating \\n\\n\\n\\nin context Ci-1. Ideally, P outputs the\\n single preferred interpretation \\n\\n\\n\\n(\\n\\n\\n\\nis subsumed by \\n\\n\\n\\nand there is no\\n\\n\\n\\nthat is preferred over \\n\\n\\n\\nand also subsumed by\\n\\n\\n\\n ), and integrating \\n\\n\\n\\ninto context Ci-1 produces\\nthe preferred output context Ci.  In a less felicitous case,\\nthe rules of P do not converge, resulting in multiple\\ninterpretations and output contexts.\\n\\n\\n  \\nContext\\n\\n\\nMy aim here is to introduce the basic components of the context C in\\nthe above discourse processing architecture that I assume in the\\nremainder of the paper.\\n\\n\\nContext Ci is a 6-tuple \\n\\n\\n\\n\\n consisting of the fast-changing components,\\n\\n\\n\\n\\n ,\\nsignificantly affected by the\\ndynamic import of utterances and the slow-changing components,\\n\\n\\n\\n\\nrelatively stable in a given stretch of discourse\\ninstance.  \\n\\n\\n\\nis the preferred interpretation (see section\\n ) of the last utterance utti in a logical form that preserves aspects of the syntactic structure of utti --\\nbest thought of as a short-term register of the surface structure of\\nthe previous utterance similar to the proposal by Sag and Hankamer\\n (1984).  Di is the discourse model -- a set of information states that the discourse has been about, which\\nalso incorporates the content of \\n\\n\\n\\n .\\nDi contains sets of\\nsituations, eventualities, entities, and relations among them,\\nassociated with the evolving event, temporal, and discourse\\n structures.  Ai is the attentional state -- a partial order of the entities and propositions in Di,  where the ordering is by salience.  Ai is separated from Di because the same Di may correspond to\\ndifferent variants of Ai depending on the particular sequence of\\nutterances in particular forms describing the same set of facts.\\n Ii is the set of indexical anchors -- the indexically accessible objects in the current discourse\\nsituation -- for instance, the values of indexical expressions such\\nas I, you, here, and now. The slow-changing components are\\n the linguistic knowledge L and  world knowledge K used by the discourse participants. Although we know that discourse participants never share\\nexactly the same mental state representing these components of the\\ncontext, there must be a significant overlap in order for a discourse\\nto be mutually intelligible. For the purpose of this paper, I will\\nsimply assume that context C is sufficiently shared by the\\nparticipants.\\n\\n\\nThe next section elaborates on the initial logical form \\n\\n\\n\\nthat\\nplays a crucial role of defining the grammar-pragmatics boundary in\\nthe discourse processing architecture.\\n\\n\\n\\n  \\nIndefeasible Semantics\\n\\n\\n The initial logical form (ILF)  \\n\\n\\n\\n represents the utterance's structure and meaning at the\\n grammar-pragmatics boundary. This section discusses the general features of ILF with examples.\\n\\n  General Considerations \\n\\nThere are specific proposals for the ILF \\n\\n\\n\\nin the computational\\nliterature (e.g., Alshawi and van Eijck, 1989; Alshawi, 1992; Alshawi\\nand Crouch, 1992; Hwang and Schubert, 1992a, 1992b; Pereira and\\nPollack, 1991).  Details in these proposals vary, but there is a\\nremarkable agreement on the general features.\\n\\n\\nThe ILF \\n\\n\\n\\ncontains ``vague'' predicates and functions\\nrepresenting what the utterance communicates.  Vague\\n predicates and functions represent various expression and construction types whose interpretation depends on the discourse context. They include\\nunresolved referring expressions such as the pronoun he,\\nunscoped quantifiers such as each, vague relations such as the\\nrelation between the nouns in a noun-noun compound, unresolved\\noperators such as the tense operator past and the mood operator\\nimperative, and attachment ambiguities such as for\\nPP-attachments. The idea can also be extended to underspecify lexical\\nsenses at the ILF level.  These predicates and functions generate\\n`assumptions' that need to be resolved or `discharged' in the union of\\nthe discourse and sentence contexts. The ILF is thus partial and\\nindefeasible -- partial because it does not always have a truth\\nvalue, and indefeasible because further contextual interpretations\\nonly prioritize possibilities and specify vagueness.\\n\\n\\nThe ILF \\n\\n\\n\\nalso represents aspects of the utterance's surface\\nstructure relevant to how the utterance communicates the\\ninformation content (e.g., the Topic-Focus\\n Articulation of Sgall et al., 1986). Such a syntax-semantics corepresentation could be achieved in either\\nof the two options: (1) the logical form is structured,\\nrepresenting aspects of phonological and surface syntactic structures\\nsuch as the grammatical functions of nominal expressions, linear\\norder, and topic-comment structure, or (2) the partial semantic\\nrepresentation and the phonological and syntactic structures are\\nseparately represented with mappings among corresponding parts.  In\\nthis paper, the choice is arbitrary as long as certain syntactic\\ninformation is available at the logical form.\\n\\n\\nThere is a general question of how far and how soon the\\nILF gets specified and disambiguated by the pragmatics. The above\\nexisting proposals in the computational literature assume that each\\nutterance is completely specified and disambiguated before the next\\nutterance comes in. This includes the integration of the utterance\\ncontent into the evolving discourse structure, event structure, and\\ntemporal structure in the context, as discussed by Lascarides and\\nAsher (1993). An utterance's complete interpretation is not in general\\navailable on the spot, however, and it often has to wait till some\\nmore information is supplied in the subsequent discourse (Grosz et\\nal., 1986). It is also possible that only the information concerning\\nthose entities that are significant or salient (or `in focus') in the\\ncurrent discourse need to be fully specified and\\n disambiguated.  The present discourse processing architecture allows such incremental and partial\\nspecification and disambiguation of the information state along\\ndiscourse progression though this perspective is not explored in any\\ntechnical detail here.\\n\\n\\nIn sum, the ILF represents the indefeasible semantics of an utterance\\nby leaving the following context-dependent interpretations\\nunderdetermined: reference of nominal expressions, modifier\\nattachments, quantifier scoping, vague relations, and lexical\\nsenses. The ILF also leaves open how the given utterance is integrated\\ninto the temporal, event, and discourse structures in the context.\\n\\n\\n  Our Working Formalism \\n\\nI will use a simplified ILF in this paper. It is an underspecified\\npredicate logic in a davidsonian style -- a version of QLF (Kameyama,\\n1995) without the aterm-qterm distinction.  The ILF for the utterance\\n``He made a robot spider'' is as follows:\\n\\n\\n\\n\\nIt contains the following vague predicates and  functions: \\n\\n\\n\\n unresolved unstressed pronoun ``he'' -- \\n\\n\\n\\n\\n\\n unscoped quantificational determiner ``a'' -- \\n\\n\\n\\n\\n\\n a vague relation for a noun-noun compound ``robot spider'' --\\n\\n\\n\\n\\n(a relation\\nbetween a spider entity and a robot property)\\n\\n\\n\\n unresolved past tense -- \\n\\n\\n\\n\\n\\n unresolved declarative mood -- \\n\\n\\n\\n\\n If the preferred interpretation of the utterance is that ``John'' made\\na robot shaped like a spider, we have the following DRS-like logical\\nform:\\n\\n\\n\\n\\nThe interpretation is complete when the content is integrated into the\\ndiscourse, event, and temporal structures in the context.  These\\nstructures are assumed to be in the discourse model D. The\\npragmatics subsystem must make all of the preferential decisions\\nincluding NP interpretation and operator interpretation as well as\\n contextual integration. \\n\\n\\n  \\nAmbiguity and Underspecification\\n\\n\\nThe initial logical form mixes both ambiguity and underspecification.\\nThe choice is largely arbitrary when the number of possible\\ninterpretations is exhaustively enumerable.  Whenever there are npossible interpretations for a linguistic item or construction type,\\nwe can have either (1) a disjunctive set of n interpretations \\n\\ni1,...,in,\\n from which the pragmatics chooses the best, or (2) one underspecified\\ninterpretation that the pragmatics further specifies.  Pragmatic\\ndisambiguation and specification involve exactly the same kind of an\\ninterplay of linguistic and commonsense preferences, and relative\\npreferences in disambiguation and specification are often\\ninterdependent.\\n\\n\\nConsider He made a robot spider with six legs.  There is a\\npreference for the interpretation ``a robot spider with six legs''\\nover the alternative ``a male person with six legs''. This preference\\nis overridden in certain contexts -- for instance, if the person is a\\nfictional figure who can freely change the number of legs to be two,\\nfour, or six, the alternative reading becomes equally plausible.  Note\\nthat the attachment disambiguation and pronoun interpretation are\\ninterdependent here.\\n\\n\\nWhen the number of possible interpretations cannot be exhaustively\\nenumerated, however, ambiguity and underspecification are not\\ninterchangeable, and we must posit an underspecified\\n relation as a semantic primitive.  A sufficient but not necessary condition for positing an underspecified\\n relation is this (Kameyama, 1995): \\nAn underspecified relation is posited when there is an open-ended set\\nof possible specific relations associated with a construction type,\\nand the interpretation is typically affected by ad hoc facts\\nknown in the discourse context.\\n\\n\\nA canonical example is the interpretation of noun-noun compounds such\\nas elephant pen. It could mean a pen shaped like an elephant, a\\npen with elephant pictures on the body, a pen with a small toy\\nelephant glued on the top, or, depending on the context, a pen that\\nthe speaker found on the ground when she was pretending to be an\\nelephant. All we can tell from the grammar of noun-noun compounds is\\nthat it is a pen that has some salient relation with elephants. It\\nmakes sense, then, to explicitly state in the grammar output the vague\\nnotion of ``some salient relation'' as a primitive. This is the basic\\nmotivation of the proposal for underspecified relations in the logical\\nform in the computational literature (e.g., Alshawi, 1990; Hobbs et\\nal., 1993).  The same thing goes with scope ambiguities. The number of\\npossible scopings is always bounded but possibly very large (on the\\norder of hundreds), and speakers are often unable to select a single\\nspecific scoping, so the grammar should defer assigning specific\\nscopings to a sentence and give it to pragmatics (Hobbs, 1983; Reyle,\\n1993; Poesio, 1993).\\n\\n\\nIn sum, with the ILF sealing off the space of grammatical reasoning,\\nthe present discourse processing architecture magnifies the importance\\nof pragmatics in utterance interpretation. Pragmatics achieves\\nanaphora resolution, attachment disambiguation, quantifier scoping,\\nvague relation specification, and contextual integration all in one\\nmodule. Is there a system in the chaos? That is the question we turn\\nto now.\\n\\n\\n\\n  \\nDefeasible Pragmatics\\n\\n\\nThis section discusses the features and examples of the defeasible\\nrules in the pragmatics subsystem.\\n\\n  General Considerations \\n\\nBy defeasible, I mean a conclusion that has to be retracted when\\nsome additional facts are introduced.  This characterizes the preferential aspect of utterance interpretation with the\\n nonmonotonicity property.  Grammatical reasoning is governed by the Tarskian notion of valid inference in standard logic -- ``Each model of the premises is also a model for\\n the conclusion.''  Pragmatic reasoning distinguishes among models as to their relevance or plausibility, and\\nis governed by the notion of plausible inference (Shoham, 1988) --\\n``Each most preferred model of the premises is a model for the\\nconclusion.''  The preference can be stated in terms of default rules\\nas well, so the general reasoning takes the form of ``as long as no\\nexception is known, prefer the default.''  In utterance\\ninterpretation, this form of reasoning chooses the best interpretation\\n from among the set of possible ones.  The present focus is the\\ninterpretation preferences of intersentential pronominal anaphora.\\n\\n\\n  \\nEarlier Computational Approaches to Pronoun\\nInterpretation\\n\\n\\nComputational research on pronoun interpretation has always recognized\\nthe existence of powerful grammatical preferences, but there are\\ndifferent views on their status in the overall processing\\narchitecture.  Hobbs (1978) discussed the relative merit of purely\\ngrammar-based and purely commonsense-based strategies for pronoun\\ninterpretation. His grammar-based strategy that accounts for 98% of\\na large number of pronouns in naturally occurring texts simply could\\nnot be extended to account for the remaining cases that only\\ncommonsense reasoning can explain. He settled in a ``deeper'' method\\nthat seeks a global coherence arguing that coreference can\\nbe determined as a side-effect of coherence-seeking interpretation.\\nThe abduction-based approach (Hobbs et al., 1993) is an example of\\nsuch a general inference system, where syntax-based preferences for\\ncoreference resolution are used as the last resort when other\\ninferences do not converge.\\n\\n\\nSidner's (1983) local focusing model used an attentional\\nrepresentation level to mediate the grammar's control of\\n discourse inferences. For each pronoun, there is an ordered list of potential referents\\ndetermined by local focusing rules, and the highest one that leads to\\na consistent commonsense interpretation of the utterance is\\nchosen. Common sense has a veto power over grammar-based focusing in\\nthe ultimate interpretation, but common sense is the last\\nresort, contrary to Hobbs's approach.  Carter (1987) implemented\\nSidner's theory combined with Wilks's (1975) preferential semantics,\\nand reported the success rate of 93% for resolving pronouns in a\\nvariety of stories -- of which only 12% relied on commonsense\\ninferences.\\n\\n\\nGrammar's role in the control of inferences was the original\\n motivation of the centering model (Joshi and Kuhn, 1979; Joshi and Weinstein, 1981).  The proposal was to use\\nthe monadic tendency of discourse (i.e., tendency to be\\ncentrally about one thing at a time) to control the amount of\\ncomputation required in discourse interpretation.  Grosz, Joshi, and\\nWeinstein (1983) proposed a refinement of Sidner's model in terms of\\ncentering, and highlighted the crucial role of pronouns in linking an\\nutterance to the discourse context.  Subsequent work on centering\\nconverged on an equally significant role of the main clause\\n SUBJECT (Kameyama, 1985, 1986; Grosz, Joshi, and Weinstein, 1986; Brennan, Friedman, and Pollard,\\n1987). Hudson D'Zurma (1988) experimentally verified that speakers had\\na difficulty in interpreting a discourse where a centering prediction\\nwas in conflict with commonsense plausibility, leading to a `garden\\npath' effect. An example from her experiment is: ``Dick had a jam\\nsession with Brad. He played trumpet while Brad played bass. ??He\\nplucked very quickly.''\\nCentering models the local attentional state management in an overall\\ndiscourse model proposed by Grosz and Sidner (1986).\\n\\n\\nThese computational approaches to discourse have recognized the\\nnon-truth-conditional effects on utterance interpretation coming\\n from the utterance's surface structure (i.e., phonological,\\nmorphological, and syntactic structures). Although this aspect of\\ninterpretation cannot be neglected in a discourse processing model,\\nits relevance to a logical model of discourse semantics and pragmatics\\nhas remained unclear. It is worth pointing out that discourse\\n pragmatics in the above computational approaches as well as in philosophy (e.g., Lewis, 1979; Stalnaker,\\n1980) has generally assumed a dynamic architecture. Would there be a\\npotential fit with the dynamic semantic theories in linguistics (e.g.,\\nKamp, 1981; Heim, 1982; Groenendijk and Stokhof, 1991) in a way that\\nforms a basis for an integrated logical model of discourse semantics\\nand pragmatics?  In this paper, I propose a pronoun interpretation\\nmodel taking ideas from both computational and linguistic\\ntraditions, and present it in such a way that it becomes tractable for\\nlogical implementation.\\n\\n\\n\\n  \\nPronoun Interpretation Preferences: Facts\\n\\n\\nPronoun interpretation must be carried out in an often vast space of\\npossibilities, somehow controlling the inferences with default\\npreferences coming from different aspects of the current context.\\nPronouns such as he, she, it and they can refer to\\nentities talked about in the current discourse, present in the current\\nindexical context, or simply salient in the model of the world\\nimplicitly shared by the discourse participants.  Since the problem\\nspace is vast and complex, we need to narrow it down to come to grips\\nwith interesting generalizations.  I will now limit our discussion to\\nthe interpretation of the anaphoric use of unstressed male\\nsingular third person pronoun he or him in English.\\n\\n  Survey and the Results \\n\\nIn 1993, I conducted a survey of pronoun interpretation preferences\\nusing the discourse examples shown in Table 1.  These examples were\\nconstructed to isolate the relevant dimensions of interest based on\\n previous work (see section ). \\n\\n\\nOne set of examples, A-H, involves pronouns that occur in the second\\nof two-sentence discourses. They were presented to competent (some\\nnonnative) speakers of English in the A-F-C-H-E-D-B-G order, avoiding\\nsequential effects of two adjancent similar examples. The speakers\\nwere instructed to read them with no special stresses on words, and to\\nanswer the who-did-what questions about pronouns in italics. The\\nanswer ``unclear'' was also allowed, in which case, the speaker was\\nencouraged to state the reason. The total number of the speakers was\\n47, of which 10 were nonlinguist natural language researchers and 4\\nwere nonnative but fluent English speakers.  The second set of\\nexamples, I-L, are longer discourses. They were given to disjoint\\nsets of native English speakers, none of whom are linguists.\\n\\n\\nThe examples fall under two general categories, as indicated in Table\\n1. One group isolates the grammatical effects by minimizing\\ncommonsense biases. In these examples, it is conjectured that there is\\nno relevant commonsense knowledge that affects the pronoun\\ninterpretation in question. The other group examines the commonsense effects of a specific causal knowledge of hitting and\\ninjuring in relation to the grammatical effects observed in the first\\ngroup.\\n\\n\\nTable 2 shows the survey results.  The \\n\\n\\n\\n\\n  significance for each example was computed by adding an evenly divided number of the ``unclear'' answers\\nto each explicitly selected answer, reflecting the assumption that an\\n``unclear'' answer shows a genuine ambiguity. Preference is considered\\nsignificant if p[.05, weakly significant if .05[p[.10,\\nand insignificant if .10[p. Insignificant preference is\\ninterpreted to mean ambiguity or incoherence. It follows from the\\nGricean Maxim that ambiguity must be avoided in order for an utterance\\nto be pragmatically felicitous. An example with an insignificant\\npreference is thus infelicitous, and should not be generated.\\n\\n\\nIt must be noted that the present survey results exhibit only one\\naspect of preferential interpretation -- namely, the final\\npreference reached after an unlimited time to think. They do not\\nrepresent the process of interepretation -- for instance, a\\nnumber of speakers commented that they had to retract the first\\nobvious choice in example I. This garden-path effect verified in\\nHudson D'Zurma's (1988) experiments does not show in the present\\nsurvey results.\\n\\n\\n  \\nDiscussion of the Results\\n\\n\\nThe present set of examples highlights four major sources of\\npreference in pronoun interpretation -- SUBJECT Antecedent\\nPreference, Pronominal Chain Preference, Grammatical Parallelism\\nPreference, and Commonsense Preference. These are stated at a\\ndescriptive level with no theoretical commitments. A theoretical\\naccount of the same set of facts will be given in section\\n . Each source of preference is discussed below. \\n\\n\\n SUBJECT Antecedent Preference. A hierarchy of the preferred intersentential antecedent of a pronoun has been proposed in the centering framework, which\\nbasically says that the main clause SUBJECT is preferred over the\\nOBJECT (Kameyama, 1985,1986; Grosz et al., 1986).  This preference is\\n confirmed in examples A and B. \\n\\n\\nThe consistency of this preference across examples A and B\\ndemonstrates that grammatical functions rather than thematic roles are\\nthe adequate level of generalization. In both A and B, the thematic\\nroles of Bill and John in the first sentence are agent and theme (or\\npatient), respectively, but the switch in grammatical functions by\\npassivization causes the preferred interpretation to switch\\naccordingly.\\n\\n\\nExample C demonstrates the defeasibility of this preference in the\\nface of the parallelism induced by the adverb too as a side\\neffect of an indefeasible conventional presupposition (see\\n section ). \\n\\n\\n Pronominal Chain Preference. This is the preference for a chain of pronouns across utterances to\\n corefer. Examples K and L are a minimal pair of structural effects without a\\ncommonsense bias.  Their contrast shows the effect of grammatical\\npositions.  The SUBJECT-SUBJECT chain of pronouns (example K)\\nsupports a significant coreference preference (p[.001), whereas the\\nOBJECT-SUBJECT chain (example L) supports a weakly significant noncoreference preference (.05[p[.10) indicating a parallelism\\neffect below.\\n\\n\\nExample I shows that the causal knowledge also in the end\\noverrides a stretch of SUBJECT pronominal chain, but as noted above,\\nthis example causes the speakers to first interpret the SUBJECT\\npronouns to corefer, then retract the choice due to the\\ninconsistency with a causal knowledge. This processing tendency\\nindicates that the grammatical preference is processed faster than the\\ncommonsense preference. We will come back to this issue later.\\n\\n\\nIn example J, the strong preference for a SUBJECT pronominal chain is\\nundermined by the indefiniteness of the referent (one of the\\nboys) that the generic causal knowledge supports and by the\\nadditional inference -- when one hits one of a group of boys, he\\nwould be revenged by the group.  The grammar-based preference and\\ncommon sense are in a tie here, showing a genuine ambiguity\\n(.50[p[.70).\\n\\n\\n Grammatical Parallelism Preference. There is a general preference for two adjacent utterances to be grammatically parallel.  The parallelism requires,\\nroughly, that the SUBJECTs of two adjacent utterances corefer and that\\nthe OBJECTs, if applicable, also corefer.  This preference is\\n demonstrated in example D that involves two pronouns. In example L, the parallelism preference overrides the pronominal chain preference.\\n\\n\\nExample E shows the defeasibility of the parallelism preference in the\\nface of the presupposition triggered by adverb back.  An ``x hit\\ny back'' event conventionally presupposes that a ``y hit x'' event has\\npreviously occurred, leading to the near-unanimous interpretation\\n ``Bill hit John back.'' \\n\\n\\n Commonsense Preference. Examples F-H illustrate the effect of a simple causal knowledge that dictates\\nthe final interpretation over and above the grammatical preferences.\\nIn example F, the SUBJECT Antecedent Preference is defeated by an\\ninference derived from the generic causal knowledge -- ``when X hits\\nY, Y is normally hurt,'' and ``being injured is being hurt.'' Since\\nthe example involves some ``normal'' fellows called John and Bill, it\\napplies with full force (46/47).\\n\\n\\nExamples G and H show what happens to this baseline default when the\\ndescribed event involves some special individuals (fictitious or\\nnonfictitious) that the speakers have some knowledge about. In example\\nH, the preferred interpretation (34/47) swings to the one where the\\nnormal fellow, John, is injured as a result of attempting to assault\\n the indestructible cyborg.  The cyborg also could have been injured (6/47) (because the movie showed\\nthat it can be destroyed after all).  In example G, John\\nattempts to assault a warm-blooded real person, Arnold, who seems a\\nlittle stronger than normal fellows.  Here, more speakers thought that\\nJohn was injured (24/47) than Arnold was (13/47), but this preference\\nis insignificant (.10[p[.20). It reflects the indeterminacy of\\nwhether Arnold is a normal fellow or not, which affects the\\n applicability of the generic causal knowledge. \\n\\n\\n  Descriptive Generalizations \\n\\nTable 3 summarizes the preference predicted by each of the four\\nsources discussed above and the final outcome verified in the\\nsurvey. We see the following general patterns of conflict\\n resolution: \\n1.\\nConventional Presuppositions\\n(triggered by adverbs in examples C and E) and Commonsense Preferences\\n(examples F, G, and H) dictate the final preference.\\n2.\\nGrammatical Preferences take charge in the absence of relevant\\nCommonsense Preferences (examples A-E, K, and L).\\n3.\\nThe SUBJECT Antecedent Preference overrides the Grammatical\\nParallelism Preference when in conflict (see examples A and B), and\\nboth are in turn stronger than the Pronominal Chain Preference\\n(example L).\\n\\n\\n\\n-- due to the conventional presupposition triggered by adverb too.\\n\\n\\n\\n\\n-- due to the conventional presupposition triggered by adverb\\nback.\\n\\n\\n\\n\\n-- Tommy is the first choice, which is later retracted.\\n\\n\\n The cases of indeterminate final preference in examples G and J are worth noting. This kind of an indeterminate preference is infelicitous and uncooperative, which\\nshould be avoided in discourse generation. The indeterminacy in\\nexample G is due to the indeterminacy of Arnold being a normal person\\nsubject to injury or an abnormally strong person who would not let\\nhimself be injured. The indeterminacy in example J is due to the\\nconflict between the general causal knowledge about an injury caused\\nby hitting and the insalience of an indefinite referent as a possible\\npronominal referent.\\n\\n\\n\\n  \\nPronoun Interpretation Preferences: Account\\n\\n\\nFour major sources of preference have been identified in the above\\npronoun interpretation examples. I propose that these sources\\ncorrespond to the data structures in the different context components\\n outlined in section .  The context components the most relevant to the present discussion are the attentional state A, the\\nLF register \\n\\n\\n\\n ,\\nand the discourse model D.\\n\\n\\nThe main thrust of the present account is the general interaction of\\n preferences that apply on different context components. It explains the basic fact that preferences may or\\nmay not be determinate. The present perspective of preference\\ninteractions also extends and explains the role of the attentional\\nstate in Grosz and Sidner's (1986) discourse theory.\\n\\n  The Role of the Attentional State \\n\\nA discourse describes situations, eventualities, and entities,\\ntogether with the relations among them. The attentional\\n state A represents a dynamically updated  snapshot of their salience. We thus assume the property salient to be a primitive representing the partial\\n order among a set of entities in A. The property salient is gradient and relative.  A certain absolute degree of salience may not be achieved by any entities in a given A, but there\\nis always a set of maximally salient entities, which is often,\\n but not necessarily, a singleton set. Thus it is crucial that a rule about the single maximally salient entity in a given A is only sometimes\\ndeterminate.\\n\\n\\nWe will now recast some elements of the centering model in the present\\ndiscourse processing architecture.  In the input context Ci-1 for\\nutterance utti, the form and content (\\n\\n\\n\\n\\n )\\nof the\\nimmediately preceding utterance utti-1 occupy an especially\\nsalient status.  The entities realized in utti-1 are among the\\nmost salient subpart of Ai-1. I assume that this is achieved by a\\ngeneral A-updating mechanism. One of the entities in Ai-1 may\\nbe the \\n\\nCenteri-1, what the current discourse is centrally\\n about, hence the high salience: \\nCENTER\\nThe Center is normally more salient than other entities in\\nthe same attentional state.\\n\\n\\nAt least two default linguistic hierarchies are relevant to the\\n dynamics of salience One is the grammatical function hierarchy (GF ORDER), and the other is the nominal expression type\\n hierarchy (EXP ORDER).  The GF ORDER in utti predicts the relative salience of entities in the\\n output attentional state Ai whereas the EXP ORDER in uttipredicts the relative salience of entities assumed in the input attentional state Ai-1.  EXP ORDER is also crucial to the management of the Center (EXP CENTER):\\nGF ORDER:\\nGiven a hierarchy, [ SUBJECT ] OBJECT ] OBJECT2 ]OTHERS], an entity realized by a higher ranked phrase is normally\\nmore salient in the output attentional state.\\nEXP ORDER:\\nGiven a hierarchy, [ ZERO\\nPRONOMINAL ] PRONOUN ] DEFINITE NP ] INDEFINITE\\n NP],  an entity realized by a higher-ranked expression type is normally more salient in the input attentional state.\\nEXP CENTER:\\nAn expression of the highest ranked type\\nnormally realizes the Center in the output attentional state.\\nEXP CENTER can be interpreted in two ways. One computes the\\n``highest-ranked type'' per utterance, sometimes allowing a\\nnonpronominal expression type to output the Center. The other takes it\\nto be fixed, namely, only the pronominals. The choice is empirical.\\nIn this paper, I will take the second interpretation.\\n\\n\\nSince matrix subjects and objects cannot be omitted in\\n English,  the highest-ranked expression type is the (unstressed) pronoun (see\\nKameyama, 1985:Ch.1).  From EXP ORDER, it follows that a pronoun normally realizes a maximally salient entity in the input\\nattentional state.  A pronoun can also realize a submaximally salient\\nentity if this choice is supported by another overriding preference.\\nThe grammatical features of pronouns also constrain the range of\\npossible referents -- for instance, a he-type entity is a male\\nagent. The maximal salience thus applies on the suitably restricted\\nsubset of the domain for each type of pronoun.\\n\\n\\nThe interactions of the above defeasible rules -- CENTER, GF ORDER,\\nEXP ORDER, and EXP CENTER -- account for various descriptive\\ngeneralizations. First, the SUBJECT Antecedent Preference follows from\\nGF ORDER and EXP ORDER -- SUBJECT is the highest ranked GF in the\\nfirst utterance, and a pronoun in the second utterance realizes the\\nmaximally salient entity in the input A.  Second, the coreference\\nand noncoreference preferences in pronominal chains are accounted for.\\nThe strong coreference preference for a SUBJECT-SUBJECT pronominal\\nchain (example K) comes from the fact that a SUBJECT Center is the\\nsingle maximally salient entity, which leads to a determinate\\n preference. In contrast, an OBJECT Center competes with the SUBJECT non-Center for the maximal salience,\\n which leads to an indeterminate preference based on salience alone (example L).  The indeterminacy is resolved, to some extent, by the Grammatical Parallelism Preference\\n (section ). \\n\\n\\nThe center transition types of ``establishing'' and ``chaining''\\n(Kameyama, 1985,1986) result from the interactions of CENTER, EXP\\n ORDER, and EXP CENTER. The Center is ``established'' when a pronoun picks a salient non-Center in the input context and makes it the Center in\\nthe output context.  It is ``chained'' when a pronoun picks the Center\\nin the input context and makes it the Center in the output context.\\nExamples A-H are thus concerned with Center-establishing pronouns,\\nwhereas examples I-L are concerned with Center-chaining\\npronouns. These transition types are not the primitives that directly\\ndrive preferences, however.\\n\\n\\n  \\nThe Role of the LF Register\\n\\n\\nThe grammatical parallelism of two adjacent utterances in discourse\\naffects the preferred interpretation of pronouns (Kameyama, 1986),\\ntense (Kameyama, Passonneau, and Poesio, 1993), and ellipsis (Pruest,\\n1992; Kehler, 1993). This general tendency warrants a separate\\nstatement.  Parallelism is achieved, in the present account, by a\\ncomputation on the pair of logical forms, one in the LF register in\\nthe context, and the other being interpreted.\\nPARA:\\nThe LF register in the input context and the ILF being\\n interpreted seek maximal parallelism. The present perspective of rule interaction explains the\\n``property-sharing'' constraint on Center-chaining (Kameyama, 1986)\\nas follows.  GF ORDER, EXP ORDER, and PARA join forces to create a\\nstrong grammatical preference for SUBJECT-SUBJECT coreference\\n(examples D,K). When they are in conflict, that is, when the maximally\\nsalient entity is not in a parallel position, PARA is defeated\\n(examples A,B). When maximal salience is indeterminate, the\\nparallelism preference affects the choice (example L), leading to a\\nnoncoreference preference for an OBJECT-SUBJECT pronominal chain.\\n\\n\\n  The Role of the Discourse Model \\n\\n The discourse model contains a set of information states about situations, eventualities, entities, and the\\nrelations among them. It also contains the evolving discourse\\nstructure, temporal structure, and event structure. Both linguistic\\nsemantics and commonsense preferences apply on the same discourse\\nmodel.\\n\\n\\n Lexically Triggered Presuppositions. Adverbs too and back trigger conventional presuppositionsabout the input discourse model. These presuppositions are part of lexical semantics, thus indefeasible.\\n\\n\\nAdverb too triggers a presupposition that appears to seek\\nparallelism between an utterance in the context and the utterance\\nbeing interpreted. This is actually due to a general similarity\\npresupposition associated with too.  Consider each of the\\nfollowing utterances immediately preceding ``John hit Bill too'':\\n``Mary hit Bill'', ``John hit Mary'', ``Mary kicked Bill'', ``John\\nkicked Mary'', ``Mary hit Jane'', and `John called Bill''. What's\\nconstrued as `similar' in each case is a function of the particular\\nutterance pair, and intuitively, preferred pairs support more\\nsimilarities. Thus similarity comes in degrees, and a parallel\\ninterpretation is due to the preference for a maximal similarity.\\n\\n\\nAdverb back triggers a presupposition for a reverse\\nparallelism. That is, the utterance ``Bill hit John back'' presupposes\\nthat it occurred after ``John hit Bill''.\\n\\n\\nCommonsense Knowledge. In contrast to the above rules that\\nbelong to the linguistic knowledge, the commonsense\\n knowledge consists of all that an ordinary speaker knows about the world and life. Formalizing common\\nsense is a major research goal of AI, where nonmonotonic reasoning has\\nbeen intensively studied. My goal here is not to propose a new\\napproach to commonsense reasoning but simply to highlight its\\n interaction with linguistic pragmatics in the overall pragmatics subsystem. We know one thing for sure -- there\\nwill be a relatively small number of linguistic pragmatic rules that\\nsystematically interact with an open-ended mass of commonsense\\nrules. Since the linguistic rules can be seen to control\\ncommonsense inferences, our aim is to describe the former as fully as\\npossible, and specify how the ``control mechanism'' works. The\\ncommonsense rules posited in connection to the examples in this paper\\nare thus meant to be exemplary. There will be different rules for each\\nnew example and domain to be treated. The linguistic rules, however,\\nshould be stable across examples and domains.\\n\\n\\nThe single powerful causal knowledge at work in our examples is that\\nhitting may cause injury on the hittee but less likely on the hitter:\\nHIT:\\nWhen an agent x hits an agent y, y is normally hurt.\\nThe effects of the Terminator and Arnold indicate that the\\napplicability of the HIT rule depends on the normality of the agents\\ninvolved. Relevant knowledge includes things like: An agent is\\nnormally vulnerable, Arnold is a normal agent or an abnormally strong\\nagent, and Terminator is an abnormally strong agent.\\n\\n\\n  Account of the Rule Interactions \\n\\n We now state the preference interaction patterns observed in Table 3 above. The SUBJECT Antecedent Preference and Pronominal Chain Preference result from\\nCENTER, GF ORDER, EXP ORDER, and EXP CENTER.  These are the defeasible\\nAttentional Rules (ATT) stating the preferred attentional state\\ntransitions. The Grammatical Parallelism Preference is PARA. This is\\nan example of the defeasible LF Rules (LF) stating the preferred\\nLF transitions. Conventional presuppositions triggered by too\\nand back are examples of the indefeasible Semantic Rules\\n(SEM) in the grammar constraining the interpretation in the discourse\\nmodel. The causal knowledge of hitting is HIT, with associated\\nknowledge ETC about agents, Terminator, and Arnold. These are examples\\nof the defeasible Commonsense Rules (WK) stating the preferred\\ndiscourse model. Table 4 identifies the rules that dominate the final interpretation in examples A-L.\\n\\n\\nRules: ATT={CENTER, GF ORDER, EXP ORDER, EXP CENTER},\\nLF={PARA}, WK={HIT, ETC}, SEM={TOO, BACK}.\\n\\n\\nGeneral Features. The first distinction among these rules is\\n defeasibility. The SEM rules are indefeasible whereas all other rules are defeasible. It is predicted that\\nindefeasible rules override all defeasible rules, as verified in\\nexamples C and E.\\n\\n\\nWhat factor determines the interaction pattern among the defeasible\\nrules?  The three context components -- discourse model D,\\nattentional state A, and LF register \\n\\n\\n\\n-- all have their\\npreferred transitions. The D preference results from proposition-level (or ``sentence-level'')\\n inferences directly determining the preferred model whereas the A and LF preferences\\nresult from entity-level (or ``term-level'')\\n inferences only indirectly determining the preferred model. We have seen that proposition-level\\npreferences, if applicable, generally override entity-level\\npreferences, albeit with a varying degree of difficulty.\\n\\n\\nTake two examples: (1) ``John met Bill. He was injured.'' and\\n(2) ``John hit Bill. He was injured.''  In (1), the ATT and LF\\npreference that the pronoun refers to John indirectly leads to the\\npreference that John was injured, which becomes the overall\\npreference in the absence of relevant WK rules.  In (2), relevant WK\\nrules directly support a proposition-level preference, Bill was\\ninjured, which wins out (with a varying degree of difficulty). These\\n``flows of preference'' during an utterance interpretation are\\nillustrated below:\\n(1)\\n\\n[S [NP he]:{John]Bill} \\n\\n\\n\\n\\n ]\\n\\n\\n\\n\\nJohn was injured\\n(2)\\n\\n[S [NP he]:{John]Bill} \\n\\n\\n\\n\\n ]:{Bill\\nwas injured ] John was injured} \\n\\n\\n\\n\\nBill was injured.\\n\\n\\n Conflict Resolution Patterns. We see a straightforward overriding pattern in examples A-H involving ``Center-establishing'' pronouns: ATT overrides LF, and\\nWK overrides ATT and LF.  Such an overriding relation can be\\nseen as a dynamic updating operation (;) (van Benthem et al., 1993)\\n-- preferences are evaluated in turn, the later ones overriding the\\n earlier ones: LF;ATT;WK. It may be the general pattern of ``changing preferences'' during\\nutterance interpretation.\\n\\n\\nExamples I-L involving ``Center-chaining'' pronouns show more or\\nless the same pattern except that the overriding gets more difficult\\nin some cases.  It is more difficult when a SUBJECT pronoun chain\\nsupports a single maximally salient entity as in example I. This shows\\nthat the LF and ATT preferences in fact join forces to interact with\\nthe WK preferences.  This intuition is expressed with brackets:\\n\\n[LF;ATT];WK.  The ``retraction'' observed in example I still fits\\nthis pattern, but the increased difficulty in overriding is only\\nimplicit.\\n\\n\\nLascarides and Asher (1993) illustrate patterns of defeasible rule\\n interactions and the Penguin Principle defined below ( \\n\\n\\n\\nmeans ``if \\n\\n\\n\\n ,\\nthen indefeasibly \\n\\n\\n\\n ,'' and \\n\\n\\n\\n\\nmeans ``if \\n\\n\\n\\n ,\\nthen\\nnormally \\n\\n\\n\\n  .''): \\nNixon Diamond\\nA conflict is unresolved resulting in an\\nambiguity or incoherence:\\n\\n\\n\\n\\n .\\n\\n\\nPenguin Principle\\nA conflict is resolved by the more specific principle\\n defeating the more general one: \\n\\n\\n\\n\\nOn their account, any resolution of a conflict between two defeasible\\nrules should be a case of the Penguin Principle.  Does it explain all\\nthe conflict resolution patterns observed in pronoun interpretation?\\n\\n\\nThe Penguin Principle explains some of the conflict resolution\\npatterns -- for instance, the knowledge about specific agents,\\nTerminator and Arnold, override the generic causal knowledge about\\nhitting (examples G and H).  There may also be a remote conceptual\\nconnection between the Penguin Principle and the pattern \\n\\n[LF;ATT];WKin the following line -- grammatical preferences (ATT and LF) tend to\\nbe more abstract than commonsense preferences (WK) about particular\\ntypes of eventualities, so the more specific support wins (Kameyama et\\nal., 1993). However, the LF, ATT, and WK rules apply on different data\\nstructures, and cannot always be reduced to an indefeasible\\nimplication (\\n\\n\\n\\n\\n )\\nas required in the Penguin\\nPrinciple. For instance, hittee(x) can be \\n\\nsubject(x) or \\n\\n\\n\\n\\ndepending on the sentence structure, so we cannot say that\\nhittee(x) implies \\n\\n\\n\\n\\nto derive the overriding pattern\\nin example F. What additional kinds of conflict resolution inferences\\ndo we have then?\\n\\n\\nThere are two additional conflict resolution patterns observed in the\\npresent examples, which I will call the Indefeasible\\n Override and the Defeasible  Override, defined below: \\nIndefeasible Override\\nAn indefeasible principle overrides a\\ndefeasible one: \\n\\n\\n\\n\\nDefeasible Override\\nGiven an explicit overriding relation, one\\ndefeasible principle defeats another (even when \\n\\n\\n\\n\\n ):\\n\\n\\n\\n\\n .\\n\\n\\nThe Indefeasible Override follows from the monotonicity of classical\\nimplication (\\n\\n\\n\\n\\n ),\\nand is an inherent principle in any nonmonotonic logic. It predicts\\nthe fact that the SEM rules override all the defeasible rules\\n(examples C and E).  The Defeasible Override captures a certain a\\npriori given ``ranks'' or ``priorities'' among different sources of\\ninformation, using the dynamic override (;) operator, where\\n\\n\\n\\n\\nmeans ``\\n\\n\\n\\noverrides \\n\\n\\n\\n .''  It is motivated by\\nthe view that preferences come from different sources, and are\\nassociated with different ``degrees of defeasibility'' not necessarily\\n in terms of the Penguin Principle.  It enables us to state the override pattern \\n\\n[LF;ATT];WK while allowing a varying\\ndegree of difficulty for WK's overriding. I hope to define a logical\\nsystem that axiomatizes these conflict resolution inferences.\\n\\n\\n\\n  Further Questions \\n\\nA number of questions related to the present topic have not been\\ndiscussed. The first are logical questions.  What are the\\n connections with update logics (e.g., Veltman, 1993)? We can see that the grammar subsystem supports straight updating, whereas the pragmatics subsystem supports preferential updating or upgrading (van Benthem et al., 1993).\\nThe preference interaction patterns discussed here can perhaps be\\nformulated as fine-grained upgrading inferences during utterance\\ninterpretation within the proposed utterance interpretation\\n architecture. Can my proposal be couched in a system of preferential dynamic logic that combines elements of dynamic semantic theories and preferential models\\n(e.g., McCarthy, 1980; Shoham, 1988)?  Does the context as a\\nmulticomponent data structure proposed here also support the general\\ncontextual inferences such as lifting in the context\\n logic (e.g., McCarthy, 1993; Buvac and Mason, 1993)?\\n\\n\\nThere are also computational questions.\\nDoes the proposed discourse processing architecture with explicit\\ncontextual control of inferences actually help manage the\\ncomputational complexity of the nonmonotonic reasoning in the\\npragmatic rule interactions?\\n\\n\\nFinally, a cognitive question -- Does the proposed discourse\\nprocessing architecture naturally extend to a more elaborate\\nmany-person discourse model that addresses the issue of coordinating\\ndifferent private contexts (e.g., Perrault, 1990; Thomason,\\n1990; Jaspars, 1994)?\\n\\n\\n  Conclusions \\n\\nA discourse processing architecture with desirable computational\\nproperties consists of a grammar subsystem representing the space of\\npossibilities and a pragmatics subsystem representing the space of\\npreferences. Underspecified logical forms proposed in the\\ncomputational literature define the grammar-pragmatics boundary.\\nUtterance interpretation induces a complex interaction of defeasible\\nrules in the pragmatics subsystem. Upon scrutiny of a set of examples\\ninvolving intersentential pronominal anaphora, I have identified\\ndifferent groups of defeasible rules that determine the preferred\\ntransitions of different components of the dynamic context.  There are\\ngrammatical preferences inducing fast entity-level inferences only\\nindirectly suggesting the preferred discourse model, and commonsense\\npreferences inducing slow proposition-level inferences directly\\ndetermining the preferred discourse model. The attentional state in\\nthe context supports the formulation of attentional rules that\\nsignificantly affect pronoun interpretation preferences. The observed\\npatterns of conflict resolution among interacting preferences are\\npredicted by a small set of inference patterns including the one that\\nassumes an explicitly given overriding relation between rules or rule\\ngroups. In general, I hope that this paper has made clear some of the\\nactual complexities of interacting preferences in linguistic\\npragmatics, and that the discussion has made them sufficiently sorted\\n out for further logical implementations. \\n\\n\\n  References \\n\\nby2em\\n\\n\\nAlshawi, Hiyan. 1990. Resolving Quasi Logical Forms.\\nComputational Linguistics, 16(3), 133-144.\\n\\n\\nAlshawi, Hiyan. ed. 1992. The Core Language Engine, The MIT\\nPress, Cambridge, MA.\\n\\n\\nAlshawi, Hiyan, and Richard Crouch. 1992. Monotonic Semantic\\nInterpretation.  In Proceedings of the 30th Annual Meeting of the\\nAssociation for Computational Linguistics, Newark, DE, 32-39.\\n\\n\\nAlshawi, Hiyan, and Jan van Eijck. 1989.  Logical Forms in the Core\\nLanguage Engine.  In Proceedings of the 27th Annual Meeting of\\nthe Association for Computational Linguistics, Vancouver, Canada,\\n25-32.\\n\\n\\nAsher, Nicholas and Michael Morreau. 1993. Commonsense Entailment: A\\nModal Theory of Nonmonotonic Reasoning. In Proceedings of the\\nInternational Joint Conference on Artificial Intelligence, Chambray,\\nFrance, 387-392.\\n\\n\\nvan Benthem, Johan, Jan van Eijck, and Alla Frolova. 1993. Changing\\nPreferences. Report CS-R9310. Institute for Logic, Language and\\nComputation, University of Amsterdam.\\n\\n\\nBrennan, Susan, Lyn Friedman, and Carl Pollard. 1987.  A Centering\\nApproach to Pronouns.  In Proceedings of the 25th Annual Meeting\\nof the Association for Computational Linguistics, 155-162.\\n\\n\\nBuvac, Sasa and Ian Mason. 1993. Propositional Logic of\\nContext. In Proceedings of the 11th National Conference on\\nArtificial Intelligence, 412-419.\\n\\n\\nCarter, David. 1987. Interpreting Anaphors in Natural Language\\nTexts. Ellis Horwood, Chichester, Sussex, UK.\\n\\n\\nGrdenfors, Peter and David Makinson. 1994. Nonmonotonic Inference\\nBased on Expectations. Artificial Intelligence, 65, 197-245.\\n\\n\\nGroenendijk, Jeroen and Martin Stokhof. 1991. Dynamic Predicate Logic. Linguistics and Philosophy, 14, 39-100.\\n\\n\\nGrosz, Barbara, Aravind Joshi, and Scott Weinstein. 1983.  Providing a Unified\\nAccount of Definite Noun Phrases in Discourse.  In Proceedings of\\nthe 21st Meeting of the Association of Computational Linguistics,\\nCambridge, MA, 44-50.\\n\\n\\nGrosz, Barbara, Aravind Joshi, and Scott Weinstein. 1986.  Towards a\\ncomputational theory of discourse interpretation.  Unpublished\\nmanuscript. [The final version to appear in Computational\\nLinguistics under the title ``Centering: A Framework for Modelling\\nthe Local Coherence of Discourse'']\\n\\n\\nGrosz, Barbara and Candy Sidner. 1986. Attention, Intention, and the Structure\\nof Discourse. Computational Linguistics, 12(3), 175-204.\\n\\n\\nHeim, Irene. 1982. The Semantics of Definite and Indefinite Noun\\nPhrases, Ph.D. Thesis, University of Massachusetts, Amherst.\\n\\n\\nHobbs, Jerry. 1978. Resolving Pronoun References. Lingua, 44,\\n311-338. Also in B. Grosz, K. Sparck-Jones, and B. Webber, eds., Readings in Natural Language Processing, Morgan Kaufmann, Los Altos,\\nCA, 1986, 339-352.\\n\\n\\nHobbs, Jerry. 1983. An Improper Treatment of Quantification in\\nOrdinary English. In Proceedings of the 21st Meeting of the\\nAssociation of Computational Linguistics, Cambridge, MA, 57-63.\\n\\n\\nHobbs, Jerry, Mark Stickel, Doug Appelt, and Paul Martin. 1993.\\nInterpretation as Abduction. Artificial Intelligence, 63, 69-142.\\n\\n\\nHudson D'Zmura, Susan. 1988. The Structure of Discourse and\\nAnaphor Resolution: The Discourse Center and the Roles of Nouns and\\nPronouns. Ph.D. Thesis, University of Rochester.\\n\\n\\nHwang, Chung Hee, and Lenhart K. Schubert. 1992a. Episodic Logic: A\\nComprehensive Semantic Representation and Knowledge Representation for\\nLanguage Understanding. Technical Report, Dept. of Computer Science,\\nUniversity of Rochester, Rochester, NY.\\n\\n\\nHwang, Chung Hee, and Lenhart K. Schubert. 1992b. Episodic Logic: A\\nSituational Logic for Natural Language Processing. In P. Aczel,\\nD. Israel, Y. Katagiri, and S. Peters, eds., Situation Theory and\\nits Applications, Volume 3, CSLI, Stanford, CA.\\n\\n\\nJaspars, Jan. 1994. Calculi for Constructive Communication,\\nPh.D. Thesis, University of Tilberg.\\n\\n\\nJoshi, Aravind, and Steve Kuhn. 1979. Centered Logic: The Role of Entity\\nCentered Sentence Representation in Natural Language Inferencing. In\\nProceedings of International Joint Conference on Artificial\\nIntelligence, Tokyo, Japan, 435-439.\\n\\n\\nJoshi, Aravind, and Scott Weinstein. 1981. Control of Inference: Role of\\nSome Aspects of Discoruse Structure -- Centering. In Proceedings\\nof International Joint Conference on Artificial Intelligence,\\nVancouver, Canada, 385-387.\\n\\n\\nKameyama, Megumi. 1985. Zero Anaphora: The Case of Japanese.\\nPh.D. Thesis, Stanford University.\\n\\n\\nKameyama, Megumi. 1986. A Property-sharing Constraints in Centering.\\nIn Proceedings of the 24th Annual Meeting of\\nthe Association for Computational Linguistics, New York, NY, 200-206.\\n\\n\\nKameyama, Megumi. 1994a. Indefeasible Semantics and Defeasible\\nPragmatics. CWI Report CS-R9441 and SRI Technical Note 544.\\n\\n\\nKameyama, Megumi. 1994b. Stressed and Unstressed Pronouns:\\nComplementary Preferences. In P. Bosch and R. van der Sandt,\\neds., Focus and Natural Language Processing, Institute for Logic\\nand Linguistics, IBM, Heidelberg, 475-484.\\n\\n\\nKameyama, Megumi. 1995.  The Syntax and Semantics of the Japanese\\nLanguage Engine. In R. Mazuka and N. Nagai, eds., Japanese\\nSentence Processing, Lawrence Erlbaum Associates, Hillsdale, NJ,\\n153-176.\\n\\n\\nKameyama, Megumi, Rebecca Passonneau, and Massimo Poesio.  Temporal\\nCentering. 1993. In Proceedings of the 31st Meeting of the\\nAssociation of Computational Linguistics, Columbus, OH, 70-77.\\n\\n\\nKamp, Hans. 1981. A Theory of Truth and Semantic Representation. In\\nJ. Groenendijk, T. Janssen, and M. Stokhof, eds., Formal Methods\\nin the Study of Language, Mathematical Center, Amsterdam, 277-322.\\n\\n\\nKamp, Hans, and Uwe Reyle. 1993. From Discourse to Logic, Kluwer\\nAcademic Publishers, Dordrecht.\\n\\n\\nKehler, Andrew. 1993. The Effect of Establishing Coherence in Ellipsis\\nand Anaphora Resolution.\\nIn Proceedings of the 31st Meeting of the\\nAssociation of Computational Linguistics, Columbus, OH, 62-69.\\n\\n\\nLascarides, Alex, and Nicholas Asher. 1993.  Temporal Interpretation,\\nDiscourse Relations, and Commonsense Entailment. Linguistics and\\nPhilosophy, 16, 437-493.\\n\\n\\nLewis, David. 1979. Scorekeeping in a Language Game. Journal of\\nPhilosophical Logic, 8, 339-359.\\n\\n\\nLifschitz, Vladimir. 1988. Circumscriptive Theories: A Logic-Based\\nFramework for Knowledge Representation. Journal of Philosophical\\nLogic, 17(4), 391-442.\\n\\n\\nMaxwell, John, and Ronald Kaplan. 1993. The Interface between Phrasal\\nand Functional Constraints. Computational Linguistics, 19(4),\\n571-590.\\n\\n\\nMcCarthy, John. 1980. Circumscription-A Form of Non-monotonic\\nReasoning. Artificial Intelligence, 13(1,2), 27-39.\\n\\n\\nMcCarthy, John. 1986. Applications of Circumscription to Formalizing\\nCommonsense Knowledge. Artificial Intelligence, 28, 89-116.\\n\\n\\nMcCarthy, John. 1993. Notes on Formalizing Context. In Proceedings of the International Joint Conference on Artificial\\nIntelligence, Chambray, France, 555-560.\\n\\n\\nPereira, Fernando and Martha Pollack. 1991. Incremental\\nInterpretation. Artificial Intelligence, 50, 37-82.\\n\\n\\nPereira, Fernando, and David Warren. 1980. Definite Clause Grammars\\nfor Language Analysis. Artificial Intelligence, 13, 231-278.\\n\\n\\nPerrault, C. Ray. 1990. An Application of Default Logic to Speech Act\\nTheory. In P. Cohen, J. Morgan, and M. Pollack, eds., Intentions\\nin Communications, MIT Press, Cambridge, MA, 161-185.\\n\\n\\nPoesio, Massimo. 1993. Discourse Interpretation and the Scope of\\nOperators. Ph.D. Thesis, University of Rochester.\\n\\n\\nPruest, Hub. 1992. On Discourse Structuring, VP Anaphora and Gapping.\\nPh.D. Thesis, University of Amsterdam.\\n\\n\\nReyle, Uwe. 1993. Dealing with Ambiguities by Underspecification:\\nConstruction, Representation, and Deduction. Journal of\\nSemantics, 10(2).\\n\\n\\nSag, Ivan, and Jorge Hankamer. 1984. Toward a Theory of Anaphoric\\nProcessing. Linguistics and Philosophy, 7, 325-345.\\n\\n\\nSgall, Petr, Eva Hajicov, and Jarmila\\nPanevov.. 1986. The Meaning of the Sentence in its Semantic\\nand Pragmatics Aspects, Reidel, Dordrecht and Academia, Prague.\\n\\n\\nShoham, Yoav. 1988. Reasoning about Change: Time and Causality\\n from the Standpoint of Artificial Intelligence, MIT Press, Cambridge,\\nMA.\\n\\n\\nSidner, Candy. 1983.  Focusing in the comprehension of definite\\nanaphora.  In M. Brady and R. C. Berwick, eds., Computational\\nModels of Discourse, The MIT Press, Cambridge, MA, 267-330.\\n\\n\\nStalnaker, Robert, C. 1972. Pragmatics. In Davidson and Harman, eds.,\\nSemantics of Natural Language, Reidel, Dordrecht, 380-397.\\n\\n\\nStalnaker, Robert, C. 1980. Assertion. In P. Cole, ed., Syntax and Semantics Vol.9: Pragmatics, Academic Press, New York,\\n315-332.\\n\\n\\nThomason, Richmond. 1990. Propagating Epistemic Coordination through\\nMutual Defaults I. In R. Parikh, ed., Theoretical Aspects of\\nReasoning about Knowledge, Proceedings of the Third Conference (TARK\\n1990), Morgan Kaufmann, Palo Alto, CA, 29-38.\\n\\n\\nVeltman, Frank. 1993. Defaults in Update Semantics. Manuscript,\\nDepartment of Philosophy, University of Amsterdam. To appear in the\\nJournal of Philosophical Logic.\\n\\n\\nWilks, Yorick. 1975. A Preferential, Pattern-seeking Semantics for\\nNatural Language Inference. Artificial Intelligence, 6, 53-74.\\n\\nFootnotes\\n\\nI would like to thank David Beaver, Johan van Benthem, Paul Dekker,\\nJan van Eijck, Jan Jaspars, Aravind Joshi, Alex Lascarides, Daniel Marcu,\\nBecky Passonneau, Henritte\\nde Swart, and Frank Veltman for helpful discussions and comments on\\nearlier versions of the paper.  The thoughtful comments by an\\nanonymous reviewer helped reshape the focus of the paper.  I also\\nprofited from the comments from the seminar participants at the\\nUniversity of Bielefeld and the University of Amsterdam.  I would\\nalso like to thank those who responded to the pronoun interpretation\\nquestionnaire whose results are discussed herein. Part of the work was\\nsponsored by project NF 102/62-356 (`Structural and Semantic\\nParallels in Natural Languages and Programming Languages'), funded by\\nthe Netherlands Organization for the Advancement of Research (N.W.O.).\\n  This separation of rule types does not imply a\\nsequential ordering between the two processing modules.  Different\\nrule types can be interleaved for interpreting or generating a\\nsubsentential constituent.\\n  The same formal system\\ncan be viewed from different viewpoints -- as a system of rules, constraints, or inferences. Rules produce and\\ntransform structures in a system, constraints reduce possible\\nstructures, and inferences are used to reason about structures (e.g.,\\nmanipulating assertions or drawing conclusions) as the ``logic'' in\\nthe standard sense.  To take a prominent example, in the ``parsing as\\ndeduction'' paradigm (Pereira and Warren, 1980), context-free rules\\nare also seen as deductive inference rules. The rule S \\n\\n\\n\\n\\n NP VP\\nis translated into the inference rule NP(i,j) \\n\\n\\n\\n  VP(j,k)\\n\\n\\n\\n\\n  S(i,k). I will not adhere to one particular viewpoint\\nin this paper, and rather take advantage of the flexibility.\\n  Grammar rules can be\\nseen from two viewpoints -- they eliminate as well as create possibilities. The former applies when communication is seen\\nas incremental elimination of possible information states. The latter\\napplies when it is seen as incremental increase of information\\ncontent. I leave the choice open here.\\n  In contrast, the abduction-based\\n system (Hobbs et al., 1993) does not separate grammar and pragmatics. All the rules are defeasible and\\ndirectly interact in one big module. (The defeasibility of grammar\\nrules is motivated by the fact of disfluencies in language use.) The\\nresult is an increased computational complexity.\\n  This architecture is in line with\\nStalnaker's (1972:385) conception:\\nThe syntactical and semantical rules for a language determine an\\ninterpreted sentence or clause; this, together with some features of\\nthe context of use of the sentence or clause, determines a truth\\nvalue. An interpreted sentence, then, corresponds to a function from\\ncontexts into propositions, and a proposition is a function from\\npossible worlds into truth values.\\n  A comment by Paul Dekker.\\n  I assume that various preferential\\ndecisions are interleaved rather than sequentially ordered within\\npragmatics.\\n  We have here an\\noperational criterion for separating out grammar and pragmatics.  It\\nleads to a discovery of cross-linguistic variation in the\\ngrammar-pragmatics boundary.  Long-distance dependency is a case in\\npoint (Kameyama, 1995).\\n  Grammatical functions will be in uppercase in order\\nto avoid the ambiguity of these words.\\n  Some speakers indicated that\\nthey had to assume additional facts in order to make a plausible\\nscenario -- for instance, in example A, ``Mary is a teacher, and she\\nsent John home as a punishment''. The speakers seem to want some more\\ninformation to make the judgment more conclusive.  What are the\\nrelationships among these three people mentioned out of the blue? I\\nrealize that impoverished examples of this sort rarely occur in our\\nreal-life discourses. To sort out some rather delicate interplay of\\npreferences, however, we need to start out with simplified examples.\\nThis is analogous to the use of the ``blocks world'' (i.e., the world\\nof blocks) in AI.\\n  I will use the simple terminology of ``referent''\\nand ``coreference'' without committing to their realist connotation\\nbecause this does not affect the points I wish to make in this paper.\\n  Another\\npossible source of preference is the causal link between the two\\ndescribed eventualities, John's hitting Bill (e1) and someone\\ndisliking someone (e2).  The preferred interpretation supports the\\ncausal link ``e1 because e2'', while the alternative\\ninterpretation, which nobody took, supports ``e1 therefore\\ne2''.  These could be stated in terms of discourse relations of Explanation and Cause (e.g., Lascarides and Asher, 1993). I'm\\nnot aware of any empirical studies of this kind of preference\\neffects.\\n  I suspect that the two speakers who\\ntook the opposite interpretation used the sense of back close to\\n``again''.\\n  The Terminator is a cyborg played\\nby Arnold Schwarzenegger in a popular science-fiction movie.\\n  Of interest\\nhere is the fact that the three speakers who knew nothing about\\nwhat a ``Terminator'' is all interpreted that John was injured\\nin example H. They clearly sensed ``something nasty and abnormal''\\n from this name alone.\\n  I\\nwill not discuss the partial order of propositions.\\n  Those entities that are\\n``inaccessible'' in the DRT sense do not participate in the salience\\nordering, or even if they do, they are below a certain minimal\\nthreshold of salience.\\n  In the centering model, the\\nentities realized in \\n\\n\\n\\n\\nare the ``forward-looking centers''\\n(Cf), and \\n\\nCenteri-1 is the ``backward-looking center'' (Cb).\\n  Consituents'\\nlinear ordering and animacy are also relevant.\\n  This order also\\napproximates the relative salience of entities in the output\\nattentional state, as demonstrated in part in example J.\\n  There is a pragmatic difference between stressed and\\nunstressed pronouns, which should be accounted for by an independent\\ntreatment of stress -- for example, in terms of a preference reversal\\nfunction (Kameyama, 1994b). This paper concerns only unstressed\\npronouns.\\n  Except in a telegraphic register.\\n  This notion of the single maximally\\nsalient entity corresponds to the ``preferred center'' Cp (Grosz et\\nal., 1986) that is determined solely by the GF ORDER. The difference\\nhere is that it is determined by both the Center and GF ORDER,\\npredicting an indeterminacy in certain cases.\\n  What I have previously called retain is now called chain. It covers both CONTINUE and RETAIN\\ntechnically distinguished by Grosz et al. (1986) and Brennan et\\nal. (1987).\\n  This statement is\\nintentionally left vague.  See Pruest's (1992) MSCD operation for a\\ngeneral definition of parallelism preference, and my property-sharing\\nconstraint (Kameyama, 1986) for a subcase relevant to pronoun\\ninterpretation.\\n\\n\\n\\n\\nmeans \\n\\n\\n\\n\\nwhere p[X] means \\n\\n\\n\\n\\n(update state X with p).\\n  In these definitions, I use the\\nnotations from Asher and Morreau's (1993) Commonsense Entailment (CE)\\nlogic as a theoretical meta-formalism without strictly adhering to\\nthe CE ontology.\\n  It follows from Cautious\\nMonotonicity [A\\n\\n\\n\\n\\n B, A\\n\\n\\n\\n\\n C /\\nA,B\\n\\n\\n\\n\\n C]:\\n\\n\\n\\n\\nbecause \\n\\n\\n\\n\\n .\\n  Grdenfors and\\nMakinson's (1994) use of expectation ordering in preferential\\nreasonning achieves essentially the same effect.\\n  In the longer\\nversion of this paper (Kameyama, 1994a), a logical implementation of\\nthe preferential rule interactions is proposed using prioritized\\ncircumscription (McCarthy, 1980, 1986; Lifschitz, 1988), a nonmonotonic\\nreasoning formalism in AI.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nAn account of utterance interpretation in discourse needs to face the\\nissue of how the discourse context controls the space of interacting\\npreferences. Assuming a discourse processing architecture that\\ndistinguishes the grammar and pragmatics subsystems in terms of\\nmonotonic and nonmonotonic inferences, I will discuss how\\nindependently motivated default preferences interact in the\\ninterpretation of intersentential pronominal anaphora.\\nIn the framework of a general discourse processing model that\\nintegrates both the grammar and pragmatics subsystems, I will propose\\na fine structure of the preferential interpretation in pragmatics in\\nterms of defeasible rule interactions. The pronoun interpretation\\npreferences that serve as the empirical ground draw from the survey\\ndata specifically obtained for the present purpose.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nThe successful treatment of morphological phenomena in some languages by\\nmeans of finite state automata appears to have led to the idea that this\\nmodel is the most efficient and universal way to deal with morphology\\ncomputationally.  Although there exist good finite-state processors for\\n Spanish -like ,  or  - we think that some phenomena can be handled more elegantly using a context-free approach, particularly if the\\nmorphological component is to be included as a part of a syntax\\ngrammar. Our model has been implemented in standard DCG using a logic\\nprogramming approach instead of a plain finite-state one.\\n\\n\\nIt is well-known that the so-called non-concatenative processes are the\\nmost difficult single problem that morphological processors must deal with.\\nExperience has shown that it is not easy for any approach.\\nUnification-based morphology uses suppletion (i.e. alternative allomorphs\\nfor a lemma) and feature description as a general mechanism for handling\\nthose processes.  Two-level morphology uses instead rules that match\\nlexical representations (lemmas) with surface representations (actual\\nspelling forms).  The latter has been been claimed to be more elegant, but\\nit is obvious that often the two-level model contains many rules needed for\\na very few cases.\\n\\n\\nThe pure two-level/finite-state automata model is not very adequate for\\ntreating certain non-concatenative processes, and in such cases one is\\nrequired to depart from this approach, for example by adding an extension\\nin which two-level rules are retained under the control of feature\\n structures .  Moreover, every language has irregularities that can only be treated as suppletive forms, e.g. soy (I\\nam), era or fui (I was).  Since suppletion is needed\\nanyway, and since it is a much simpler approach than rules, we consider\\n that the ``elegance'' objection is not well-founded. \\n\\n\\nOn the other hand, our goal is to generate and recognize all (and only)\\nwell-formed inflected forms, and thus we do not accept ``missing\\nforms'' for defective verbs (see below), but do accept duplicate but\\ncorrect forms.\\n\\n\\n  Major issues in Spanish computational morphology \\n\\nSpanish morphology is not a trivial subject.  As an inflectional\\nlanguage, Spanish shows a great variety of morphological processes,\\nparticularly non-concatenative ones.  We will try to summarize the\\noutstanding problems which any morphological processor of Spanish has to\\ndeal with:\\n\\n\\n1.\\nA highly complex verb paradigm. For simple tenses, we consider 53\\n inflected forms (see Table ), excluding the archaic Future Subjunctive, but including the duplicate Imperfect Past Subjunctive\\n(6 forms). If we add the 45 possible forms for compound tenses, then 98\\ninflected forms are possible for each verb.\\n\\n\\n2.\\nThe frequent irregularity of both verb stems and endings.  Very common\\nverbs, such as tener (to have), poner (to put),\\npoder (to be able to), hacer (to do), etc., have up\\nto 7 different stems: hac-er, hag-o, hic-e, ha-r,\\nhiz-o, haz, hech-o. This example shows internal vowel\\nmodification triggered by different morphemes having the same external form:\\nhag-o; hiz-o, hech-o (The first /-o/ is first person\\nsingular present indicative morpheme; the second /-o/ is third singular\\npreterit indicative morpheme; and the third /-o/ is past participle\\nmorpheme - an irregular one, by the way).  As well as\\nthese non-concatenative\\nprocesses, there exist other, very common, kinds of internal variation,\\nas illustrated by the following example.\\n\\n\\n2,300 out of 7,600 verbs in our dictionary are classified as irregular, and\\n5,300 as regular -i.e. only one stem for all the forms as in am-ar\\n-- am-o, etc. (to love).\\n\\n\\n3.\\nGaps in some verb paradigms. In the so-called defective verbs\\nsome forms are missing or simply not used. For instance, meteorological verbs\\nsuch as llover, nevar (to rain, to snow), etc. are\\nconjugated only in third person singular. Other ones are more peculiar, like\\nabolir (to abolish) that lacks first, second and third singular\\nand third plural present indicative forms, all present subjunctive forms, and\\nthe second singular imperative form. In other verbs, the compound tenses are\\nexcluded from the paradigm, like in soler (to do usually).\\n\\n\\n4.\\nDuplicate past participles:  a number of verbs have two\\nalternative forms, both correct, like impreso, imprimido\\n(printed).  In such cases, the analysis has to treat both.\\n\\n\\n5.\\nThere exist some highly irregular verbs that can be\\nhandled only by including many of their forms directly in the lexicon (like\\nir (to go), ser (to be), etc).\\n\\n\\n6.\\nNominal inflection can be of two major types: with\\ngrammatical gender (i.e. concatenating the gender morpheme to the stem) and\\nwith inherent gender (i.e. without gender morphemes). Most pronouns and\\nquantifiers belong to the first class, but nouns and adjectives can be in\\nany of the two classes, with a different distribution: 4% of the nouns\\nhave grammatical gender and 92% have inherent gender, while 70% of the\\nadjectives are in the first group.  Some nouns and adjectives present\\nalternative correct forms for plural -e.g. for bamb (bamboo), bamb-s and, bamb-es.\\n\\n\\n7.\\nThere is a small group (3%) of invariant nouns with the same form for\\nsingular and plural, e.g. crisis. On the other hand, 30% of the\\nadjectives present the same form for masculine and feminine, e.g. azul\\n(blue).  There exist also singularia tantum, where only the\\nsingular form is used, like estrs (stress); and pluralia tantum, where only the plural form is allowed, e.g. matemticas, (mathematics).\\n\\n\\n8.\\nIn contrast with verb morphology, nominal processes do not produce\\ninternal change in the stem caused by the addition of a gender or plural\\nsuffix, although there can be many allomorphs produced by spelling changes:\\nluz, luc-es (light, lights).\\n\\n\\nFor a detailed description of all verb and nominal phenomena, including a\\n classification into paradigmatic models, see . \\n\\n\\nAll these phenomena suggest that there is no such a universal model\\n(e.g. two-level, unification, or others) for (surface) morphology.\\nInstead, we have approaches more suited for some processes than others.\\nThe computational morphologist must decide which is more appropriate for a\\nparticular language. We support the idea that unification and feature-based\\nmorphology is more adequate for languages, such as Spanish and other Latin\\nlanguages, that have alternative stems triggered by specific suffixes,\\nmissing forms in the paradigm, and duplicate correct forms.\\n\\n\\n  The model \\n\\nIt is well known that morphological processes are divided into two types:\\nprocesses related to the phonological and/or graphic form (morpho-graphemics),\\nand processes related to the combination of morphemes (morpho-syntax). Each\\nmodel treats these facts from its particular perspective. Two-level morphology\\nuses phonological rules and continuation classes (in the lexical component).\\n Mixed systems such as  or  have different sets of rules.\\n\\n\\nAs we stated before, our model relies on a context-free feature-based grammar,\\nthat is particularly well suited for the morpho-syntactic processes.  For\\nmorpho-graphemics, our model depends on the storage -or computation- of all\\nthe possible allomorphs both for stems and endings. This feature permits that\\nboth analysis and synthesis be limited to morpheme concatenation, as the\\ngeneral and unique mechanism. This simplifies dramatically the rule component.\\n\\n\\nWe present some examples of dictionary entries: two verbal ending entries\\n(allomorphs) for the past participle morphemes and two allomorph stems for\\nimprimir, compatible with those endings.\\n\\n\\n\\nWhere vm and vl stands for the values of the ``morphological\\ncategory'' that we are using to drive the DCG rule invocation. All the\\ndictionary entries are coded with a predicate that corresponds to its\\nmorphological category. The full inventory of such categories follows:\\n\\n\\nw\\nFor complete inflected word forms.\\nwl\\nFor words (nouns and adjectives) that can accept a number\\nmorpheme.\\nvl\\nFor verb lexemes (stems).\\nnl\\nFor nominal -nouns and adjectives- lexemes.\\nvm\\nFor verb morphemes.\\nng\\nFor nominal gender morphemes.\\nnn\\nFor nominal number morphemes.\\n\\n\\nFor reference, and to check the meaning of the examples, a short\\nself-description of the arguments of those predicates follows:\\n\\n\\n\\nWe have introduced some contextual atomic features that impose restrictions\\non the concatenation of morphemes through standard unification rules. Such\\nfeatures are never percolated up to the parent node of a rule. Multi-valued\\natomic features are permitted in the unification mechanism, being\\ninterpreted as a disjunction of atomic values. We represent this\\ndisjunction as a Prolog list. Disjunction of values is used only for\\ncontextual features (stem_type, suffix_type, conjugation,\\ngender_type and number_type) just to improve storage efficiency,\\nsince this device is actually not needed if different entries were encoded\\nin the lexicon.\\n\\n\\n In the conjugation table (Table ), the stem_type values of the grammatical features person-number and tense-mood are displayed in\\nboldface. For example, sing_1 means first person, singular number;\\nwhile pres_ind means present tense, indicative mood.\\n\\n\\n Each of the 49 inflected forms is  represented by a numeric code, and the additional value 100 is used as a shorthand for the disjunction\\nof all of them (used for regular verbs; see the entry for imprim\\nabove). The contextual feature stem_type (stt) is used to\\nidentify the verb stem and ending corresponding to each form, and the\\ncontextual feature suffix_type (sut) distinguishes among\\nseveral allomorphs of the inflectional morpheme by means of a set of\\nvalues:\\n\\n\\n\\nSince this value set is much smaller than the stem_type set, we have\\nchosen an alphabetic code. With the combination of both features, and the\\naddition of a third feature conj (conjugation), we can state\\nunequivocally which is the correct sequence of stem and ending for each case\\n(see examples above, where imprim only matches ido for all\\nfeatures, and impres matches o, thus preventing ill-formed\\nconcatenations -for these morphemes- such as imprim-o or impres-ido).\\n\\n\\nIn the same fashion, we have two special contextual features for the\\nnominal inflection, nut (number_type) and get (gender_type),\\nto identify the various allomorphs for the plural and gender morphemes, and\\nassociate them with the proper nominal stems. The following examples show\\nthose contextual features both in nominal morphemes and in nominal lexeme\\nentries:\\n\\n\\n\\nThese entries allow the analysis/generation of the word forms presidente, presidenta, presidentes and presidentas for the lemma\\npresidente; doctor, doctora, doctores and doctoras for\\ndoctor; and bamb, bambs and bambes for bamb.\\n\\n\\nThe grammatical features (category, lemma, tense, mood, person,\\nnumber and gender are the only features that are delivered to\\nthe w node, and from this level can be used by a syntactic DCG\\ngrammar.\\n\\n\\nA unification-based system relies very much on the lexical side. It is\\nneeded a robust and large dictionary, properly coded. Additionally, our\\nmodel depends on the accessibility of all possible allomorphs, so their\\nstorage is also necessary. Fortunately, there is no need for typing all of\\nthem by hand, since this would be an impractical, time consuming and\\nerror-prone task.  Morpho-graphemics for Spanish is quite regular and we\\nhave formalized and implemented the automatic computation of the allomorphs\\nof any verb from the infinitive form.\\n\\n\\nThe formalized description of the morphological phenomena of Spanish was\\n presented in , where some interesting and well founded  linguistic generalizations are made: Paradigms for verbs are described to capture regularities in the inflectional behaviour of the Spanish verbs, and the same is done with\\nnouns.  All the lemmas belonging to a particular paradigmatic model\\nnot only share most of contextual and grammatical features but also have\\nthe same allomorph number and distribution. For instance, our model 11\\nhas three allomorph stems, and their distribution is as follows:\\n\\n\\n\\n In  regular-expression based rules are devised to compute automatically these allomorphs, capturing morpho-graphemic\\ngeneralizations in the paradigmatic models.\\n\\n\\n  The grammar \\n\\nThe rule component of the model is quite small, because most of the\\ninformation is in the lexicon. In particular, inflected verb forms are\\nanalysed or generated by two rules. Actually, only one rule is needed, but as\\nwe used the value 100 for the stt feature for regular verbs\\ninstead of a disjunction of all the possible stt values, we split the\\nrule in two:\\n\\n\\n\\nNominal inflection is a bit more complicated, because of the combination of\\ntwo inflectional morphemes (gender and number) in some cases.  Our model\\nneeds the 4 rules shown to handle this. The first one is for singular\\nwords, when the stem has to be concatenated to a gender suffix (ni-o, ni-a); the second is for plural words, where an additional\\nnumber suffix is added (nio-s) ; the third builds plurals from an\\nallomorph stem and a plural morpheme (len / leon-es); and the\\nfourth rule validates as words the singular forms (wl) obtained from\\nthe first rule without further concatenation:\\n\\n\\n\\nThe predicate member included in the procedural part of the DCG rule\\nimplements disjunction in atomic contextual features, although it could\\nhave been eliminated with a different encoding of the lexical entries.\\n\\n\\n  The Processor \\n\\nThe grammar rules are stated using the DCG formalism included in most\\nProlog implementations, thus we have used the DCG interpreter both for\\nparsing and generating word forms. Since the interpreter is supplied with\\nmorphemes included in the dictionary for its proper operation, a segmenter\\nhas to be included to provide the parser with candidate word\\nsegmentations. This is achieved by means of a non-deterministic predicate\\nthat finds all the possible segmentations of a word. This is one of the\\nefficiency drawbacks of the current implementation of GRAMPAL.\\n\\n\\nTo avoid such inefficiency the system could be augmented with a letter\\n trie index -or trie-  to the lexical entries. With this device, segmentation will be no longer\\nnon-deterministically blind and the search would be efficiently guided.\\nGeneration does not have those efficiency problems, and the system is\\nbidirectional without any change in the rules.\\n\\n\\n  Conclusions \\n\\nA Prolog prototype, GRAMPAL, was developed to intensively test the model,\\nboth as analyser and as generator. This processor implemented in Prolog has\\nshown that logic programming can be used successfully to handle the Spanish\\ninflectional morphology. We have also implemented a C version of GRAMPAL,\\nbut it needs separate components for analysis and generation, due to the\\nlack of reversibility that Logic Programming has provided us with.\\n\\n\\nThe model presented is based on two basic principles:\\n\\n\\n\\nEmpirical rigour: all and only correct forms are analysed and\\ngenerated, whether regular or not; gaps in verb paradigms are observed;\\nsuppletive forms are considered valid, and so on. It is important to\\nstress that GRAMPAL does not overgenerate or overanalyse.\\n\\nSimplicity and generalization: GRAMPAL employs a really\\nstraightforward rule component, that captures the logical generalization of\\nthe combination of a stem and an ending to form a inflected\\nword. ``Standard scientific considerations such as simplicity and\\ngenerality apply to grammars in much the same way as they do to any other\\ntheories about natural phenomena. Other things being equal, a grammar with\\n seven rules is to be preferred to one with 93 rules''  . \\n\\n\\n\\n\\nThe current dictionary has a considerable size: 43,000 lemma\\n entries, including 24,400 nouns, 7,600 verbs, and 11,000 adjectives.  The model could be used for derivative\\nmorphology and compounds as well, but this has not been done yet, since\\nfurther linguistic analysis must be done to specify the features needed to\\npermit derivatives and compounds.\\n\\nBibliography \\n\\nAho, A.V.; Hopcroft, J.E. and Ullman, J.D.\\n(1983).\\nTries.\\nIn Data Structures and Algorithms, ch. 5, sec. 3, pp. 163-169.\\nAddison Wesley.\\n\\n\\nAntworth, E.\\n(1990).\\nPC-KIMMO: A Two-level Processor for Morphological Analysis.\\nAcademic Computing Department, Summer Institute of Linguistics.\\nDallas, TX.\\n\\n\\nBear, J.\\n(1986).\\nA Morphological Recogniser with Syntactic and Phonological Rules.\\nProceedings of the 11th International Conference on\\nComputational Linguistics (COLING 86), pp. 272-276.\\n\\n\\nGazdar, F. and Mellish, C.S.\\n(1989).\\nNatural Language Processing in Prolog.\\nAddison Wesley.\\n\\n\\nGoi, J.M.; Gonzlez, J.C. and Lpez, J.\\n(1994).\\nA Framework for Lexical Representation.\\nTechnical Report UPM/DIT/LIA 5/94. Universidad\\nPolitcnica de Madrid, 1994.\\n\\n\\nKoskenniemi, K.\\n(1985).\\nA general two-level computational model for word-form recognition\\nand production.\\nIn Karlsson, F. (ed): Computational Morphosyntax, pp. 1-18.\\nDept. of Linguistics. University of Helsinki.\\n\\n\\nMart, M.A.\\n(1986).\\nUn sistema de anlisis morfolgico por ordenador.\\nProcesamiento del Lenguaje Natural, n. 4, pp. 104-110.\\n\\n\\nMeya, M.\\n(1986).\\nAnlisis morfolgico como ayuda a la recuperacin de\\ninformacin.\\nProcesamiento del Lenguaje Natural, n. 4, pp. 91-103.\\n\\n\\nMoreno, A.\\n(1991).\\nUn modelo computacional basado en la unificacin para el\\n        anlisis y la generacin de la morfologa\\n        del espaol.\\nTesis Doctoral. Universidad Autnoma de\\nMadrid, 1992.\\n\\n\\nRitchie, G.; Pulman, S.; Black, A. and Russell, G.\\n(1987).\\nA Computational Framework for Lexical Description.\\nComputational Linguistics, vol. 13, n. 3-4, pp. 290-307.\\n\\n\\nTzoukermann, E. and Liberman, M.\\n(1990).\\nA Finite-State Morphological Processor for Spanish.\\nProceedings of the 13th International Conference on\\n    Computational Linguistics (COLING 90), vol. 3, pp. 277-281.\\n\\n\\nTrost, H.\\n(1990).\\nThe application of two-level morphology to non-concatenative German\\nmorphology.\\nProceedings of the 13th International Conference on\\nComputational Linguistics (COLING 90), vol.2, pp. 371-376.\\n\\nFootnotes\\n\\n  This work has been partially supported by the Spanish Plan Nacional de I+D, under the Research Project An Architecture for\\npara Natural Language Interfaces with User Modeling (TIC91-0217C02-01).\\n  See the next\\nsection for further discussion of the adequacy of the two-level model for\\nSpanish, including defective forms (i.e. null forms in the conjugation) and\\nalternative correct forms.\\n  The codes 83 and 86 stand for the\\ncourtesy imperative: imprima usted, impriman\\nustedes. These word forms are the same as the 53 and 56 ones.\\n  Actually, this number encodes a\\nparticular combination of person, number, tense and mood features.\\n  These are not the\\ntraditional ones, since they capture the problems arising in written\\nlanguage, such as diacritical marks, different surface letters for the same\\nphoneme, etc.\\n  In these figures are neither included closed categories,\\nnor allomorphs for verb and nominal morphemes.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nA model for the full treatment of Spanish inflection for verbs, nouns and\\nadjectives is presented. This model is based on feature unification and it\\nrelies upon a lexicon of allomorphs both for stems and morphemes.  Word\\nforms are built by the concatenation of allomorphs by means of special\\ncontextual features. We make use of standard Definite Clause Grammars (DCG)\\nincluded in most Prolog implementations, instead of the typical\\nfinite-state approach. This allows us to take advantage of the\\ndeclarativity and bidirectionality of Logic Programming for NLP.\\nThe most salient feature of this approach is simplicity: A really\\nstraightforward rule and lexical components. We have developed a very simple\\nmodel for complex phenomena.\\nDeclarativity, bidirectionality, consistency and completeness of the model\\nare discussed: all and only correct word forms are analysed or generated,\\neven alternative ones and gaps in paradigms are preserved.  A Prolog\\nimplementation has been developed for both analysis and generation of\\nSpanish word forms.  It consists of only six DCG rules, because our lexicalist approach -i.e. most information is in the\\ndictionary. Although it is quite efficient, the current implementation\\ncould be improved for analysis by using the non logical features of Prolog,\\nespecially in word segmentation and dictionary access.\\nKeywords:  Applications of Logic Programming to NLP,\\nComputational Morphology.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nStatistical approaches to natural language processing\\nare becoming increasingly popular, being applied\\nto a wide variety of tasks.  For example,\\nWeischedel et al. (1993) explores part-of-speech\\ntagging, parsing and acquisition of lexical frames.\\nNonetheless, all these tasks share some important\\ncharacteristics, not the least of which is the requirement\\nfor a sizable corpus of training data.  One\\nquestion which has largely been ignored is how much\\ndata is enough?  For example, given a limited body\\nof training data, it is essential to know which\\nstatistical NLP methods are likely to be accurate before\\npursuing any one.  Also, given a particular\\nmethod, when will acquiring further training data cease\\nto improve the system accuracy?  Currently, the\\nfield is conspicuously lacking a general theory of\\ndata requirements for statistical NLP.\\n\\n\\nIn this paper, I present the first steps towards the\\ndevelopment of such a theory.  I begin by formulating\\na framework for statistical NLP systems\\ndesigned to capture some of the elements crucial\\nto data requirements analysis.  I will\\nthen review a sample of existing approaches, showing\\nhow they fit into the framework, and where they vary from it.\\nEven though several of these introduce complexities which\\nare not captured by the framework, reasoning in the framework\\nstill supports some important insights into these systems.\\nFinally, I present some preliminary work on establishing\\na closed form upper bound on data requirements for a class\\nof statistical NLP systems. This latter work owes much to Mark\\nJohnson, Brown University, who is responsible for several key\\n mathematical ideas in Section . \\n\\n\\nStatistical NLP systems are designed to make choices;\\nhopefully in an informed manner.  To do this they use\\nindicators, upon which their choices are conditioned.\\nThe purpose of computing statistics is to inductively establish\\nthe relationship between the indicators and the choices to be made.\\nConsider for example a next word predictor which attempts to predict\\nthe next word on the basis of the preceding word.  To do this it must\\nhave an understanding of the relationship  between the indicator (the\\npreceding word) and the choice (the next word).  It is possible\\nto acquire this understanding by computing statistics over a large\\ncorpus, a process called training.  Once trained, the system may then\\nbe applied to a new text and its accuracy evaluated.  This paper\\nis concerned with the dependence of a system's accuracy\\non the size of the training corpus.  In the following section, the\\nnotions of indicators, choices and training data will\\nbe made more formal.\\n\\n\\n  Statistical Processors \\n  A Framework \\n\\nA statistical NLP system deals with a certain linguistic\\nuniverse.  Formally, there is a set of linguistic events\\n\\n\\n\\nfrom which every training example and every\\ntest instance will be drawn.  In the next word predictor,\\nthis need only be the set of all pairs of words which may\\nbe adjacent in text.\\n\\n\\nLet V be a finite set of values that we would like\\nto assign to a given linguistic input.  This defines\\nthe range of possible answers that the analyser can\\nmake.  In the predictor, this is the set of words plus\\nan end of sentence symbol.\\nLet \\n\\n\\n\\n\\nbe the random\\nvariable describing the distribution of values that\\nlinguistic events take on.  We also require a set of\\nindicators, B, to use in selecting a value for a\\ngiven linguistic event.  I will refer to each element of\\nB as a bin. In the predictor, the set of bins is\\nthe set of words plus a start of sentence symbol.\\nLet \\n\\n\\n\\n\\nbe\\nthe random variable describing the distribution of bins\\ninto which linguistic events fall.\\n\\n\\nThe task of the analyser is to choose which value\\nis most likely given only the\\nindicator.  Therefore, it is a function \\n\\n\\n\\n\\nThe task of the learning algorithm is to acquire\\nthis function by computing statistics on the training set.\\n\\n\\nPutting these components together, we can define a\\nStatistical Processor, S as a\\ntuple \\n\\n\\n\\n\\n ,\\nwhere:\\n\\n\\n\\nis the set of all possible linguistic\\nevents\\n\\nB and V are finite sets, the bins and values\\nrespectively\\n\\nA is the trained analysis function\\n\\n\\n\\n\\nAmongst all such statistical processors, there is a\\nspecial class in which we are interested.  Define a\\nprobabilistic analyser to be a statistical processor\\nwhich computes a function \\n\\n\\n\\n\\nsuch that \\n\\n\\n\\n\\nfor all \\n\\n\\n\\nand then computes A as:\\n\\n\\n\\n\\n\\nThe problem of acquiring A is thus transformed into\\none of estimating the function p using the\\ntraining corpus. Generally, p(b,v) is viewed as an estimate\\nof the probability \\n\\n\\n\\n\\n .\\n\\n\\n  Training Data \\n\\nFormally, a training corpus, c,\\nof m instances, is an element from\\n\\n\\n\\n\\nwhere each pair (b,v) is sampled\\naccording to the random variables I and J from\\n\\n\\n\\n .\\nFor probabilistic analysers, there are a variety of\\nmethods by which an appropriate function p can be\\nestimated from a corpus; one simple example being the\\nMaximum Likelihood Estimate.  Regardless of\\nthe learning algorithm used, each possible training\\ncorpus, c, results in the acquisition of some function, Pc.\\nOur aim is to explore the dependence\\nof the expected accuracy of Pc on the magnitude\\nof m.\\n\\n\\nSurprisingly, it is not always obvious how many training\\ninstances have been used to train a statistical\\nmethod.  It is not generally sufficient\\nto report the size of the corpus in words.  A system\\nwhich collects word associations using a window of\\ncooccurrence 10 words wide will find 819 instances\\nin a 100 word corpus, while one collecting the objects\\nof the preposition on from the same corpus,\\nwould most likely find only a few instances.\\nTherefore, before any conclusions can be drawn about data\\nrequirements, the training corpus must be measured in terms of\\ninstances.\\n\\n\\nEach of these instance falls into a particular bin\\nby virtue of its associated indicator.  In choosing the indicators,\\nwe have implicitly defined equivalence classes for instances.  The\\nstatistical processor will treat every instance in a bin identically.\\nFurther, once the bins are chosen, the greater the number of\\ntraining instances that fall into a bin, the greater our\\nconfidence in the statistical inference made by the processor\\nfor test cases in that bin.  For instance, the next word predictor\\nis more likely to be correct when the preceding word is common\\nthan when it is a rare word.\\n\\n\\nIt is not always obvious how many bins a\\ngiven statistical method employs.  Often multiple\\nindicators are used.\\nFor instance, a trigram tagger uses the tags of the two\\npreceding words and the current word to choose a new tag.\\nIn this case, \\n\\n\\n\\n\\nwhere T is the tagset\\nand W is the vocabulary.\\n\\n\\nThis example demonstrates an important point.  By choosing to take\\ninto account the tags of two preceding words, the trigram\\ntagger requires |T| times as many bins as a bigram\\ntagger (where \\n\\n\\n\\n\\n ).  With more bins, the\\ntrigram tagger is sensitive to a broader range of context and\\nthus can in principle achieve a greater accuracy.  However,\\nbecause there are more bins, there are fewer training instances\\nin each bin.  Thus, statistical estimation will be less accurate.\\nIn practice, high accuracy requires at least a few training\\ninstances per bin.  Thus increasing the number of indicators\\nmay actually decrease the overall accuracy.\\n\\n\\nFor probabilistic analysers it is useful to define the number\\nof slots, L, to be \\n\\n|B|(|V|-1), which is the number of\\nindependent parameters needed to define the function p.\\n\\n\\n  Error Rates and Optimality \\n\\nFor any non-trivial general statistical processor the\\nindicators used cannot perfectly represent the entire\\nlinguistic event space.  Thus, in general there exist\\nvalues \\n\\n\\n\\n\\n ,\\nfor which\\nboth \\n\\n\\n\\n\\n and \\n\\n\\n\\n\\nfor some \\n\\n\\n\\n .\\nSuppose without loss of generality that\\n\\nA(b) = v1.  The analyser will be in\\nerror with probability at least \\n\\n\\n\\n\\n .\\nThis is the root of a rather difficult problem in\\nstatistical NLP because no matter how inaccurate a\\ntrained statistical processor is, the inaccuracy may\\nbe due to the imperfect representation of \\n\\n\\n\\n  by B.   \\n\\n\\nProbabilistic analysers always select just one value for each bin,\\nthe one which maximises p.  Let \\n\\n\\n\\n\\nThis is the probability of the analyser being in error\\non a randomly selected element of \\n\\n\\n\\n .\\nLet\\n\\n\\n\\n\\nbe any function which minimises the expected\\nerror rate and \\n\\n\\n\\n\\n .\\nGiven B and V, \\n\\n\\n\\n\\nis the smallest possible\\nexpected error rate.  Any probabilistic analyser\\nwhich achieves an accuracy close to this is\\nunlikely to benefit from further training data.\\n\\n\\nUnless large volumes of manually annotated data exist,\\nmeasuring the size of \\n\\n\\n\\n\\nin any given statistical\\nprocessor presents a difficult challenge.  Hindle\\nand Rooth (1993) have attempted a similar task using human\\nsubjects on the problem of prepositional phrase\\nattachment.  Subjects were given only the preposition\\nand the preceding verb and noun and then were\\nasked to select the attachment.  This was precisely\\nthe task facing their statistical processor.  The subjects\\ncould only perform the attachment correctly in around 86%\\nof cases.  If we assume that the subjects incorrectly analysed\\nthe remaining 14% of cases because these cases depended on\\nknowledge of the wider context, then any statistical learning\\nalgorithm based only on these indicators cannot do better\\nthan 86%.  Of course, if there is insufficient training data\\nthe system may do considerably worse.\\n\\n\\nAssuming that human performance on the task\\naccurately reflects the value of \\n\\n\\n\\n\\n is the only means known at present to estimate the value\\nof \\n\\n\\n\\n\\n .\\nUnfortunately, this approach is expensive to apply\\nand makes a number of questionable\\npsychological assumptions.  For example,\\nit assumes that humans can accurately reproduce parts\\nof their language analysis behaviour on\\ncommand.  It may also suffer when representational\\naspects of the analysis task cannot be explained\\neasily to experimental subjects.  A worthwhile goal\\nfor future research is to establish a statistical\\nmethod for estimating or bounding  \\n\\n\\n\\n\\nusing\\nlanguage data.\\n\\n\\n\\n  Statistical Learning \\n  Existing Methods \\n\\nIn this section, I show how a number of existing statistical\\nNLP works fit into the framework, including a\\ntagger, a sense disambiguator and three syntactic analysers.\\n For each, I consider how the various\\nelements of the general statistical processor are instantiated.\\n\\n\\nWeischedel et al. (1993) uses (among other experiments)\\na trigram hidden Markov model to tag text for\\npart of speech.  The training data is four million\\nwords of the University of Pennsylvania Treebank,\\ntagged with a set of 47 different tags.  I shall regard\\nB as consisting of the two previous tags (\\n\\n\\n\\n\\n ),\\nwhile V is simply the tagset.  The system\\nalso takes into account lexical tag frequencies (that is,\\n\\n\\n\\n\\n ).\\nI will assume however that data sparseness does not affect\\nthe lexical tag frequency estimates.  Since the trigram estimates and\\nthe lexical tag frequencies are combined as independent\\nfactors, ignoring the lexical component does not seem unreasonable.\\nThe situation is further complicated\\nbecause probability is maximised over a sequence of\\nwords, rather than for a single word.  The framework needs to be extended\\nto capture these mechanisms, but for the moment the approximations\\nI have made may be useful.\\nSince every word in the corpus (bar the first two) is used for\\ntraining, we have m=4 million and \\n\\n\\n\\n\\nThe accuracy is reported to be\\naround 97%, which is approximately the accuracy of\\nhuman taggers using the whole context.\\n\\n\\nYarowsky (1992) describes a sense disambiguation system\\nwhich uses a 100 word window of\\ncooccurrences.  He uses a mutual information-like measure\\nwhich combines the cooccurrence statistics\\nfor all words in each category of Roget's thesaurus.\\n The result is a profile of contexts for a category\\nwhich can be used to estimate how likely each category\\nis within a certain context.  Comparing the\\ndifferent possible categories for the word provides\\na broad sense discrimination.  The training corpus is\\nGrolier's encyclopedia which contains on the order\\nof 10 million words.  Each of these provides 100\\ntraining instances (every other word in the window),\\nso \\n\\n\\n\\n\\nbillion.  Since the evidence from\\neach word in the context is combined independently,\\nit is reasonable to regard B as simply the set of distinct\\nwords in Grolier's. Again, further work is needed to make this\\napproximation unnecessary.  V is the set of Roget's categories\\n(\\n\\n|V| = 1042), so assuming the vocabulary is around\\n100,000, \\n\\n\\n\\n\\nmillion.\\n  The average accuracy reported is 92%.\\n\\n\\nHindle and Rooth (1993) propose a system to syntactically\\ndisambiguate prepositional phrase\\nattachments.  Unambiguous examples of attachments are\\nused to find lexical associations (a likelihood\\nratio) between prepositions and the nouns or verbs\\nthey attach to.  They cyclically apply this\\ntechnique, adding disambiguated attachments into the\\ntraining set, until all the training data (ambiguous\\nor not) has been used.  This approach can be\\napproximated by a probabilistic analyser.\\nEach association value is ascribed\\nto a pair (w,p) where w is a verb or noun\\nand p is a preposition.  Thus B is a product of\\ntwo indicator spaces: the set of verbs and nouns and\\nthe set of prepositions.  Assuming they used 10,000\\nnouns and verbs (5,000 of each) and 100\\nprepositions, |B| = 1 million.  The analyser computes\\na probability for each of two possible attachments,\\nnominal and verbal, so V is binary.\\nThe training set consists of 754,000 noun attachments\\nand 468 thousand verb ones giving m = 1.22 million.\\n  The accuracy reported is close to 80%, while human subjects given the\\nsame indications could achieve 85-88% accuracy.\\nIf the latter figure reflects the optimal error\\nrate, it appears there is still room for improvement\\nby adding training data or changing the statistical\\nmeasures.\\n\\n\\nLauer (1994) describes a system for syntactically analysing\\ncompound nouns.  Two-word compounds\\nextracted from Grolier's encyclopedia were used to\\nmeasure mutual information between every pair of\\nthesaurus categories (using Roget's thesaurus) and\\nthe results used to select a bracketing for three-word\\ncompounds.  Since an association value is computed\\nfor every pair of thesaurus categories, |B|is equal to \\n\\n\\n\\n\\n .\\nThere are only two possible\\nbracketings to choose from, so again V is binary.  The training\\ncorpus consists of about 35,000 two-word\\ncompounds, giving \\n\\nm = 35,000 and \\n\\n\\n\\n\\nmillion.\\n The accuracy reported is 75%.\\n\\n\\nResnik and Hearst (1993) aim to enhance Hindle and\\nRooth's (1993) work by incorporating\\ninformation about the head noun of the prepositional\\nphrase in question.  Thus B is now a product of\\nthree spaces: the set of nouns and verbs, the set of\\nprepositions and the set of nouns.  To reduce the data\\nrequirement, a freely available on-line thesaurus,\\ncalled WordNet is used (Beckwith et al., 1991).\\nWordNet groups words into synsets, categories of synonymous\\nwords.  These synsets are arranged in a taxonomy, so that\\nevery word is also provided with a list of hypernyms.\\nThe system then adds together the frequency counts for nouns\\nwithin a synset, providing more data about each.\\nThis reduces the number of bins, since it is the synsets\\nwhich are taken as indicators rather than individual words.\\nIf we assume roughly 1000 synsets,\\n1000 verbs and 100 prepositions, then \\n\\n|B|\\n=  200 million.  V is still binary.  Their training corpus\\nis an ``order of magnitude smaller than'' Hindle and Rooth's,\\nso m is around 100,000.  Unlike Hindle\\nand Rooth's, their corpus is parsed, which should give\\nbetter results.  Interestingly, they combine\\nevidence from large groups of synsets within WordNet's\\nhypernym hierarchy using a t-test.  This causes\\nthe effective number of synsets for nouns to be reduced,\\nperhaps by as much as a factor of 10 (thus \\n\\n\\n\\n\\nmillion).\\nI will therefore assume that \\n\\n\\n\\n\\nmillion.  Even\\ngiven the additional information about the head\\nnoun of the prepositional phrase, the accuracy reported\\nfails to improve on that of Hindle and Rooth,\\nbeing 78%.  It is possible that insufficient training\\ndata is the cause of this shortfall.\\n\\n\\n Table  shows a summary of the above systems, ordered on the ratio m:L.  A strong\\ncorrelation is evident between the value of this ratio\\nand the success rate.  This suggests that the success\\nof a statistically based system is strongly dependent\\non the confidence permitted by the training set size\\nas measured by this ratio.\\n\\n\\n  An Important Trade-off \\n\\nThe model formulated above and the empirical data presented\\nsupport a number of qualitative\\ninferences about the potential of systems given a fixed\\ntraining set size.  Because training data will\\nalways be limited, such reasoning is an important part\\nof system design.  Therefore before turning to\\nsome quantitative analyses, I will examine a few such\\ninferences.\\n\\n\\nThe most important of these is in regard to linguistic\\nsophistication, that is the degree to which the\\nsystem uses knowledge of the patterns of language.\\n This kind of knowledge is extremely important,\\nsince it often allows just the right distinctions to\\nbe made.  More simplistic systems will inevitably\\nassign one choice to two different inputs\\nbecause their linguistic knowledge fails to support a\\ndistinction.  Therefore, it seems desirable to\\nincorporate as much linguistic sophistication as\\npossible.  While this is a tempting direction to take\\nfor improving system performance, there is a\\nbarrier.\\n\\n\\nConsider, for instance, the effect on data requirements\\nof incorporating new indicators.  Each indicator\\nincreases the number of distinctions which the system\\ncan make.  For example, Resnik and Hearst (1993)\\ntake into account the object of the preposition. In\\ndoing so, they distinguish cases which Hindle and Rooth (1993)\\ndid not.  As a result, the number of cases their system\\nconsiders is substantially larger than those considered\\nby Hindle and Rooth's.  In terms of the framework, Resnik\\nand Hearst have many more bins than Hindle and Rooth.\\n\\n\\nIt is easy to see that incorporating a new indicator\\nincreases the number of bins combinatorially.\\nThe size of B is multiplied by the\\nrange of the new indicator.  This results in the ratio m:Lfalling by the same factor, which, as I have argued above, can be\\ndetrimental to the overall accuracy.\\n\\n\\nThe situation is worse still if the training set is not hand annotated.\\nIn this case, introducing the new indicator creates additional ambiguity\\nin the training set since the value of the new indicator must be\\ndetermined for each training example.  This\\neffectively decreases the number of training instances\\nresulting in a further decrease in m:L.\\n\\n\\nThus, linguistic sophistication presents a trade-off\\nbetween accuracy and data sparseness.  It is a\\nbalance between poor modeling of the language and insufficient\\ndata for accurate statistics.  If we are to\\nstrike a satisfactory compromise, we need a strong\\ntheory of data requirements and ways to make more\\neconomic use of data.\\n\\n\\nOne such method is termed conceptual association,\\nas defined in Resnik and Hearst (1993).  By\\ncollecting statistics based on concepts rather than\\nindividual words, the number of bins is usually\\nreduced.  The idea is to generalise findings about\\nwords to cover other words which have the same\\nmeaning.  The advantages of this approach are extensively\\nargued in Resnik (1993) and the method is\\nused in Lauer (1994).  While concepts can help, the\\nambiguity introduced (namely what concept does a\\ngiven word belong to) may undermine the increased accuracy.\\n  Further work is needed to establish the\\neffects on data requirements of employing this strategy.\\n\\n\\nA novel extension to this approach that has not yet\\nbeen employed, would be to collect statistics at\\nvarious levels of granularity.  Statistics computed on\\ncounts of individual words would provide fine sensitivity,\\nwhile statistics computed on counts of a small set of\\nsemantic primitives (such as ANIMATE, ABSTRACT, etc.)\\nwould provide the coarsest evidence.  As many levels\\nas desired between these two extremes could be employed\\nin this way.  The level used to make\\neach choice could then be selected according\\nto the degree of confidence available at each level.\\nIf insufficient data has been seen to allow a confident\\nselection at one level, a coarser grained level\\nwould be tried.  Resnik and Hearst (1993) seem to be\\nsimulating this when they perform a t-test across\\nall levels of the WordNet hierarchy.\\n\\n\\n\\n   First Steps Towards a Theory\\n\\n  A Simple Learning Scheme \\n\\nIn this section I shall establish some lower bounds\\non the accuracy of a simple training scheme within\\nthe framework developed.  The mathematics presented in\\n Sections  through  was for the most part developed by Mark Johnson of Brown\\nUniversity and completed by the author.  I wish to thank\\nhim for his many communications in this regard.\\n\\n\\nLet \\n\\n\\n\\n\\n ,\\nthe training instances in a corpus c that fall into bin b.\\n\\n\\nLet \\n\\n\\n\\n\\n ,\\nthe frequency of the value v in the set of\\ntraining instances, t.\\n\\n\\nLet \\n\\n\\n\\n\\n ,\\nthe most common value for instances from a corpus c in a bin b.\\nWhere several values have equal frequencies, one should be\\nchosen at random.\\n\\n\\nDefine the learning algorithm such that:\\n\\n\\n\\nSince each bin has only one value with non-zero\\nprobability, V is effectively a binary set (either\\nthe instance has the non-zero value or it does not).\\nThus, L = |B|.  Notice also that the value assigned\\nhighest probability by Pc is the one most\\nfrequently falling into the bin.  That is,\\n\\n\\n\\n\\n .\\n\\n\\nTwo possible cases arise when the analyser is faced\\nwith making a decision on the basis of some\\nindication, b.  Either the corpus contains no occurrences\\nof (b, v) for any value \\n\\n\\n\\n(Case A)\\nor there is some training data which falls into the\\nbin (Case B).\\n\\n\\n   Empty Bins\\n\\n\\nCase A arises when none of the training instances fall\\ninto the bin.  Let pb denote \\n\\n\\n\\n   Non-empty Bins\\n\\n\\nIn Case B, we have at least one instance in the corpus\\nfor the given bin.  Let \\n\\n\\n\\n\\n  Skewed Bins \\n\\nAn obvious question is why systems such as Lauer (1994)\\nand Resnik and Hearst (1993) work at all\\ngiven that far less than one instance is expected for\\neach slot.  One possible answer is that different bins\\nhave widely differing frequencies.  The system quickly\\nlearns about the most frequent cases at the\\nexpense of less frequent ones.\\n\\n\\nThis can be modeled by considering the different\\ndistributions, pb defined for Case A above.\\nIn that analysis, the probability of encountering an\\nempty bin was maximised over all possible distributions.\\nHowever, if something is known about the distribution,\\nin principle a tighter bound is possible.  For example,\\nsuppose some fraction of the bins have very low\\nprobability.  That is, \\n\\n\\n\\n\\nsuch that\\n\\n\\n\\n\\nfor some small c.\\nLet \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\n .\\nNow:\\n\\n\\n\\nNow the second term is maximised when \\n\\n\\n\\n\\n\\n\\n\\n\\nSo letting \\n\\n\\n\\n\\n :\\n\\n\\n\\nSo knowing a pair of values c and \\n\\n\\n\\nis a useful,\\nif primitive, means of lowering the upper\\nbound on data requirements.  Since the distribution\\nof bins does not depend on the values we are\\nseeking to learn, it should be possible to develop\\nsimple techniques for estimating values of c and\\n\\n\\n\\n .\\n\\n\\n  Future Work \\n\\nA great deal of work remains to be done.  I will mention\\nonly a few directions where the work begs\\nto be extended.  First, the mathematical model doesn't\\ncapture several aspects of existing models, such as maximising\\nprobabilities over sequences of words and combining evidence\\nfrom multiple sources.  Second, the simple learning algorithm\\npresented differs from those used in practice in several\\nways.  It would be useful to explore the\\nrelationship between the algorithm I have proposed\\nand others in existing statistical methods (for\\nexample, the backing off method in Katz, 1987).  Third,\\nsmoothing is frequently used to alleviate data\\nsparseness (see Dagan et al., 1993), but the model does\\nnot include any means to represent the process\\nof smoothing.  Finally, almost all statistical NLP\\nsystems deal with some noise in the training data.\\nThis is especially important in systems like Yarowsky\\n(1992) where training is unsupervised.  The\\nmathematical results need to be extended to reflect\\nnoisy training data and to support reasoning about\\nthe sensitivity of data requirements to noise.\\n\\n\\n\\n  Conclusion \\n\\nIn this paper I have indicated the lack of a general\\ntheory of data requirements within the field of\\nstatistical NLP.  As a first step in the development\\nof such a theory I have presented a framework\\nfor statistical NLP systems.  I have shown how several\\nprominent works in the field fit this model and demonstrated\\na number of mathematical results which\\nsupport inferences about data requirements.  I believe\\nthis represents a significant first step along the\\nroad to a better understanding of when and how statistical\\nNLP methods can be applied.\\n\\n\\n  Acknowledgments \\n\\nWithout Mark Johnson's interest and collaboration, this paper\\nwould not exist.  Many of the key ideas originate with him\\nand I am indebted to him for his patience and attention.\\nThe work has also benefited from assistance from Richard Buckland,\\nRobert Dale, Mark Dras, Mike Johnson and Steven Sommer.\\nFinancial support is gratefully acknowledged from the Australian\\nGovernment and the Microsoft Institute.\\n\\nBibliography \\n\\n1\\n Beckwith, R., Fellbaum, C., Gross, D. and Miller, G.\\n(1991) WordNet: A lexical database organized on psycholinguistic\\nprinciples, in Zernik, Uri (ed.) Lexical Acquisition:\\nExploring On-line Resources to Build a Lexicon, Lawrence Erlbaum,\\npp211-32\\n\\n\\n2\\n Brown, P. F., Della Pietra, S. A., Della Pietra, V. J.,\\nLai, J. C. and Mercer, R. L. (1992) An Estimate of an Upper\\nBound for the Entropy of English Computational Linguistics\\n18(1) pp31-40\\n\\n\\n3\\n Dagan, I., Marcus, S. and Markovitch, S.\\n(1993) Contextual Word Similarity and Estimation\\nfrom Sparse DataProceedings of the 31st Annual Meeting\\nof the Association for Computational Linguistics,\\nColumbus, Ohio. see also cmp-lg/9405001\\n\\n\\n4\\n Dras, Mark and Lauer, Mark (1993) Lexical Preference Estimation Incorporating Case\\nInformation Proceedings of the Natural Language\\nProcessing Workshop, Australian Joint Conference on\\nArtificial Intelligence, Melbourne, Australia.\\n\\n\\n5\\n Hindle, Don and Mats Rooth (1993) Structural Ambiguity and Lexical Relations\\nComputational Linguistics Vol. 19(1), Special Issue\\non Using Large Corpora I, pp 103-20\\n\\n\\n6\\n Katz, S. M. (1987) Estimation of\\nprobabilities from sparse data for the language model\\nof a speech recognizer IEEE Transactions on Acoustics,\\nSpeech, and Signal Processing, Vol. 35(3)\\n\\n\\n7\\n Lauer, Mark (1994) Conceptual\\nAssociation for Compound Noun Analysis Proceedings\\nof the 32nd Annual Meeting of the Association for\\nComputational Linguistics, Student Session, Las Cruces,\\nNew Mexico. cmp-lg/9409002\\n\\n\\n8\\n Resnik, P. (1993) Selection and\\nInformation: A Class-Based Approach to Lexical\\nRelationships PhD dissertation, University of\\nPennsylvania, Philadelphia, PA.\\n\\n\\n9\\n Resnik, Philip and Marti Hearst (1993) Structural Ambiguity and Conceptual Relations\\nProceedings of the Workshop on Very Large Corpora:\\nAcademic and Industrial Perspectives, June 22, Ohio\\nState University, pp 58-64\\n\\n\\n10\\n Weischedel, R., Meteer, M., Schwartz, R.,\\nRamshaw, L. and Palmucci, J. (1993) Coping\\nwith Ambiguity and Unknown Words through\\nProbabilistic Models Computational Linguistics\\nVol. 19(2), Special Issue on Using Large\\nCorpora II, pp 359-82\\n\\n\\n11\\n Yarowsky, David  (1992) Word-Sense\\nDisambiguation Using Statistical Models of Roget's\\nCategories Trained on Large Corpora Proceedings of\\nCOLING-92, Nantes, France, pp 454-60\\n\\nFootnotes\\n\\n  This\\npaper has been published in the Proceedings of the Second\\nConference of the Pacific Association for Computational\\nLinguistics, Brisbane, Australia, 1995.\\n  Unless a more accurate statistical\\nprocessor based on the same indicators already exists.\\n  Brackets indicate measured on different data\\nand/or under different conditions.\\n  Reported in Resnik(1993).\\n  Reported in Dras and Lauer(1993).\\n  Some stemming is performed, so it is the number\\nof stems in the vocabulary that we want.\\n  I have allowed all the training\\nexamples as instances, even though some are\\nacquired by cyclic refinement.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nFeeding training data to statistical representations of language\\nhas become a popular past-time for computational\\nlinguists, but our understanding of what constitutes\\na sufficient volume of data remains shadowy.\\nFor example, Brown et al. (1992) used over 500 million\\nwords of text to train their language model.\\nIs this enough?  Could devouring even more data\\nfurther improve the accuracy of the model learnt?\\nIn this paper I explore a number of issues in the analysis of\\n data requirements for statistical NLP systems. A framework for viewing such systems is proposed and a sample\\nof existing works are compared within this framework.\\nFinally, the first steps toward a theory of\\ndata requirements are made by establishing\\nan upper bound on the expected error rate of a class of\\nstatistical language learners as a function of the volume\\nof training data.\\n\\n'],\n",
              " [\"\\n\\n  Background \\n  Do We Need To Know? \\n\\nEven though text is becoming increasingly available, it is often\\nexpensive, especially if it must be annotated.  Consider the\\ndecisions facing the  SLL technology consumer, that is,\\nthe architect of a planned commercial  NLP system.\\nFor each module which is to employ  SLL, an appropriate technique\\nmust be selected.  If different techniques require different\\namounts of data to achieve a given accuracy,\\nthe architect would like to know what these requirements are in advance\\nin order to make an informed choice.\\n\\n\\nFurther, once the technique is chosen, she must decide how much\\ndata to collect or purchase for training.  Because this data can be expensive,\\nforeknowledge of data requirements is highly valuable.\\nThus, in order to make statistical  NLP technology practical,\\na predictive theory of data requirements is needed.\\nDespite this need, very little attention has been paid to the\\n problem. \\n\\n\\n    Foundations For A Theory\\n\\n\\nAll the  SLL systems mentioned above employ knowledge gained\\nfrom a corpus to make decisions.  Abstractly, this knowledge\\ncan be represented as a mapping from observable features (inputs)\\nto decision outcomes (outputs).  Following Lauer (1995) I will\\ncall each distinguished input a  BIN and each possible output\\na  VALUE.  There is a probability distribution across the bins\\nrepresenting how instances fall into bins.  Also, for each bin,\\nthere is a probability distribution across the set of values\\nrepresenting how instances in that bin take on values.\\nFor the system to perform accurately, most (but not necessarily\\nall) of the instances falling in a particular bin must have the\\nsame value.\\n\\n\\nIn what follows I will make several assumptions: Training and test\\ndata are drawn from the same distributions.  The set of possible\\nvalues is binary (examples include Hindle and Rooth, 1993 and\\nLauer, 1994).  The probability of the most likely value in each\\n bin is constant. Finally, I will only consider a simple learning algorithm: collect\\nthe training instances falling into each bin and then select\\nthe most frequent value for each.  This mode-based learner is\\nemployed directly in the unigram tagger of Charniak (1993, p49)\\nand is at the heart of many systems.\\n\\n\\n  Optimal Accuracy \\n\\nThere are two sources of error in statistical language\\nlearners of the kind we are considering.  First, since the\\nvalues are not necessarily fully determined by the bins,\\nno matter what value the learner\\nassigns to a bin there will always be errors (the optimal\\nerror rate).\\nSecond, since training\\ndata is limited, the learner may not have sufficient data\\navailable to acquire accurate rules.\\nThe combination of these sources of error results in some\\ndegree of inaccuracy for the system.  We are interested in\\nestimating the accuracy for various volumes of training\\ndata.  Since the optimal error rate is independent of the\\namount of training data,  it will always exist no matter how\\nmuch data is used.\\nAs the amount of training data increases we\\nexpect the accuracy to get closer to this optimal.\\n\\n\\nLet B be the set of bins, V the set of values, \\n\\n\\n\\nthe\\nprobability that an instance falls into the bin b and\\n\\n\\n\\n\\nthe probability of the value v given the\\nbin b.  If we denote the most likely value in each bin as\\n\\n\\n\\n\\n ,\\nthen the\\nexpected value of the optimal accuracy is determined by\\nthe likelihood of this value occurring in each bin.\\n\\n\\n\\n\\n\\nIf we know the probability that an algorithm will learn the\\nvalue v for the bin b (denote this\\n\\n\\n\\n\\n ), then we can also calculate the\\nexpected accuracy rate:\\n\\n\\n\\n\\n\\nIn Lauer (1995) several results are shown concerning the\\nrelationship of these two values.  I will summarise these\\n in section  (see equations ()  and ()). \\n\\n\\n\\n  Existing Work \\n    Empty Bins and Non-empty Bins\\n\\n\\nThe most severe result of insufficient training data is that\\nsome bins can go without any training instances.  Since the\\nlearner has no indications about likely values for the bin it\\nwill be forced to guess.  To estimate how often this will\\noccur, consider the way in which m training instances\\nwould fall into the bins.  For each bin, the probability\\nthat no training instances fall into it is:\\n\\n\\n\\nI will call such bins  EMPTY BINS.\\n\\n\\nIn Lauer (1995) it is shown that for any bin b:\\n\\n\\n\\n\\n\\nLauer (1995) also bounds the expected\\naccuracy of the mode-based learner when all bins are\\nguaranteed to have at least one training instance.  When\\nthis is the case, it is shown that the expected error rate\\nis always no worse than twice the optimal error rate.\\n\\n\\n\\n\\n\\nThis is quite a useful result, since we expect the optimal\\naccuracy to be fairly high.  If the optimal\\npredictions are 90% accurate, then a mode-based learner\\nwill be at least 80% accurate after learning on just\\none instance per bin.\\n\\n\\n  Overall Expected Accuracy \\n\\nUnfortunately, we cannot normally guarantee that no bins\\nwill be empty, since the corpus is typically a random\\n sample.  However, we can combine equations ()  and () to arrive at a bound for the overall expected accuracy after training on a random sample.\\nOver non-empty bins, we know that the error rate is no worse than\\ntwice the optimal error rate for those bins.\\nSince we have assumed that \\n\\n\\n\\n\\nis constant\\n(call this p), we can infer\\nthat the optimal accuracy for the non-empty bins is\\nthe same as the optimal accuracy on all bins.  Thus:\\n\\\\mbox{EA}  =  \\\\Pr(\\\\mbox{non-empty}) \\\\mbox{EA}(\\\\mbox{non-empty})\\n+ \\\\Pr(\\\\mbox{empty}) \\\\mbox{EA}(\\\\mbox{empty}) \\\\nonumber \\\\\\\\\\n         \\\\ge  (1-e^{-m/{\\\\mid B \\\\mid}}) \\\\mbox{EA}(\\\\mbox{non-empty})\\n                + (e^{-m/{\\\\mid B \\\\mid}}) \\\\mbox{EA}(\\\\mbox{empty}) \\\\nonumber \\\\\\\\\\n         \\\\ge  (1-e^{-m/{\\\\mid B \\\\mid}}) (1-2(1-\\\\mbox{OA}))\\n                + \\\\frac{1}{2}e^{-m/{\\\\mid B \\\\mid}} \\\\nonumber \\\\\\\\\\n         =  (1-e^{-m/{\\\\mid B \\\\mid}}) (2p-1)\\n                + \\\\frac{1}{2}e^{-m/{\\\\mid B \\\\mid}}\\n\\\\end{eqnarray} -->\\nThe second step follows from the fact that\\n\\n\\n\\n\\nand\\n equation ().  The third step follows from  equation (). \\n\\n\\n\\n    Theory\\n\\n  Estimating Expected Accuracy \\n\\n Given the assumptions in section , we can arrive at a better estimate of the expected accuracy\\nwhen the distribution of bins is uniform (that is,\\n\\n\\n\\n\\n ).\\nLet the total number of training instances in a bin b be nand the number of these instances with value v be\\n\\n\\n\\n\\n :\\n\\n\\n\\nIf n is even, we must also add an additional term of\\n\\n\\n\\n\\nThis is because when there are equal numbers\\nof both values in the bin, a random guess yields an\\nexpected accuracy of 50%.  In the arguments below, I will\\ntreat all values of n as odd in order to simplify.  The\\nreader may check for herself that the results hold generally\\nwhen the above extra term is included.\\n\\n\\nUsing the fact that V is binary, the total expected accuracy\\nfor test instances in bin bwhen it contains n training instances is:\\n\\n\\n\\nBy summing over all possible numbers of training\\ninstances in a bin, we can arrive at an expression for the\\nexpected accuracy across all bins as follows:\\n\\n\\n\\nwhere \\n\\n\\n\\n\\nTo simplify this I have defined a function as follows:\\n\\n\\n\\nA result which may be easily obtained by expansion is:\\n\\n\\n\\n\\n\\n Using the assumptions in section  and the uniform bin probabilities we can now proceed to simplify:\\n\\n The last step uses equation () and \\n\\n\\n\\n\\n .\\n\\n\\n    A Computable Bound for G\\n\\n\\nThe main difficulty with the function G is the\\nappearance of \\n\\n\\n\\n\\n .\\nMost corpus-based language learners\\nuse large corpora, so we expect the number of training\\ninstances, m, to be very large.\\nSo we need a\\nmore easily computable version of G.  The following argument\\nleads to a fairly tight lower bound to G for suitably chosen\\nvalues of kj (see below):\\n\\n\\n\\nThe first step rearranges the order of addition.  The final\\nstep introduces a series of variables which limit the\\nnumber of terms in the inner sum.  The inequality holds for\\nall \\n\\n\\n\\n .\\nNotice that the kj may vary for each\\nterm of the outer sum.  Since \\n\\n\\n\\n\\nwe can use\\nthe following relation:\\n\\n\\n\\n\\n\\nLetting \\n\\n\\n\\n\\nwe can simplify as\\nfollows:\\n\\n\\n\\nThe last step introduces g and holds for all \\n\\n\\n\\n\\nThis is because in practice only the first few terms\\nof the outer sum are significant.  Thus for suitably chosen\\ng, kj this is a cheaply computable lower bound for\\nG.  A program to compute this to a high degree of\\naccuracy has been implemented.\\n\\n\\n\\n  Experiment \\n    Skewed Bins\\n\\n\\nThe assumption that bin probabilities are uniform is problematic.\\nWhen bins are uniformly probable, the expected number of\\ntraining instances in the same bin as a random test instance is\\n\\n\\n\\n\\n(\\n\\n\\n\\n\\n).\\nBut most distributions in language are highly skewed.\\nZipf's law states that word types are distributed logarithmically\\n(the nth most frequent word has probability proportional\\nto \\n\\n\\n\\n\\n ).  When this is true the expected number of\\ntraining instances in the same bin as a random test instance is\\napproximately \\n\\n\\n\\n\\n (\\n\\n\\n\\n\\n ).  Thus we can expect much more\\ninformation to be available about typical test cases.\\n\\n\\n  Simulations \\n\\n Since the mathematics in section  cannot easily be generalised to different distributions,\\nI have conducted several simulations in order to verify\\nthe mathematical results above and to explore the effect\\nof using a skewed distribution of bins.\\n\\n\\nThese simulations use a fixed number of bins (10,000), allocating\\nm training instances to the bins according to either\\na uniform or logarithmic distribution.  It then measures the\\ncorrectness of the mode-based learner on 1000 randomly generated\\ntest instances to arrive at an observed correctness\\n rate. \\n\\n\\nThis process (training and testing) is repeated 30 times for each run,\\nwith the mean being recorded as the observed accuracy.\\nThe standard deviation is used to estimate a 5% t-score confidence interval.\\n\\n\\n  Results \\n\\n Figure  shows five traces of accuracy as the volume of training data is varied.\\nThe lowest curve shows the old bound which can be achieved\\nusing the results in Lauer (1995), as represented by\\n equation ().  The other dotted curve shows  the expected accuracy predicted using equation ()  as approximated by the program described in section . The two further curves (with confidence interval bars) then show\\nthe results of simulations, using uniform and logarithmic bin\\ndistributions.\\n\\n\\nAs can be seen, the new bound given in this paper is accurate\\nfor uniform bin probabilities.  However, when the bins are\\nlogarithmically distributed learning converges significantly\\nmore quickly, as suggested by the reasoning about expected\\n number of relevant training instances (see section ). Perhaps surprisingly though, the logarithmic distribution\\nappears to eventually fall behind the uniform one once there is plenty\\nof data.  This might be explained by the presence of very rare bins\\nin the logarithmic distribution which thus take longer to learn.\\nBoth these observations are crucial to reasoning\\nabout data requirements for  SLL.\\n\\n\\n\\n  Conclusion \\n\\nIf commercial  NLP systems are to be developed from the current\\nbatch of research prototypes for  SLL, then a predictive theory\\nof the data requirements of such systems is necessary.  In this paper\\nI have explored the dependence of the expected accuracy of a simple\\nstatistical learner on the volume of training data.  When the\\nprobability distribution of inputs is uniform, I have shown how to\\ncompute the expected accuracy, a result backed up by simulations.\\nIn particular, an average of four training instances per bin can\\nbe expected to yield an error rate only 50% worse than the\\noptimal error rate.\\n\\n\\nWhen the distribution is non-uniform, simulations show that\\nconvergence can be much more rapid.  Error rates only 50% worse\\nthan optimal result from only three training instances per\\nbin.  However,\\nwhen data is abundant,\\nnon-uniform distributions result in higher error rates\\nthan the estimate produced by assuming uniformity.\\n\\n\\n  Acknowledgements \\n\\nI am grateful to Mark Johnson, without whom this work\\nwould not exist, and also to Robert Dale, Mark Dras, Mike Johnson and\\nJohn Potter.  Financial support is gratefully acknowledged from the\\nAustralian Government and the Microsoft Institute.\\n\\nBibliography \\n\\n1\\n Brent, Michael.\\n1993.\\nFrom Grammar to Lexicon: Unsupervised Learning of Lexical Syntax.\\nIn Computational Linguistics, Vol 19(2),\\npp243-62.\\n\\n\\n2\\n Charniak, Eugene.\\n1993.\\nStatistical Language Learning.\\nMIT Press, Cambridge, MA.\\n\\n\\n3\\n Dagan, I. and Itai, A.\\n1990.\\nA Statistical Filter For Resolving Pronoun References.\\nIn Proceedings of the Seventh Israeli Conference on\\nArtificial Intelligence and Computer Vision, Ramat Gan, Israel. pp125-35.\\n\\n\\n4\\n de Haan, Pieter.\\n1992.\\nOptimum Corpus Sample Size?\\nIn Leitner, Gerhard (ed.) New Directions in\\nEnglish Language Corpora.\\nMouton de Gruyter, Berlin.\\n\\n\\n5\\n Hindle, D. and Rooth, M.\\n1993.\\nStructural Ambiguity and Lexical Relations.\\nIn Computational Linguistics Vol. 19(1),\\npp103-20.\\n\\n\\n6\\n Lauer, Mark.\\n1995.\\nHow Much Is Enough? Data Requirements for Statistical NLP.\\nIn Proceedings of the 2nd Conference of the\\nPacific Association for Computational Linguistics, Brisbane, Australia.\\ncmp-lg/9509001\\n\\n\\n7\\n Lauer, M. and Dras, M.\\n1994.\\nA Probabilistic Model of Compound Nouns.\\nIn Proceedings of the 7th Australian Joint\\nConference on Artificial Intelligence, Armidale, NSW, Australia.\\nWorld Scientific Press, pp474-81. cmp-lg/9409003\\n\\n\\n8\\n Marcus, M., Marcinkiewicz, M. A. and Santorini, B.\\n1993.\\nBuilding a Large Annotated Corpus of English: The Penn Treebank.\\nIn Computational Linguistics Vol 19(2),\\npp313-30.\\n\\n\\n9\\n Yarowsky, David.\\n1992.\\nWord-Sense Disambiguation Using Statistical Models of\\nRoget's Categories Trained on Large Corpora.\\nIn Proceedings of COLING-92, France, pp454-60.\\n\\nFootnotes\\n\\n  This paper has been\\n accepted for publication at the Eigth Australian Joint Conference\\n on Artificial Intelligence, Canberra, 1995.\\n  See de Haan (1992) for an investigation of sample\\nsizes for linguistic studies.\\n  Note that this does not require that the most\\nlikely value be the same value in each bin; only that whatever\\nthe most likely value is has a constant probability.\\n  The results were generated using an optimal\\nvalue probability of p = 0.9 (thus the optimal accuracy rate is 90%).\\nSimulations with other values of p did not differ qualitatively.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThe paradigm for  NLP known as  STATISTICAL LANGUAGE\\nLEARNING ( SLL) has flourished in recent times, being seen\\nas a quick and easy way to get off the ground.\\nResearch systems have been launched at many  NLP\\nproblems including sense disambiguation (Yarowsky, 1992),\\nanaphora resolution (Dagan and Itai, 1990),\\nprepositional phrase attachment (Hindle and Rooth, 1993)\\nand lexical acquisition (Brent, 1993).  This has all been fueled by\\nthe large text corpora which are\\nincreasingly available (Marcus et al., 1993).\\nSince these systems learn to navigate language by consuming\\ntext, they are critically dependent on the data that\\ndrives them.\\nIn this paper I address the practical concern of predicting\\nhow much training data is sufficient for a given system.  First,\\nI briefly review earlier results and show how these can be\\ncombined to bound the expected accuracy of a mode-based\\nlearner as a function of the volume of training data.\\nI then develop a more accurate estimate of the expected\\naccuracy function under the assumption that inputs\\nare uniformly distributed.  Since this estimate is expensive\\nto compute, I also give a close but cheaply computable\\napproximation to it.  Finally, I report on a series of simulations\\nexploring the effects of inputs that are not uniformly\\ndistributed.\\n\\n'],\n",
              " ['\\n\\n  Introduction \\n\\nRecently, RD efforts involving machine translation of different\\nlanguage families, such as Japanese and English, have become popular\\n ,,.  However, differences in perspective and how objects are thought of, in such different\\nlanguage families, affect how expressions are structured.  These\\ndifferences in expression structure make it difficult to convert from\\none language to another mechanically.  For example, in\\nJapanese-to-English machine translation, the more typical the Japanese\\nexpression, the more difficult it is to translate into English, due\\nto differences in the way that thought processes are expressed\\nlinguistically.\\n\\n\\nAs a means of solving this problem, efforts have been made in the area\\n of sub-languages  or knowledge-based translation  .  But these methods currently require human intervention, that is, Japanese expressions must be rewritten into\\neasily translatable Japanese.  In other words, there is a need to\\nre-write the text into a more English type of concept before machine\\ntranslation can be performed.\\n\\n\\nThis action of re-writing is normally known as pre-editing\\n .  Pre-editing techniques include: use of a single word so as to have only one meaning; limiting the use of `joshi\\'\\n(Japanese post-positional words) and of auxiliary verbs and other words\\nlikely to be interpreted several ways; replacing, in advance, any\\nwords which may have been omitted; and the re-writing of idiomatic\\nexpressions to more general expressions.  These all represent efforts\\nto re-write the source into unambiguous Japanese which can be\\ntranslated into English, literally.\\n\\n\\nThe theory and rationale of pre-editing in Japanese-to-English\\ntranslation would appear to be closely related to the principle of\\nelementary compositionality.  Elementary compositionality hypothesizes\\nthat ``the meaning of the entire expression is the sum of the meanings\\n of the various portions of the expression\\'\\' .  This principle is taken as basic in existing machine translation systems\\nand supports a most effective method between languages of the same\\nfamily.  When seeking high quality machine translation, however, there\\nstill remain serious problems to be dealt with.\\n\\n\\nJapanese-to-English machine translation has reached the stage where\\nsentences that allow word-by-word transfer from Japanese to English,\\nfollowed by assembly into the final sentence form (i.e. where literal\\ntranslation is possible), can be translated by current technology. But\\nthere is a wide difference in the thought process constituting the\\nbackground of linguistic expression between the Japanese and English\\nlanguages.  Therefore, translations using existing systems require\\npre-editing to re-write the original Japanese sentences into a form\\nthat will enable application of the elementary compositional method,\\nor in other words, a form that can undergo literal translation.\\n\\n\\nTo go beyond the limits of conventional translation methods based on\\nelementary compositionality, we have proposed the Multi-Level\\n Translation Method, ,, based on the Constructive Process Theory of Language\\n , and have made the experimental system ALT-J/E, the Automatic Language Translator -- Japanese to English. \\n\\n\\nThis method focuses attention on the fact that many expressions have\\nmeanings that cannot be deduced directly from the combination of the\\nmeanings of the individual words.  It is a method of translation which\\ngrasps the structure and meanings of expressions as a whole.  The\\nmeanings of words will vary according to the manner and context in\\nwhich the words are used.  Many expressions have meanings that cannot\\nbe explained directly from the meanings of each individual word.  With\\nattention focused on these characteristics, those units having\\nstructural meanings have been arranged systematically into a form of\\nlinguistic knowledge. This knowledge is being used in analysis of\\nthe Japanese language and conversion of the Japanese into English.  As\\nsuch, it represents a big step forward towards the fundamental solution of\\npreviously existing problems, hitherto only solvable by pre-editing.\\n\\n\\n  The Constructive Process Theory and the Multi-Level Translation  Method \\n  The Constructive Process Theory of Language \\n  Problems of Conventional Translation Systems \\n\\nThe transfer method and the pivot method have been regarded as the\\n methods most commonly used in machine translation . Whereas the pivot method hypothesizes an intermediate language common\\nfor both the original and the target language, the transfer system\\ndiffers in that it uses an intermediate language for each language in\\norder to convert meanings from one language to another.  Both have in\\ncommon the fact that they establish an intermediate language to\\nrepresent meaning that is separate from the surface expression.\\n\\n\\nIt is possible to seek the background regarding these methods in the\\ndualism of computational linguistics\\n ,, that discriminates between surface and deep structures.\\n\\n\\nBut the deep structure as suggested by computational linguistics\\ncannot be said to have achieved success.  In fact, concepts which deny\\nthe existence of deep structure have been suggested of late\\n ,,. \\n\\n\\nComputational linguistics can be thought of as derived from\\n computational logic .  It hypothesizes that the meanings of expressions do not rely on languages but are a form of\\ncommon existence, and it also hypothesizes that the meaning of the\\nexpression in its entirety is the sum total of the meanings of\\nsections of the expression.  But these hypotheses are only partially\\nvalid for actual languages.  Thus, it would be difficult to apply this\\ntheory of computational linguistics to machine translation which deals\\nwith actual text, particularly to translation involving a pair of\\nlanguages with different origins such as Japanese and English.\\n\\n\\n  The Concept of the Constructive Process Theory of Language \\n\\nWe believe that the key to solving this problem lies with the linguistic\\n evolution theory of Tokieda Grammar , one of the main schools of traditional study of the Japanese language.\\nTokieda Grammar is derived from the theory of Norinaga Motoori\\n  and it was developed from a critique of the  linguistic theory propounded by Saussure .  It is regarded as one of the 4 major theories of grammar of Japan.\\n\\n\\nAccording to the Constructive Process Theory of Language, language is\\nto be understood as a compound body of processes as in the field of\\nphysics, and can be viewed as the relationship between the `object\\',\\n`(speaker\\'s) recognition\\' and `expression\\'.  The relationship between\\n`object\\' and `recognition\\' can be explained by `Epistemology\\' or\\n`Reflection Theory\\', and between `recognition\\' and `expression\\' by\\n`Linguistic Norm\\'.  The sole element that is common between two\\ndiffering languages would be the `object\\' and since there are\\ndifferences in how the `object\\' is viewed and understood between\\nlanguages, everything beyond `recognition\\' will differ depending on\\nthe language in question.  The very existence of `deep structure\\'\\nwhich is neither `object\\' nor `recognition\\' is denied altogether.\\n\\n\\n Also, according to Tsutomu Miura  who built on the Constructive Process Theory, the meaning of linguistic expressions is\\nthe relationship between `object\\', `recognition\\' and `expression\\'.\\nThis relationship is objectively connected to the `expression\\' itself.\\nThe concept of regarding ``relationship\" as meaning resembles the\\n recent work in situation semantics .  But where situation semantics confuses ``meanings of expression\\'\\' with ``meanings\\nof the field where the expression is placed\\'\\', Miura Grammar draws a\\ndistinct line between the two and propounds a theory pertaining to\\n``meanings of expression\".\\n\\n\\nWhen language is regarded thus as a compound body of various\\nprocesses, the following two points, placing importance on the\\nmeaning, are seen as important for machine translation.\\n\\n\\n1.\\n Expressions are classified into   `subjective\\' which are a direct expression of the emotions,\\n  intentions, and judgment of the speaker and `objective\\' which\\n  express the object in the form of a concept, and reproduce it within\\n  the framework of the target language.\\n\\n\\n2.\\nThe structure, which involved with the object, is reflected by\\n  its recognition and this is further reflected in the structure of\\n  the expression.  Therefore, the structure of an expression is to be\\n  considered as a part of its meaning, and the meaning is to be handled\\n  accordingly.\\n\\n\\n\\n  The Multi-Level Translation Method \\n\\nALT-J/E has implemented the Multi-Level Translation Method with\\ndue consideration of the foregoing two points.  The translation\\n process is outlined in Figure .  First, the Japanese expression is analyzed and separated into subjective and objective\\nparts.  The subjective part (for example, tense and aspect) is\\ntranslated separately from the objective part.  Second, the objective\\npart is translated in three stages (the Multi-Level Transfer Method).\\nIf there are any idiomatic expressions, these are translated first, in\\nthe Idiomatic Expression Transfer.  Then any expressions whose\\npredicates and arguments match an entry in the semantic pattern\\ndictionary are translated as part of the Semantic Valency Transfer.\\nFinally, any remaining expressions are translated by the General Pattern\\nTransfer.  The entire process is designed to prevent loss of meaning\\nthrough elementary decomposition.\\n\\n\\n\\n  Organization of Linguistic Knowledge \\n  Semantic Categories of Words \\n\\nNouns are used to express existing objects as concepts.  Depending on\\nhow the object is viewed and understood, various profiles of the\\nobject are picked up or discarded.  Which noun is to be used is\\nselected based on a profile corresponding to the view of the speaker.\\n\\n\\nIn conceiving the object, the special and individual characteristics\\nare discarded and the features are recognized as a single unit.  Among\\nthe concepts analyzing semantic features, there have been attempts to\\nexplain the meaning of nouns as a bundle of detailed meanings or\\nfeatures.  But the concept that is represented by a noun is a single\\nconclusive unit of recognition.  It is, therefore, to be handled as an\\nirreducible concept, that can only be captured as a whole.  We\\nclassify these concepts as  SEMANTIC CATEGORIES.\\n\\n\\nFor example, the objective concept represented by the word   school would include ``the school as an organization\\'\\' and ``the\\nschool as a given location\\'\\'. In machine translation, there is a need\\nto know which of these the word school signifies.  In order to\\ndo this, thought was given to what type of profile is conceived for\\nthe object when it is used. These profiles were then classified as\\nsemantic categories held by each noun.\\n\\n\\nAround 3,000 categories were specified, about the number of important\\nwords which the normal person feels comfortable in using.  The\\nsemantic categories are ordered into two  IS-A hierarchies. These\\nare the common noun semantic categories, some 2,800 categories (12\\nlevels deep), and the proper noun semantic categories, some 200\\ncategories (9 levels deep).  Based on this ontology, a semantic\\n  word dictionary was compiled with 400,000 index words.  The maximum\\nnumber of semantic categories per word is 5 common noun categories and\\n10 proper noun categories.  Overall, an average of 2 categories are\\nassigned to each noun in the dictionary.\\n\\n\\nProjects using conceptual classifications similar to our semantic\\ncategorization, have previously had around 30 to 50 categories. EDR\\n , is implementing plans to extend to 500 categories. ALT-J/E is the first case of a system with a precision of some 3,000 categories and a large scale dictionary (around 400,000 index words).\\n\\n\\n  The Meaning of Expression Structures  as viewed from Declinable Words \\n\\nIn Japanese both verbs and adjectives are declinable.  The basic\\nstructure of Japanese sentences revolves mainly around\\npredicates.  Looking at the declinable words, the meanings of the\\npredicates themselves, and of their basic structure, can be understood\\nby examining the types and meaning of nouns that fill the predicate\\'s\\ncase frames. A semantic structure dictionary with some 6,000\\nindex words (verbs and adjectives) consisting of 15,000 patterns has\\nbeen prepared for use in analysis, transfer and generation.\\n\\n\\nWith this method, analysis is performed by having units of semantics\\nand structure correspond to one another so that ambiguity in\\nstructural analysis is reduced.  Each Japanese entry has an English\\ntranslation. As soon as the structure of the Japanese is determined in\\nthe source language analysis, the basic English structure can be\\ndetermined from the English form structure in the semantic structure\\ndictionary.  This is helpful in avoiding the need for an additional\\nconversion process.\\n\\n\\n\\n  Realization of New Functions \\n\\nAmong the functions which have been realized through this method, the\\nfollowing  will solve problems previously requiring pre-editing.\\n\\n  Precise Selection of Translation    According to Meaning \\n\\nPreviously, re-writing of the original text was required so that each\\nword in the source would have an unambiguous translation in the\\ntarget language.  But, due to the rich information in the semantic\\nstructure and word dictionaries, it has now become possible to\\n differentiate into precise translations as shown in Figure . Manual rewriting is no longer necessary.\\n\\n\\nIt has also become possible to translate typically Japanese\\nexpressions which were previously difficult to translate into English\\nas well as to differentiate between translation of idiomatic\\nexpressions and general expressions.\\n\\n\\nFurther, it has become clear, after experimenting, that in order to\\ntranslate the meanings of Japanese declinable words (verbs and\\n adjectives) as shown in Figure  into English, it is necessary to have a description of detailed rules. It has been\\nascertained that this, in turn, requires a classification of detailed\\nsemantic categories.  A look at rules involving the 15,000 cases\\nregistered in the expression structure dictionary reveals the frequent\\nuse of semantic categories classified in the 8th to 9th step in the\\nsemantic category system.  This shows that at least the top nine\\nlevels of our ontology (about 2,000 semantic categories) are needed to\\nsuccessfully disambiguate most predicates.\\n\\n\\n  Automatic Re-Writing Function in Japanese \\n\\nThere are many cases in which typical Japanese expressions, where two\\nor more words are combined to form idiomatic expressions, cannot be\\nliterally translated and even if they were literally translated, would\\nbe inappropriate in the English language.  It would be advantageous to\\nhave such expressions automatically converted within the system into\\nmore easily translatable Japanese.  But previous attempts to do this\\nhave foundered due to the problems of unwanted side effects.\\n\\n\\nThe Multi-Level Translation Method has enabled a precise enumeration\\nof conditions for the application of rules through detailed semantic\\ncategories.  This has enabled side effects to be reduced and\\neffectively re-writes the Japanese prior to translation.\\n\\n\\n Figure  shows an example of a Japanese sentence, which normally has numerous predicates but which has been automatically\\nrewritten so as to have fewer.  Three Japanese verb phrases are\\nchanged into prepositional phrases in English.\\n\\n\\n  Supplementation of ellipsed elements through Context Processing \\n\\nThe Japanese language normally omits elements that are easily\\nrecoverable from context, particularly subjects and objects.  But in\\nEnglish, these elements are in most cases obligatory.  Previously,\\nsupplementing these constituted an important part of pre-editing.\\n\\n\\nALT-J/E has, in addition to the semantic structure dictionary and\\nsemantic categories, introduced an analysis of the semantic categories\\nof predicates which allows the supplemention of ellipses using the\\nsemantic relations between sentences.\\n\\n\\n  Translation  of Compound Words \\n\\nThe Japanese language generates new words (compound words) which are\\nan amalgamation of a number of nouns, prefixes and suffixes (a\\ncharacteristic of agglutinative languages).  This type of compound\\nword is generated without limitation and so it is impossible to have\\nthem all registered in a dictionary in advance.  With conventional\\ntranslation methods, registration of these compound words in the\\ndictionary was an important issue for pre-processing.\\n\\n\\nALT-J/E uses semantic categories to analyze compound words to\\nfind the semantic relationships of their constituents.  This function\\nmakes the translation of unknown compound words possible.  It also\\nenables the automatic translation of compound words whose meanings\\nvary  depending on the manner in which they are used within a\\nsentence.\\n\\n\\n\\n  The Benefits of the Multi-Level Translation  Method and Future Issues \\n  Benefits of the Multi-Level Translation Method \\n\\nThe experimental Japanese-to-English machine translation system   ALT-J/E, based on the Multi-Level Translation Method, is currently\\nbeing debugged.  To examine the potential of this method, newspaper\\nlead sentences (a summary preceding the newspaper article proper,\\ngenerally consisting of 3 to 5 sentences per article, and averaging 20\\nwords per sentence) were translated in the following experiments.\\n\\n\\nBlind Test:\\n(BT) Experiments conducted with articles chosen\\nat random with no registration of unknown words, nor rule revisions.\\n\\n\\nWindow Test:\\n(WT) Experiments on a sample of text with\\nrevision of the system allowed. Registration of unknown words and rule\\n  revisions are conducted during the test.\\n\\n\\n(In both cases, the original text was translated without any pre-editing) \\n\\n\\nThe standards used for the evaluation are an improved version of the\\n ALPAC standards  with 10 points being a `perfect\\' translation and grades 6 or higher being a pass (the sentence is\\nunderstandable from reading the translation only).  Grading was\\nconducted by outside company specialists in translation.  The average\\nof grades as judged by three specialists in Japanese-to-English\\ntranslation were taken to determine passing or failing grades for each\\nindividual sentence.\\n\\n\\nThe condition for a passing grade was that the meaning could be\\nunderstood by looking only at the translation.  Thus, sentences that\\nwere ruled as passing are not guaranteed to be stylistically\\nappropriate (or even grammatical).  But it is estimated that a quality\\nlevel equal to or better than that of existing Japanese-to-English\\nmachine translation systems has been achieved.\\n\\n\\nAccording to this test, the pass rates for the blind test were 40 to\\n50% , and over 60% for the window test. This indicates a passing\\nratio of about double that of existing Japanese-to-English machine\\ntranslation systems.  For tests pertaining to technical subjects\\n(which are easier to translate than the newspaper lead sentences), a\\npass rate of 80% was achieved.\\n\\n\\nBased on the above results, we judge that with the Multi-Level\\nTranslation Method, a major step toward realization of a\\nJapanese-to-English machine translation system requiring no\\npre-editing has been achieved.\\n\\n\\n  Future Issues \\n\\nThe major problem currently being faced is the need for improvement of\\nthe translation quality of long sentences (of 30 words or longer) and\\nfor the overall improvement of the English in the translated text.  To\\nmeet this challenge, research efforts are presently being focussed on\\nan extended Japanese-to-English transfer method designed to analyze\\nthe meaning of the structure of declinable words and to directly\\nestablish an appropriate English structure to correspond to this.\\nThis direct parse-tree transfer method will be adding a new path to\\nthe three transfer paths for objective expression in the Multi-Level\\nTransfer Method, further improving and strengthening it.\\n\\n\\nOver the long term, research efforts are being extended to include a\\nreview of the system of parts of speech in the Japanese language and\\nto extend the semantic hierarchy to multiple dimensions.\\n\\n\\n\\n  Summary \\n\\nThis paper has presented the results of using the Multi-Level\\nTranslation Method, based on the Constructive Process Theory. It has\\nshown that the method enables a Japanese-to-English machine\\ntranslation system to function effectively without manual pre-editing.\\nIn fact, the major reasons for pre-editing the source text are no\\nlonger valid. But there remain problems with translating typically\\nlong Japanese sentences and a need to improve the quality of finished\\ntranslations.\\n\\n\\nWe call the limited use of semantic information used in the\\nMulti-Level Translation Method meaning analysis.  It is\\nestimated that this level of technology is limited to a maximum\\nsuccess rate of approximately 80%.  To attain a higher level of\\naccuracy it is essential to establish an understanding of meaning in\\ncontext, based on the expansion of general and specialized knowledge\\nof the target domains.  We call this meaning comprehension.\\nHowever, since it is difficult to establish the comprehension of\\nmeaning in extremely broad or general fields, it is planned to\\nestablish the limits for processing based on meaning analysis first,\\nand then follow up with research into the area of meaning\\ncomprehension.\\n\\n\\n  Acknowledgment \\n\\nThe authors wish to thank Masahiro Miyazaki, Kentaro Ogura\\nand other members of the research group on machine translation for\\ntheir valuable contribution to discussions; and especial thanks to\\nFrancis Bond for revising this paper before archiving it.\\n\\nBibliography \\n\\n ALLWOOD, J.,  L.G. ANDERSON,   O. DAHL.\\n1971.\\nLogics in Linguistics.\\nCambridge Univ. Press.\\n\\n\\n AUTOMATIC LANGUAGE PROCESSING ADVISORY COMMITTEE.\\n1966.\\nLanguage and machine: Computers in translation and linguistics.\\nTechnical report, National Academy of sciences, U.S. National\\n  Research Council.\\n\\n\\n BARWISE, J.,   J. PERRY.\\n1981.\\nSituation and attotudes.\\nJournal of Philosophy 78.668-691.\\n\\n\\n BRESNAN, J. (ed.)\\n1982.\\nThe Mental Representation of Grammatical Relations.\\nCambridge, Mass.: MIT Press.\\n\\n\\n CHOMSKY, NOAM.\\n1956.\\nThree Models for the Description of Language, volume IT-2.\\nIRE Trans.\\n\\n\\n CHOMSKY, NOAM.\\n1965.\\nAspects of the Theory of Syntax.\\nMIT Press.\\n\\n\\n CRESSWELL, M.J.\\n1973.\\nLogics and Languages.\\nLondon: Methuen  Co. Ltd.\\n\\n\\n EDR.\\n1990.\\nConcept dictionary.\\nTechnical report, Japan Electronic Dictionary Research Institute,\\n  Ltd.\\n\\n\\n FILLMORE, C.J.\\n1975.\\nTOWARD A MODERN THEORY OF CASE  OTHER ARTICLES.\\nSanseidou Publishing.\\n(Japanese Translation).\\n\\n\\n IKEHARA, SATORU.\\n1989.\\nMulti-level machine translation system.\\nFuture Computer Systems 1.261-274.\\n\\n\\n IKEHARA, SATORU,  MASAHIRO MIYAZAKI,  SATOSHI\\n  SHIRAI,   Y. HAYASHI.\\n1987.\\nSpeaker\\'s recognition and multi-level-translating method based on it.\\nTransactions of the Information Processing Society of Japan\\n  28.\\n(in Japanese).\\n\\n\\n IKEHARA, SATORU,  MASAHIRO MIYAZAKI,  SATOSHI\\n  SHIRAI,   AKIO YOKOO.\\n1989.\\nAn approach to machine translation method based on constructive\\n  process theory.\\nReview of the Electrical Communications Laboratories\\n  37.39-44.\\n\\n\\n LANCELOT, CLAUDE,   ANTOINE ARNAULD.\\n1972.\\nGrammaire generale et raisonnee, les fondements de l\\'art de parler.\\nIn Port Royal Grammar, ed. by Paul Reitsch. Tokyo: Taishukan\\n  Publishing.\\n(Japanese Translation).\\n\\n\\n MENDELSON, E.\\n1979.\\nIntroduction to Mathematical Logics.\\nD. Van Nostrand Company.\\n\\n\\n MIURA, T.\\n1967.\\nTheory of Recognition and Languages.\\nKeiso Shobou.\\n(in Japanese).\\n\\n\\n MOTOORI, NORINAGA.\\n1779.\\nKotoba no tama-no-o.\\nChikuma Publishing.\\n(in Japanese).\\n\\n\\n MT SUMMIT-I (ed.)\\n1987.\\nProceedings of MT Summit-I.\\n\\n\\n MT SUMMIT-II (ed.)\\n1989.\\nProceedings of MT Summit-II.\\n\\n\\n NAGAO, MAKOTO.\\n1985.\\nEvaluation of the quality of machine-translated sentences and the\\n  control of language.\\nJournal of the Information Processing Society of Japan\\n  26.1197-1202.\\n\\n\\n NAGAO, MAKOTO.\\n1989.\\nJapanese view future of machine translation.\\n In , 123-140. \\n\\n\\n NIRENBURG, SERGEI.\\n1989.\\nKbmt-89-a knowledge-based MT project at Carnegie Mellon\\n  University.\\n In , 141-147. \\n\\n\\n NOMOTO, K.\\n1986.\\nFrege\\'s Philosophy of Languages.\\nKeiso publishing.\\n(in Japanese).\\n\\n\\n SAUSSURE, F.D.\\n1909.\\nCOURS DE LINGUISTIQUE GENERALE.\\nKeiso Publishing.\\n(Japanese Translation).\\n\\n\\n TOKIEDA, MOTONAGA.\\n1941.\\nThe Principle of Linguistics.\\nIwanami Bookstore.\\n(in Japanese).\\n\\n\\n TOMABECHI, H.\\n1987.\\nDirect memory translation.\\nIn Proceedings of IJCAI-87.\\n\\n\\n TOMITA, M.\\n1987.\\nAn efficient augmented-context-free parsing algorithm.\\nComputational Linguistics 13.31-46.\\n\\nFootnotes\\n\\n  Now:  NTT Communication Science\\n            Laboratories.\\n   Regarding the difference\\nbetween subjective and objective expressions, there is the theory\\n     of Port Royal , before Norinaga Motoori. \\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nRecently, several types of Japanese-to-English machine translation\\n  systems have been developed, but all of them require an initial\\n  process of rewriting the original text into easily translatable\\n  Japanese.  Therefore these systems are unsuitable for translating\\n  information that needs to be speedily disseminated.  To overcome\\n  this limitation, a Multi-Level Translation Method based on the\\n  Constructive Process Theory has been proposed.  This paper describes\\n  the benefits of using this method in the Japanese-to-English machine\\n  translation system ALT-J/E.\\nIn comparison with conventional compositional methods, the\\n  Multi-Level Translation Method emphasizes the importance of the\\n  meaning contained in expression structures as a whole.  It is shown\\n  to be capable of translating typical written Japanese based on the\\n  meaning of the text in its context, with  comparative ease.\\n  We are now hopeful of carrying out useful machine translation with\\n  no manual pre-editing. \\n\\n'],\n",
              " ['\\n\\n  Introduction  \\n\\nThe following paper presents an approach for automatic acquisition\\nof linguistic knowledge from observations within a given domain;\\nit thus addresses a topic from the field of machine learning\\n(cf. Michalski (1986)), or more precisely, from the field of machine learning\\nof natural language, as it is sometimes called\\n(cf. Powers and Reeker (1991)).\\nThe last decade has seen a growing interest in the application of\\nmachine learning to different kinds of linguistic domains\\n(cf. Powers and Reeker (1991)) and has been motivated by different objectives,\\nsuch as modelling cognitive processes or overcoming the knowledge-acquisition\\nbottleneck in natural-language processing systems. The principal motivation\\nfor our approach is theoretical. The automatically induced analyses can\\nbe compared to existing linguistic descriptions and thus can confirm\\nproposed analyses or provide alternative representations. On the other\\nhand, the approach can be used by descriptive linguists as a tool to\\nobtain a rough structuring of an entirely new domain (e.g. a language\\nthat has not yet been investigated).\\nTheoretical\\napproaches and implemented systems cover subjects from many different\\nlinguistic areas and use different kinds of learning strategies. Some are\\nspecially designed for a particular task, while others are more general and\\ncan thus be applied to other tasks as well. Whereas the present paper\\npursues a general approach that is not restricted to a specific\\nlinguistic domain, the system is crucially determined by the properties\\nof the chosen representation language.\\n[*] The presented work was partly supported by the\\nDeutsche Forschungsgemeinschaft (DFG).\\nFor helpful comments and discussions on the topic\\nwe would like to thank Gerald Gazdar, Dafydd Gibbon, James Kilbury, Ingrid\\nRenz, and Markus Walther.\\nIn designing a learning system the choice of a language for representing\\nthe acquired knowledge is crucial for the quality of the output of the\\nlearning task. One requirement for a linguistic representation formalism\\nis that the information can be structured in a way that captures\\ngeneralizations over linguistic objects in order to minimize redundancy.\\nSince many linguistic generalizations have exceptions that can only be\\ntreated adequately if the representation formalism includes some device for\\nhandling default information, we have chosen the language  DATR (cf. Evans and Gazdar (1989), (1990)), which allows regularities and\\nsub-/irregularities to be expressed in a uniform way. \\nWe briefly summarize the main features of  DATR here but presuppose a basic\\nfamiliarity with the language as described in (Evans and Gazdar (1989)).\\n DATR is a declarative formalism for the definition of inheritance networks.\\nIt includes orthogonal multiple inheritance and a\\ndefault-inheritance mechanism.\\nA network description in  DATR is called a theory\\nand describes a set of objects (nodes). The properties of an object\\nare defined by path-definition pairs, where a path consists of an\\nordered sequence of atoms (enclosed in angle brackets). The definition can\\nbe either the directly stated value (atomic value or sequence\\nof atomic values) of the property, an inheritance descriptor that\\nstates where the value of that property can be inherited from, or a\\nsequence of inheritance descriptors. An inheritance descriptor can\\nrefer to another node, path or node-path pair of the theory. The triple\\nconsisting of a node, a path, and a definition is called a\\ndefinitional sentence.\\n The simple  DATR theory in Fig.  encodes information about English verb morphology. It contains the three\\nnode definitions VERB, Love, and Come, which contain three,\\ntwo, and four definitional sentences, respectively. The node definition\\nVERB encodes the information that all past tense forms of a verb\\nare like the root plus _ed, and all present tense forms are like the\\nroot, with the exception of the form for third singular. As a regular verb\\nLove inherits all information except the morphological root from\\nthe node VERB. In contrast Come deviates from the regular\\nverbs in its past tense forms, which are therefore\\nspecified in the node definition. All other information can be inherited\\nfrom VERB.\\n\\n\\n     a simple  DATR theory\\n\\n\\n VERB: [mor past] == (\"[mor root]\" _ed)\\n\\t\\t \\t\\t [mor pres tense] == \"[mor root]\"\\n\\t\\t \\t\\t [mor pres tense sing three] == (\"[mor root]\" _s).\\n\\n\\nLove: \\t\\t [] == VERB\\n\\t\\t \\t\\t [mor root] == love.\\n\\n\\nCome: \\t\\t [] == VERB\\n\\t\\t \\t\\t [mor root] == come\\n\\t\\t \\t\\t [mor past] == came\\n\\t\\t \\t\\t [mor past participle] == [mor root].\\nThe information expressed in a  DATR theory is accessed by queries\\nconcerning objects and their properties. A query consists of a\\nnode-path pair and returns an atomic value (or a sequence of atomic values)\\nor fails. Seven inference rules and a default mechanism are given to\\ndeterministically evaluate the queries.\\nThe query Love:[mor pres tense sing two] evaluates to love\\n for the theory in Fig. . A query together with its returned value is called an extensional sentence.\\n\\n\\n  Inference of DATR theories  \\n\\nMany learning systems use the same formal language to represent the input data\\nand the acquired knowledge. Extensional sentences (which constitute the\\noutput of the conventional inference in  DATR) form a natural sublanguage\\nof  DATR which is suitable to represent the input data. Since extensional\\nsentences all have atomic values and thus are not related to each\\nother, they can be taken as representing independent and unstructured facts\\nabout a given linguistic domain. The learning task then consists in\\nforming a  DATR theory which accounts for the observed facts through\\n adequate structuring. \\nFor an acquired  DATR theory to be regarded as adequately characterizing\\na given set of observations it has to meet at least the following criteria\\n(in addition to the general syntactic wellformedness conditions that hold\\nfor every  DATR theory):\\n\\nconsistency with respect to the input data\\n\\ncompleteness with respect to the input data\\n\\nstructuring of the observed data by inheritance relationships\\n\\nstructuring of the observed data by generalizing them\\n\\n\\nThe first two of these criteria constitute minimal, formal requirements\\nthat can be verified easily. A  DATR theory is consistent with respect to\\na given set of extensional sentences if, for every query that constitutes\\nthe left-hand side of one of the extensional sentences, the returned value\\nis that of the extensional sentence. If this holds for all left-hand sides\\nof the extensional sentences the theory is also complete with respect to\\nthe input data.\\nThe last two criteria rely more on intuitions and cannot be checked so easily.\\nThe inferred  DATR theory should structure the observed data so that it\\nreveals relationships that exist between the extensional sentences. A  DATR theory expresses such relationships by the use of inheritance descriptors.\\nThe generalization of the observed data is twofold. First of all, a set of\\nspecific facts should be generalized, whenever this is possible, to a\\nsingle more general assumption that covers all of the specific facts.\\nIn  DATR such generalizations are captured by defaults expressed in sentences\\nthat cover more than one property of an object (as opposed to the input data,\\nwhere each sentence is supposed to represent a single observed property).\\nFor example, the sentence VERB:[mor past] == (\"[mor root]\" _ed)\\n of the theory in Fig.  covers all past tense forms of a verb. In addition to this process of generalization which is used in many\\nmachine-learning systems (e.g. Mitchell (1982), Michalski (1983)), acquired\\n DATR theories should identify information that several objects have in common.\\nThis information should be abstracted and stored in more general objects\\nfrom which the others inherit. Such generalized objects further structure\\nthe domain because hierarchies evolve where objects are grouped into classes.\\n\\n  Acquisition of inheritance relationships  \\n\\nThe observed data constitute a trivial  DATR theory which forms the initial\\nhypothesis H0 of the learning task. This  DATR theory is complete and\\nconsistent with respect to the input\\nbut does not meet the other two criteria. This section addresses\\nthe question of how a given  DATR theory can be transformed into another\\ntheory that contains more inheritance descriptors or changes the\\nlatter in order to structure the domain.\\nThe knowledge of how a given  DATR theory can be transformed into a new one with\\ndifferent inheritance descriptors is defined by rewrite rules of the\\nfollowing format:\\n\\n\\n     form of a transformation rule\\n\\n\\n\\n\\n\\n\\nwhere si is the input sentence and si\\' is the transformed sentence. Since\\ninheritance descriptors are stated as right-hand sides (RHS) or parts of\\nRHSs of sentences, the transformation rules operate on RHSs of  DATR sentences.\\nThus, si\\' differs from si in that it contains a different RHS.\\n\\nc1,..,cn are constraints that define under what conditions a given\\nsentence can be transformed into another one. In order to carry out a\\ntransformation that maintains the completeness and consistency of the theory\\na major constraint for the application of most transformation rules\\nto a hypothesis Hi consists in the requirement that Hi contain\\nanother sentence with the same RHS as the sentence that\\nis to be transformed.\\nCorresponding to the different kinds of inheritance relationships that\\ncan be expressed in a  DATR theory, there are four major groups of\\ntransformation rules: rules that return sentences with local descriptors\\n(local paths, local nodes, local node-path pairs), rules that transform\\nsentences into others that have a global descriptor, rules where the\\ntransformed sentence contains a descriptor that refers to a sentence with\\na global descriptor, and rules that create new, abstract sentences for\\n the acquisition of a hierarchy.  In Fig.     the rule for creating local node\\ndescriptors is formulated. Here, Hi is the given  DATR theory,\\nVa is the set of atomic values in Hi, and N is the set of nodes in\\nHi. The rule transforms a sentence s with atomic value into one with\\na node descriptor v\\', if the theory contains another sentence sithat belongs to node v\\' and has the same path and value as s.\\n\\n\\n     rule for local node inheritance\\n\\n\\n\\nBy means of transformation rules all the different kinds of inheritance\\ndescriptors can be obtained with the exception of evaluable paths.\\nEvaluable paths capture dependencies between properties with\\ndifferent values and therefore cannot be acquired by\\ntransformation rules that crucially depend on the existence of sentences\\nwhich have the same RHSs. Therefore, they have here been excluded from\\nthe learning task.\\n\\n\\n  Acquisition of default information \\n\\nWhile inheritance relationships are represented with the RHSs of sentences,\\ndefault information is basically expressed through paths of the left-hand\\nsides (LHSs), namely by paths that cover more than one fact. Since\\ntransformation rules leave the LHSs of sentences unchanged, an additional\\ndevice is necessary that operates on LHSs of sentences. For this purpose a\\ndefault-inference algorithm (DIA) was developed that reduces any given  DATR theory that does not (yet) contain default information, where \\'\\'reduction\\'\\'\\nmeans shortening the paths of sentences (by cutting off a path suffix) or\\ndeletion of whole sentences. Since extensive generalization is normally\\na desirable property, the resulting theory must be (and indeed is)\\nmaximally reduced.\\nIn order to acquire a  DATR default theory that remains consistent with\\nrespect to the input data\\nthe DIA has to check that a reduction of a sentence\\ndoes not lead to any conflicts with the remaining sentences of the theory.\\nConflicts can only arise between sentences which have the same node and path,\\nbecause in all other cases the longest matching path can be determined.\\nTherefore, if a given sentence is to be shortened, it has to be checked\\nwhether the theory already contains another sentence with the same node\\nand shortened path. If it does, and if the other sentence has a different\\nRHS, the first sentence cannot be shortened and must remain in the resulting\\ntheory. If the other sentence has the same RHS, the first sentence can\\nbe removed from the theory altogether. If the theory does not contain the\\nshortened sentence, the shortening is a legitimate operation since no\\nconflicts can arise.\\nThe following additional restrictions must be imposed to guarantee a\\ntheory that is complete and consistent with respect to the input data.\\nFirst of all, the sentences of a node have\\nto be considered in descending order according to the length of their paths.\\nThis guarantees that for every sentence, the sentences it can conflict with\\nare still contained in the theory and are not shortened or removed.\\nFor similar reasons, sentences can only be shortened by one element (the last)\\nat a time. In the case of path references or node-path pairs, some additional\\ntests are carried out since potential conflicts arise from  DATR\\'s\\nmechanism governing the inheritance of path extensions.\\n\\n\\n  Inference strategy  \\n\\nThe inference strategy determines how a result hypothesis HR is acquired\\nfrom an initial hypothesis H0. It relies on the notion of a permissible\\nderivation which arises through applications of transformation rules and DIA.\\nA permissible derivation of H0 results from any sequence of transformation\\nrules followed by the DIA. For reasons of consistency it is not possible\\nto apply transformation rules after the DIA or to apply the DIA several times.\\nMany different theories can be derived from H0, but only some of them can be\\n regarded as good  DATR theories with respect to the input data. In order to acquire a good theory the space of permissible derivations\\nhas to be searched. Since an exhaustive search leads to a combinatorial\\nexplosion for every non-trivial problem, a heuristic search is used as\\nin many other systems. We employ a forward pruning strategy that works\\nas follows: First of all, by further restricting the transformation rules\\nand DIA, not all of the possible successor hypotheses\\nare generated for a given hypothesis. Most importantly, the rules\\nfor building hierarchies are\\nrestricted in order to gain sensible classes. Here the notion of similarity\\nof objects (i.e. the number of sentences that two objects have in common)\\nplays a crucial role as in clustering approaches\\n(cf. Stepp and Michalski (1986), Lebowitz (1987)).\\nOf the generated successor hypotheses only the few most promising ones\\nare further expanded, while all others are discarded from the search.\\nTo decide which hypotheses are promising, criteria are needed to evaluate  DATR theories. Since only monotonic  DATR theories can be further transformed, these\\ncriteria have to be formulated for such theories. On the other hand,\\nonly default theories are considered as possible solutions, since the\\nrepresentation of default information constitutes a major demand on an\\nappropriate theory. Therefore, the default theories resulting from the\\nmost promising monotonic theories are the candidates for the result\\nhypothesis. Again, criteria are needed in order to select the best of these\\ncandidates. The search terminates when no more transformation rules can be\\n applied. \\nEach kind of criteria forms a complex that is composed of various different\\nsingle criteria that are ordered according to priority. As the inference\\nstrategy is not restricted to any specific domain, different learning tasks\\nusually require different evaluation criteria or different orderings.\\nAmong the criteria that were found to be most useful are the following:\\n\\nsize of a  DATR theory, measured by the absolute or average number of sentences\\nper object (useful only for default theories)\\n\\nhomogeneity of RHSs, measured by the number of different RHSs\\n\\ncomplexity of RHSs (length of paths and sequences)\\n\\ncapturing of particular relationships such as\\n\\nrelationships between objects (relative number of node references)\\n\\nrelationships within objects (relative number of\\npath references)\\n\\n\\n\\n\\n\\n\\n\\n  Inference of German noun inflection  \\n\\nAn implementation of the approach has been applied to a number of different\\nlearning tasks, including the acquisition of German noun inflection\\n(cf. Wurzel (1970)). The input for these tasks can be drawn from sample\\nevaluations of a corresponding  DATR theory that is included in the  DATR papers (cf. Evans and Gazdar (1990)). It consists of sentences whose paths contain\\nattributes for case and number and whose values are the inflected word forms\\n(here, abstract morphemes) associated with them, as illustrated in\\n Fig. . In addition, information about the root form and the gender are included.\\n\\n\\n     input sentence for German noun inflection\\n\\n\\nFels: [plur nom] = (fels _n).\\nFor the learning task observations about nouns of various inflectional\\nclasses are given: Fels \\'rock\\', Friede \\'peace\\',\\nHerr \\'gentleman\\' and Affe \\'monkey\\' are weak nouns,\\nStaat \\'state\\', Hemd \\'shirt\\' and Farbe \\'colour\\'\\nare mixed, and Acker \\'field\\', Kloster \\'convent\\', Mutter\\n\\'mother\\', Onkel \\'uncle\\', Ufer \\'shore\\', Klub \\'club\\',\\nAuto \\'car\\', and Disco \\'disco\\' are strong.\\nThe criteria for selecting the most promising theories during search were the\\nnumber of different references, followed by the complexity of inheritance\\ndescriptors and number of levels in the hierarchy. The\\ncriteria for determining the best hypotheses were the number of sentences with\\na node-path pair on the RHS, the relative number of sentences with\\nno node reference, and the average size of objects. All of the mentioned\\ncriteria were to be minimized.\\n The acquired  DATR theory is depicted graphically in Fig. . Here, the automatically generated abstract node names are replaced\\n(manually) by more linguistically motivated names. Edges that are not\\nannotated correspond to inheritance via the empty path.\\n\\n\\n\\n The inferred hierarchy in Fig.  structures the domain of German noun inflection in a linguistically plausible way.\\nAccording to similarity nouns are grouped into six major classes, from\\nwhich they inherit most of their information.\\nThe first three of them (UMLAUT_NULL, NULL, _S) correspond to strong\\nclasses that have in common the formation of singular forms but differ in\\ntheir plural forms, which are therefore stated explicitly.\\nThe last two classes (WEAK_ANIMATE, WEAK_INANIMATE)\\nrepresent weak noun classes that differ only in the formation of their\\nforms for genitive singular. The commonalities of strong nouns on the one\\nhand and weak nouns on the other hand are further abstracted from these\\nclasses and specified in the two more general node definitions\\nSTRONG and WEAK respectively. As an interesting fact,\\nthe class of mixed nouns (MIXED)\\nhas been identified, whose members behave like strong nouns\\nwith respect to the formation\\nof their singular forms and like weak nouns in the formation of their plural\\nforms. These facts are captured by inheriting information\\nfrom the classes STRONG and WEAK.\\nFinally, the top node NOUN of the hierarchy\\nrepresents information that is typical for German nouns in general.\\n\\n\\n  Conclusion  \\n\\nThis paper has presented an approach to the acquisition of linguistic\\nknowledge from unstructured data.\\nThe approach is general in the sense that it is not restricted\\nto a specific linguistic domain. This has been achieved by choosing the\\ngeneral representation language DATR for the representation of the acquired\\nknowledge and by postulating a learning strategy that is tailor-made for\\nthis formalism. A similar approach could be conceived for other\\nknowledge-representation formalisms (e.g. KL-ONE,\\ncf. Brachman and Schmolze (1985))\\nwhich are more familiar within the artificial-intelligence paradigm.\\nThe system was applied to a learning task involving German noun inflection.\\nThe results are sensible in that nouns are grouped into classes according\\nto their inflectional behavior in such a way that generalizations are captured.\\nThe acquired theories are restricted in that they do not make use of evaluable\\npaths; thus, although they are clearly non-trivial, the theories constitute\\na proper sublanguage of  DATR. In the future, further applications of the system\\nwithin different domains must be made in order to get a more detailed view of\\nits possibilities. This pertains especially to the criteria for guiding\\nthe search and selecting best hypotheses.\\n\\nFootnotes\\n\\n  Light (1994) addresses a related topic,\\nthe insertion of a new object (described with extensional  DATR sentences) into an existing  DATR theory. In contrast to our approach\\nthe assumption of a structured initial theory is made.\\n  Barg (1995) gives a full account\\nof all transformation rules.\\n  The\\nquestion of what constitutes a good  DATR theory is addressed later.\\nAssume for the moment that it has been defined and that two  DATR theories can be compared with each other with respect to quality.\\n  This presupposes that the search space is finite,\\nwhich is guaranteed by further restricting the transformation rules\\n(more precisely the rules for creating abstract sentences).\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nAn approach for the automatic acquisition of\\nlinguistic knowledge from unstructured data is presented. The acquired\\nknowledge is represented in the lexical knowledge representation\\nlanguage DATR. A set of transformation rules that establish inheritance\\nrelationships and a default-inference algorithm make up the basis components\\nof the system. Since the overall approach is not restricted to a special\\ndomain, the heuristic inference strategy uses criteria to evaluate\\nthe quality of a DATR theory, where different domains may require\\ndifferent criteria. The system is applied to the linguistic learning task\\nof German noun inflection.\\n\\n'],\n",
              " [\"\\n\\n  The GLR* Parser \\n  The GLR Parsing Algorithm \\n\\nThe Generalized LR Parser, developed by Tomita [], extended the\\noriginal LR parsing algorithm to the case of non-LR languages, where the\\nparsing tables contain entries with multiple parsing actions.\\nTomita's algorithm uses a Graph Structured Stack (GSS) in order to\\nefficiently pursue in parallel the different parsing options that arise\\nas a result of the multiple entries in the parsing tables. A second data\\nstructure uses pointers to keep track of all possible parse trees throughout\\nthe parsing of the input, while sharing common subtrees of these different\\nparses. A process of local ambiguity packing allows the parser to pack\\nsub-parses that are rooted in the same non-terminal into a single structure\\nthat represents them all.\\n\\n\\nThe GLR parser is the syntactic engine of the Universal Parser Architecture\\ndeveloped at CMU []. The architecture supports grammatical\\nspecification in an LFG framework, that consists of context-free grammar rules\\naugmented with feature bundles that are associated with the non-terminals of\\nthe rules. Feature structure computation is, for the most part, specified and\\nimplemented via unification operations. This allows the grammar to constrain\\nthe applicability of context-free rules. The result of parsing an input\\nsentence consists of both a parse tree and the computed feature structure\\nassociated with the non-terminal at the root of the tree.\\n\\n\\n  The GLR* Parser \\n\\nGLR* is a recently developed robust version of the Generalized LR Parser, that\\nallows the skipping of unrecognizable parts of the input sentence\\n[]. It is designed to enhance the parsability of domains such as\\nspontaneous speech, where the input is likely to contain deviations from the\\ngrammar, due to either extra-grammaticalities or limited grammar coverage.\\nIn cases where the complete input sentence is not covered by the grammar,\\nthe parser attempts to find a maximal subset of the input that is parsable.\\nIn many cases, such a parse can serve as a good approximation to the true parse\\nof the sentence.\\n\\n\\nThe parser accommodates the skipping of words of the input string by allowing\\nshift operations to be performed from inactive state nodes in the Graph\\nStructured Stack (GSS). Shifting an input symbol from an inactive state is\\nequivalent to skipping the words of the input that were encountered after the\\nparser reached the inactive state and prior to the current word that is being\\nshifted. Since the parser is LR(0), previous reduce operations remain\\nvalid even when words further along in the input are skipped. Information\\nabout skipped words is maintained in the symbol nodes that represent parse\\nsub-trees.\\n\\n\\nTo guarantee runtime feasibility, the GLR* parser is coupled with a ``beam''\\nsearch heuristic, that dynamically restricts the skipping capability of the\\nparser, so as to focus on parses of maximal and close to maximal substrings\\nof the input. The efficiency of the parser is also increased by an enhanced\\nprocess of local ambiguity packing and pruning. Locally ambiguous symbol nodes\\nare compared in terms of the words skipped within them. In cases where one\\nphrase has more skipped words than the other, the phrase with more skipped\\nwords is discarded in favor of the more complete parsed phrase. This operation\\nsignificantly reduces the number of parses being pursued by the parser.\\n\\n\\n\\n  The Parse Evaluation Heuristics \\n\\nAt the end of the process of parsing a sentence, the GLR* parser returns with\\na set of possible parses, each corresponding to some grammatical subset of\\nwords of the input sentence. Due to the beam search heuristic and the\\nambiguity packing scheme, this set of parses is limited to maximal or close to\\nmaximal grammatical subsets. The principle goal is then to find the maximal\\nparsable subset of the input string (and its parse). However, in many cases\\nthere are several distinct maximal parses, each consisting of a different\\nsubset of words of the original sentence. Furthermore, our experience has\\nshown that in many cases, ignoring an additional one or two input words may\\nresult in a parse that is syntactically and/or semantically more coherent. We\\nhave thus developed an evaluation heuristic that combines several different\\nmeasures, in order to select the parse that is deemed overall ``best''.\\n\\n\\nOur heuristic uses a set of features by which each of the parse candidates can\\nbe evaluated and compared. We use features of both the candidate parse and the\\nignored parts of the original input sentence. The features are designed to be\\ngeneral and, for the most part, grammar and domain independent. For each parse,\\nthe heuristic computes a penalty score for each of the features. The penalties\\nof the different features are then combined into a single score using a linear\\ncombination. The weights used in this scheme are adjustable, and can be\\noptimized for a particular domain and/or grammar. The parser then selects the\\n parse ranked best (i.e. the parse of lowest overall score).  \\n\\n  The Parse Evaluation Features \\n\\nSo far, we have experimented with the following set of evaluation features:\\n1.\\nThe number and position of skipped words\\n2.\\nThe number of substituted words\\n3.\\nThe fragmentation of the parse analysis\\n4.\\nThe statistical score of the disambiguated parse tree\\n\\n\\nThe penalty scheme for skipped words is designed to prefer parses that\\ncorrespond to fewer skipped words. It assigns a penalty in the range of\\n\\n(0.95 - 1.05) for each word of the original sentence that was skipped.\\nThe scheme is such that words that are skipped later in the sentence receive\\nthe slightly higher penalty. This preference was designed to handle the\\nphenomena of false starts, which is common in spontaneous speech.\\n\\n\\nThe GLR* parser has a capability for handling common word substitutions when\\nthe parser's input string is the output of a speech recognition system. When\\nthe input contains a pre-determined commonly substituted word, the parser\\nattempts to continue with both the original input word and a specified\\n``correct'' word. The number of substituted words is used as an evaluation\\nfeature, so as to prefer an analysis with fewer substituted words.\\n\\n\\nThe grammars we have been working with allow a single input sentence to be\\nanalyzed as several grammatical ``sentences'' or fragments. Our experiments\\nhave indicated that, in most cases, a less fragmented analysis is more\\ndesirable. We therefore use the sum of the number of fragments in the\\nanalysis as an additional feature.\\n\\n\\nWe have recently augmented the parser with a statistical disambiguation module.\\nWe use a framework similar to the one proposed by Briscoe and Carroll\\n[], in which the shift and reduce actions of the LR parsing\\ntables are directly augmented with probabilities. Training of the probabilities\\nis performed on a set of disambiguated parses. The probabilities of the parse\\nactions induce statistical scores on alternative parse trees, which are used\\nfor disambiguation. However, additionally, we use the statistical\\nscore of the disambiguated parse as an additional evaluation feature\\nacross parses. The statistical score value is first converted into a\\nconfidence measure, such that more ``common'' parse trees receive a lower\\npenalty score. This is done using the following formula:\\n\\n\\npenalty = (0.1 * (- log10(pscore)))\\n\\n\\nThe penalty scores of the features are then combined by a linear\\ncombination. The weights assigned to the features determine the way they\\ninteract. In our experiments so far, we have fined tuned these weights\\nmanually, so as to try and optimize the results on a training set of data.\\nHowever, we plan on investigating the possibility of using some known\\noptimization techniques for this task.\\n\\n\\n  The Parse Quality Heuristic \\n\\nThe utility of a parser such as GLR* obviously depends on the semantic\\ncoherency of the parse results that it returns. Since the parser is designed\\nto succeed in parsing almost any input, parsing success by itself can no\\nlonger provide a likely guarantee of such coherency. Although we believe this\\ntask would ultimately be better handled by a domain dependent semantic analyzer\\nthat would follow the parser, we have attempted to partially handle this\\nproblem using a simple filtering scheme.\\n\\n\\n(1) = simple heuristic, (2) = full heuristics\\n\\n\\nThe filtering scheme's task is to classify the parse chosen as best by the\\nparser into one of two categories: ``good'' or ``bad''. Our heuristic takes\\ninto account both the actual value of the parse's combined penalty score and a\\nmeasure relative to the length of the input sentence. Similar to the penalty\\nscore scheme, the precise thresholds are currently fine tuned to try and\\noptimize the classification results on a training set of data.\\n\\n\\n\\n  Parsing of Spontaneous Speech Using GLR* \\n\\nWe have recently conducted some new experiments to test the utility of the GLR*\\nparser and our parse evaluation heuristics when parsing speech recognized\\nspontaneous speech in the ATIS domain. We modified an\\nexisting partial coverage syntactic grammar into a grammar for the ATIS domain,\\nusing a development set of some 300 sentences. The resulting grammar has 458\\nrules, which translate into a parsing table of almost 700 states.\\n\\n\\nA list of common appearing substitutions was constructed from the development\\nset. The correct parses of 250 grammatical sentences were used to train the\\nparse table statistics that are used for disambiguation and parse evaluation.\\nAfter some experimentation, the evaluation feature weights were set in the\\nfollowing way. As previously described, the penalty for a skipped word ranges\\nbetween 0.95 and 1.05, depending on the word's position in the sentence. The\\npenalty for a substituted word was set to 0.9, so that substituting a word\\nwould be preferable to skipping the word. The fragmentation feature was\\ngiven a weight of 1.1, to prefer skipping a word if it reduces the\\nfragmentation count by at least one. The three penalties are then summed,\\ntogether with the converted statistical score of the parse.\\n\\n\\nWe then used a set of 120 new sentences as a test set. Our goal was three-fold.\\nFirst, we wanted to compare the parsing capability of the GLR* parser with\\nthat of the original GLR parser. Second, we wished to test the effectiveness\\nof our evaluation heuristics in selecting the best parse. Third, we wanted to\\nevaluate the ability of the parse quality heuristic to correctly classify\\nGLR* parses as ``good'' or ``bad''. We ran the parser three times on the test\\nset. The first run was with skipping disabled. This is equivalent to running\\nthe original GLR parser. The second run was conducted with skipping enabled\\nand full heuristics. The third run was conducted with skipping enabled, and\\nwith a simple heuristic that prefers parses based only on the number of words\\nskipped. In all three runs, the single selected parse result for each sentence\\nwas manually evaluated to determine if the parser returned with a ``correct''\\nparse.\\n\\n\\n The results of the experiment can be seen in Table . The results indicate that using the GLR* parser results in a significant\\nimprovement in performance. When using the full heuristics, the percentage of\\nsentences, for which the parser returned a parse that matched or almost\\nmatched the ``correct'' parse increased from 50% to 75%. As a result of its\\nskipping capabilities, GLR* succeeds to parse 58 sentences (48%) that were\\nnot parsable by the original GLR parser. Fully 96% of the test sentences (all\\nbut 5) are parsable by GLR*. However, a significant portion of these sentences\\n(23 out of the 58) return with bad parses, due to the skipping of essential\\nwords of the input. We looked at the effectiveness of our parse quality\\nheuristic in identifying such bad parses. The heuristic is successful in\\nlabeling 21 of the 25 bad parses as ``bad''. 67 of the 90 good/close parses\\nare labeled as ``good'' by the heuristic. Thus, although somewhat overly harsh,\\nthe heuristic is quite effective in identifying bad parses.\\n\\n\\nOur results indicate that our full integrated heuristic scheme for selecting\\nthe best parse out-performs the simple heuristic, that considers only the\\nnumber of words skipped. With the simple heuristic, good/close parses were\\nreturned in 24 out of the 53 sentences that involved some degree of skipping.\\nWith our integrated heuristic scheme, good/close parses were returned in 30\\nsentences (6 additional sentences). Further analysis showed that only 2\\nsentences had parses that were better than those selected by our integrated\\nparse evaluation heuristic.\\n\\nFootnotes\\n\\n  The\\nsystem can display the n best parses found, where the parameter n is\\ncontrolled by the user at runtime. By default, we set n to one, and the\\nparse with the lowest score is displayed.\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nGLR* is a recently developed robust version of the Generalized LR Parser\\n[], that can parse almost any input sentence by ignoring\\nunrecognizable parts of the sentence. On a given input sentence, the parser\\nreturns a collection of parses that correspond to maximal, or close to maximal,\\nparsable subsets of the original input. This paper describes recent work on\\ndeveloping an integrated heuristic scheme for selecting the parse that is\\ndeemed ``best'' from such a collection. We describe the heuristic measures\\nused and their combination scheme. Preliminary results from experiments\\nconducted on parsing speech recognized spontaneous speech are also reported.\\n\\n\"],\n",
              " ['\\n\\n  Introduction \\n\\nA sublanguage (SL) is the written or spoken language\\nthat is used in a particular field or\\ndiscipline by people working in the field,\\nespecially to communicate with their colleagues [].\\nAn SL\\ndiffers from the general language used by people under ordinary\\ncircumstances in both its structure and its vocabulary.\\nFor example, it may include syntactic constructions that would be considered\\nungrammatical in general language, such as omitting verbs\\n[].\\nIt may include words or phrases that are not used in general\\nlanguage, and may also use familiar words in unfamiliar ways.\\nIt differs from jargon, although jargon may be part of an SL,\\nin that jargon refers exclusively to the vocabulary used in the\\ndiscourse.\\n\\n\\nWhen specialists in a particular field or discipline communicate\\ninformally, the\\ndiscipline\\'s SL facilitates their communication by allowing\\nthem to be precise in their terminology, and frequently, to\\nbe more concise in their expression.  When specialists communicate\\nin more formal settings (e.g., a journal article),\\ntheir language acquires some of the characteristics\\nof general language.  The grammar of their language will\\nbe that of general language and will conform to the norms of standard\\nlanguage.  Their language will still, however, contain\\nthe vocabulary of the SL.  These specialized terms may be\\nthe best (or only) way to describe their topic.\\nThe language in this formal setting will thus\\nbe a blend of general language and SL characteristics.\\n\\n\\nOne place that\\nthis type of language, with mostly standard structure, but a highly\\nspecialized vocabulary, appears is in journal titles, abstracts, and articles.\\nHere, it provides a challenge for a variety of language analysis,\\nfiltering,\\nand retrieval systems.   A highly specialized vocabulary contains\\nterms that are not in standard dictionaries, resulting in coverage\\ngaps for systems that depend on a dictionary for syntactic and/or\\nsemantic information.\\nHaas haa:cove found that 20%\\nof the word tokens in a set of computer science\\nabstracts were not found in a standard college dictionary.  Sixty-two percent\\nof the SL word tokens (technical terms) identified by an expert\\nwere not found in the dictionary.\\nThe lack of coverage of the\\nSL terms is not surprising, given the wealth of terms used in\\nvarious disciplines, the speed with which new terminology is adopted,\\nand the propensity of authors to coin new terms.\\nSL words that do appear in the standard dictionary are frequently marked as\\nbelonging to that domain, for instance, with the name of the\\ndomain appearing in italics after the headword.\\n\\n\\nSpecialized dictionaries may be sought to expand the coverage of the\\nstandard dictionary.  A dictionary written to describe words and\\nconcepts in a particular discipline may be expected to contain\\nmore of the SL terms, and more of the SL senses or meanings of\\nterms, than a general dictionary.  Specialized dictionaries also indicate\\nwhat the important terms and concepts in their disciplines are.\\nThe coverage of these specialized\\ndictionaries, however, may vary widely.  Haas haa:cove studied the\\ncoverage\\nprovided by a set of computer science dictionaries of vocabulary in\\na set of abstracts, and found that coverage of SL word tokens ranged\\nfrom 20% to 78%.\\n\\n\\n  Goals of Research \\n\\nThe overall goal of this research is to examine language usage in\\ntitles and abstracts drawn from an array of disciplines to determine\\ninteresting usage differences between them.  We looked at usage from\\nseveral perspectives.\\nFirst, we looked at word frequencies, and especially at the\\nfrequencies of SL terms.  Next, we looked at the coverage of the SL\\nterms in the titles and abstracts\\nby special dictionaries for each of the domains,\\nand by a general dictionary.  Finally, we\\nexamined the use of vocabulary as a means for identifying the\\ndomain of an abstract.\\n\\n\\nIt has been shown that abstracts from different disciplines are\\ndistinguishable by topic and structure [].\\nThe focus of this research is\\non the usage of SL vocabulary in abstracts from scholarly work in a\\nvariety of fields.  Differences in usage may reflect differences in\\ntopic and structure, as well as other as yet unidentified factors.\\n\\n\\nWord frequencies, the overlap of usage between related and\\nunrelated domains, inclusion of SL terms in general and specialized\\ndictionaries, and similar data are informative about the nature of\\nthe language used in different disciplines and the differences between the\\ndisciplines [].\\nIn particular, there are interesting differences between\\nthe usage patterns in scientific disciplines, such as\\nMathematics and Physics, and humanities and social science\\nfields such as History and Sociology.\\nThese differences may include the way in which exposition is structured,\\nthe kind of information that is considered necessary in reporting on\\nresearch [], sentence structure [], or the\\namount of synonomy [].\\n\\n\\nUsage refers to both the words that are used and the word senses that are used.\\nThe list of SL terms used in a discipline may include words that are not\\nused (or are used very infrequently)\\nin general language, such as the Physics term ``dipole\".\\nSL terms may also include words that are familiar in general language,\\nbut are used in a special, very specific sense in the language of the\\ndomain.  An example of this is the Biology term ``linear\", which refers\\neither to a long, narrow leaf, or a row of pollen grains.  Word and word\\nsense usage in a particular discipline may also be ``borrowed\" from related\\ndomains.  In this case, it can be difficult to say that a term is or\\nis not in the SL of the discipline.  The most common example of borrowing\\nseen in this research is the use of mathematical terms in Physics,\\nElectrical Engineering, and Biology abstracts.\\nThis kind of borrowing is to be\\nexpected, since Mathematic  is an important tool in these fields.\\nWhether\\nborrowed terms should be considered part of the SL may be a matter of\\ndefinition.\\n\\n\\nThere is another class of terms used in the discourse of a discipline that\\nis harder to classify as SL or not SL.  These are terms that are\\ncrucial to the discussion of the important topics in\\nthe discipline, but are not specific to that discipline.  For example, many\\ndifferent disciplines use the term ``model\" to describe a set of concepts\\nand the relationships between them.  It would be difficult to discuss\\nresearch without using this term.  We cannot say that this is an SL term\\nin Physics or in Sociology or some other discipline because its usage is\\nthe same in all the disciplines.  On the other hand, in studying\\nthe language that experts in the field use to discuss\\ntheir work, this is clearly an integral term.\\nBonzi (1984)\\nchose not to\\ninclude such terms in her study.\\n\\n\\nUsage in a specific discipline can also be studied by looking at special\\ndictionaries for the discipline.  Although editorial policy differs\\nfrom dictionary to dictionary, the terms chosen for inclusion in a\\ndictionary are generally those used often enough that a reader\\nof literature in the field may be\\nexpected to encounter them, and which are central to the field, so\\nthat it is important that a reader knows what they mean.  Terms may be\\nidentified for inclusion by a variety of means,\\nincluding looking at textbooks, journals and\\nother standard works in the field. The dictionary can be viewed as\\nan authority on SL usage in the discipline.  It is important to point out,\\nhowever, that the words included in such a dictionary will be only a portion\\nof the words in the discipline\\'s SL.  One problem is that of new terms; a\\ndictionary, especially in print format, will always be a few years\\nbehind current usage.  Nor can a dictionary include words that are\\ncoined by a single researcher, but not picked up by researchers in\\nthe field as a whole.  The reader must expect that such terms are\\ndefined in the particular paper in which they are used.\\nFinally, no dictionary is complete:  the restrictions of space and time will\\nalways lead to the omission from the dictionary of terms that another person\\nmight consider important.\\n\\n\\nA specialized dictionary may also include some of the terms described\\nin the preceding section that are crucial to the discourse of the domain, but\\nnot\\nspecific to just that SL.\\nOne way these can be identified is by examining specialized\\ndictionaries from several disciplines.  Not only will these terms\\nappear in more than one dictionary, they will\\nalso be defined in approximately the same way.\\n\\n\\nGeneral, or standard, college or unabridged dictionaries contain\\nSL terms from many disciplines.  Frequently the words or word senses\\nare explicitly marked with the name of the field in which they are\\nused.  Definitions given in a general dictionary may not be\\nas detailed or complete as those in a specialized dictionary.\\nFor example, the term ``linear\" referred to earlier had two senses given\\nin the Biology dictionary.  The standard dictionary, however, lists\\nonly the first one as a Biology SL sense.\\nThe other sense is not listed at all.\\nDamerau dam:gene,dam:eval\\ndiscusses some additional problems with using\\nstandard dictionaries as an aid in identifying SL terms.\\n\\n\\nIn summary, which terms are considered to be part of the SL may vary\\naccording to several factors.  Dictionaries will necessarily be\\nincomplete, and will also include words that may be borrowed from\\nanother SL, or that are common to several disciplines\\' SLs.  Domain\\nexperts will similarly have some disagreement on whether such terms are\\npart of the SL.  To define an SL as consisting of only those core terms\\nthat are unique to it, and agreed upon by all sources ignores the\\nrealities of scholarly discourse. Yet it is clear that there are\\nboundaries between SLs, however fuzzy they may be.  By examining terms\\nin light of both dictionary and text based standards, we hope to\\ndiscover some ways\\nin which SLs can be differentiated.\\n\\n\\n  Materials \\n\\nAbstracts representing work in eight different disciplines were\\ncollected by querying CD-ROM databases for each discipline.\\nFive queries were constructed for each discipline to obtain five\\nsets of abstracts, containing between 22 and 50 abstracts\\neach.  (Fifty was arbitrarily chosen as the upper limit.  For\\nqueries that returned more than 50 abstracts, only the first 50\\nwere used.)\\nQueries were designed to describe an entire topic, rather than\\npinpoint a specific aspect of a topic.  It was thought that in\\nthis way, a broader sample of the SL could be obtained.\\nQueries were developed using a variety of basic reference and textbook\\nmaterials from the disciplines.  Strategies included extracting phrases\\nfrom back of the book indexes, and using topics from review\\nquestions or exercises in textbooks.\\nThe queries consisted of one to three keywords.\\nAs examples, a Physics query was ``planetary atmospheres\", and\\na sociology query was ``prison and violence\".\\n Table  shows the average and total number of abstracts and word tokens for each discipline\\nand for the entire collection.\\nNo limitations on date were placed on the\\nabstracts other than those of retrieval order for those queries that\\nreturned more than 50 abstracts, and those inherent in the topic.\\nFor example,\\na query on ``planetary atmospheres\" would be influenced by when data was\\nreceived via space probes, whereas one on ``Archimedes\" would not have\\nsuch strong temporal influences.\\n\\n\\nSpecialized dictionaries for each domain were chosen from the UNC-CH\\nlibrary.  In cases where more than one dictionary was available,\\nrecent (within the last decade) dictionaries were chosen over older\\ndictionaries, and those claiming a larger number of entries over those with\\na smaller number.  The recency of a dictionary may have some effect\\non its coverage of SL terms.  Given the time lag inherent in publishing\\na printed dictionary, a dictionary published in 1990, for example, may\\nnot include important terms from abstracts published in the same year.\\nThis disparity will be greater in disciplines where the vocabulary\\nchanges rapidly, such as Computer Science, and less noticeable in\\ndisciplines whose vocabularies evolve less rapidly.  Since no date\\nrestrictions were explicitly placed on the abstracts, we do not expect\\nthis to be a problem for this research.  Rather, this will reflect the\\nrealities of using specialized dictionaries as a knowledge source.  The\\nrate of change of SL vocabulary, and its effect on the use of\\ndictionaries in various information tasks, deserves further\\ninvestigation, particularly with the advent of online dictionaries.\\nAppendix A contains a complete list of the\\ngeneral and specialized dictionaries.\\n\\n\\nA systematic sample of 500 terms was taken from  each database.\\nA list of  all terms in a given database was\\nsorted by frequency of occurrence\\nand then a systematic sample (every nth term so that the\\nsample size was 500) taken from that database.\\nEach author then separately identified those words from the sample that\\ncould possibly be a SL terms, omitting stop words (function words)\\nand other obviously general words such as ``present\".  Those words\\nthat were considered to be possible SL terms by either author\\nwere\\nlooked up in the appropriate specialized dictionary.\\nWords were coded according\\nto the following four categories:\\n- No entry in the specialized dictionary.\\n1\\n- Headword of entry exactly matches the target word.\\n2\\n- Headword is phrase that starts with the target word.\\n3\\n- Headword of entry is inflectional variant of the target word\\n(e.g., a different verb form,\\nor a singular/plural variant).\\nThose few terms that\\nmight be ``true\" SL terms that were\\nnot included in the dictionary were considered not to\\nbe SL terms for the purposes of the data analysis described below.\\n\\n\\n  Term Frequencies in Sublanguages \\n\\n Table  shows the percent of occurrences for the terms found or not found in the sublanguage dictionaries.\\nThe largest percent of terms found in the dictionary in one form or another was\\na tie between Electrical Engineering and Physics,\\neach of which had 66 percent of their terms found\\nin the specialized dictionary.\\nThe smallest percent of terms found in a sublanguage\\ndictionary was 17 percent for Sociology.\\n\\n\\nIn most cases, more terms occurred at the start of  a phrase\\nin the dictionary than as an exact match for\\nthe single term in the dictionary.\\nIf one combines the number of terms that are exact matches with\\nthe exact matches at the start of a phrase, the percent of terms\\nvaries from 7 and 11 percent\\n(Sociology and History) to 34 percent\\n(Electrical Engineering and Physics).\\n\\n\\nThe last two categories offer the possibility of ``false\\npositives\", where the term in the dictionary is not the same as that\\nused in the abstracts.  Many words in the ``start of\\nphrase\" category occurred at\\nthe beginning of more than one entry.  In Physics, for example, the word\\n``atomic\" was the initial word of six entries.  One or more of these\\nentry phrases may have been used in the text, or the word may have been\\nused in a different phrase.  In general, however, entries starting with\\nthe same word are somewhat related.  It should also be noted that merely\\nfinding a word in the dictionary does not mean that it was used in its\\nSL sense in the abstracts.\\n\\n\\nIn the case of the ``term variant\" category, a false positive would mean\\nthat an inflectional variant of a term had a different meaning from the\\nterm itself, for example, that a past tense meant something different\\nfrom the infinitive.  While this is theoretically\\npossible, it was not noticed in this\\ndata.\\n\\n\\n Table  shows that the average term frequencies for those terms that\\nare exact matches or occur at the start of the phrase\\nare much higher than for terms not in the dictionary or are\\nterm variants of terms that occur in the dictionary.\\nFor all domains except for Economics,\\nthe average term frequency for exact matches is higher than\\nthe average term frequency for terms not in the dictionary.\\nFor all domains except for Mathematics,\\nthe average term frequency for terms occurring at the start of a phrase\\nwas higher than for terms not in the dictionary.\\n\\n\\nIt is interesting to note that Mathematics had the highest average\\nterm frequency for the exact match category.  This indicates that the\\nvocabulary used in the Mathematics abstracts matches that listed in the\\ndictionary better than that of the other disciplines.  It is possible\\nthat this reflects a slower rate of change in the Mathematics domain,\\ni.e., in the adoption of new terms, thus allowing the dictionary to\\nremain current for a longer period of time.  This possibility requires\\nmore investigation.\\n\\n\\nThese term frequencies may be useful in separating terms\\nlikely to occur\\nin a specialized dictionary from terms\\nnot likely to occur in a specialized dictionary,\\nthat is, the separation of SL terms from general terms.\\nWhile the averages suggest that we may be able to discriminate\\nbased on term frequencies,\\npreliminary investigations into the correlations between\\nterm discrimination measures and term sublanguage status\\n showed that little useful\\ndiscrimination is actually obtained.\\nThis suggests that term frequencies alone are inadequate\\nfor accurate automatic identification of specialized terminology.\\nWe did not pursue this line of investigation.\\n\\n\\n  Usage \\n\\nUsage was examined from several perspectives, including term frequency and\\nspecialized and standard dictionary definitions of terms,\\nto identify differences between disciplines and families of disciplines.\\n\\n  Frequencies \\n\\nTerms were ranked for each database by their Poisson percentile.\\nThis percentile provides a measure of the degree to which a term has a\\nhigher than expected frequency of occurrence in the database in question.\\nThe average frequency of term occurrence is computed by dividing the total\\nnumber of occurrences\\nof the term in question from across all the different databases\\nby the number of databases.\\nThis average is then compared statistically with the number of term occurrences\\nin the database in question.\\n\\n\\nThe number of times a term occurs in a body of text,\\neither an individual title, abstract, full text, or a database,\\nmay be described probabilistically using the Poisson distribution\\n[].\\nThis describes a body of text as being ``about\" a topic\\nto a certain degree.\\nCurrent researchers often assume that the Poisson distribution\\nis close to the actual empirical distribution describing a given\\nset of term frequencies and,\\ndespite the fact that term frequency data is not exactly Poisson\\ndistributed, the\\nPoisson distribution of term frequencies is a useful approximation.\\nThe research described below explicitly assumes the Two Poisson\\nEffectiveness Hypothesis [], which states,\\nin part, that\\n``even though terms are not Poisson distributed and independent, the term\\nfrequencies are distributed in a manner close enough to the Poisson\\ndistribution that the [performance]\\ndegradation due to the failure of the distribution\\nassumption to be met is compensated for by the increase in information\\nprovided by non-binary term frequencies.\"\\nWe do not attempt here to determine whether the terms are in fact\\ndistributed in a Poisson manner; for research on the degree to\\nwhich terms are Poisson distributed, the reader is referred to\\n[].\\nInstead, we assume that terms are close\\nto being Poisson distributed and that\\nmaking this assumption may increase classification performance\\nenough through the incorporation\\nof term frequencies,\\nthat this will counteract the obvious decrease in performance that\\noccurs due to the failure of the\\nPoisson term distribution assumption to be met.\\n\\n\\nThe two-Poisson model []\\nassumes that there are two bodies of text; one about a topic and\\nthe other not about the topic, and that term frequencies\\nare Poisson distributed in each body of text.\\nMore recently, the three-Poisson model []\\nhas been developed to model\\nthree bodies of text that\\nvary in their degrees of\\nbeing ``about\" the term.\\nThe two-Poisson model can be shown to be a special case of the three-Poisson\\nmodel and a model assuming that terms in all bodies of text are described\\nby the Poisson distribution, a one-Poisson model, can be shown to be a\\nspecial case of the two-Poisson model.\\nWe treat ``discipline\" as topicality here and use the one-Poisson\\nmodel as a model of term distributions in disciplines.\\n\\n\\nAssuming that terms in a body of text are generated by a Poisson process\\nallows one to measure the\\nprobability that one will have x occurrences of a term\\ngiven an average frequency of \\nas,\\n\\n\\n\\n\\n\\nThis distribution assumes that x is an integer and\\nthat the production process is memoryless,\\nthat is,\\nthe presence of a term in one location in a text doesn\\'t affect\\nits presence either way in later produced sections of the text.\\nWhile this may be a weak assumption to make\\nwhen working with single documents,\\nwhere stylistic considerations have a great impact on the choice\\nof terms used by an author,\\nthe Poisson distribution assumption\\nis reasonable in a database consisting of\\nhundreds of different titles and abstracts written independently\\nby different authors.\\n\\n\\nThe Poisson percentile for a term occurring t times in a database,\\nwhere the average frequency of occurrence across all databases is ,\\nmay be measured as \\n\\n.\\nThis percentage will approach 1 when a term occurs in a database with\\na frequency that is much higher in this database\\nthan is usually found in the databases as a whole.\\nSimilarly, a low positive value would be found with a term\\nthat is commonly found in the databases but\\nis rare in this particular database:\\nsuch terms are seldom found.\\n\\n\\nWhile the technique described here assumes that terms are distributed in\\na manner roughly similar to the Poisson distribution,\\nother distributions may prove useful in other circumstances.\\nFor example,\\nusing the normal distribution instead of the Poisson distribution\\nmay be desirable in cases where the average frequency\\nof occurrence approaches the hundreds or higher and\\nminimizing computation speed is important for the application.\\n\\n\\n Table  lists the 5 top and bottom terms for each database after the terms were ranked by their Poisson percentile.\\nNote  that these are extracted from the list of all ranked terms\\nand not just from those identified as SL terms.\\nThose at the top can in some way be considered especially\\ncharacteristic of the SL used in that database, not because\\nthey were used frequently, but because they were\\nused more frequently than expected, given their frequency in the\\nrest of the collection.  These words are clearly associated with\\ncentral topics in their disciplines.\\nThe bottom ranked terms are somewhat harder to\\ncharacterize - they are more of a mixed bag.  Some of them are\\ngeneral language words, such as ``during\" and ``their\".  Some\\nare general `research description\\' terms, such as ``discussed\"\\nand ``method\".\\n\\n\\nIt is interesting to note that although there is\\nno overlap between disciplines in the top\\nranked terms, there is some in the bottom ranked terms.\\nThis is due in part to the ranking procedure,\\nwhich gives a high rank to terms that are relatively rare\\n(whether as SL terms or not), while those terms\\nat the bottom of the lists are those terms likely to occur more\\nevenly throughout the databases.\\nThese latter terms, while being specialized vocabulary for some\\ndomains, do occur in other domains and therefore receive a low\\nposition using this ranking method.\\n\\n\\n  Comparison with Specialized Dictionary Definitions \\n\\nMerely finding a word from a particular set of abstracts in the specialized\\ndictionary for that discipline does not guarantee that the word is being\\nused in the abstracts in its SL sense, as defined in the dictionary.\\nMany words that have a very specific meaning in a particular domain are\\nalso used with a different, more general meaning, in general language.\\nThe usage of words classified as 1\\'s, that is,\\nwith exact matches in the dictionaries, were of greatest interest\\nto us.  We wished to determine what portion of these words were being\\nused in their SL sense and what portion in a general sense.  We were\\nspecifically interested in two types of comparisons.\\nFirst, we wondered if SL words with the highest Poisson rankings\\nwere  used differently from those with the lowest rankings within\\nany discipline.  Second, we wondered if there\\nwere any differences in usage between disciplines, or between classes\\nof disciplines.\\n\\n\\nThe top 10 and bottom 10 ranked terms classified as 1\\'s from each discipline\\nwere\\nselected for examination.  Each instance of their usage in the discipline\\'s\\nabstracts\\nwas categorized into one of the following groups.\\n\\nSSL - Same sense, SL. \\nThe word token in the abstract used the same SL word sense as\\nthat defined in the special dictionary.  For example, in the\\nElectrical Engineering dictionary, the word ``array\" is\\ndefined as ``1) photovoltaic converter - a combination of panels\\ncoordinated in structure and function. 2) solar cell - a combination\\nof solar cell panels or paddles coordinated\\nin structure and function.\"\\nThe majority (96%) of its occurrences in the Electrical Engineering abstracts\\nwere used in one of these two senses.\\n\\nSG - Same sense, general.\\nThe word token in the abstract used the same sense as that\\ndefined in the special dictionary, which was a general definition.\\nThat is, the special dictionary defined the word to mean the same\\nas its meaning in general language.  For example, the economics\\ndictionary defined the word ``merger\" as ``An amalgamation of two or\\nmore firms into a new firm.\"  In general language, it is used in the same way.\\n\\nDSL - Different sense, SL.\\nThe word token in the abstract is used in a different sense from\\nthat defined in the special dictionary, but it is still used as\\nan SL term.  For example, the Biology dictionary defined the word\\n``linear\" as ``1) a leaf having parallel sides, and at least 4\\nto 5 times as long as broad. 2) a tetrad of pollen grains in a\\nsingle row.\"  In the Biology abstracts, it was generally used to\\nrefer to a mathematical array.\\n\\nDG - Different sense, general.\\nThe word token in the abstract is used in a different sense from\\nthat defined in the special dictionary, and its usage was in a general language\\nsense.  For example, the Physics dictionary defined the word ``period\"\\nas ``the time occupied in one complete movement of a vibration or\\noscillation\".  In the Physics abstracts, it was used in the general sense of\\n``a span of time.\"\\n\\n\\n\\n\\n Table  shows the average percentage and variance of occurrences of\\nthe top 10 and bottom 10 1\\'s.  The disciplines are divided into two\\ngroups, one group contains what may be considered the\\n``scientific\" or ``hard science\" disciplines,\\nand the other group contains\\nthe humanities and social science disciplines.\\nThis division minimized the variances.\\n(We will use the term ``scientific\" for the first group merely\\nfor the sake of exposition; we do not intend to comment on the nature of\\nthe work done by any discipline.)\\n\\n\\nThere are some very interesting differences in the distribution of the\\nterms.  In the scientific disciplines, a total of 73.8% of the\\noccurrences were SL usages (SSL + DSL),\\nwhile in the humanities and social science\\ndisciplines only  9.9% were.  In the humanities and social sciences, the\\nmajority of the usages were classified as SG.  Not only were terms\\nmostly used in a general sense, it is these general senses that\\nwere defined in the specialized dictionaries.  For example, the History\\ndictionary contained an entry for ``France\", which was defined as\\na country in Western Europe, etc.  Obviously, the word ``France\"\\nhas the same meaning in general discourse.\\n\\n\\nThe picture changes somewhat in looking at the bottom 10 1\\'s.\\nIn the scientific disciplines, 60.9% of the term occurrences were used\\nin a general sense.  Interestingly, 70.8% of the humanities and social science\\nterm\\noccurrences were used in a general sense, a smaller proportion than\\nthe top-ranked terms.  However, about half of these were used in a\\ngeneral sense different from that defined in the special dictionary.\\nNote that none of the terms in the humanities and social sciences were used in\\nan SL sense different from that defined in the special dictionary.\\n\\n\\nIn summary, most of the occurrences of the top-ranked words used\\nthe same sense as that defined in the discipline\\'s special dictionary,\\nwhether that was an SL or a general sense. Most of the occurrences\\nof the bottom-ranked words in the scientific disciplines\\nwere used in a different sense from that given in the dictionary.\\nOnly one third of the humanities and social science terms\\' occurrences were\\nused in\\na different sense.  This finding indicates some interesting\\ncharacteristics about the vocabulary used in scholarly abstracts,\\nand how the disciplines differ in this regard.\\nIn scientific disciplines such as Mathematics or Physics,  the words that occur\\nmore frequently than expected (the top-ranked terms)\\nare generally used in a specific, SL sense.  The infrequent words\\nthat could also be used in an SL sense are not used that way.\\nThere is a shift in usage between the top ranked words and the\\nbottom ranked words.  This particular shift does not occur in the\\nother disciplines.\\nIn these disciplines, the vocabulary is less distinct\\nfrom that of general language, as seen by the\\nnumber of occurrences classified as SG.\\nThe shift in usage between the top and bottom ranked words seen here is that\\nthe top ranked words are primarily used in the same general\\nsense as that defined in the specialized dictionary, while the bottom\\nranked words are used almost as frequently in a different general sense.\\nIn addition, there are more SSL occurrences.\\nThis shift, however, is not as marked as that\\nseen in the scientific disciplines.\\n\\n\\n  Measures of Sublanguage Characteristics \\n\\nGiven the usage patterns found in the\\nscientific and humanities and social science\\ndisciplines, the next question is whether these patterns can be used to measure\\nthe characteristics of a particular SL.\\nWe propose a usage based measure,\\n\\n\\n\\n\\n\\nthat may be interpreted as the percent of term uses (for\\nterms in the dictionary) that\\nare used in the text in a sublanguage sense, or\\nthe probability that a term is used in a sublanguage sense given\\nthat the term is ``present\" in the dictionary.\\nMu values may be indicative of the degree to which terms in\\nthe sublanguage are\\ndifferentiated from those in general language.\\nA small Mu indicates that ``technical\" terms, (those in the SL),\\nare frequently used in\\nnon-technical senses, while an Mu of 1 represents these\\nterms being used only in a technical sense.\\nValues for the measure for the top  ten type 1 terms and the bottom\\n ten type 1 terms for each database are given in Table . \\n\\n\\nThe ratio of the usage measure Mu for the top terms to that for\\nthe bottom terms provides a measure of the difference between\\nthese two areas:\\n\\n\\n\\n\\n\\nwhere \\n\\n\\nrepresents the Mu value for the top\\nten terms, with similar notation used for the bottom ten terms.\\n Table   shows the  values for each database.\\nIt may be the case that \\nis a measure of the\\ndiscipline ``hardness\" or degree\\nof precision of the\\nSL for an academic field.\\nScientific disciplines may exhibit a greater degree of\\nvariation\\nbetween the rate of terms most likely to be used in an SL sense\\nfor those terms that are expected to be most characteristic of the SL vs.\\nfor those terms that are least expected to characterize the SL.\\nThis is an area that merits further investigation.  Questions\\nof interest include whether this ratio is fairly constant across\\nan SL, and what other features of the SL correlate with this\\nmeasure.\\n\\n\\n  Comparison with Standard Dictionary Definitions \\n\\nA further difference in the distinctiveness of the scientific and\\nhumanities and social science\\nvocabularies can be seen by examining entries in the standard college\\ndictionary.  The terms\\nclassified as 1\\'s from each discipline\\nwere looked up in the dictionary, first to see whether there was an\\nentry for the word, and next to see if one or more of the word senses\\nwas marked as being a term from  that discipline.  There was little\\ndifference in the number of terms with entries. 78.7% of the\\nscientific disciplines\\' terms had entries,  ranging from 94% (Biology)\\nto 40.4% (Physics).\\nIn the humanities and social sciences, 81.2% of the terms had entries,\\nranging from 100% of the Sociology terms to 52% of the History terms.\\nIt is interesting to note that many of the History terms were proper\\nnouns (e.g., names of countries or people), which are not listed in the\\nmain entries of most standard dictionaries.  The History dictionary, on\\nthe other hand, contained a large number of proper noun entries.\\nAn average of 12.7% of the technical entries were marked with the\\nspecific discipline name.  None of the Sociology and History terms were\\nmarked, and 4.2% of the economics terms were marked, yielding an\\naverage number of marked terms for the humanities\\nand social sciences of 1.4%.\\nFew of the humanities and social science terms have meanings which\\nare distinct from those of general usage.  When one considers the topics\\ncovered by these disciplines, this finding makes sense.  They\\nstudy objects and concepts that make up ordinary life, which are\\nalso the topics of general conversation and writing.  In contrast, the\\nhighly scientific disciplines such as Physics or Mathematics are concerned\\nwith more esoteric\\nconcepts that are not the subject of general discourse.\\n\\n\\nDifferent fields have\\nSLs that may be understood to be distinctive in\\nseveral different senses.\\nFor example, a particular SL may be seen as distinct from another SL\\nwhen\\nthe two SLs  use different terminology.\\nThe term ``muon\" occurring in the Physics SL would\\nnot be expected to occur in a  Psychology SL.\\nThis form of distinctiveness is probably the easiest form to measure\\nwith automated techniques\\ncapable of looking for string occurrences in either or both SLs.\\nA different form of distinction occurs when an SL uses a term\\nin a different sense than in another\\nSL.\\nFor example, the term ``affect\" occurs in both the psychological and physics\\nliterature, but has a specialized meaning for psychologists in addition\\nto the meaning common to both of the SLs.\\nAn extreme example of the use of different senses for different SLs would\\nbe the case where two SLs have exactly the same vocabulary but different\\nsenses in every case.\\nIf philosophers who suggest that we each have our own meaning for\\nterms are correct, the SL that each of us uses (idiolect)\\nis distinctive in\\nthis ``sense\" sense.\\n\\n\\nWe may measure\\nthe general distinctiveness of an SL by noting the\\noverlap between terms defined in an SL dictionary with those terms\\ndefined in a general dictionary.\\nIt may be computed as\\n\\n\\n\\n\\n\\nwhere SL is the set of terms in the sublanguage dictionary,\\nGthe set of terms in the general dictionary,\\nand where |x| is the number of items in set x.\\nThe distinctiveness measure\\nDS,G will have a value of 1 when\\nthere is no overlap, that is, the sublanguage is completely separated\\nfrom the general language,\\nwhile DS,G=0 when the two sets of terms are identical.\\nThe percentages of entries of technical terms in the general dictionary may be\\nconverted to this DS,Gvalue by subtracting them from 100 and then dividing the result by\\n100.\\nFor example, the fact that \\nof the physics terminology was included\\nin the general dictionary results in \\n\\n,\\nwhile\\n\\n,\\nsuggesting that the SL for Physics is more\\ndistinctive than that for Sociology, which is at the minimum value.\\n\\n\\nThese distinctiveness results may be interpreted in two ways, depending on the\\nassumptions one makes concerning\\nthe inherent technicality of SLs.\\nIf we believe that SLs are  unequal in terms of their technicality,\\nthat is, some languages are inherently more specialized than others,\\nthe DS,G may measure the degree to which more specialized\\nterminology is used in the more technical languages.\\n\\n\\nIf, on the other hand,\\nall SLs are assumed to be specialized to the same degree,\\nthe differing DS,G values may be understood\\nas the degree to which\\nthe differences between the SL and the general language moves from\\nbeing that of a terminological difference to being a difference\\nin sense or meaning.\\nPhysics might be interpreted as having a greater degree of\\nterminological\\ndistinctiveness than Sociology;  the latter more frequently\\nuses the same terminology as\\nthe general language but must use the terms in different senses if the SLs are\\nto be equally specialized.\\n\\n\\nThe definitions in general dictionaries for terms in the humanities and\\nsocial sciences were suggested above to seldom have different\\nsenses in the SL and the general language.\\nIf we accept these dictionary definitions as capturing the true\\nmeanings of the terms, then disciplines such as Physics do have\\nmore distinctive SLs than do disciplines such as Sociology and the hypothesis\\nthat SLs have the same degree of specialization is wrong.\\nIf, on the other hand, we assume that the Sociological definition of\\n``crowd\" is the same as the general definition of ``crowd\" but that,\\nfor a sociologist,\\nthe term brings to mind a specialized constellation of images,\\nthen we might wish to claim that the senses of\\nthe general and SL terms are in fact different, despite the similarity of the\\ndefinitions, and we do not have\\nempirical support for the claim that sublanguages have different degrees\\nof distinctiveness.\\n\\n\\nIf one accepts\\nthe hypothesis that sublanguages are of equal technical\\nspecificity, it becomes necessary to address the question of the degree to\\nwhich an SL can\\nbe different from a general language.\\nOur data for Physics suggests that it has less than half of its\\nspecialized terminology occurring in the general language, while\\nall of the specialized terminology from Sociology occurred in the\\ngeneral language.\\nThis may be seen as providing a possible range of values for\\ntechnical terminology differences in SLs.\\nIf we accept the assumption that SLs are of equal technical\\nspecificity, there must be about this much variation\\nin the sense differences between the SLs.\\n\\n\\nIn addition to comparing SLs with the general language,\\ntwo SLs may be compared with one another.\\nMore generally, the asymmetric\\ndistinctiveness of an SL x from an SL\\ny may be measured as\\n\\n\\n\\n\\n\\nwhere SLx represents the set of terms in the dictionary for sublanguage\\nx.\\n\\n\\nThese two distinctiveness measures may be used to\\nexamine a number of SL phenomena.\\nAs defined, they may measure the distinctiveness of those terms\\nincluded in a specialized dictionary.\\nTo the extent that a sublanguage may be defined as those specialized\\nterms occurring in a discipline specific dictionary, these dictionary based\\nmeasures may be used as measures of sublanguage distinctiveness.\\nA second approach would be to examine the\\ndefinitions of terms in different SLs to study the\\nextent to which the definitions overlap.\\nThis definition based approach is probably superior to an entry based\\nmeasure at capturing the true distinctiveness of an SL.\\nA third possibility would be to combine dictionary definitions with\\nmeanings extracted from the context in which the terms are used in the\\nabstracts.  It would be difficult to represent these meanings in a\\nuseful way, but this approach would probably be superior to the other\\ntwo.\\nOther types of differences in meaning may similarly be used\\nas the basis for measurement.\\n\\n\\n\\n  Automatic Classification into Discipline \\n\\nThe final question we investigated was whether term frequencies could\\nbe used to classify abstracts by discipline.\\nIndividual documents were classified or assigned to a particular database\\nbased on the Poisson percentile described above.\\n The databases used in the analyses above and described in Table  were used for the classification experiments.\\nThe number of documents classified in each discipline\\nranged from a low of 185 for Biology to 250 for Mathematics.\\n The results, shown in Table , suggest that documents may be automatically classified using this procedure with\\na high degree of accuracy using either the title alone or the\\nabstract alone or when both are combined.\\n\\n\\nExperimental\\nlearning systems often gain knowledge from one set of data and\\nthen use this knowledge to process another set of data.\\nFor example, classification systems may learn from half the data\\nand then classify the other half.\\nThey may also learn from an entire data set and then classify the\\nset.\\nThis latter approach results in the classification system learning\\nfrom an item and then using this knowledge to classify  the item.\\nA superior experimental\\napproach to learning for classification is to learn from\\nevery item in the data set except for the item to be classified.\\nWhen this is done for every item in the data set, the maximum information\\nabout database characteristics\\nis\\nobtained that can be obtained\\nwithout tainting the learning with knowledge from the\\nspecific item to\\nbe classified.\\nThe latter experimental technique is more realistic than learning from half\\nthe data and then classifying the other half of the data.\\nIt simulates the production environment in which incoming documents are\\nclassified on the basis of all previously classified ones.\\nThe results obtained with our method of learning from all but\\nthe document to be classified provides classification performance that\\nis both superior to that obtained with half-and-half testing and\\nis closer to that which would be obtained with a production\\nclassification system.\\n\\n\\nThe Poisson percentile is computed somewhat differently\\nfor use in this classification procedure to provide a more conservative\\nand\\nrealistic test of the power of this classification scheme.\\nThe term frequency for both the database in which the term occurs\\nand for the set of databases as a whole\\nis decreased by the term frequency for the document in question.\\nThis effectively allows for classification of a document to be based on\\nterm frequencies from all the other documents and not from the document\\nbeing classified.\\nIn other words, the document being classified is not used in learning\\nthe Poisson percentile used to classify that individual document.\\nThis mimics the situation found in a document\\nfiltering system where a set of percentiles\\nwould be developed from one set of documents, already filtered,\\nand then an arriving document would be classified based on\\nearlier learning.\\n\\n\\nThe weight for a given document was computed as the sum of the\\nPoisson percentiles for the individual terms.\\nEarlier tests using the product of term weights resulted in little\\nconsistent change in classification performance from that obtained\\nusing additive methods.\\nNo normalization for abstract or title size was used.\\nDocuments were classified by assigning weights (for each database)\\nand the document being classified as a member of the database which\\nresulted in the highest document weight for that particular document.\\n\\n\\nTerms from a list of 203 stopwords were removed from the\\ndatabase.\\nFor a few titles,\\nterms occurred only in that document and, when these frequencies are\\nsubtracted from the database frequencies,\\nthe terms have a frequency of 0 with an average frequency of 0.\\nThese terms have an indeterminate and thus unusable Poisson percentile.\\nThis, combined with\\ndeletion of the stopwords, results in some titles effectively having no terms\\nthat could be used to classify the document.\\nFor our experiments\\nthese titles were randomly assigned to databases for classification purposes.\\nThe classification results reported here are thus somewhat\\nlower than would be obtained if more ad hoc methods had been used to classify\\nthese particular titles.\\n\\n\\nThese classification results appear to be very good.\\nSince the classification procedure ``learned\" from other documents than the\\none being classified, there must be something  about the titles and\\nabstracts that allowed them to be classified correctly at such a high rate.\\nWe believe that the high degree of accuracy obtained was due to\\na combination of the relatively\\nlarge number of\\nterms present in a title or an abstract and thus\\nused in the classification procedures and the discriminating quality\\nof the terms.\\nWhile traditional information retrieval applications often discriminate\\nbased\\non a few keywords in the query,\\nour abstracts and title/abstract combinations\\nwere classified based on the much larger number of terms\\noccurring in the title, abstract, or the title and abstract combined.\\nClassification of titles alone probably performed so well because of the\\nhigh discrimination ability of terms included in the titles.\\n\\n\\n  Qualitative Analysis of Classification Failures \\n\\nThe results of the classification were extremely good, as can be\\n seen in Table . It is, however, informative to examine the errors\\n in classification (Table ). Abstracts from History and Physics seemed to be the\\nmost prone to misclassification. Given that many History\\narticles discuss patterns of behavior that lead to specific events,\\nthe fact that 4% of the History abstracts were classified as\\nSociology is not surprising.\\nThis misclassification can be considered a\\n``near miss\".  In fact, Sociology ``attracted\" erroneous classifications\\nfrom all disciplines except Biology.  Those from Economics and\\nPsychology may also be considered ``near misses\", but those from the\\nmore scientific fields are more mysterious.  In some cases, the topic\\nof the abstract was the impact of a technological development on people,\\nor the implications of a policy decision, rather than the technology\\nitself.  In other cases, the cause of the misclassification is not\\nobvious.\\n\\n\\nThe other discipline that ``attracted\" erroneous\\nclassifications was Mathematics.  This is not surprising at all, since\\nMathematics is used as a language or form of expression in\\nseveral fields, including Biology, Electrical Engineering, and\\nPhysics. The misclassifications of Physics abstracts as Biology,\\nElectrical Engineering, and Mathematics can also be understood as\\nbeing a result of their common mathematical language.\\n\\n\\n  Conclusions and Discussion \\n\\nVocabulary in abstracts from different disciplines\\nvaries considerably from discipline to\\ndiscipline.\\nThis is not surprising;\\nafter all, the vocabulary must represent the topics\\nand concepts of the discipline.\\nThe more important results of our research are how\\nthese differences can be characterized and measured.\\n\\n\\nThe first technique we developed is a method of measuring\\nthe technical specificity of the\\nSL vocabulary.  This measure is based on the shift of usage\\nfrom SL to general meanings\\nbetween the top and bottom ranked words.\\nSLs whose vocabularies are clearly identifiable\\nand distinct from general language will have a larger\\nMu and a larger \\n\\n.\\nSLs that are not as distinct will have smaller Mu and \\n\\n.\\nThese SLs reflect\\nthe nature of the topics that are the target of study in their disciplines,\\nwhich are\\nconcerned with issues that surround people in everyday life.  The measurement\\nof the\\ndistinctiveness of an SL will facilitate the development of a variety of\\nlanguage\\nanalysis and retrieval tools.  For example, if one assumes that it is easier to\\nanalyze\\nlanguage that is similar to the more familiar general language, then it will be\\npossible\\nto predict the effort required to provide coverage for the language of a new\\ndiscipline.\\nOf course, the measurement is also interesting for the theoretical study of\\nSLs, and\\nsimilarities and differences between them.\\n\\n\\nThe second result, related to the first, is a measure of the distinctiveness of\\nan SL.  This measure can be based on either the terms of the SL or on\\nthe terms\\' definitions.  The difference between these two measures may further\\ncharacterize the SL.\\nThe consequences of assuming that SLs are equal in degree of technicality\\nwere examined, as well as the more traditional assumption that they\\ndiffer in technicality.\\n\\n\\nThe third result of this research is the development of a highly accurate\\nmethod of\\nclassifying abstracts by discipline, based on word frequencies.  This technique\\nwill be\\nuseful in a variety of filtering and retrieval tasks, for example, as a first\\ntask in\\nidentifying a set of abstracts that are potentially relevant to a query or\\ninformation need.\\nThe misclassifications that may be characterized as ``near misses\" would not\\nnecessarily\\nbe irrelevant to the query, and the few remaining misclassifications could\\nprobably be\\neasily filtered out in subsequent processing stages.\\n\\n\\n   Dictionaries used in this study \\n\\nThe size of each of the dictionaries below was estimated by multiplying\\nthe average number of entries on sample pages by the number\\nof pages.\\n\\n\\nGeneral:  The American Heritage Dictionary,\\nSecond College Edition. Boston: Houghton\\nMifflin Company. 1985.  57,000 entries.\\n\\n\\nBiology:  Chambers Biology Dictionary.\\nPeter M. B. Walker, Ed. Cambridge: Chambers.\\n1989. 10,000 entries.\\n\\n\\nEconomics:  Dictionary of Economics.\\nDonald Rutherford. New York: Routledge. 1992. 4,000 entries.\\n\\n\\nElectrical Engineering: IEEE Standard Dictionary\\nof Electrical and Electronics Terms,\\nThird Edition.  Frank Jay, Ed. New York:  IEEE. 1984. 22,000 entries.\\n\\n\\nHistory: Macmillan Concise Dictionary of World History.\\nBruce Wetterau, Ed.\\nNew York:  Macmillan Books.  1986. 16,000 entries.\\n\\n\\nMath:  Mathematics Dictionary,\\nFifth Edition. G. James  R. James. New York: Van\\nNostrand Reinhold. 1992. 2,000 entries.\\n\\n\\nPhysics:  Dictionary of Physics,\\nThird Edition. H. J. Gray  Alan Isaacs, Eds. Harlow,\\nEssex:  Longman Group. 1991. 8,000 entries.\\n\\n\\nPsychology:  The International Dictionary\\nof Psychology. Stuart Sutherland.  New York:\\nContinuum. 1989. 9,000 entries.\\n\\n\\nSociology:  Sociology. David Jary  Julia Jary.\\nNew York:  Harper Collins. 1991. 2,000 entries.\\n\\nFootnotes\\n\\nWe wish to thank  Nicole Magas for her assistance with the\\ndata collection described here.\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nThe use of terms from natural and social scientific\\ntitles and abstracts is studied from the perspective\\nof sublanguages and their specialized dictionaries.\\nDifferent notions of sublanguage distinctiveness are explored.\\nObjective methods for separating hard and soft sciences are suggested based\\non measures of sublanguage use, dictionary characteristics, and sublanguage\\ndistinctiveness.\\nAbstracts were automatically classified with a high\\ndegree of accuracy by using a formula that considers the\\ndegree of uniqueness of terms in each sublanguage.\\nThis may prove useful for text filtering or information retrieval systems.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nSpoken language processing challenges for integration of speech recognition\\ninto\\nnatural language processing, and must deal with multi-level knowledge sources\\nfrom signal level to symbol level. The multi-level knowledge integration and\\nhandling increase the\\ntechnical difficulty of both the speech and the natural language processing.\\nIn the speech\\nrecognition side, the recognition must be at phoneme-level for large\\nvocabulary continuous speech, and the speech recognition module must provide\\nright level of outputs to the natural language module in the form of not\\nsingle solution but many\\nalternatives of solution hypotheses. The n-best list\\n[], word-graph [], and word-lattice\\n[] techniques are mostly used in this purpose.\\nThe speech recognition module can also ask the linguistic\\nscores from the language processing module in a more tightly coupled\\nbottom-up/top-down hybrid integration scheme [].\\nIn the natural language side, the insertion, deletion, and\\nsubstitution errors of continuous speech must be compensated by robust parsing\\nand partial parsing techniques, e.g. [].\\nOften the spoken languages are ungrammatical, fragmentary, and contain\\nnon-fluencies and speech repairs, and must be processed incrementally under the\\ntime constraints [].\\n\\n\\nMost of the speech and natural language systems which were developed\\nfor English and other Indo-European languages neglect the morphological\\nprocessing,\\nand integrate speech and natural language at the word level\\n[]. Often these systems\\nemploy a pronunciation dictionary for speech recognition and independent\\ndictionaries for natural language processing. However, for the agglutinative\\nlanguages such as Korean and Japanese, the morphological processing plays a\\nmajor role in the language processing since these languages have very complex\\nmorphological phenomena and relatively simple syntactic functionality.\\nUnfortunately\\neven the Japanese researchers apply degenerated morphological techniques for\\nthe\\nspoken Japanese processing []. Obviously\\ndegenerated morphological processing limits the usable vocabulary size for the\\nsystem,\\nand word-level dictionary results in exponential explosion in the number of\\ndictionary\\nentries. For the agglutinative languages, we need sub-word level integration\\nwhich leaves rooms for general morphological processing.\\n\\n\\nThe spoken language processing calls for multi-strategic approaches in order to\\ndeal with signal level as well as symbol level information in a symbiotic and\\nunified way. Recent development of connectionist speech recognition\\n[] and connectionist natural language processing\\n[] shed lights on the connectionist/symbolic hybrid models of\\nspoken language processing, and some of\\nthe researches are already available for English and other Indo-European\\nlanguages\\n[]. We feel that it is the right time to\\ndevelop connectionist/symbolic hybrid spoken languages processing systems for\\nthe\\nagglutinative languages such as Korean and Japanese.\\n\\n\\nThis paper presents one of the such endeavors, SKOPE (Spoken Korean Processing\\nEngine), that has the following unique features:\\n1) The connectionist and symbolic techniques are selectively used according\\nto their strength and weakness. The learning capability, fault-tolerant\\nproperty, and ability of simultaneous integration of multiple\\nsignal-level sources make\\nthe connectionist techniques suitable to the phoneme recognition from the\\nspeech\\nsignals, but the structure manipulation and powerful matching (binding)\\nproperties\\nof the symbolic techniques are the better choices for the complex morphological\\nprocessing of Korean. However, the parallel multiple constraint\\nrelaxation capability of the connectionist techniques are applied together with\\nthe symbolic structure binding techniques for the syntactic processing. 2) The\\nlinguistic characteristics of Korean are fully considered in phoneme\\nrecognition, speech and language integration, and morphological/syntactic\\nprocessing. 3) The SKOPE provides multi-level application program interfaces\\n(APIs)\\nwhich can utilize the phoneme-level or the morphological level or the syntactic\\nlevel services for the applications such as spoken language interface,\\nvoice information retrieval and spoken language translation.\\n\\n\\nWe hope the experience of SKOPE development provide viable answers to\\nsome of the open questions to the speech and language\\nprocessing, such as 1) how learning and encoding can be synergetically\\ncombined in speech and language\\nprocessing, 2) which aspects of system architecture have to be considered in\\nspoken language processing, especially in connectionist/symbolic hybrid\\nsystems,\\nand finally 3) what are the most efficient way of speech and language\\nintegration,\\nespecially for agglutinative languages.\\n\\n\\n    Characteristics of spoken Korean\\n\\n\\nThis section briefly explains the linguistic characterists of spoken\\nKorean before describing the SKOPE system. In this paper, Yale romanization is\\nused for representing the Korean phonemes.\\n1) A Korean word, called Eojeol, consists of more than one morphemes with\\nclear-cut morpheme boundaries.\\n2) Korean is a postpositional language with many kinds of noun-endings,\\nverb-endings, and\\nprefinal verb-endings. These functional morphemes determine the noun's\\ncase roles, verb's tenses, modals, and modification relations between\\nEojeols.\\n3) Korean is a basically SOV language but has relatively free word order\\ncompared to the rigid word-order languages, such\\nas English, except for the constraints that the verb must appear\\nin a sentence-final position. However, in Korean, some word-order\\nconstraints do exist\\nsuch that the auxiliary verbs representing modalities must follow the main\\nverb,\\nand the modifiers must be placed before the word (called head) they modify.\\n4) The unit of pause in speech (which is called Eonjeol) may be different\\nfrom that of a written text (an Eojeol). The spoken morphological analysis\\nmust deal with\\nan Eonjeol since no Eojeol boundary can be provided in the speech.\\n5) Phonological changes can occur in a morpheme, between morphemes in an\\nEojeol,\\nand even between Eojeols in an Eonjeol. These changes include consonant and\\nvowel assimilation, dissimilation, insertion, deletion, and contraction.\\n6) Korean has many rising diphthongs that are very similar to mono-vowels at\\nsignal level. Korean has well-developed syllable structures, and unlike\\nJapanese\\n that has only CV type syllable, Korean has all different types such as CV, VC, V, CVC. Moreover, in CVC type syllable, first\\nand second consonants are almost same in pronunciation. These signal\\ncharacteristics make it difficult to directly use phonemes or syllables as\\nsub-word recognition units.\\n\\n\\n  The SKOPE architecture \\n\\nThe above spoken Korean characteristics and the relative strength and weakness\\nof symbolic/connectionist techniques result in the general SKOPE architecture\\n which is shown in figure . The architecture consists of three different but closely interrelated modules:\\nphoneme recognition, morphological analysis, and syntactic analysis module.\\nThe phoneme\\nrecognition module processes the signal-level information, and changes it to\\nthe\\nsymbol-level information (phoneme lattice). The morphological analysis begins\\nthe primitive language processing, and connects the speech recognition to the\\nlanguage processing at the phoneme-level. The syntactic analysis\\n module finishes the language processing, and produces\\nthe domain independent syntactic structures for application systems.\\nThe following subsections briefly describe each module.\\n\\n  Diphone-based connectionist phoneme recognition \\n\\nThe phoneme recognition is performed by developing the hierarchically organized\\ngroup of\\nTDNNs (time delay neural networks) [].\\nConsidering the signal characteristics of the Korean phonemes,\\nwe define diphones as a new sub-word recognition\\n unit. The defined diphones are shown in figure , and are classified into four different types. The diphones\\nhave the co-articulation handling features similar to the\\npopular triphones [] but are much fewer in numbers.\\n\\n\\n Figure  shows the architecture of the component TDNNs in the phoneme recognition module.\\nThe whole module consists of total 19\\ndifferent TDNNs for recognition of the defined Korean diphones. The top-level\\nTDNN identifies the 18 vowel groups of diphones (we re-classified the total 672\\ndiphones into 18 different groups according to the vowels that are contained in\\nthe diphones). The 18 different sub-TDNNs recognize the target diphones.\\n\\n\\nFor the training of TDNNs, we manually segment the digitized speech into 200\\nmsec range (which includes roughly left-context phoneme, target diphone, and\\nright context phoneme), and perform 512 order FFTs and 16 step mel-scaling\\n[] to get the filter-bank coefficients. Each frame size is\\n10\\nmsec, so 20 (frames) by 16 (mel-scaling factor) values are fed to the TDNNs\\nwith\\nthe proper output symbols, that is, vowel group name or target diphone names.\\nAfter the training of each TDNN, the phoneme recognition is performed by\\nfeeding 200 msec signals to the vowel group identification network and\\nsubsequently to the\\nproper diphone recognition network. The 200 msec signals are shifted by 30 msec\\nsteps and continuously fed to the networks to process the continuous speech\\nin an Eonjeol.  From the resulting diphone\\nsequences, the necessary phoneme lattice has to be constructed.\\nWe use a simple deterministic decoding heuristics and try to maintain all the\\npossible diphone spotting results since the later\\nphonological/morphological processing can safely prune the incorrect\\nrecognitions.  The decoding begins by grouping the diphones into the same\\n types (see figure ). The frequency count for each diphone, that is,\\nthe number of specific diphones per 30 msec frame shift, is utilized to\\nfix the insertion errors by deleting the lower frequency count diphones,\\nand finally the diphones are split into the constituent phonemes by\\nmerging the same phonemes in the neighboring diphones.\\n\\n\\n  Table-driven morphological and phonological analysis \\n\\nThe morphological analysis starts with the phoneme lattice.\\nThe phoneme lattice delivers the alternative phonetic\\n transcriptions of input speech, which must be searched by the morphological/phonological\\nanalyzer to reconstruct the orthographic morpheme strings. The conventional\\nmorphological analysis procedure [], that is, morpheme\\nsegmentation,\\nmorphotactics modeling, and orthographic rule (or phonological rule) modeling,\\nmust be augmented and extended as the followings: 1) The conventional\\nmorpheme segmentation is extended\\nto deal with the exponential number of phoneme sequences and\\nbetween-morpheme phonological changes during the segmentation, 2) the\\nmorphotactics modeling is extended to cope with the complex verb and\\nnoun-endings\\n(or postpositions), and 3) the orthographic rule modeling is combined with\\nthe phonological rule modeling to correctly transform the phonetic\\ntranscriptions to the orthographic morpheme sequences.\\n\\n\\nThe central part of the morphological analysis lies in the dictionary\\nconstruction. In our dictionary, each phonetic transcription of single morpheme\\n has a separate dictionary entry. Figure  shows the unified dictionary both for speech and language processing (called morpheme-level\\nphonetic dictionary) with three different morpheme entries ci-wu, l, swu.\\n\\n\\nThe extended morphological analysis is based on the well-known tabular\\nparsing technique for context-free language [] and augmented to\\nhandle the Korean\\n phonological rules and phoneme-lattice input. Figure  shows our extended table-driven morphological analysis process. The example phoneme\\nlattice was obtained from the input speech\\nci-wul-sswu (removable), and the morphological analysis\\nproduces ci-wu+l+swu (remove+ADNOMINAL+BOUND-NOUN),\\nwhere '+' is the morpheme boundary, and '-' is the syllable boundary.\\n\\n\\nThe extended morpheme segmentation is basically performed using the\\ndictionary search. During\\nthe left-to-right scan of the input phoneme lattice, when a morpheme boundary\\nis\\nfound\\nin the lattice, the morpheme is enrolled in the triangular table in an\\nappropriate\\nposition. For example,\\n in figure , morphemes such as ci-wu, l, swu, etc are enrolled in the table position (1,3), (4,4), (5,6), etc. The position (i,j)\\ndesignates the starting and ending position of the enrolled morphemes.\\nHowever since the\\ninput is a phoneme-lattice, total exponential time is required to\\nfind all the possible morpheme boundaries. To cope with such exponential\\nexplosion, the dictionary is organized as trie\\nstructure [] using the phonetic transcriptions as trie indices,\\nand breadth-first search of the trie can prune the unnecessary phoneme\\nsequences earlier in the search.\\n\\n\\nThe morphotactics modeling is necessary after all the morphemes are enrolled in\\nthe table in order to combine only legal morphemes into an Eojeol (Korean\\nword),\\nand the process is called morpheme-connectivity-checking.\\nSince Korean has well developed postpositions (noun-ending, verb-ending,\\nprefinal verb-ending) which play as grammatical functional morphemes, we must\\nassign each morpheme proper part-of-speech (POS) tags for the efficient\\nconnectivity\\nchecking. Our more than 200 POS tags which are refined from the 13 major\\nKorean lexical categories are\\nhierarchically organized, and contained in the dictionary\\n (in the name of morphological connectivity, see figure ). In the case of idiomatic\\nexpressions, we place such idioms directly in the dictionary for efficiency,\\nwhere two different POS tags are necessary for the left and the right\\nmorphological connectivity.\\nFor single morpheme, the left and the right POS tags are always same.\\nThe separate morpheme-connectivity-matrix indicates\\nthe legal morpheme combinations, and the morphotactics modeling\\nis performed using the POS tags (in the dictionary) and\\nmorpheme-connectivity-matrix.\\n\\n\\nThe orthographic rule modeling must be integrated with the phonological rule\\nmodeling in spoken language processing. Since we must deal with the phoneme\\nlattice, the conventional rule-based modeling requires exponential number of\\nrule application []. So our solution is based on the\\ndeclarative modeling of both orthographic and phonological rules in uniform\\nway.\\nThat is, in our dictionary, the conjugated verb forms as well as the original\\nverb forms are enrolled, and the same morphological connectivity information is\\napplied.\\nThe phonological rule modeling is also accomplished declaratively by having\\nthe phonemic\\nconnectivity information in the dictionary. The phonemic connectivity\\ninformation for each morpheme declares the possible phonemic changes in the\\nfirst\\n(left) and last (right) positioned phonemes in the morpheme, and the separate\\nphoneme-connectivity-matrix indicates the legal sound combinations in Korean\\n phonology. For example, in figure , the morpheme l can be combined with the morpheme swu during the morpheme connectivity checking\\neven if swu is actually pronounced as sswu because\\nthe phoneme-connectivity-matrix supports the legality of the\\n combination of l sound with ss sound.  In this way, we can declaratively model all the major Korean phonology rules such as second consonant standardization, consonant\\nassimilation,\\npalatalization, glotalization, insertion, deletion, and contraction.\\n\\n\\n  Table-driven connectionist/symbolic syntax analysis \\n\\nThe phoneme lattice-based morphological analysis produces the morphologically\\nanalyzed (segmented and stem reconstructed) morpheme sequences. Since there are\\nusually more than one analysis results due to the errors of speech recognition\\nprocess, the outputs are usually organized as morpheme lattice.\\nFor the seamless integration of the\\nmorphological analysis with the syntax analysis, we employ the same\\ntable-driven\\ncontrol for the syntax analysis as well as the morphological analysis.\\n\\n\\nWe extend the category formation and functional application rules in the\\nprevious categorial unification\\ngrammar[]\\nto deal with the word order variations in Korean:\\n\\nif category a \\nC, then a \\nC'\\n\\nif category a \\nC', and category set S \\nC', then a/S \\nC'\\nand\\naS \\nC'\\n\\n\\nwhere S is an unordered set of categories.\\n\\nleft cancellation: ai b{a1,a2, ..., an}\\nresults in\\nb{a1, a2, ..., ai-1, ai+1, ..., an}\\n\\nright cancellation: b/{a1,a2, ..., an} ai results in\\nb/{a1, a2, ..., ai-1, ai+1, ..., an}\\n\\n\\n\\n\\nThe syntax analysis is performed by interactive relaxation (spreading\\nactivation)\\nparsing on the categorial grammar where the position of the functional\\napplications are controlled by a triangular table. The original interactive\\nrelaxation parsing\\n[] was extended to provide efficient constituent searching and\\nexpectation generation through positional information provided by categorical\\n grammar and triangular table. Figure  shows table-driven interactive relaxation parsing.\\n\\n\\nThe interactive relaxation process consists of the following three steps that\\nare repetitively executed: 1) add nodes, 2) spread activation, and 3) decay.\\nadd nodes\\nGrammar nodes (syntactic categories from the dictionary)\\nare added for each sense of the morphemes when\\nthe parsing begins. A grammar node which has more activation than the\\npredefined threshold \\ngenerates new nodes in the proper positions (to\\nbe\\ndiscussed shortly). The newly\\ngenerated nodes search for the constituents (expectations)\\nwhich are in the appropriate table\\npositions, and are of proper function applicable categories. For example,\\n in figure , when npnp(2,2) fires, it generates np(1,2). The\\ngenerated np(1,2) searches for the constituents np(1,1) to be combined with\\nnpnp(2,2).\\nspread activation\\nThe bottom-up spreading activation is as follows:\\n\\n\\nwhere predefined portion \\nof total activation a is passed upward to the\\nnode with activation ai among the n parents each with node activation\\naj. In other words, the node with\\nlarge activation gets more and more activation, and it gives an inhibition\\neffects without explicit inhibitory links [].\\nThe top-down spreading activation uniformly distributes:\\n\\n\\namong the children where ' is predefined portion of the source\\nactivation a.\\ndecay\\nThe node's activation is decayed with time. The node with less\\nconstituents than needed gets penalties plus decays:\\n\\n\\nwhere a is an activation value, d is a decay ratio, and Ca, Cr is the\\nactual\\nand required constituents. After the decay, the node with less activation than\\nthe predefined threshold \\nis removed from the table.\\n\\n\\nThe node generation and constituent search positions are controlled by the\\ntriangular table. When the node a(i,j) acts as an argument, it generates node\\nonly in the position (k,j) where 1[k[j, and the generated node searches\\nfor the constituents\\n(functors) only in the position (k,i-1). Or when the node is generated in the\\nposition\\n(i,k) where \\n\\nj[k[number-of-morphemes, it searches for the position (j+1,k)\\nfor its constituents.\\nWhen the node acts as a functor, the same position restrictions also\\napply for the node generation and the argument searching.\\nThe position control combined with the interactive relaxation guarantees an\\nefficient,\\nlexically oriented, and robust syntax analysis of spoken languages.\\n\\n\\n\\n  Implementation and experiments \\n\\nThe SKOPE was fully implemented in UNIX/C platform, and have been extensively\\ntested in practical domains such as natural language interface to operating\\nsystems.\\nThe phoneme recognition module targets 1000 morpheme continuous speech,\\ncurrently speaker dependent due to the short of standard speech database for\\nKorean. The unified morpheme-level phonetic dictionary has about 1000\\nmorpheme entries and compiled into the trie structure. The\\nmorpheme-connectivity-matrix and phoneme-connectivity-matrix are encoded with\\nthe special Korean POS (part-of-speech) symbols and compressed.\\n\\n\\nThis section demonstrates the SKOPE's performance in continuous diphone\\nrecognition, morphological analysis, and syntax analysis experiments.\\nFor the continuous diphone recognition experiment, we generated about 5500\\ndiphone patterns from the 990 Eojeol patterns (66 Eojeols, 15 times\\npronunciation) for the training of TDNNs.\\nIn the performance phase, the new 2600 test Eojeol patterns (260 Eojeol, 10\\ntimes pronunciation) are\\ncontinuously shifted with 30 msec step, and generate 7772 test\\ndiphone patterns disjoint from the training patterns.\\n Figure -a shows the continuous diphone recognition performance.  The correct designates that the correct\\ntarget diphones were spotted in the testing position, and the delete\\ndesignates the other case.\\nThe insert designates that the non-target diphones were spotted in the\\ntesting position. To compare the ability of handling the continuous speech,\\nwe also tested the diphone recognition using the hand-segmented test patterns\\nwith the same 7772 target diphones.\\n Figure -b shows the segmented diphone recognition performance. Since the test data are already hand-segmented before input, there are no\\ninsertion and deletion errors in this case.\\nThe fact that the segmented speech performance is not much better than the\\ncontinuous one (93.8% vs. 93.4%) demonstrates the\\ndiphone's suitability to handling the continuous speech.\\n\\n\\nFor the morphological analysis performance, we used the same\\n990 Eojeol patterns to train the phoneme recognition module, and the\\n2600 Eojeol patterns to test the morphological analysis performance directly\\nfrom the speech input.\\n Figure  shows the results. \\n\\n\\nThis experiment shows that most of the morphological errors are\\npropagated from the incorrect (deleted) or spurious (inserted) phoneme\\nrecognition results.\\nTo see the original performance of the morphological and syntactic analysis\\nmodules assuming no speech recognition error, we\\nartificially made the phoneme lattices by mutating the correctly\\nrecognized phoneme sequences\\naccording to the phoneme recognizer's confusion matrix. Each phoneme\\nlattice was made to contain at least one correct recognition result,\\nso the phoneme recognition performance is assumed to be perfect except the\\nartificially made insertion errors (mutations). In this way, we\\nmade 6 or 7 lattices for each of the 50 sentences, altogether 330\\nphoneme lattices.  The average phoneme alternatives per single correct phoneme\\nin the lattice are 2.3,\\nand average sentence length is 31 phonemes. This means there are average\\n2.3[31] phoneme chains in each lattice. The used sentences are natural\\nlanguage commands to UNIX [] and are fairly complex which have\\none or two embedded sentences or conjunctions.\\n Figure  shows the morphological and syntactic analysis results for these artificially made phoneme lattices. For\\nthe syntactic level interactive relaxation, we used the following parameters\\n(which are experimentally\\ndetermined): upward propagation portion \\n0.05,\\ndownward propagation portion ' 0.03, decay ratio d 0.87, the node\\ngeneration threshold \\n0.51, and the node removal threshold .066.\\n\\n\\nThe morphological analysis was perfect as shown in the table. Since the\\nphoneme lattice was made to contain at least one correct phoneme recognition\\nresult, the morphological analysis must be perfect as long as the morpheme is\\nenrolled in the dictionary and the connectivity information can cover all\\nthe morpheme combinations. This was possible due to the small number of\\ntested sentences (50 sentences). This results verify that most of the\\nmorphological\\nanalysis errors from real speech input are actually propagated from the phoneme\\nrecognition errors as discussed before. However, the syntax analysis results\\nare\\nmarginal here since we only count the single best scored tree, and we don't use\\nyet any semantic feature in the analysis.\\nThe syntax analysis failures mainly come from 1) the\\ninsertion errors (artificial mutations) in the phoneme\\n lattices, which result in ambiguous\\nmorpheme lattice, and finally produce redundant syntax trees, and 2)\\nthe inherent structural ambiguities in the sentence. These failures\\nshould be greatly reduced if we generate n-best scored parse trees, and let\\nthe semantic processing module select the correct ones as is usually done\\nin most of the probabilistic parsing schemes[].\\n\\n\\n  Conclusions and future works \\n\\nThis paper explains the design and implementation of spoken Korean processing\\nengine, which is a connectionist/symbolic hybrid model of spoken language\\nprocessing by utilizing the linguistic characteristics of Korean.\\nThe SKOPE model demonstrates the synergetic integration of connectionist\\nand symbolic techniques by considering the relative strength and weakness of\\ntwo\\ndifferent techniques, and also demonstrates the phoneme level speech and\\nlanguage integration for\\ngeneral morphological processing for agglutinative languages. Besides the\\nabove two\\nmajor contributions, the SKOPE architecture has the following unique features\\nin spoken language processing: 1) the diphones are newly developed as a\\nsub-word\\nrecognition unit for connectionist Korean speech recognition, 2) the\\nmorphological and syntactic analysis are tightly coupled by using the uniform\\ntable-driven control, 3) the phonological and orthographic rules are\\nuniformly co-modeled declaratively, and 4) the table-driven interactive\\nrelaxation parsing and extension of the categorial grammar can provide robust\\nhanding of word-order variations in Korean.\\n\\n\\nHowever, current implementation of the system still suffers\\nfrom excessive continuous speech recognition errors. Since the large vocabulary\\ncontinuous\\nspeech recognition is still an open problem, we cannot hope for the 100%\\ncorrect speech recognition results in the near future. Currently, we are\\npursuing multi-strategic approaches to the advanced spoken language\\nprocessing model, including optimizing TDNN-based phoneme recognition\\nmodule, integrating HMM-based morpheme recognition module into the\\nconnectionist phoneme recognition, and incorporating\\nprobabilistic searches into the morphological analysis process as well as the\\nsyntactic analysis process. We are also developing applications on top of our\\nSKOPE, including speech-to-speech translation system and intelligent interface\\nagent for UNIX operating system. We hope our approach could be extended to\\nother\\nagglutinative languages such as Japanese, Finish, and Turkish, and also to the\\nlanguages that have complex morphological phenomena such as German and Dutch.\\n\\n\\n  Acknowledgments \\n\\nThis research was supported partly by a grant from KOSEF (Korea Science\\nand Engineering Foundation) and PIRL (Postech Information Research\\nLaboratory).  The SKOPE's various modules were programmed by our students: the\\nphoneme recognition module by Kyunghee Kim  Kyubong Baac, the morphological\\nanalysis module by EunChul Lee  Wonil Lee, and finally the syntax analysis\\nmodule by Wonil Lee.\\n\\nFootnotes\\n\\n  C: consonant, V: vowel\\n  We believe that the semantic\\nand pragmatic processing should be integrated into the domain knowledge for\\npractical application under the current NLP technology, so we\\nexcluded the semantic and pragmatic processing from our general model.\\n  Unlike English, the Korean alphabet is truly phonetic\\nin the sense that each phoneme is pronounced as it\\nis written. That is why we sometimes use phonetic and\\nphonemic interchangeably.\\n  This legality comes\\nfrom the Korean phonology rule glotalization (one form of consonant\\ndissimilation) stating that s sound becomes ss sound after\\nl sound.\\n  Recall we\\ngenerated average 2.3 phonemes per single correct phoneme.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nSpoken language processing requires speech and natural language\\nintegration. Moreover, spoken Korean calls for unique processing\\nmethodology due to its linguistic characteristics. This paper presents\\nSKOPE, a connectionist/symbolic spoken Korean processing engine, which\\nemphasizes that: 1) connectionist and symbolic techniques must be selectively\\napplied according to their relative strength and weakness, and  2) the\\nlinguistic\\ncharacteristics of Korean must be fully considered for phoneme recognition,\\nspeech and language integration, and morphological/syntactic processing.\\nThe design and implementation of SKOPE demonstrates how connectionist/symbolic\\nhybrid architectures can be constructed for spoken agglutinative language\\nprocessing.  Also SKOPE\\npresents many novel ideas for speech and language processing. The phoneme\\nrecognition, morphological analysis, and syntactic analysis\\nexperiments show that SKOPE is a viable approach for the spoken Korean\\nprocessing.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nPart-of-speech (POS) tagging is a basic step to several natural language\\nprocessing applications including text-based information retrieval, speech\\nrecognition, and text-to-speech synthesis. The POS tagging has been\\nusually performed by statistical (or data/corpus-driven) approaches mainly\\nusing hidden markov model (HMM)\\n[].\\nHowever, since statistical approaches only consider the neighboring tags\\nwithin a limited window (usually two or three),\\nsometimes the decision cannot cover all the linguistic rules necessary for the\\ndisambiguation. Also the approaches are inappropriate for the idiomatic\\nexpressions in which the lexical term itself needs to be consulted for the\\ndisambiguation. The statistical approaches are insufficient for the\\nagglutinative languages (such as Korean) which have usually complex\\nmorphological structures.\\nIn these languages, a word consists of single stem morpheme plus several\\nfunctional\\nmorphemes, and the POS tags should be assigned to each morpheme to best\\nexploit the complex morphological structures.\\nConsidering just the neighboring morphemes regardless of their grammatical\\nfunctions is not enough for the morpheme-level POS disambiguation.\\nRecently, rule-based approaches are re-studied to cope with the limitations\\nof statistical approaches by learning the tagging rules automatically from\\nthe corpus []. Some systems even perform the POS\\ntagging as part of syntactic analysis process [].\\nHowever, the rule-based\\napproaches alone are in general not robust to handle the unknown words,\\nand is not flexible to adjust to the new tag-sets and\\nlanguages. Also the performance is usually no better than the statistical\\ncounterparts []. To gain flexibility and robustness\\nand also to overcome the\\nlimited window range of statistical approaches, we need a method that can\\ncombine both statistical and rule-based approaches [].\\n\\n\\nThis paper presents a hybrid POS disambiguation methods that cascaded\\nstatistical and\\nrule-based approaches in a two-phase learning architecture.\\nOur system TAKTAG (Two-phase learning Architecture for Korean part-of-speech\\nTAGger) combines the state-of-the-art hidden markov model with\\nBrill [1992] style rule learning error correction. The system is trained in\\ntwo phases: HMM parameter estimation and comparison-based rule\\nlearning for the HMM tagging output. The TAKTAG has the unique following\\nproperties of the Korean POS disambiguation:\\n\\nThe system is designed to be very accurate in tagging especially the\\nambiguous\\nKorean morphemes that have more than one part-of-speeches. The accuracy is\\nvery important in Korean tagging since Korean has much poorer tagging\\nperformance compared with English due to its linguistic characteristics.\\nAlthough some of\\nthe POS ambiguities cannot be resolved at the morphology level, we\\ntried to correct as much as tagging errors by introducing the rule-based\\nerror correction scheme.\\n\\nThe system fully considers many linguistic characteristics of\\nKorean in HMM/rule tagging. Unlike English and other Indo-European languages,\\nthe\\ncomplex functional morphemes determine the grammatical roles of Korean\\n words (which is called Eojeol, see section ). \\n\\nThe system is flexible so that it can tune to the new tag-sets and\\nnew languages. In other words, the system doesn't rely on the enormous\\namounts of pre-existing\\ntagged corpus for its training. This is very important since the Korean tag\\nsets are not stabilized yet, nor are the standard Korean tagged corpus\\nprovided yet.\\n\\nIn TAKTAG, the tag-sets are hierarchically organized so that they can\\nbe adjustable according to the given applications such as\\ninformation retrieval, speech synthesis, text data extraction, and so on.\\n\\n\\nThe rest of the sections are organized as follows.\\n Section  explains the linguistic characteristics of Korean and the\\nhierarchically organized tag sets for multiple applications.\\n Section  discusses the two-phase learning architecture, its  process model and the training procedures. Section  demonstrates the performance of TAKTAG with extensive experiments and finally\\n section  draws some conclusions of the works. \\n\\n\\n    Hierarchical tag-sets for Korean morphology\\n\\n\\nKorean is classified as agglutinative languages in which the words (which is\\ncalled Eojeol in Korean) consist\\nof several morphemes that have clear-cut morpheme boundaries. Below are the\\ncharacteristic of Korean that must be considered for POS tag-set and tagging\\nsystem design.\\n1.\\nKorean is a postpositional language\\nwith many kinds of noun-endings, verb-endings, and\\nprefinal verb-endings. It is the functional morphemes, not Eojeol's order that\\ndetermine the most of the grammatical relations such as the noun's\\ncase roles, verb's tenses, modals, and modification relations between\\nEojeols. So contextual information for POS disambiguation must be\\nselectively applied to the functional (bound) morphemes or content (free)\\nmorphemes.\\n2.\\nSometimes a Korean Eojoel corresponds to an English phrase, not to a\\nsingle\\nword, so the tagging must be done on morpheme basis, not Eojeol basis. The\\nmorphological analyzer must precede the tagging system because the\\nmorpheme segmentation is much more important and difficult than POS\\nassignment in Korean.\\n3.\\nKorean is basically SOV\\nlanguages but has relatively free word order\\ncompared to English, except for the constraints that the verb must appear\\nin a sentence-final position. However, in Korean, some word-order\\nconstraints do exist\\nsuch that the modifiers must be placed before the word (called head) they\\nmodify. So some order constraints must be applied as contextual\\ninformation, but some must not.\\n4.\\nComplex spelling changes can occur between morphemes when two morphemes\\ncombine to form an Eojeol. These spelling changes make it difficult to\\nsegment the morphemes before assigning the POS tags. Also a lot of allomorphs\\nare generated from the spelling changes.\\nFor the above reasons, a morphological analysis play important roles in Korean\\nPOS tagging system. It is the morphological analysis process which\\ninitially segments\\nthe morphemes out of the Eojoels, reconstructs the spelling changes, and\\nassigns the initial POS tags to each morpheme by consulting the dictionary.\\nLater, the tagging system disambiguates the POS assignments by selecting\\nthe single morpheme sequence for each sentence and the single POS tag\\nfor each morpheme by\\nconsulting the lexical and contextual information acquired from the corpus.\\n\\n\\nWe classified over 200 POS tags that can be used in\\nmorphological analysis as\\nwell as the POS disambiguation. Our POS tags, which are originally\\ndesigned for\\nmorphotactics modeling in CYK-based Korean morphological\\nanalysis [], consists of the\\nhierarchically organized 200 symbols that are refined from the seven major\\ngrammatical categories of Korean, which are nominal, predicate, modifier,\\nparticle, ending, symbol, interjection. For single morpheme,\\na path name in the POS symbol hierarchy\\n(e.g. nominal:noun:proper-noun:person-name:no-final-consonant)\\nis assigned as a POS tag.\\nThe tag can be a full path name or part of the path name to adjust the\\nnumber of tags in the tag-set.\\nIn this way, the tag-set can be adjusted by refining the\\nmore pertinent grammatical categories to the applications at hand. For\\nexample, for the text information retrieval application, we can more refine the\\nnominals than the predicates since the indexing terms are usually nominals.\\n Figure  shows one example of tag-set extracted from the POS symbol hierarchy. This tag-set will be used in our experiment in\\n section . \\n\\n\\n    Two-phase learning of POS disambiguation\\n\\n\\n Figure  shows a two-phase learning architecture for Korean POS tagging\\nsystem. There are three major components: the morphological analyzer,\\nthe HMM tagger, and the error-corrector.\\nThe morphological analyzer segments the constitutional morphemes out\\nof the input\\ntexts and assigns the initial POS tag for each morpheme by\\nconsulting the dictionary.\\nThe Korean morphological analysis procedure consists of the following\\nthree steps:\\nmorpheme segmentation, morphotactics modeling, spelling change handling\\n[]. The input texts are scanned from left to right,\\ncharacter by character, to be matched to the morphemes in the dictionary. For\\nthe efficient text search, the modified CYK parsing method based on the\\ndynamic programming technique is applied []. For the\\nmorphotactics modeling, we defined the hierarchical POS symbol system as\\n mentioned in section . At the bottom of the hierarchy, there are about 200 POS tags that\\nreflects the morphosyntactic properties of Korean. The full path name in\\nthe POS symbol hierarchy is\\nencoded in the dictionary for each morpheme entry, and is called morpheme\\nconnectivity information. To model the morpheme's\\nconnectability to each other (so called morphotactics),\\nthe separate morpheme connectivity table encodes\\nthe connectability of each 200 morpheme connectivity information.\\nSo when the input Eojeol is\\nsegmented by the dictionary search, the analyzer checks\\nwhether the segmentation is legal or not by consulting the morpheme\\nconnectivity table to find out the connectability of the two segmented\\nmorphemes.\\nIn the dictionary, we also enroll the inflected forms as\\nwell as original (uninflected) morphemes as header information, so that we\\ncan reconstruct the original form from the spelling changes of morphological\\ncombination.\\n\\n\\nThe HMM tagger takes the morpheme sequence with the initial tag assignment\\nby the morphological analyzer, and\\nusing the Viterbi algorithm [], searches the optimal tag\\nsequence for the POS disambiguation. Sometimes, there can be multiple morpheme\\nsequences for one sentence due to the multiple segmentation results in\\nKorean.\\nIn that case, we perform Viterbi search for each morpheme sequence, and\\nselect the maximum probability tag sequence as a solution.\\nTo reduce the computational complexity, we can share common morphemes in the\\ndifferent morpheme sequence during the Viterbi search as studied in\\n[]. The equation of the HMM tagging model we use is the\\nordinary bi-gram model with left to right search:\\n\\n,\\nwhere T[*] is the optimal tag sequence that maximizes the forward\\nViterbi scores.\\nThe \\n\\nPr(ti|ti-1) is a bi-gram tag transition probability,\\nand the \\n\\nPr(mi|ti) is a morpheme lexical probability.\\n\\n\\nWe call the results of this HMM tagging as the\\nfirst-tagged-morpheme-sequence,\\nand usually the tagging accuracy is not satisfactory because of the\\n characteristics of Korean as mentioned in section . The error-corrector transforms the first tagged morpheme\\nsequences to the final tagged text. The error-corrector is a rule-based\\ntransformer, and it matches the condition\\npart of the rules, and change the erroneous tags to the tags in the action\\npart. The rules are in the form of:\\n\\n,\\nwhere the rule condition part consists of the current and context morphemes\\nwith their\\ntags, and the action part is the current morpheme with the corrected tag.\\nThe \\nmeans\\nthat the rule can see the several composite contexts at one time.\\nThe next section\\nexplains the training algorithms of the HMM tagger and the error-corrector in\\ndetail, and shows what kinds of error correcting rules are learned to overcome\\nthe statistical tagging limitations.\\n\\n  Learning HMM-based disambiguation \\n\\nThe first phase of learning in the two-phase POS disambiguation is\\nthe HMM parameter\\ntraining. Since the HMM tagger takes morpheme sequences as input, unlike\\nEnglish, the\\ntraining corpus must be morphologically analyzed, too. The POS tags are\\nassigned to each original morpheme (in the training corpus) which is\\nreconstructed from the spelling\\nchanges. There can be many morphological analysis results\\nfor one sentence in Korean. In that case, we include only correct\\nmorphological analysis results in the training corpus by following\\n[].\\n\\n\\nThere are two types of HMM parameter training methods that are widely used.\\nThe first method is to use the enormous amounts of tagged corpus such as\\nBrown corpus [] to extract the lexical and\\ntransition probabilities from the\\nfrequences of tags associated with words and of pairs of tag\\n[]. This method is not desirable at the moment for\\nKorean because 1) there is no large tagged corpus available yet, and\\n2) the tag-sets\\nare not standardized yet. The second method of training does not require\\nlarge tagged corpus for training []. In this case, the\\nBaum-Welch algorithm [] can be used for estimation of\\nthe HMM parameters by iterative relaxation (which is one form of the\\nestimation-maximization (EM) algorithm). However, several studies show that\\nusing as much as tagged corpus for training gives much better performance\\non tagging\\n[], and the fact favors for the Church [1988] style\\ntagging as long as large tagged corpus is available for Korean.\\nHowever, for the parameter estimation from the small amount of tagged\\ncorpus, the Baum-Welch algorithm always helps to increase the tagging\\nperformance. In this regard, we used\\nsmall amounts of tagged corpus (about 2000 morphemes) for bootstrapping the\\nBaum-Welch training, and mainly use the Baum-Welch algorithm for the\\nwhole training using the morphologically analyzed but untagged corpus.\\n\\n\\n  Learing rule-based tagging correction \\n\\nThe statistical morpheme tagging only covers the limited range of\\ncontextual information. Moreover, it does not see the lexical form itself in\\ndisambiguation. As mentioned before, Korean has very complex morphological\\nstructure so it is necessary to see the functional morphemes selectively to\\nget the relation between Eojeols. For these reasons, we designed the error\\ncorrecting rules to compensate the missings of the statistical tagging.\\nHowever, designing the tagging rules with knowledge engineering is\\ntedious and error-prone. Instead, we adopted Brill`s approach [1992] to\\nautomatically learn the error correcting rules from the tagged corpus.\\nFortunately, Brill showed that we don't need large tagged corpora\\nto extract the symbolic rules, especially\\n compared with the ones in need for the statistical tagging. Table  shows the rule schema we used to extract the error correcting rules, where\\nthe rule schema designates the context, i.e., the place and the lexical/tag\\ndecision in the rule\\n (see rule format in section ). More than one rule schema can be simultaneously applied to the error correction so that the rule can see\\nmore than one contexts at one time.\\nThe rules are learned according to the schema by comparing the correctly\\ntagged corpus (morphologically analyzed and hand tagged) with the output\\nof the HMM tagger (called the first-tagged-morpheme-sequence).\\nThe acquired rules are sorted by their effectiveness\\nwhich is defined by the number of successful corrections using the rules\\nas used in [].\\n\\n\\n\\n    Experiments\\n\\n\\nWe collected 70000 morpheme corpus which was from diverse domains such as\\nnational ethics code,\\nelementary school textbooks, composition handbooks, and so\\n on. All the sentences in the corpus\\nwere morphologically analyzed before use. In each domain, about 15% of the\\ncorpus are manually\\ntagged for error correcting rule learning, about 15% are set aside for\\nthe test, and the remaining 70% are used for the Baum-Welch training. For\\ninitial bootstrapping of HMM, we are provided with other 2000 morpheme tagged\\ncorpus which is disjoint from our original corpus.\\nFrom the 15% (about 10000 morphemes) of the corpus, we extracted about\\n 445 error-correction rules using the rule schema in table .  Table  shows the final tagging results. The accuracy is calculated from the formula: \\n\\n.\\nThe results show that the error correcting rules are quite useful to\\nincrease the overall tagging accuracy, and the overall results are\\nmuch better than the previous well-engineered HMM tagging results (which was\\nabout 89.1% in a similar environment) even\\nthough our HMM tagging alone are not quite successful [].\\nThis results demonstrate that the well-engineered HMM tagging with our error\\ncorrecting rules can increase the\\noverall tagging performance up to over 97% which was considered to be\\nimpossible with statistical tagging alone in English [].\\n\\n\\n    Conclusions\\n\\n\\nWe presented a new POS tagging architecture which\\nintegrates the statistical approach with the rule learning approach in a\\nsynergistic way. Our hybrid tagging architecture is proved to be useful,\\nespecially for the morphologically complex agglutinative languages such as\\nKorean. The system TAKTAG can provide the\\nfollowing two unique properties for desirable Korean tagging: 1) The system can\\nprovide accurate results even with the morpheme\\ntagging which usually results in very poor performance, and 2) The system can\\nbe flexibly tuned to the new tag-sets without massive retraining.\\nThe performance of the two-phase learning for tagging is determined how well\\nthe error-corrector can compensate the deficiencies of the statistical\\ntagging, and in that sense, our TAKTAG is much successful since it increased\\nthe overall tagging results more than 10%. The next step will be to analyze\\nthe learned rules carefully to extract the more desirable rule schema for\\nKorean.\\nThe robust unknown word handling scheme with more efficient morphological\\nanalyzers also should be studied with the well-engineered HMM taggers that\\nfully consider the linguistic characteristics of Korean.\\n\\n\\n  Acknowledgments \\n\\nThis research was partially supported by POSCO (Pohang iron and steel\\ncompany). We thank to NamHee Hong and Wonil Lee for re-classification of Korean\\npart-of-speech and re-implementation of Korean morphological analyzer.\\nThe corpus was selected from the one provided by ETRI (Electronic and\\nTelecommunication Research Institute), Korea.\\n\\nFootnotes\\n\\n  The 300000 Eojeol (about 700000 morpheme)\\nuntagged corpus was provided from the ETRI (Electronics and Telecommunication\\nResearch Institute) in Korea. We selected 70000 morpheme\\nsentences among them\\nwith careful consideration to the corpus balance.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nBoth statistical and rule-based approaches to part-of-speech (POS)\\ndisambiguation have their own advantages and limitations. Especially for\\nKorean, the narrow windows provided by hidden markov model (HMM) cannot cover\\nthe necessary lexical and long-distance dependencies for POS disambiguation.\\nOn the other hand, the\\nrule-based approaches are not accurate and flexible to new tag-sets and\\nlanguages. In this regard, the statistical/rule-based hybrid method\\nthat can take\\nadvantages of both approaches is called for the robust and flexible POS\\ndisambiguation. We present one of such method, that is, a two-phase\\nlearning architecture for the hybrid statistical/rule-based\\nPOS disambiguation, especially for Korean.\\nIn this method, the statistical learning of morphological tagging is\\nerror-corrected by\\nthe rule-based learning of Brill [1992] style tagger. We also design the\\nhierarchical and flexible Korean tag-set to cope with the multiple\\ntagging applications, each of which requires different tag-set.\\nOur experiments show\\nthat the two-phase learning method can overcome the undesirable features\\nof solely HMM-based or solely rule-based tagging, especially for\\nmorphologically\\ncomplex Korean.\\n\\n'],\n",
              " [\"\\n\\n  Motivation \\n\\nCurrent work in surface realization concentrates on the use\\nof general, abstract algorithms that interpret declaratively defined,\\nnon-directional grammars.\\nIt is claimed that this way, a grammar can be reused for parsing and\\ngeneration, or a generator can interpret different grammars (e.g. in machine translation). A prominent example for this type of\\nabstract algorithm is semantic-head-driven generation\\n[] \\nthat has been used with HPSG, CUG, DCG and several other formalisms.\\n\\n\\nIn practice, this type of surface realization has several drawbacks.\\nFirst, many existing grammars have been developed with parsing as the\\nprimary type of processing in mind. Adapting their semantics layer to a\\ngeneration algorithm, and thus achieving reversibility, \\ncan turn out to be a difficult enterprise\\n[]. Second, many linguistically motivated \\ngrammars do not cover \\ncommon means of information presentation, such as filling in a table,\\nbulletized lists, or semi-frozen formulae used for greetings in letters. \\nFinally, the grammar-based \\nlogical form representation hardly serves as a suitable\\ninterface to deep generation processes. \\nGrammar-based semantics is, to a large extent, a compositional\\nreflex of the syntactic structure and hence corresponds too closely\\nto the surface form to be generated. As a consequence, only little\\nattention has been paid to interfacing this type of realizers \\nadequately to deep generation\\nprocesses, e.g. by allowing the latter to influence the order\\nof results of the former.\\n\\n\\nThe system TG/2, which is presented in this contribution,\\novercomes many flaws of grammar-based surface realization systems\\nthat arise in concrete applications. In particular, TG/2\\n\\ncan be smoothly integrated with 'deep' generation processes,\\n\\nintegrates canned text, templates, and context-free rules into a\\nsingle formalism,\\n\\nallows for both textual and tabular output,\\n\\nefficiently reuses generated substrings for additional solutions, and\\n\\ncan be parameterized according to linguistic properties\\n(regarding style, grammar, fine-grained rhetorics etc.).\\n\\n\\n\\n\\nTG/2 is based on restricted production system techniques that\\npreserve modularity of processing and linguistic knowledge, hence\\nmaking the system transparent and reusable for various applications. Production\\nsystems have been used both for modeling human thought (e.g. [])\\nand for the construction of knowledge-based expert systems (e.g. []). In spite of the modularity gained by separating\\nthe rule basis from the interpreter, production systems have disappeared from\\nthe focus of current research  because of their limited transparency\\ncaused by various types of side effects. In particular, side effects \\ncould modify the data base in such a way that other rules become\\napplicable [].\\n\\n\\nHowever, precondition-action pairs can be used in a more restricted way,\\npreserving transparency by disallowing side effects that affect the\\ndatabase. In TG/2 preconditions are tests over the database contents\\n(the generator's input structure), and actions typically\\nlead to a new subset of\\nrules the applicability of which would be tested on some\\nselected portion of the database. By constraining the effects of\\nproduction rules in such a way, the disadvantages of early\\nproduction systems are avoided. At the same time, considerable\\nflexibility is maintained with regard to linguistic knowledge used.\\nA production rule may \\n\\ninvolve a direct mapping to surface forms (canned\\ntext),\\n\\nrequire to fill in some missing portion from a surface text\\n(template), or\\n\\ninduce the application of other rules (classical grammar rules)\\n\\n\\n\\n\\nEarly template-based generation methods have correctly been criticized for\\nbeeing too inflexible to account adequately for the communicative and \\nrhetorical demands of many applications. On the other hand, templates\\nhave been successfully used when these demands could be hard-wired\\ninto the rules. In TG/2 the rule writer can choose her degree of\\nabstraction according to the task at hand. She can freely intermix\\nall kinds of rules.\\n\\n\\nThe rest of the paper is organized as follows. TG/2 assumes as its input a\\npredicate-argument structure, but does not require any particular\\nformat. Rather, a separate translation step is included that translates the\\noutput of feeding components into expressions of \\n the Generator Interface Language (GIL) (Section ).   In Section  the formalism TGL (Template Generation Language) for production rules is introduced. \\nThe properties of TGL allow for efficient generation of all\\npossible solutions in any order. The TGL interpreter and its\\n generic backtracking regime are presented in Section . It is used to parameterize\\nTG/2 by inducing an order in which the solutions are generated\\n (Section ).  \\n\\n\\n Figure  gives an overview of the system and its components.\\n\\n\\n  The Generation Interface Language (GIL) \\n\\n Although the level of logical form is considered a good candidate for an interface to surface\\nrealization, practice shows that notational idosyncrasies can pose\\nsevere translation problems. TG/2 has an internal language, GIL, that\\ncorresponds to an extended predicate argument structure. GIL is the\\nbasis for the precondition test predicates and the selector functions\\nof TGL. Any input to TG/2 is first translated into GIL before being\\nprocessed. It is of considerable practical benefit to keep the\\nrule basis as independent as possible from external conditions (such\\nas changes to the output specification of the feeding system).\\n\\n\\nGIL is designed to be a target language suited for deep generation\\nprocesses. Similar aims have been pursued with the development of the \\nSentence Plan Language (SPL) [] that is used in a variety of\\ngeneration systems. Like SPL, GIL assumes only little grammatical\\ninformation. GIL can represent DAG-like feature\\nstructures. \\n Figure  contains a sample GIL expression. The example shows the major language elements:\\n\\nThe top level consists of a speech act predicate and arguments\\nfor author, addressee and theme (the speechact proper).\\n\\nDiscourse objects can be assigned unique constants (ID) that denote\\nSETs of discourse objects. \\n\\nSMOOD expresses sentence modalities including sentence type, time,\\na specification of\\nwhich constituents to topicalize in a German declarative sentence, etc.\\n\\nThe predicate argument structure is reflected by corresponding\\nfeatures; ARGS contains a list of arguments.\\n\\nDifferent sorts of free temporal and local adjuncts can be\\n specified by corresponding features. In Figure , a temporal adjunct is represented under TIME-ADJ.\\n\\nArguments and, in part, adjuncts are specified for their role, for\\ncardinality, for quantificational force (under CONTENT.QFORCE),\\nand further details such as name strings and natural gender.\\n\\nTemporal adjuncts relate to some context (e.g. tomorrow) or\\nare indexical (e.g. on Wednesday, February 7, 1996). All\\ncommon combinations in German are covered.\\n\\n\\n\\n\\n  The Template Generation Language (TGL) \\n\\nTGL defines a general format for expressing production rules as\\n precondition-action pairs (cf. Figure ). A TGL rule is applicable if its preconditions are met. A TGL rule is successfully\\napplied, if the action part has been executed without failure.\\nFailure to apply a TGL rule signals that the\\nrule does not cover the portion of the input structure submitted to it.\\n\\n\\n Figure  shows a sample TGL rule. It corresponds to an infinitival VP covering \\na direct object, an optional temporal adjunct, an optional expression\\nfor a duration (such as for an hour), \\nan optional local adjunct (such as at the DFKI building), and the\\ninfinite verb form. \\n Given the input GIL structure of Figure , the  VP Sie am Freitag treffen [to meet you on Friday]\\ncould be generated from this rule. Among the optional constituents,\\nonly the temporal adjunct would find appropriate material in the\\nGIL input structure (under THEME.TIME-ADJ). \\n\\n\\nEvery TGL rule has a unique name, denoted by the initial string. This\\n name is used for expressing preferences on alternative rules (cf. Section ). \\n\\n\\nCategory:\\nThe categories can be defined as in a context-free grammar.\\nCorrespondingly, categories are used for rule selection (see below).\\nThey ensure that a set of TGL rules possesses a context-free backbone.\\n\\n\\nTest:\\nThe Lisp code under :TEST is a boolean predicate\\n(usually about properties of the portion of input structure under\\ninvestigation or about the state of some memory). In the sample rule,\\nan argument is required that fills the patient role.\\n\\n\\nTemplate:\\n Actions under :TEMPLATE  include the selection of other rules (:RULE, :OPTRULE), executing a function (:FUN), or returning an ASCII string as a (partial) result.\\n\\n\\nWhen selecting other rules by virtue of a category, a Lisp function is\\ncalled that identifies the\\nrelevant portion of the input structure for which a candidate rule\\n must pass its associated tests. In Figure , the first action selects all rules with category NP; the\\n relevant substructure is the argument filling the patient role (cf. the second element of the ARGS list in Figure ). If there is no such substructure, an error\\n is signalled unless an OPTRULE slot (for ``optional rule'') was executed. In this case, processing continues without results from that slot.\\n\\n\\nFunctions  must return an ASCII string. They are mostly used\\nfor word inflection; otherwise, for German every inflectional variant would have\\nto be encoded as a rule. TG/2 uses the morphological inflection component\\nMORPHIX [].\\n\\n\\nSide effects:\\nThe Lisp code under\\n:SIDE-EFFECTS is a function whose value is ignored. \\nIt accounts for non-local dependencies between\\nsubstructures, such as  updates of a discourse memory. Note that\\nthese effects can be traced and undone in the case of backtracking.\\n\\n\\nConstraints:\\nAgreement relations are encoded into the rules\\nby virtue of a PATR \\nstyle [] feature percolation mechanism. The rules can\\nbe annotated by equations that either assert equality of a feature's \\nvalue at two or more constituents or introduce a feature value at\\na constituent. Attempting to overwrite \\na feature specification yields an error. \\n In Figure , the right-hand side constituent  NP is assigned accusative case.  \\nAny of these effects are subject to backtracking.\\n\\n\\nUsing TGL, small task- and domain-specific grammars can be written\\nquickly. For instance, in the domain of appointment scheduling the\\nsystem  COSMA  []  has to accept, reject, \\nmodify, or refine suggested meeting dates via email. The sublanguage\\nencoded in TGL only needs a few speech acts, about twenty sentential\\ntemplates, and a complete account of German date expressions. Moreover,\\nformal as well as informal opening and closing phrases for emails are covered. \\n\\n\\nLarger grammars may become difficult to maintain unless special care is\\ntaken by the grammar writer to preserve a global structure of rules, \\nboth by defining suitable  categories and by documenting the rules. \\nTGL rules are presently written using a text editor. A specialized\\nTGL grammar editor could improve the development and the organization\\nof grammars considerably.\\nSyntactic correctness is checked at\\ncompile-time by an LR-Parser generated by Zebu []\\non the basis of a BNF syntax for TGL.\\n\\n\\n  An interpreter with generic backtracking \\n\\n TG/2 has a simple interpretation procedure that corresponds to the classical three-step evaluation cycle in production systems\\n(matching, conflict resolution, firing) [].\\nThe algorithm receives a GIL structure as its input and uses\\na distinguished category, TXT, to start from.\\n1. Matching:\\nSelect all rules carrying the current category.\\nExecute the tests for each of these rules on the input structure\\nand add those passing their test to the conflict set.\\n2. Conflict resolution:\\nSelect an element from the conflict set.\\n3. Firing:\\nExecute its side effect code (if any). Evaluate its constraints (if any).\\nFor each action part, read the category, determine the\\nsubstructure of the input by evaluating the associated function, and\\ngoto 1.\\n\\n\\nThe processing strategy is top-down and depth-first. The set\\nof actions is fired from left to right. Failure of executing some\\naction causes the rule to be backtracked. \\n\\n\\nThe interpreter yields all solutions the grammar can generate. It\\nattempts to generate and output a first solution, producing\\npossible alternatives only on external demand. Any alternative is based\\non backtracking at least one rule. Backtrack points correspond to\\nconflict sets containing more than one element.\\n\\n\\nBacktracking may turn out to be inefficient if it involves\\nrecomputation of previously generated substrings. In TG/2 this effort\\nis reduced considerably because it is only necessary to recompute\\nthe part licensed by the newly selected rule. What has been\\ngenerated before or after it remains constant (modulo some word forms\\nthat need to agree with new material) and can thus be\\nreused for subsequent solutions. This is possible\\ndue to the design properties of TGL: rules cannot\\nirrevocably influence other parts of the solution. In particular, the\\ncontext-free backbone implicit in any solution and the restrictions\\nto side effects mentioned above keep the structural effects of TGL rules local.\\n\\n\\nIn the sequel, technical aspects of the backtracking regime are\\ndiscussed. \\nLet us assume that the interpreter compute a backtrack point. \\nLet us call the sequence of strings \\ngenerated by previous actions its pre-context, the set of\\nstring sequences\\ngenerated from the elements of the conflict set its ego, \\nand the sequence of strings generated \\nfrom subsequent actions its post-context. For every ego, the pre- or \\nthe post context may be empty.\\n\\n\\nEach time a backtrack point is encountered during processing, \\nan entry into a global table is made\\nby specifying its pre-context (which is already known due to the\\nleft-to-right processing), a variable for the ego (which\\nwill collect the sequences of strings generated by the elements\\nof the conflict set), \\n and a variable for the post-context (which is unknown so far).  Figure     shows the state of a sample table comprising three backtrack points  after all solutions have been\\ncomputed. The ego variable is shown using indices running over\\nthe elements of the respective conflict sets.  \\nThe operator `\\n\\n\\n  Parameterization \\n\\n Parameterization of TG/2 is based on specifying the way how the generic backtracking regime should operate. It can be\\ninfluenced with regard to\\n\\nthe element in the conflict set to be processed next, and\\n\\nthe backtrack point to be processed next.\\n\\n\\nBoth possibilities taken together allow a system that feeds TG/2 to\\nspecify linguistic criteria of preferred solutions to be generated\\nfirst. \\n\\n\\nThe criteria are \\ndefined in terms of rule names, and a criterion is fulfilled\\nif some corresponding rule is successfully applied. We call such a\\nrule c-rule.\\nTG/2 implements a simple strategy that processes those backtrack points\\nfirst that have conflict sets \\ncontaining c-rules, and preferrably choses a c-rule from a\\nconflict set. When applied incrementally, this procedure\\nyields all solutions fulfilling (some of) the criteria first. \\n\\n\\nIt would be desirable to see the solution fulfilling most criteria first.\\nHowever, incremental application enforces decisions to be taken locally\\nfor each conflict set. Any c-rule chosen may be the last one in a\\nderivation, whereas chosing a non-c-rule may open up further\\nopportunities of chosing c-rules. \\nThese limits are due to a lack of look-ahead information: it is not known in \\ngeneral which decisions will have to be taken \\n until all solutions have been generated.  Clearly, sacrificing incrementality is not what should be desired\\nalthough it may be acceptable for some\\napplications.  The drawbacks include a loss\\nof efficiency and run-time. This leaves us with two possible\\ndirections that can lead to improved results.\\n\\n\\nAnalyzing dependencies of criteria:\\nThe solution fulfilling most criteria is generated first if  \\nsets of mutually independent\\ncriteria are applied: fulfilling one criterion must not exclude the\\napplicability of\\nanother one, unless two criteria correspond to rules of the same conflict\\nset. In this case, they must allow for the the application of the\\nsame subset of criteria. \\nIf these conditions are met, chosing a c-rule from every\\nconflict set, if possible, will lead to a globally best solution first.\\nThere is, however, the practical problem that the\\nconditions on the criteria can only be fulfilled by analyzing, and\\npossibly modifying, the TGL grammar\\nused. This contradicts the idea of having the user specify her preferences\\nindependent of TG/2 properties.\\n\\n\\nLearning dependencies of criteria:\\nMissing look-ahead information could be acquired  automatically by\\nexploiting the derivational history of previously generated texts.\\nFor every applied rule, \\nthe set of c-rules applied later in the current subtree of a derivation\\nis  stored.\\nFrom this information, we can derive off-line for any set of criteria\\nwhich c-rules have applied in the corpus\\nand how often each c-rule has applied within a\\nderivation. Computing such information from \\nthe context-free backbone of TGL grammars instead\\nwould be less effective since it\\nneglects the drastic filtering effects of preconditions.\\nHowever, checking the grammar this way indicates which c-rules will not\\nappear in some subtree.\\n\\n\\nDuring processing, TG/2 can then judge the global impact of chosing \\nthe locally best c-rule and decide to fulfill or violate a criterion.\\nThe success of this method depends on how well the derivation under\\nconstruction fits with the sample data. \\nThe more examples the system observes, the more reliable will be its\\ndecisions. \\n\\n\\nThe latter approach is in fact independent on how the criteria influence each\\nother. In addition, it can be extended to cope with weighted criteria.\\nA weight is specified by the user (e.g. a feeding system) and expresses the\\nrelative importance of the criterion being fulfilled in a solution.\\nTG/2 would give preference to derivations leading to the\\nmaximum global weight. The global\\nweight of a solution is the sum \\nof the c-rule weights, each divided by the number of times the c-rule occurs.\\n\\n\\nHowever, different GIL structures may, for a TGL rule, lead to\\ndifferent sets of follow-up c-rules. This causes\\nthe decision to be nondeterministic unless the reasons for the\\ndifference are learned and applied to the case at hand. We must leave it to\\nfuture research to identify and apply suitable learning algorithms to\\nsolving this problem. \\n\\n\\nCriteria have been implemented for choosing a language,\\nfor chosing between active and passive \\nsentences, for preferring paratactical over hypotactical style, and for \\nchoice of formal vs. informal wordings. Additional uses could include\\nsome rhetorical structuring (e.g. order of nucleus and satellites\\nin RST-based analyses []).\\n\\n\\nThe approach presented offers a technical framework \\nthat allows a deep generation process \\nto abstract away from many idiosyncrasies of\\nlinguistic knowledge by virtue of meaningful weighting functions.\\nIdeally, these functions must implement a theory of how\\nmutual dependencies of criteria should be dealt with.\\nFor instance, lexical choice and constituent order constraints \\nmay suggest the use of passive voice (cf. e.g. []).\\nIt is a yet open question whether such a theory can be encoded by\\nweights. However, for some sets of preferences, this approach\\nhas proven to be sufficient and very useful.\\n\\n\\n  Conclusion \\n\\nIn this contribution, we have introduced TG/2, a production-rule based\\nsurface generator that can be parameterized to generate the best solutions\\nfirst. The rules are encoded in TGL, a language that allows the definition of\\ncanned text items, templates, and context-free rules within the same formalism.\\nTGL rules can, and should, be written with generation in mind, i.e. the goal\\nof reversibility of grammars pursued with many constraint-based approaches\\nhas been sacrificed. This is justified because of the limited usefulness\\nof large reversible grammars for generation. \\n\\n\\nTGL is particularly well suited for the description\\nof limited sublanguages  specific to the domains and\\nthe tasks at hand. Partial reuse of such descriptions depends\\non whether the grammar writer keeps general, reusable definitions \\nindependent from the specific, non-reusable parts of the grammar.\\nFor instance, time and date descriptions encoded for the  COSMA\\ndomain can be reused in other TG/2 applications. \\nOn the other hand, TGL sublanguage grammars can be developed using existing\\nresources. For instance, suitable fragments of context-free grammars \\ntranslated into TGL could be augmented by the domain and task specific \\nproperties needed. Practical experience must show whether this approach\\nsaves effort.\\n\\n\\nThe system is fully implemented in Allegro Common Lisp and runs on\\ndifferent platforms (SUN workstations, PC, Macintosh). Computing \\nthe first solution of average-length sentences (10-20 words) takes between\\none and three seconds on a SUN SS 20.\\nTG/2 is being used in the domain of appointment scheduling within DFKI's\\n COSMA system. In the near future, the system will be used within an \\nNL-based information kiosk, where information about environmental data must be\\nprovided in both German and French language, including tabular presentations if\\nmeasurements of several substances are involved. \\n\\nFootnotes\\n\\n  This work has been supported by a grant from \\nThe Federal Ministry for Research and Technology (FKZ ITW 9402). I am\\ngrateful to Michael Wein, who implemented the\\ninterpreter, and to Jan Alexandersson\\nfor influential work on a previous version of the system. Finally, I\\nwish to thank two anonymous reviewers for useful suggestions.\\nAll errors contained in this paper are my own.\\n  The notion of template is preserved\\nfor historical reasons: the predecessor system TG/1 was strictly\\ntemplate-based.\\n  In the case at hand, the grammar writer preferred\\nto ensure availability of the substructure by virtue of the test\\npredicate.\\n  In \\nfact, it is preterminal rather than terminal elements that are stored in\\nthe table in order to account for modified\\nconstraints. This can be neglected in the present \\ndiscussion, but will be taken up again below.\\n  Note that this\\nconclusion  does not depend on the processing strategy chosen.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nCurrent work in surface realization concentrates on the use\\nof general, abstract algorithms that interpret large, reversible grammars.\\nOnly little attention has been paid so far to \\nthe many small and simple applications that require coverage of\\na small sublanguage at different degrees of sophistication. \\nThe system TG/2 described in this paper\\ncan be smoothly integrated with deep generation processes,\\nit integrates canned text, templates, and context-free rules into a\\nsingle formalism, it allows for both textual and tabular output, and\\nit can be parameterized according to linguistic preferences.\\nThese features are based on suitably restricted production system\\ntechniques and on a generic backtracking regime.\\n\\n'],\n",
              " [\"\\n\\n  BACKGROUND \\n\\n Previous work by Manny Rayner and the author, see  attempts to tailor an existing natural-language\\nsystem to a specific application domain by extracting a specialized grammar\\nfrom the original one using a large set of training examples. The training\\nset is a treebank consisting of implicit parse trees that each specify a\\nverified analysis of an input sentence.\\nThe parse trees are implicit in the sense that each node in the tree is the\\n(mnemonic) name of the grammar rule resolved on at that point, rather than\\nthe syntactic category of the LHS of the grammar rule as is the case in an\\n ordinary parse tree. Figure  shows five examples of implicit parse trees.  The analyses are verified in the sense that each analysis\\nhas been judged to be the preferred one for that input sentence by a human\\nevaluator using a semi-automatic evaluation method.\\n\\n\\nA new grammar is created by cutting up each implicit parse tree in the\\ntreebank at appropriate points, creating a set of new rules that consist\\nof chunks of original grammar rules.\\nThe LHS of each new rule will be the LHS phrase of the original grammar\\nrule at the root of the tree chunk and the RHS will be the RHS phrases\\nof the rules in the leaves of the tree chunk. For example, cutting up the\\n first parse tree of Figure  at the NP of the rule  vp_v_np yields rules 2 and 3 of Figure . \\n\\n\\nThe idea behind this is to create a specialized grammar that retains a\\nhigh coverage but allows very much faster parsing. This has turned out to\\nbe possible -- speedups compared to using the original grammar of in\\nmedian 60 times were achieved at a cost in coverage of\\n about ten percent, see . Another benefit from the method is a decreased error rate when the system\\nis required to select a preferred analysis.\\nIn these experiments the scheme was applied to the grammar of a version of\\n the SRI Core Language Engine  ] adapted to the Atis domain  for\\n a speech-translation task [] and large corpora of real user data\\n collected using Wizard-of-Oz simulation. The resulting specialized\\ngrammar was compiled into LR parsing tables, and a special LR parser\\nexploited\\n their special properties, see .\\n\\n\\n\\nThe technical vehicle previously used to extract the specialized grammar\\n is explanation-based generalization (EBG), see e.g. []. Very briefly, this consists of redoing the derivation of each training\\nexample top-down by letting the implicit parse tree drive a rule expansion\\nprocess, and aborting the expansion of the specialized rule currently being\\nextracted if the current node of the implicit parse tree meets a set of\\n tree-cutting criteria. In this case the extraction process is invoked recursively to extract subrules rooted in the\\ncurrent node. The tree-cutting criteria can be local (``The LHS of the\\noriginal grammar rule is an NP,'') or dependent on the rest of the\\nparse tree (``that doesn't dominate the empty string only,'') and previous\\nchoices of nodes to cut at (``and there is no cut above the current node\\nthat is also labelled NP.'').\\n\\n\\nA problem not fully explored yet is how to arrive at an optimal choice of\\ntree-cutting criteria. In the previous scheme, these must be specified\\nmanually, and the choice is left to the designer's intuitions.\\nThis article addresses the problem of automating this process and presents\\na method where the nodes to cut at are selected automatically using the\\ninformation-theoretical concept of entropy.\\nEntropy is well-known from physics, but the concept of perplexity is perhaps\\nbetter known in the speech-recognition and natural-language communities.\\nFor this reason, we will review the concept of entropy at this point, and\\ndiscuss its relation to perplexity.\\n\\n  Entropy \\n\\nEntropy is a measure of disorder. Assume for example that a physical system\\ncan be in any of N states, and that it will be in state si with\\nprobability pi. The entropy S of that system is then\\n\\n\\n\\n\\n\\nIf each state has equal probability, i.e. if \\n\\n\\nfor\\nall i, then\\n\\n\\n\\n\\n\\nIn this case the entropy is simply the logarithm of the number of states\\nthe system can be in.\\n\\n\\nTo take a linguistic example, assume that we are trying to predict the next\\nword in a word string from the previous ones. Let the next word be wkand the previous word string \\n\\nw1,...,wk-1. Assume further that we have\\na language model that estimates the probability of each possible next word\\n(conditional on the previous word string). Let these probabilities be pifor \\n\\ni = 1,...,N for the N possible next words wk[i],\\ni.e. \\n\\n.\\nThe entropy is then a measure of how hard this prediction problem is:\\n\\n\\n\\n\\n\\nIf all words have equal probability, the entropy is the logarithm of the\\nbranching factor at this point in the input string.\\n\\n\\n  Perplexity \\n\\nPerplexity is related to entropy as follows.\\nThe observed perplexity Po of a language model with respect\\nto an (imaginary) infinite test sequence \\n\\nw1,w2,... is defined through\\n the formula (see ) \\n\\n\\n\\n\\n\\nHere \\n\\np(w1,...,wn) denotes the probability of the word string \\n\\nw1,...,wn.\\n\\n\\nSince we cannot experimentally measure infinite limits, we terminate after\\na finite test string \\n\\nw1,...,wM, arriving at the measured perplexity\\nPm:\\n\\n\\n\\n\\n\\nRewriting \\n\\np(w1,...,wk) as\\n\\n\\ngives us\\n\\n\\n\\n\\n\\nLet us call the exponential of the expectation value of\\n\\n\\nthe local perplexity \\n\\n,\\nwhich can be used as a measure of the\\ninformation content of the initial String.\\n\\n\\n\\n\\n\\nHere \\n\\n\\nis the expectation value of \\nand the summation is\\ncarried out over all N possible next  words wk[i]. Comparing this\\nwith the last equation of the previous section, we see that this is\\nprecisely the entropy S at point k in the input string.\\nThus, the entropy is the logarithm of the local perplexity at a given\\npoint in the word string. If all words are equally probable, then the\\nlocal perplexity is simply the branching factor at this point. If the\\nprobabilities differ, the local perplexity can be viewed as a\\ngeneralized branching factor that takes this into account.\\n\\n\\n  Tree entropy \\n\\nWe now turn to the task of calculating the entropy of a node in a parse\\ntree. This can be done in many different ways; we will only describe two\\ndifferent ones here.\\n\\n\\n Consider the small test and training sets of Figure . \\nAssume that we wish to calculate the entropy of the phrases of the\\nrule PP \\n\\n Prep NP, which is named pp_prep_np.\\nIn the training set, the LHS PP is attached to the RHS PP\\nof the rule np_np_pp in two cases and to the RHS PP of the\\nrule vp_vp_pp in one case, giving it the entropy \\n\\n.\\nThe RHS preposition Prep is always a lexical lookup, and the entropy\\n is thus zero, while the RHS NP in one case attaches to the LHS of rule\\nnp_det_np, in one case to the LHS of rule np_num, and in one\\ncase is a lexical lookup, and the resulting entropy is thus\\n\\n.\\n\\n\\nThe complete table is given here:\\n\\n\\nIf we want to calculate the entropy of a particular node in a parse\\ntree, we can either simply use the phrase entropy of the RHS node, or\\ntake the sum of the entropies of the two phrases that are unified in\\nthis node. For example, the entropy when the RHS NP of the rule\\npp_prep_np is unified with the LHS of the rule np_det_n\\nwill in the former case be 1.10 and in the latter case be\\n\\n1.10 + 1.33 = 2.43.\\n\\n\\n\\n  SCHEME OVERVIEW \\n\\nIn the following scheme, the desired coverage of the specialized grammar is\\nprescribed, and the parse trees are cut up at appropriate places without\\nhaving to specify the tree-cutting criteria manually:\\n1.\\nIndex the treebank in an and-or tree where the or-nodes correspond to\\nalternative choices of grammar rules to expand with and the and-nodes\\ncorrespond to the RHS phrases of each grammar rule.\\nCutting up the parse trees will involve selecting a set of or-nodes in the\\nand-or tree. Let us call these nodes ``cutnodes''.\\n2.\\nCalculate the entropy of each or-node. We will cut at each node whose\\nentropy exceeds a threshold value. The rationale for this is that we wish\\nto cut up the parse trees where we can expect a lot of variation i.e. where\\nit is difficult to predict which rule will be resolved on next. This\\ncorresponds exactly to the nodes in the and-or tree that exhibit high\\nentropy values.\\n3.\\nThe nodes of the and-or tree must be partitioned into equivalence classes\\ndependent on the choice of cutnodes in order to avoid redundant derivations\\n at parse time. Thus, selecting some particular node as a cutnode may cause other nodes to\\nalso become cutnodes, even though their entropies are not above the threshold.\\n4.\\nDetermine a threshold entropy that yields the desired coverage.\\nThis can be done using for example interval bisection.\\n5.\\nCut up the training examples by matching them against the and-or\\ntree and cutting at the determined cutnodes.\\n\\n\\nIt is interesting to note that a textbook method for constructing\\ndecision trees for classification from attribute-value pairs is to\\n minimize the (weighted average of the) remaining entropy over all  possible choices of root attribute, see . \\n\\n\\n  DETAILED SCHEME \\n\\nFirst, the treebank is partitioned into a training set and a test set. The\\ntraining set will be indexed in an and-or tree and used to extract the\\nspecialized rules. The test set will be used to check the coverage of the\\nset of extracted rules.\\n\\n  Indexing the treebank \\n\\nThen, the set of implicit parse trees is stored in an and-or tree. The parse\\ntrees have the general form of a rule identifier Id dominating a list\\nof subtrees or a word of the training sentence. From the current or-node of\\nthe and-or tree there will be arcs labelled with rule identifiers\\ncorresponding to previously stored parse trees.\\nFrom this or-node we follow an arc labelled Id, or add a new one if\\nthere is none. We then reach (or add) an and-node indicating the RHS phrases\\nof the grammar rule named Id.\\nHere we follow each arc leading out from this and-node in turn to\\naccommodate all the subtrees in the list. Each such arc leads to an or-node.\\nWe have now reached a point of recursion and can index the corresponding\\nsubtree. The recursion terminates if Id is the special rule identifier\\nlex and thus dominates a word of the training sentence, rather than\\na list of subtrees.\\n\\n\\n Indexing the four training examples of Figure  will result in  the and-or tree of Figure . \\n\\n\\n  Finding the cutnodes \\n\\nNext, we find the set of nodes whose entropies exceed a threshold value.\\nFirst we need to calculate the entropy of each or-node. We will here\\ndescribe three different ways of doing this, but there are many others.\\nBefore doing this, though, we will discuss the question of redundancy\\nin the resulting set of specialized rules.\\n\\n\\nWe must equate the cutnodes that correspond to the same type of phrase.\\nThis means that if we cut at a node corresponding to e.g. an NP,\\ni.e. where the arcs incident from it are labelled with grammar rules whose\\nleft-hand-sides are NPs, we must allow all specialized\\nNP rules to be potentially applicable at this point, not just the\\nones that are rooted in this node. This requires that we by transitivity\\nequate the nodes that are dominated by a cutnode in a structurally equivalent\\nway; if there is a path from a cutnode c1 to a node n1 and a path from\\na cutnode c2 to a node n2 with an identical sequence of labels, the\\ntwo nodes n1 and n2 must be equated. Now if n1 is a cutnode, then\\nn2 must also be a cutnode even if it has a low entropy value.\\nThe following iterative scheme accomplishes this:\\nFunction N[*(N0])\\n1.\\ni := 0;\\n2.\\nRepeat i := i+1; \\n\\nN[i] := N(N[i-1]);\\n3.\\nUntil \\n\\nN[i] = N[i-1]4.\\nReturn N[i];\\nHere N(N[j]) is the set of cutnodes N[j] augmented with those induced in\\none step by selecting N[j] as the set of cutnodes. In practice this was\\naccomplished by compiling an and-or graph from the and-or tree and the set\\nof selected cutnodes, where each set of equated nodes constituted a vertex\\nof the graph, and traversing it.\\n\\n\\nIn the simplest scheme for calculating the entropy of an or-node, only the RHS\\nphrase of the parent rule, i.e. the dominating and-node, contributes to the\\nentropy, and there is in fact no need to employ an and-or tree at all, since\\nthe tree-cutting criterion becomes local to the parse tree being cut up.\\n\\n\\nIn a slightly more elaborate scheme, we sum over the entropies of the nodes\\nof the parse trees that match this node of the and-or tree. However, instead\\nof letting each daughter node contribute with the full entropy of the LHS\\nphrase of the corresponding grammar rule, these entropies are weighted with\\nthe relative frequency of use of each alternative choice of grammar rule.\\n\\n\\nFor example, the entropy of node n3 of the and-or tree of\\n Figure  will be calculated as follows: The mother rule vp_v_np will contribute the entropy associated\\nwith the RHS NP, which is, referring to the table above, 0.64.\\nThere are 2 choices of rules to resolve on, namely np_det_n and\\nnp_np_pp with relative frequencies \\n\\n\\nand \\n\\nrespectively. Again referring to the entropy table above, we find that\\nthe LHS phrases of these rules have entropy 1.33 and 0.00 respectively.\\nThis results in the following entropy for node n3:\\n\\n\\n\\n\\n\\nThe following function determines the set of cutnodes N that either\\nexceed the entropy threshold, or are induced by structural equivalence:\\nFunction \\n\\nN(Smin)\\n1.\\n\\n;\\n2.\\nReturn N[*](N);\\nHere S(n) is the entropy of node n.\\n\\n\\nIn a third version of the scheme, the relative frequencies of the daughters\\nof the or-nodes are used directly to calculate the node entropy:\\n\\n\\n\\n\\n\\nHere A is the set of arcs, and \\n\\n\\n is an arc from nto ni. This is basically the entropy used in . Unfortunately, this tends to promote daughters of cutnodes to in turn become\\ncutnodes, and also results in a problem with instability, especially in\\nconjunction with the additional constraints discussed in a later section,\\nsince the entropy of each node is now dependent on the choice of cutnodes.\\nWe must redefine the function N(S) accordingly:\\nFunction \\n\\nN(Smin)\\n1.\\n\\n;\\n2.\\nRepeat i := i+1; \\n\\n;\\n\\nN[i] := N[*](N);\\n3.\\nUntil \\n\\nN[i] = N[i-1]4.\\nReturn N[i];\\nHere S(n|N[j]) is the entropy of node n given that the set of cutnodes\\n is N[j]. Convergence can be ensured by modifying the termination criterion to be\\n3. Until \\n\\nfor some appropriate set metric \\n\\n\\n(e.g. the size of the\\nsymmetric difference) and norm-like function \\n\\n\\n(e.g. ten\\npercent of the sum of the sizes), but this is to little avail, since we\\nare not interested in solutions far away from the initial\\nassignment of cutnodes.\\n\\n\\n  Finding the threshold \\n\\nWe will use a simple interval-bisection technique for finding the appropriate\\nthreshold value. We operate with a range where the lower bound\\ngives at least the desired coverage, but where the higher bound doesn't.\\nWe will take the midpoint of the range, find the cutnodes corresponding to\\nthis value of the threshold, and check if this gives us the desired coverage.\\nIf it does, this becomes the new lower bound, otherwise it becomes the new\\nupper bound. If the lower and upper bounds are close to each other, we stop\\nand return the nodes corresponding to the lower bound. This termination\\ncriterion can of course be replaced with something more elaborate.\\nThis can be implemented as follows:\\nFunction N(C0)\\n1.\\n\\nSlow := 0; Shigh := largenumber;\\n\\nNc := N(0);2.\\n If  \\n\\n then goto   else \\n\\n;\\n3.\\n\\nN := N(Smid);4.\\nIf \\n\\nC(N) [ C0 \\nthen \\n\\nShigh := Smid \\nelse \\n\\nSlow := Smid; Nc := N;\\n5.\\n Goto ; 6.\\n Return Nc; Here C(N) is the coverage on the test set of the specialized grammar\\ndetermined by the set of cutnodes N.\\n\\n\\nActually, we also need to handle the boundary case where no assignment of\\ncutnodes gives the required coverage. Likewise, the coverages of the upper\\nand lower bound may be far apart even though the entropy difference is\\nsmall, and vice versa. These problems can readily be taken care of by\\nmodifying the termination criterion, but the solutions have been\\nomitted for the sake of clarity.\\n\\n\\nIn the running example, using the weighted sum of the phrase entropies as\\nthe node entropy, if any threshold value less than 1.08 is chosen,\\nthis will yield any desired coverage, since the single test example of\\n Figure  is then covered. \\n\\n\\n  Retrieving the specialized rules \\n\\nWhen retrieving the specialized rules, we will match each training example\\nagainst the and-or tree. If the current node is a cutnode, we will cut at\\nthis point in the training example. The resulting rules will be the set of\\ncut-up training examples. A threshold value of say 1.00 in our example will\\nyield the set of cutnodes \\n\\n\\nand result in the set of\\n specialized rules of Figure . \\n\\n\\nIf we simply let the and-or tree determine the set of specialized rules,\\ninstead of using it to cut up the training examples, we will in general\\narrive at a larger number of rules, since some combinations of choices in\\nthe and-or tree may not correspond to any training example. If this latter\\nstrategy is used in our example, this will give us the two extra rules of\\n Figure . Note that they not correspond to any training example.\\n\\n\\n\\n  ADDITIONAL CONSTRAINTS \\n\\nAs mentioned at the beginning, the specialized grammar is compiled into\\nLR parsing tables.\\nJust finding any set of cutnodes that yields the desired coverage will\\nnot necessarily result in a grammar that is well suited for LR parsing.\\nIn particular, LR parsers, like any other parsers employing a bottom-up\\nparsing strategy, do not blend well with empty productions.\\nThis is because without top-down filtering, any empty production is\\napplicable at any point in the input string, and a naive bottom-up\\nparser will loop indefinitely. The LR parsing tables constitute a type\\nof top-down filtering, but this may not be sufficient to guarantee\\ntermination, and in any case, a lot of spurious applications of empty\\nproductions will most likely take place, degrading performance.\\nFor these reasons we will not allow learned rules whose RHSs are empty,\\nbut simply refrain from cutting in nodes of the parse trees that do not\\ndominate at least one lexical lookup.\\n\\n\\nEven so, the scheme described this far is not totally successful, the\\nperformance is not as good as using hand-coded tree-cutting criteria.\\nThis is conjectured to be an effect of the reduction lengths being\\nfar too short.\\nThe first reason for this is that for any spurious rule reduction to take\\nplace, the corresponding RHS phrases must be on the stack. The likelihood\\nfor this to happen by chance decreases drastically with increased rule\\nlength. A second reason for this is that the number of states visited will\\ndecrease with increasing reduction length. This can most easily be\\nseen by noting that the number of states visited by a deterministic LR parser\\nequals the number of shift actions plus the number of reductions, and equals\\nthe number of nodes in the corresponding parse tree, and the longer the\\nreductions, the more shallow the parse tree.\\n\\n\\nThe hand-coded operationality criteria result in an average rule length of\\nfour, and a distribution of reduction lengths that is such that only 17\\npercent are of length one and 11 percent are of length two. This is in sharp\\ncontrast to what the above scheme accomplishes; the corresponding figures\\nare about 20 or 30 percent each for lengths one and two.\\n\\n\\nAn attempted solution to this problem is to impose restrictions on\\nneighbouring cutnodes. This can be done in several ways; one that has been\\ntested is to select for each rule the RHS phrase with the least entropy,\\nand prescribe that if a node corresponding to the LHS of the rule is chosen\\nas a cutnode, then no node corresponding to this RHS phrase may be chosen\\nas a cutnode, and vice versa. In case of such a conflict, the node (class)\\nwith the lowest entropy is removed from the set of cutnodes.\\n\\n\\nWe modify the function N[*] to handle this:\\n2. Repeat i := i+1; \\n\\n;\\nHere B(N[j]) is the set of nodes in N[j] that should be removed to\\navoid violating the constraints on neighbouring cutnodes. It is also\\nnecessary to modify the termination criterion as was done for the\\nfunction \\n\\nN(Smin) above.\\nNow we can no longer safely assume that the coverage increases with\\ndecreased entropy, and we must also modify the interval-bisection\\nscheme to handle this. It has proved reasonable to assume that the\\ncoverage is monotone on both sides of some maximum, which simplifies\\nthis task considerably.\\n\\n\\n  EXPERIMENTAL RESULTS \\n\\nA module realizing this scheme has been implemented and applied to the\\nvery setup used for the previous experiments with the hand-coded\\n tree-cutting criteria, see . 2100 of the verified parse trees constituted the training set, while 230\\nof them were used for the test set.\\nThe table below summarizes the results for some grammars of different\\ncoverage extracted using:\\n1.\\nHand-coded tree-cutting criteria.\\n2.\\nInduced tree-cutting criteria where the node entropy was taken to be the\\nphrase entropy of the RHS phrase of the dominating grammar rule.\\n3.\\nInduced tree-cutting criteria where the node entropy was the sum of the\\nphrase entropy of the RHS phrase of the dominating grammar rule and the\\nweighted sum of the phrase entropies of the LHSs of the alternative choices\\nof grammar rules to resolve on.\\nIn the latter two cases experiments were carried out both with and without\\nthe restrictions on neighbouring cutnodes discussed in the previous section.\\n\\n\\nWith the mixed entropy scheme it seems important to include the restrictions\\non neighbouring cutnodes, while this does not seem to be the case with the\\nRHS phrase entropy scheme.\\nA potential explanation for the significantly higher average parsing times\\nfor all grammars extracted using the induced tree-cutting criteria is that\\nthese are in general recursive, while the hand-coded criteria do not allow\\nrecursion, and thus only produce grammars that generate finite languages.\\n\\n\\nAlthough the hand-coded tree-cutting criteria are substantially better than\\nthe induced ones, we must remember that the former produce a grammar that in\\nmedian allows 60 times faster processing than the original grammar and parser\\ndo. This means that even if the induced criteria produce grammars that are a\\nfactor two or three slower than this, they are still approximately one and a\\nhalf order of magnitude faster than the original setup. Also, this is by no\\nmeans a closed research issue, but merely a first attempt to realize the\\nscheme, and there is no doubt in my mind that it can be improved on most\\nsubstantially.\\n\\n\\n  SUMMARY \\n\\nThis article proposes a method for automatically finding the appropriate\\ntree-cutting criteria in the EBG scheme, rather than having to hand-code\\nthem. The EBG scheme has previously proved most successful for tuning a\\nnatural-language grammar to a specific application domain and thereby\\nachieve very much faster parsing, at the cost of a small reduction in\\ncoverage.\\n\\n\\nInstruments have been developed and tested for controlling the coverage\\nand for avoiding a large number of short reductions, which is argued to\\nbe the main source to poor parser performance. Although these instruments\\nare currently slightly too blunt to enable producing grammars with the\\nsame high performance as the hand-coded tree-cutting criteria, they can\\nmost probably be sharpened by future research, and in particular refined\\nto achieve the delicate balance between high coverage and a distribution\\nof reduction lengths that is sufficiently biased towards long reductions.\\nAlso, banning recursion by category specialization, i.e. by for example\\ndistinguishing NPs that dominate other NPs from those that do\\nnot, will be investigated, since this is believed to be an important\\ningredient in the version of the scheme employing hand-coded tree-cutting\\ncriteria.\\n\\n\\n  ACKNOWLEDGEMENTS \\n\\nThis research was made possible by the basic research programme at the\\nSwedish Institute of Computer Science (SICS). I wish to thank Manny Rayner\\nof SRI International, Cambridge, for help and support in matters pertaining\\nto the treebank, and for enlightening discussions of the scheme as a whole.\\nI also wish to thank the NLP group at SICS for contributing to a very\\nconductive atmosphere to work in, and in particular Ivan Bretan for valuable\\ncomments on draft versions of this article. Finally, I wish to thank the\\nanonymous reviewers for their comments.\\n\\nBibliography \\n\\nHiyan Alshawi, editor.\\nThe Core Language Engine,\\nMIT Press 1992.\\n\\n\\nFred Jelinek.\\n``Self-Organizing Language Models for Speech Recognition'',\\nin Readings in Speech Recognition, pp. 450-506,\\nMorgan Kaufmann 1990.\\n\\n\\nTom M. Mitchell, Richard M. Keller and Smadar T. Kedar-Cabelli.\\n``Explanation-Based Generalization: A Unifying View'',\\nin Machine Learning 1, No. 1, pp. 47-80, 1986.\\n\\n\\nJ. Ross Quinlan.\\n``Induction of Decision Trees'',\\nin Machine Learning 1, No. 1, pp. 81-107, 1986.\\n\\n\\nM. Rayner, H. Alshawi, I. Bretan, D. Carter, V. Digalakis, B. Gambck,\\nJ. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman and C. Samuelsson.\\n``A Speech to Speech Translation System Built From Standard\\nComponents'',\\nin Procs. ARPA Workshop on Human Language Technology,\\nPrinceton, NJ 1993.\\n\\n\\nChrister Samuelsson.\\nFast Natural-Language Parsing Using Explanation-Based Learning,\\nPhD thesis, Royal Institute of Technology, Stockholm, Sweden 1994.\\n\\n\\nChrister Samuelsson.\\n``Notes on LR Parser Design'' to appear in\\nProcs. 15th International Conference on Computational Linguistics,\\nKyoto, Japan 1994.\\n\\n\\nChrister Samuelsson and Manny Rayner.\\n``Quantitative Evaluation of Explanation-Based Learning as an\\nOptimization Tool for a Large-Scale Natural Language System'',\\nin Procs. 12th International Joint Conference on Artificial\\nIntelligence, pp. 609-615, Sydney, Australia 1991.\\n\\nFootnotes\\n\\n  Other more easily\\nobtainable publications about this are in preparation.\\n  These are usually referred to as\\n``operationality criteria'' in the EBG literature.\\n  Since there is only one alternative, namely a lexical\\nlookup. In fact, the scheme could easily be extended to encompass including\\nlexical lookups of particular words into the specialized rules by\\ndistinguishing lexical lookups of different words; the entropy would then\\ndetermine whether or not to cut in a node corresponding to a lookup,\\njust as for any other node, as is described in the following.\\nThis can most easily be seen as follows: Imagine two identical, but\\ndifferent portions of the and-or tree. If the roots and leaves of\\nthese portions are all selected as cutnodes, but the distribution of\\ncutnodes within them differ, then we will introduce multiple ways of\\nderiving the portions of the parse trees that match any of these two\\nportions of the and-or tree.\\nDefined slightly differently, as described below.\\n  albeit in exponential time\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nExplanation-based generalization is used to extract a specialized grammar\\nfrom the original one using a training corpus of parse trees. This allows\\nvery much faster parsing and gives a lower error rate, at the price of a\\nsmall loss in coverage. Previously, it has been necessary to specify the\\ntree-cutting criteria (or operationality criteria) manually; here they are\\nderived automatically from the training set and the desired coverage of the\\nspecialized grammar.\\nThis is done by assigning an entropy value to each node in the parse trees\\nand cutting in the nodes with sufficiently high entropy values.\\n\\n'],\n",
              " [\"\\n\\n  Background \\n\\nPart-of-speech tagging is the process of assigning grammatical categories to\\nindividual words in a corpus. One widely used approach makes use of a\\nstatistical technique called a Hidden Markov Model (HMM). The model is defined\\nby two collections of parameters: the transition probabilities, which\\nexpress the probability that a tag follows the preceding one (or two for a\\nsecond order model); and the lexical probabilities, giving the\\nprobability that a word has a given tag without regard to words on either side\\nof it. To tag a text, the tags with non-zero probability are hypothesised for\\neach word, and the most probable sequence of tags given the sequence of words\\nis determined from the probabilities. Two algorithms are commonly used, known\\nas the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability\\nto every tag on every word, while Viterbi prunes tags which cannot be chosen\\nbecause their probability is lower than the ones of competing hypotheses, with\\na corresponding gain in computational efficiency. For an introduction to the\\nalgorithms, see Cutting et al. (1992), or the lucid description by\\nSharman (1990).\\nThere are two principal sources for the parameters of the model. If a tagged\\ncorpus prepared by a human annotator is available, the transition and lexical\\nprobabilities can be estimated from the frequencies of pairs of tags and of\\ntags associated with words. Alternatively, a procedure called Baum-Welch (BW)\\nre-estimation may be used, in which an untagged corpus is passed through the\\nFB algorithm with some initial model, and the resulting probabilities used to\\ndetermine new values for the lexical and transition probabilities. By\\niterating the algorithm with the same corpus, the parameters of the model can\\nbe made to converge on values which are locally optimal for the given\\ntext. The degree of convergence can be measured using a perplexity measure,\\nthe sum of plog2p for hypothesis probabilities p, which gives\\nan estimate of the degree of disorder in the model.  The algorithm is again\\ndescribed by Cutting et al. and by Sharman, and a mathematical\\njustification for it can be found in Huang et al. (1990).\\n\\n\\nThe first major use of HMMs for part of speech tagging was in CLAWS\\n     in the 1970s. With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number\\nof variations on and alternatives to the FB, Viterbi and BW algorithms have\\n been tried; see the work of, for example, Church , Brill  ,, DeRose  and Kupiec  . One of the most effective taggers based on a pure HMM is  that developed at Xerox  . An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged\\ntraining data. 96% accuracy correct assignment of tags to word token,\\ncompared with a human annotator, is quoted, over a 500000 word corpus.\\n\\n\\nThe Xerox tagger attempts to avoid the need for a hand-tagged training corpus\\nas far as possible. Instead, an approximate model is constructed by hand,\\nwhich is then improved by BW re-estimation on an untagged training corpus. In\\nthe above example, 8 iterations were sufficient. The initial model set up so\\nthat some transitions and some tags in the lexicon are favoured, and hence\\nhaving a higher initial probability. Convergence of the model is improved by\\nkeeping the number of parameters in the model down. To assist in this, low\\nfrequency items in the lexicon are grouped together into equivalence classes,\\nsuch that all words in a given equivalence class have the same tags and\\nlexical probabilities, and whenever one of the words is looked up, then the\\ndata common to all of them is used. Re-estimation on any of the words in a\\n class therefore counts towards re-estimation for all of them. \\n\\n\\nThe results of the Xerox experiment appear very encouraging. Preparing tagged\\ncorpora by hand is labour-intensive and potentially error-prone, and\\n although a semi-automatic approach can be used   , it is a good thing to reduce the human involvement as much as possible. However, some\\ncareful examination of the experiment is needed. In the first place, Cutting\\net al. do not compare the success rate in their work with that achieved\\nfrom a hand-tagged training text with no re-estimation. Secondly, it is\\nunclear how much the initial biasing contributes the success rate. If\\nsignificant human intervention is needed to provide the biasing, then the\\nadvantages of automatic training become rather weaker, especially if such\\nintervention is needed on each new text domain. The kind of biasing Cutting\\net al. describe reflects linguistic insights combined with an\\nunderstanding of the predictions a tagger could reasonably be expected to make\\nand the ones it could not.\\n\\n\\nThe aim of this paper is to examine the role that training plays in the\\ntagging process, by an experimental evaluation of how the accuracy of the\\ntagger varies with the initial conditions. The results suggest that a\\ncompletely unconstrained initial model does not produce good quality results,\\nand that one accurately trained from a hand-tagged corpus will generally do\\nbetter than using an approach based on re-estimation, even when the training\\ncomes from a different source. A second experiment shows that there are\\ndifferent patterns of re-estimation, and that these patterns vary more or less\\nregularly with a broad characterisation of the initial conditions. The outcome\\nof the two experiments together points to heuristics for making effective use\\nof training and re-estimation, together with some directions for further\\nresearch.\\n\\n\\nWork similar to that described here has been carried out by Merialdo (1994),\\nwith broadly similar conclusions. We will discuss this work below. The\\nprincipal contribution of this work is to separate the effect of the lexical\\nand transition parameters of the model, and to show how the results vary with\\ndifferent degree of similarity between the training and test data.\\n\\n\\n  The tagger and corpora \\n\\nThe experiments were conducted using two taggers, one written in C at\\nCambridge University Computer Laboratory, and the other in C++ at Sharp\\nLaboratories.  Both taggers implement the FB, Viterbi and BW algorithms. For\\ntraining from a hand-tagged corpus, the model is estimated by counting the\\nnumber of transitions from each tag i to each tag j, the total occurrence\\nof each tag i, and the total occurrence of word w with tag i. Writing\\nthese as f(i,j), f(i) and f(i,w) respectively, the transition\\nprobability from tag i to tag j is estimated as \\n\\nf(i,j)/f(i) and the\\nlexical probability as \\n\\nf(i,w)/f(i). Other estimation formulae have been used\\n in the past. For example, CLAWS  normalises the lexical probabilities by the total frequency of the word rather than of the\\ntag. Consulting the Baum-Welch re-estimation formulae suggests that the\\napproach described is more appropriate, and this is confirmed by slightly\\ngreater tagging accuracy. Any transitions not seen in the training corpus are\\ngiven a small, non-zero probability.\\n\\n\\nThe lexicon lists, for each word, all of tags seen in the training corpus with\\ntheir probabilities. For words not found in the lexicon, all open-class tags\\nare hypothesised, with equal probabilities. These words are added to the\\nlexicon at the end of first iteration when re-estimation is being used, so\\nthat the probabilities of their hypotheses subsequently diverge from being\\nuniform.\\n\\n\\nTo measure the accuracy of the tagger, we compare the chosen tag with one\\nprovided by a human annotator. Various methods of quoting accuracy have been\\nused in the literature, the most common being the proportion of words (tokens)\\nreceiving the correct tag. A better measure is the proportion of ambiguous words which are given the correct tag, where by ambiguous we mean\\nthat more than one tag was hypothesised. The former figure looks more\\nimpressive, but the latter gives a better measure of how well the tagger is\\ndoing, since it factors out the trivial assignment of tags to non-ambiguous\\nwords. For a corpus in which a fraction a of the words are ambiguous, and\\np is the accuracy on ambiguous words, the overall accuracy can be recovered\\nfrom 1-a+pa. All of the accuracy figures quoted below are for\\nambiguous words only.\\n\\n\\nThe training and test corpora were drawn from the LOB corpus and the Penn\\ntreebank. The hand tagging of these corpora is quite different. For example,\\nthe LOB tagset used 134 tags, while the Penn treebank tagset has 48. The\\ngeneral pattern of the results presented does not vary greatly with the corpus\\nand tagset used.\\n\\n\\n  The effect of the initial conditions \\n\\nThe first experiment concerned the effect of the initial conditions on the\\naccuracy using Baum-Welch re-estimation. A model was trained from a\\nhand-tagged corpus in the manner described above, and then degraded in various\\nways to simulate the effect of poorer training, as follows:\\nLexicon\\nD0\\nUn-degraded lexical probabilities, calculated from \\n\\nf(i,w)/f(i).\\nD1\\nLexical probabilities are correctly ordered, so that the most\\nfrequent tag has the highest lexical probability and so on, but the absolute\\nvalues are otherwise unreliable.\\nD2\\nLexical probabilities are proportional to the overall tag\\nfrequencies, and are hence independent of the actual occurrence of the word in\\nthe training corpus.\\nD3\\nAll lexical probabilities have the same value, so that the \\nlexicon contains no information other than the possible tags for each word.\\nTransitions\\nT0\\nUn-degraded transition probabilities, calculated from \\n\\nf(i,j)/f(i).\\nT1\\nAll transition probabilities have the same value.\\nWe could expect to achieve D1 from, say, a printed dictionary listing parts of\\nspeech in order of frequency. Perfect training is represented by case D0+T0.\\n The Xerox experiments   correspond to something between D1 and D2, and between T0 and T1, in that there is some initial biasing of the\\nprobabilities.\\n\\n\\nFor the test, four corpora were constructed from the LOB corpus: LOB-B from\\npart B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-B-J\\nfrom parts B to J inclusive. Corpus LOB-B-J was used to train the model, and\\nLOB-B, LOB-L and LOB-B-G were passed through thirty iterations of the BW\\nalgorithm as untagged data. In each case, the best accuracy (on ambiguous\\nwords, as usual) from the FB algorithm was noted. As an additional test, we\\ntried assigning the most probable tag from the D0 lexicon, completely ignoring\\n tag-tag transitions. The results are summarised in table , for various corpora, where F denotes the ``most frequent tag'' test.\\ncontains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J,\\nand the overall accuracy in the D0+T0 case is hence 98.69%. The general\\npattern of the results is similar across the three test corpora, with the only\\ndifference of interest being that case D3+T0 does better for LOB-L than for\\nthe other two cases, and in particular does better than cases D0+T1 and D1+T1.\\nA possible explanation is that in this case the test data does not overlap\\nwith the training data, and hence the good quality lexicons (D0 and D1) have\\nless of an influence. It is also interesting that D3+T1 does better than\\nD2+T1. The reasons for this are unclear, and the results are not always the\\nsame with other corpora, which suggests that they are not statistically\\nsignificant.\\n\\n\\nSeveral follow-up experiments were used to confirm the results: using corpora\\nfrom the Penn treebank, using equivalence classes to ensure that all lexical\\nentries have a total relative frequency of at least 0.01, and using larger\\ncorpora. The specific accuracies were different in the various tests, but the\\noverall patterns remained much the same, suggesting that they are not an\\nartifact of the tagset or of details of the text.\\n\\n\\nThe observations we can make about these results are as follows. Firstly, two\\nof the tests, D2+T1 and D3+T1, give very poor performance. Their accuracy is\\nnot even as good as that achieved by picking the most frequent tag (although\\nthis of course implies a lexicon of D0 or D1 quality). It follows that if\\nBaum-Welch re-estimation is to be an effective technique, the initial data\\nmust have either biasing in the transitions (the T0 cases) or in the lexical\\nprobabilities (cases D0+T1 and D1+T1), but it is not necessary to have both\\n(D2/D3+T0 and D0/D1+T1).\\n\\n\\nSecondly, training from a hand-tagged corpus (case D0+T0) always does best,\\neven when the test data is from a different source to the training data, as it\\nis for LOB-L. So perhaps it is worth investing effort in hand-tagging training\\ncorpora after all, rather than just building a lexicon and letting\\nre-estimation sort out the probabilities. But how can we ensure that\\nre-estimation will produce a good quality model? We look further at this issue\\nin the next section.\\n\\n\\n  Patterns of re-estimation \\n\\nDuring the first experiment, it became apparent that Baum-Welch re-estimation\\nsometimes decreases the accuracy as the iteration progresses. A second\\nexperiment was conducted to decide when it is appropriate to use Baum-Welch\\nre-estimation at all. There seem to be three patterns of behaviour:\\nClassical\\nA general trend of rising accuracy on each iteration, with\\nany falls in accuracy being local. It indicates that the model is converging\\ntowards an optimum which is better than its starting point.\\nInitial maximum\\nHighest accuracy on the first iteration, and falling\\nthereafter. In this case the initial model is of better quality than BW can\\nachieve. That is, while BW will converge on an optimum, the notion of\\noptimality is with respect to the HMM rather than to the linguistic judgements\\nabout correct tagging.\\nEarly maximum\\nRising accuracy for a small number of iterations (2-4),\\nand then falling as in initial maximum.\\n An example of each of the three behaviours is shown in figure . The values of the accuracies and the test conditions are unimportant here; all\\nwe want to show is the general patterns.\\nThe second experiment had the aim of trying to discover which pattern applies\\nunder which circumstances, in order to help decide how to train the model.\\nClearly, if the expected pattern is initial maximum, we should not use BW at\\nall, if early maximum, we should halt the process after a few iterations, and\\nif classical, we should halt the process in a ``standard'' way, such as\\ncomparing the perplexity of successive models.\\n\\n\\nThe tests were conducted in a similar manner to those of the first experiment,\\nby building a lexicon and transitions from a hand tagged training corpus, and\\nthen applying them to a test corpus with varying degrees of degradation.\\nFirstly, four different degrees of degradation were used: no degradation at\\nall, D2 degradation of the lexicon, T1 degradation of the transitions, and the\\ntwo together. Secondly, we selected test corpora with varying degrees of\\nsimilarity to the training corpus: the same text, text from a similar domain,\\nand text which is significantly different. Two tests were conducted with each\\ncombination of the degradation and similarity, using different corpora (from\\nthe Penn treebank) ranging in size from approximately 50000 words to 500000\\nwords. The re-estimation was allowed to run for ten iterations.\\n\\n\\n The results appear in table , showing the best accuracy achieved (on ambiguous words).  the iteration at which it occurred, and the pattern\\nof re-estimation (I = initial maximum, E = early maximum,\\n C = classical). The patterns are summarised in table , each entry in the table showing the patterns for the two tests under the given\\nconditions.\\n[*] These tests gave an early peak, but the graphs of accuracy against\\nnumber of iterations show the pattern to be classical rather than early\\nmaximum.\\n``similar/D0+T0'' case, we can draw some general conclusions about the\\npatterns obtained from different sorts of data. When the lexicon is degraded\\n(D2), the pattern is always classical. With a good lexicon but either degraded\\ntransitions or a test corpus differing from the training corpus, the pattern\\ntends to be early maximum. When the test corpus is very similar to the model,\\nthen the pattern is initial maximum. Furthermore, examining the accuracies in\\n table , in the cases of initial maximum and early maximum, the accuracy tends to be significantly higher than with classical behaviour. It\\nseems likely that what is going on is that the model is converging to towards\\nsomething of similar ``quality'' in each case, but when the pattern is\\nclassical, the convergence starts from a lower quality model and improves, and\\nin the other cases, it starts from a higher quality one and deteriorates.  In\\nthe case of early maximum, the few iterations where the accuracy is improving\\ncorrespond to the creation of entries for unknown words and the fine tuning of\\nones for known ones, and these changes outweigh those produced by the\\nre-estimation.\\n\\n\\n  Discussion \\n\\nFrom the observations in the previous section, we propose the following\\nguidelines for how to train a HMM for use in tagging:\\n\\nIf a hand-tagged training corpus is available, use it\\n.\\nIf the test and training\\ncorpora are near-identical, do not use BW re-estimation;\\notherwise use for a small number of iterations.\\n\\nIf no such training corpus is available, but a lexicon with at least\\nrelative frequency data is available, use BW re-estimation for a small number\\nof iterations.\\n\\nIf neither training corpus nor lexicon are available, use BW\\nre-estimation with standard convergence tests such as perplexity.\\nWithout a lexicon, some initial biasing of the transitions is needed if good\\nresults are to be obtained.\\n\\n\\n\\n\\nSimilar results are presented by Merialdo (1994), who describes experiments to\\ncompare the effect of training from a hand-tagged corpora and using the\\nBaum-Welch algorithm with various initial conditions.  As in the experiments\\nabove, BW re-estimation gave a decrease in accuracy when the starting point\\nwas derived from a significant amount of hand-tagged text. In addition,\\nalthough Merialdo does not highlight the point, BW re-estimation starting from\\nless than 5000 words of hand-tagged text shows early maximum behaviour.\\nMerialdo's conclusion is that taggers should be trained using as much\\nhand-tagged text as possible to begin with, and only then applying BW\\nre-estimation with untagged text. The step forward taken in the work here is\\nto show that there are three patterns of re-estimation behaviour, with\\ndiffering guidelines for how to use BW effectively, and that to obtain a good\\nstarting point when a hand-tagged corpus is not available or is too small,\\neither the lexicon or the transitions must be biased.\\n\\n\\nWhile these may be useful heuristics from a practical point of view, the next\\nstep forward is to look for an automatic way of predicting the accuracy of the\\ntagging process given a corpus and a model.  Some preliminary experiments with\\nusing measures such as perplexity and the average probability of hypotheses\\nshow that, while they do give an indication of convergence during\\nre-estimation, neither shows a strong correlation with the accuracy. Perhaps\\nwhat is needed is a ``similarity measure'' between two models M and M',\\nsuch that if a corpus were tagged with model M, M' is the model obtained\\nby training from the output corpus from the tagger as if it were a hand-tagged\\ncorpus. However, preliminary experiments using such measures as the\\nKullback-Liebler distance between the initial and new models have again showed\\nthat it does not give good predictions of accuracy. In the end it may turn out\\nthere is simply no way of making the prediction without a source of\\ninformation extrinsic to both model and corpus.\\n\\n\\n  Acknowledgements \\n\\nThe work described here was carried out at the Cambridge University Computer\\nLaboratory as part of Esprit BR Project 7315 ``The Acquisition of Lexical\\nKnowledge'' (Acquilex-II). The results were confirmed and extended at Sharp\\nLaboratories of Europe. I thank Ted Briscoe for his guidance and advice, \\nand the ANLP referees for their comments.\\n\\nBibliography \\n\\nEric Brill and Mitch Marcus (1992).\\nTagging an Unfamiliar Text With Minimal Human Supervision.\\nIn AAAI Fall Symposium on Probabilistic Approaches to Natural\\n  Language, pages 10-16.\\n\\n\\nEric Brill (1992).\\nA Simple Rule-Based Part of Speech Tagger.\\nIn Third Conference on Applied Natural Language Processing.\\n  Proceedings of the Conference. Trento, Italy, pages 152-155, Association\\n  for Computational Linguistics.\\n\\n\\nKenneth Ward Church (1988).\\nA Stochastic Parts Program and Noun Phrase Parser for Unrestricted\\n  Text.\\nIn Second Conference on Applied Natural Language Processing.\\n  Proceedings of the Conference, pages 136-143, Association for Computational\\n  Linguistics.\\n\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun (1992).\\nA Practical Part-of-Speech Tagger.\\nIn Third Conference on Applied Natural Language Processing.\\n  Proceedings of the Conference. Trento, Italy, pages 133-140, Association\\n  for Computational Linguistics.\\n\\n\\nSteven J. DeRose (1988).\\nGrammatical Category Disambiguation by Statistical Optimization.\\nComputational Linguistics, 14(1):31-39.\\n\\n\\nRoger Garside, Geoffrey Leech, and Geoffrey Sampson (1987).\\nThe Computational Analysis of English: A Corpus-based Approach.\\nLongman, London.\\n\\n\\nX. D. Huang, Y. Ariki, and M. A. Jack (1990).\\nHidden Markov Models for Speech Recognition.\\nEdinburgh University Press.\\n\\n\\nJ. M. Kupiec (1989).\\nProbabilistic Models of Short and Long Distance Word Dependencies in\\n  Running Text.\\nIn Proceedings of the 1989 DARPA Speech and Natural Language\\n  Workshop, pages 290-295.\\n\\n\\nJulian Kupiec (1992).\\nRobust Part-of-speech Tagging Using a Hidden Markov Model.\\nComputer Speech and Language, 6.\\n\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz (1993).\\nBuilding a Large Annotated Corpus of English: The Penn Treebank.\\nComputational Linguistics, 19(2):313-330.\\n\\n\\nBernard Merialdo (1994).\\nTagging English Text with a Probabilistic Model.\\nComputational Linguistics, 20(2):155-171.\\n\\n\\nR. A. Sharman (1990).\\nHidden Markov Model Methods for Word Tagging.\\nTechnical Report UKSC 214, IBM UK Scientific Centre.\\n\\nFootnotes\\n\\n  The\\n technique was originally developed by Kupiec . \\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nIn part of speech tagging by Hidden Markov Model, a statistical model is used\\nto assign grammatical categories to words in a text. Early work in the field\\nrelied on a corpus which had been tagged by a human annotator to train the\\nmodel. More recently, Cutting et al. (1992) suggest that training can be\\nachieved with a minimal lexicon and a limited amount of a priori\\ninformation about probabilities, by using Baum-Welch re-estimation to\\nautomatically refine the model. In this paper, I report two experiments\\ndesigned to determine how much manual training information is needed. The\\nfirst experiment suggests that initial biasing of either lexical or transition\\nprobabilities is essential to achieve a good accuracy. The second experiment\\nreveals that there are three distinct patterns of Baum-Welch re-estimation. In\\ntwo of the patterns, the re-estimation ultimately reduces the accuracy of the\\ntagging rather than improving it. The pattern which is applicable can be\\npredicted from the quality of the initial model and the similarity between the\\ntagged training corpus (if any) and the corpus to be tagged. Heuristics for\\ndeciding how to use re-estimation in an effective manner are given. The\\nconclusions are broadly in agreement with those of Merialdo (1994), but give\\ngreater detail about the contributions of different parts of the model.\\nCitation details: appears in Proceedings of the 4th ACL Conference on\\nApplied Natural Language Processing, Stuttgart, October 13-15th 1994, pp.\\n53-58. Some typos corrected from the published version.\\n\\n'],\n",
              " [\"\\n\\n  INTRODUCTION \\n\\nMethods for automatically classifying words according to their\\ncontexts of use have both scientific and practical interest. The\\nscientific questions arise in connection to distributional views of\\nlinguistic (particularly lexical) structure and also in relation to\\nthe question of lexical acquisition both from psychological and\\ncomputational learning perspectives. From the practical point of view,\\nword classification addresses questions of data sparseness and\\ngeneralization in statistical language models, particularly\\nmodels for deciding among alternative analyses proposed by a grammar.\\n\\n\\nIt is well known that a simple tabulation of frequencies of certain\\nwords participating in certain configurations, for example of\\nfrequencies of pairs of a transitive main verb and the head noun of\\nits direct object, cannot be reliably used for comparing the\\nlikelihoods of different alternative configurations. The problem is\\nthat for large enough corpora the number of possible joint events is\\nmuch larger than the number of event occurrences in the corpus, so\\nmany events are seen rarely or never, making their frequency counts\\nunreliable estimates of their probabilities.\\n\\n\\nHindle  proposed dealing with the sparseness\\nproblem by estimating the likelihood of unseen events from that of\\n``similar'' events that have been seen. For instance, one may estimate\\nthe likelihood of a particular direct object for a verb from the\\nlikelihoods of that direct object for similar verbs. This\\nrequires a reasonable definition of verb similarity and a similarity\\nestimation method.  In Hindle's proposal, words are similar if we have\\nstrong statistical evidence that they tend to participate in the same\\nevents. His notion of similarity seems to agree with our intuitions\\nin many cases, but it is not clear how it can\\nbe used directly to construct word classes and corresponding models of\\nassociation.\\n\\n\\nOur research addresses some of the same questions and uses similar\\nraw data, but we investigate how to factor word association\\ntendencies into associations of words to certain hidden senses classes and associations between the classes themselves. While\\nit may be worthwhile to base such a model on preexisting sense classes\\n , in the work described here we look at how to derive the classes directly from distributional data. More\\nspecifically, we model senses as probabilistic concepts or clusters c with\\ncorresponding cluster membership probabilities p(c|w) for each\\nword w. Most other class-based modeling techniques for natural\\nlanguage rely instead on ``hard'' Boolean classes\\n . Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving\\nparticular words, a potentially unreliable source of information as we\\nnoted above. Our approach avoids both problems.\\n\\n    Problem Setting\\n\\n\\nIn what follows, we will consider two major word classes, and ,\\nfor the verbs and nouns in our experiments, and a\\nsingle relation between them, in our experiments relation between a\\ntransitive main verb and the head noun of its direct object. Our raw\\nknowledge about the relation consists of the frequencies fvn of\\noccurrence of particular pairs (v,n) in the required configuration\\nin a training corpus. Some form of text analysis is required to\\ncollect such a collection of pairs. The corpus used in our first\\nexperiment was derived from newswire text automatically parsed by\\n Hindle's parser Fidditch . More recently, we have constructed similar tables with the help of a statistical\\npart-of-speech tagger\\n  and of tools for regular expression pattern  matching on tagged corpora . We have not yet compared the accuracy and coverage of the two methods, or what\\nsystematic biases they might introduce, although we took care to filter\\nout certain systematic errors, for instance the misparsing of the\\nsubject of a complement clause as the direct object of a main verb for\\nreport verbs like ``say''.\\n\\n\\nWe will consider here only the problem of classifying nouns according\\nto their distribution as direct objects of verbs; the converse problem\\nis formally similar. More generally, the theoretical basis for our\\nmethod supports the use of clustering to build models for any n-ary\\nrelation in terms of associations between elements in each coordinate\\nand appropriate hidden units (cluster centroids) and associations\\nbetween those hidden units.\\n\\n\\nFor the noun classification problem, the empirical distribution of a\\nnoun n is then given by the conditional density\\n\\n.\\nThe problem we study is how to use the\\npn to classify the \\n\\n.\\nOur classification method will\\nconstruct a set \\nof clusters and cluster membership\\nprobabilities p(c|n). Each cluster c is associated to a cluster\\ncentroid pc, which is discrete density over obtained by averaging appropriately the pn.\\n\\n\\n  Distributional Similarity \\n\\nTo cluster nouns n according to their conditional verb distributions\\npn, we need a measure of similarity between distributions. We use\\nfor this purpose the relative entropy or Kullback-Leibler (KL)\\ndistance between two distributions\\n\\n\\n\\n\\n\\nThis is a natural choice for a variety of reasons, which we will just\\n sketch here. \\n\\n\\nFirst of all, \\n\\n\\nis zero just in case p=q, and it\\nincreases as the probability decreases that p is the relative\\nfrequency distribution of a random sample drawn according to p. More\\nformally, the probability mass given by q to the set of all samples\\nof length n with relative frequency distribution p is bounded by\\n\\n\\n. Therefore, if we are trying to distinguish among hypotheses qi when p is the relative\\nfrequency distribution of observations, \\n\\n\\ngives the\\nrelative weight of evidence in favor of qi. Furthermore, a similar\\nrelation holds between \\n\\n\\nfor two empirical\\ndistributions p and p' and the probability that p and p' are\\ndrawn from the same distribution q. We can thus use the relative\\nentropy between the context distributions for two words to measure how\\nlikely they are to be instances of the same cluster centroid.\\n\\n\\nFrom an information theoretic perspective \\n\\nmeasures how inefficient on average it would be to use a code based on\\nq to encode a variable distributed according to p. With respect to\\nour problem, \\n\\n\\nthus gives us the loss of\\ninformation in using cluster centroid pc instead of the actual\\ndistribution for word pn when modeling the distributional\\nproperties of n.\\n\\n\\nFinally, relative entropy is a natural measure of similarity between\\ndistributions for clustering because its minimization leads to cluster\\ncentroids that are a simple weighted average of member distributions.\\n\\n\\nOne technical difficulty is that \\n\\n\\nis not defined\\nwhen p'(x) = 0 but p(x) ] 0. We could sidestep this problem (as we\\ndid initially) by smoothing zero frequencies appropriately\\n .  However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of\\ndata sparseness by grouping words into classes. It turns out that the\\nproblem is avoided by our clustering technique, since it does not need\\nto compute the KL distance between individual word distributions, but\\nonly between a word distribution and average distributions, the\\ncurrent cluster centroids, which are guaranteed to be nonzero whenever\\nthe word distributions are.  This is a useful advantage of our method\\ncompared with agglomerative clustering techniques that need to compare\\nindividual objects being considered for grouping.\\n\\n\\n\\n    THEORETICAL BASIS\\n\\n\\nIn general, we are interested on how to organize a set of linguistic\\nobjects such as words according to the contexts in which they occur,\\nfor instance grammatical constructions or n-grams.  We will show\\nelsewhere that the theoretical analysis outlined here applies to that\\nmore general problem, but for now we will only address the more\\nspecific problem in which the objects are nouns and the contexts are\\nverbs that take the nouns as direct objects.\\n\\n\\nOur problem can be seen as that of learning a joint distribution of\\npairs from a large sample of pairs. The pair coordinates come from two\\nlarge sets \\nand ,\\nwith no preexisting topological\\nor metric structure, and the training data is a sequence S of Nindependently drawn pairs\\n\\n\\n\\n\\n\\nFrom a learning perspective, this problem falls somewhere in between\\nunsupervised and supervised learning. As in unsupervised learning, the\\ngoal is to learn the underlying distribution of the data. But in contrast\\nto most unsupervised learning settings, the objects involved\\nhave no internal structure or attributes allowing them to be compared\\nwith each other. Instead, the only information about the objects is\\nthe statistics of their joint appearance. These statistics can thus be\\nseem as a weak form of object labelling analogous to supervision.\\n\\n  Distributional Clustering \\n\\nWhile clusters based on distributional similarity are interesting on\\ntheir own, they can also be profitably seen as a means of summarizing\\na joint distribution. In particular, we would like to find a set of\\nclusters \\nsuch that each conditional\\ndistribution pn(v) can be approximately decomposed as\\n\\n\\n\\n\\n\\nwhere p(c | n ) is the membership probability of n in c and \\n\\npc(v) = p(v|c) is v's conditional probability given by the\\ncentroid distribution for cluster c.\\n\\n\\nThe above decomposition can be written in a more symmetric form as\\n\\\\hat{p}(n,v)  = \\\\sum_{c \\\\in \\\\cal C} p(c,n) p(v|c) \\\\nonumber \\\\\\\\\\n =  \\\\sum_{c \\\\in \\\\cal C} p(c) p(n|c) p(v|c)\\n\\\\end{eqnarray} -->\\nassuming that p(n) and \\n\\n\\ncoincide.\\n We will take () as our basic clustering model. \\n\\n\\nTo determine this decomposition we need to solve the two connected\\nproblems of finding find suitable forms for the cluster membership and\\ncentroid distributions p(v|c), and of maximizing the goodness of fit\\nbetween the model distribution \\n\\n\\nand the observed data\\n\\n\\nGoodness of fit is determined by the model's likelihood of the\\nobservations. The maximum likelihood (ML) estimation principle is thus\\nthe natural tool to determine the centroid distributions pc(v).\\n\\n\\nAs for the membership probabilities, they must be determined solely by\\nthe relevant measure of object-to-cluster similarity, which in the\\npresent work is the relative entropy between object and cluster\\ncentroid distributions.  Since no other information is available, the\\nmembership is determined by maximizing the configuration entropy\\nsubject for a fixed average distortion.  With the\\nmaximum entropy (ME) membership distribution, ML estimation is\\nequivalent to the minimization of the average distortion of the data.\\nThe combined entropy maximization entropy and distortion minimization\\nis carried out by a two-stage iterative process similar to the\\nEM method\\n . The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids\\ngiven fixed membership probabilities.  In the second iteration stage,\\nthe entropy of the membership distribution is maximized with a fixed\\naverage distortion.  This joint optimization searches for a saddle point in the distortion-entropy parameters, which is\\nequivalent to minimizing a linear combination of the two\\nknown as free energy in statistical mechanics. This analogy with\\nstatistical mechanics is not coincidental, and provide us with a\\nbetter understanding of the clustering procedure.\\n\\n  Maximum Likelihood Cluster Centroids \\n\\nFor the maximum likelihood argument, we start by estimating the\\nlikelihood of the sequence S of N independent observations of\\npairs \\n\\n(ni , vi ).  Using\\n (), the sequence's model log likelihood is \\n\\n\\n\\n\\n\\nFixing the number of clusters (model size) ,\\nwe want to\\nmaximize l(S) with respect to the distributions\\np(n | c ) and p(v|c).  The variation of l(S) with\\nrespect to these distributions is\\n\\n\\n\\n\\n\\nwith p(n|c) and p(v|c) kept normalized.  Using Bayes's\\n formula, we have  \\n\\n\\n\\n\\n\\nor\\n\\n\\n\\n\\n\\n for any c, which we substitute into () to obtain \\n\\n\\n\\n\\n\\nsince \\n\\n.\\nThis expression is particularly useful when the cluster distributions\\np(n|c) and p(v|c) are of exponential form,\\nprecisely what will be provided by the ME step described below.\\n\\n\\nAt this point we need to specify the clustering model in more detail.\\nIn the derivation so far we have treated p(n|c) and p(v|c)symmetrically, corresponding to clusters not of verbs or nouns but of\\nverb-noun associations. In principle such a symmetric model may be\\nmore accurate, but in this paper we will concentrate on asymmetric models in which cluster memberships are associated to just\\none of the components of the joint distribution and the cluster\\ncentroids are specified only by the other component.  In particular,\\nthe model we use in our experiments has noun clusters with cluster\\nmemberships determined by p(n|c) and centroid distributions\\ndetermined by p(v|c).\\n\\n\\nThe asymmetric model simplifies the estimation significantly by\\ndealing with a single component, but it has the disadvantage that the\\njoint distribution, p(n,v) has two different and not necessarily\\nconsistent expressions in terms of asymmetric models for the two\\ncoordinates.\\n\\n\\n  Maximum Entropy  Cluster Membership \\n\\nWhile variations of p(n|c) and p(v|c) in equation\\n ( are not independent, we can treat them separately. First, for fixed average distortion between the cluster centroid\\ndistributions p(v|c) and the data p(v|n), we find the\\ncluster membership probabilities, which are the Bayes's inverses of\\nthe  p(n|c), that maximize the entropy of the cluster distributions.\\nWith the membership distributions thus obtained, we then look\\nfor the p(v|c) that maximize the log likelihood l(S). It turns out\\nthat this will also be the values of p(v|c) that minimize the\\naverage distortion between the asymmetric cluster model and the data.\\n\\n\\nGiven any similarity measure d(n,c) between nouns and cluster\\ncentroids, the\\naverage cluster distortion is\\n\\n\\n\\n\\n\\nIf we maximize the cluster membership entropy\\n\\n\\n\\n\\n\\n subject to normalization of p(n|c) and fixed (), we obtain the following standard exponential forms for the class and membership\\ndistributions\\n\\n\\n\\n\\n\\n\\n\\n\\nwhere the normalization sums (partition functions) are \\n\\n\\nand \\n\\n.\\nNotice\\nthat d(n,c) does not need to be symmetric for this derivation, as\\nthe two distributions are simply related by Bayes's rule.\\n\\n\\n Returning to the log-likelihood variation (),  we can now use () for p(n|c) and the assumption for the asymmetric model that the cluster membership\\nstays fixed as we adjust the centroids, to obtain\\n\\n\\n\\n\\n\\nwhere the variation of p(v|c) is now included in the variation of\\nd(n,c).\\n\\n\\nFor a large enough sample, we may replace the sum over observations in\\n () by the average over \\n\\n\\n\\n\\n\\nwhich, applying Bayes's rule, becomes\\n\\n\\n\\n\\n\\nAt the log-likelihood maximum, the\\n variation () must vanish. We will see below that the use of relative entropy for similarity measure makes\\n\\n\\nvanish at the maximum as well, so the\\nlog likelihood can be maximized by minimizing the average distortion\\nwith respect to the class centroids while class\\nmembership is kept fixed\\n\\n\\n\\n\\n\\nor, sufficiently, if each of the inner sums vanish\\n\\n\\n\\n\\n\\n  Minimizing the Average KL Distortion \\n\\nWe first show that the minimization of the relative entropy yields the\\nnatural expression for cluster centroids\\n\\n\\n\\n\\n\\nTo minimize the\\n average distortion (), we observe that the variation of the KL distance between noun and centroid distributions with respect\\nto the centroid distribution p(v|c), with\\neach centroid distribution normalized by the Lagrange multiplier\\n,\\nis given by\\n\\n\\n\\n\\n\\nSubstituting this expression into (), we obtain \\n\\n\\n\\n\\n\\nSince the \\n\\nare now independent, we obtain immediately the desired centroid\\n expression (), which is the desired weighted average of noun distributions.\\n\\n\\nWe can now see that the variation \\n\\n\\nvanishes for\\n centroid distributions given by  (), since it follows  from () that \\n\\n\\n\\n\\n\\n  The Free Energy Function \\n\\nThe combined minimum distortion and maximum entropy optimization is\\nequivalent to the minimization of a single function,\\nthe free energy\\n\\n\\n\\n\\n\\nwhere \\n\\n\\nis the average distortion\\n () and H is the cluster membership entropy  (). \\n\\n\\nThe free energy determines both the distortion and the membership entropy\\nthrough\\n\\n\\n\\n\\n\\nwith temperature \\n\\n.\\n\\n\\nThe most important property of the free energy is that its minimum\\ndetermines the balance between the ``disordering'' maximum entropy and\\n``ordering'' distortion minimization in which the system is most\\nlikely to be found. In fact\\nthe probability to find the system at a given configuration is\\nexponential in F\\n\\n\\n\\n\\n\\nso a system is most likely to be found in its minimal free energy\\nconfiguration.\\n\\n\\n\\n  Hierarchical Clustering \\n\\nThe analogy with statistical mechanics suggests a deterministic\\n annealing procedure for clustering , in which the number of clusters is determined through a sequence of phase\\ntransitions by continuously increasing the parameter following an annealing schedule.\\n\\n\\nThe higher ,\\nthe more local is the\\ninfluence of each noun on the definition of centroids.  The\\ndissimilarity plays here the role of distortion.  When the scale\\nparameter \\nis close to zero, the dissimilarities are almost\\nirrelevant, all words contribute about equally to each centroid, and\\nso the lowest average distortion solution involves just one cluster\\nwhich is the average of all word densities. As \\nis slowly\\nincreased, a point (phase transition) is eventually reached which the\\nnatural solution involves two distinct centroids. We say then that the\\noriginal cluster has split into the two new clusters.\\n\\n\\nIn general, if we take any cluster c and a twin c' of csuch that the centroid pc' is a small random pertubation of\\npc, below the critical \\nat which c splits the membership\\nand centroid reestimation procedure given by equations\\n () and () will make pc and pc'converge, that is, c and c' are really the same cluster. But with \\nabove the critical value for c, the two centroids will\\ndiverge, giving rise to two daughters of c.\\n\\n\\nOur clustering procedure is thus as follows.  We start with very low\\n\\nand a single cluster whose centroid is the average of all noun\\ndistributions. For any given ,\\nwe have a current set of leaf clusters corresponding to the current free energy (local)\\nminimum. To refine such a solution, we search for the lowest which is the critical value for some current leaf cluster splits.\\nIdeally, there is just one split at that critical value, but\\nfor practical performance and numerical accuracy reasons we may have\\nseveral splits at the new critical point. The splitting procedure can\\nthen be repeated to achieve the desired number of clusters or model\\ncross-entropy.\\n\\n\\n\\n    CLUSTERING EXAMPLES\\n\\n\\nAll our experiments involve the asymmetric model described in the\\nprevious section. As explained there, our clustering procedure yields\\nfor each value of \\na set \\nof clusters minimizing the\\nfree energy F, and the asymmetric model for \\nestimates the\\nconditional verb distribution for a noun n by\\n\\n\\n\\n\\n\\nwhere p(c|n) also depends on .\\n\\n\\nAs a first experiment, we used our method to classify the 64 nouns\\nappearing most frequently as heads of direct objects of the verb\\n``fire'' in one year (1988) of Associated Press newswire. In this\\ncorpus, the chosen nouns appear as direct object heads of a total of\\n2147 distinct verbs, so each noun is represented by a density over\\nthe 2147 verbs.\\n\\n\\n Figure  shows the five words most similar to the each cluster centroid for the four clusters resulting from the first two\\ncluster splits.  It can be seen that first split separates the objects\\ncorresponding to the weaponry sense of ``fire'' (cluster 1) from the\\nones corresponding to the personnel action (cluster 2). The second\\nsplit then further refines the weaponry sense into a projectile sense\\n(cluster 3) and a gun sense (cluster 4). That split is\\nsomewhat less sharp, possibly because not enough distinguishing\\ncontexts occur in the corpus.\\n\\n\\n Figure  shows the four closest nouns to the centroid of each of a set of hierarchical clusters derived from verb-object pairs\\ninvolving the 1000 most frequent nouns in the June 1991 electronic\\nversion of Grolier's Encyclopedia (10 million words).\\n\\n\\n  MODEL EVALUATION \\n\\nThe preceding qualitative discussion provides some indication of what\\naspects of distributional relationships may be discovered by\\nclustering. However, we also need to evaluate clustering more\\nrigorously as a basis for models of distributional relationships. So,\\nfar, we have looked at two kinds of measurements of model quality: (i)\\nrelative entropy between held-out data and the asymmetric model, and\\n(ii) performance on the task of deciding which of two verbs is more\\nlikely to take a given noun as direct object when the data relating\\none of the verbs to the noun has been witheld from the training data.\\n\\n\\nThe evaluation described below was performed on the largest data set\\nwe have worked with so far, extracted from 44 million words of 1988\\nAssociated Press newswire with the pattern matching techniques\\nmentioned earlier. This collection process\\nyielded 1112041 verb-object pairs. We selected then the subset\\ninvolving the 1000 most frequent nouns in the corpus for clustering,\\nand randomly divided it into a training set of 756721 pairs and a test\\nset of 81240 pairs.\\n\\n  Relative Entropy \\n\\n Figure  plots the average relative entropy of several data sets to asymmetric clustered models of different sizes, given by\\n\\n\\n\\n\\n\\nwhere tn is the relative frequency distribution of verbs taking nas direct object in the test set. For each critical value of ,\\nwe\\nshow the relative entropy with respect to the asymmetric model\\nbased on \\nof the training set (set train), of\\nrandomly selected held-out test set (set test), and of held-out\\ndata for a further 1000 nouns that were not clustered (set new).\\nUnsurprisingly, the training set\\nrelative entropy decreases monotonically. The test set relative\\nentropy decreases to a minimum at 206 clusters, and then starts\\nincreasing, suggesting that larger models are overtrained.\\n\\n\\nThe new noun test set is intended to test whether clusters based on\\nthe 1000 most frequent nouns are useful classifiers for the\\nselectional properties of nouns in general. As the figure shows, the\\ncluster model provides over one bit of information about the\\nselectional properties of the new nouns, but the overtraining effect\\nis even sharper than for the held-out data involving the 1000\\nclustered nouns.\\n\\n\\n  Decision Task \\n\\nWe also evaluated asymmetric cluster models on a verb decision task\\ncloser to possible applications to disambiguation in language\\nanalysis. The task consists judging which of two verbs v and v' is\\nmore likely to take a given noun n as object, when all occurrences\\nof (v,n) in the training set were deliberately deleted. Thus this\\ntest evaluates how well the models reconstruct missing data in the\\nverb distribution for n from the cluster centroids close to n.\\n\\n\\nThe data for this test was built from the training data for the\\nprevious one in the following way, based on a suggestion by\\nDagan et al. . A small\\nnumber (104) of (v,n) pairs with a fairly frequent verb (between 500\\nand 5000 occurrences) was randomly picked, and all occurrences of each\\npair in the training set were deleted. The resulting training set was\\nused to build a sequence of cluster models as before. Each model was\\nused to decide which of two verbs v and v' are more likely to\\nappear with a noun n where the (v,n) data was deleted from the\\ntraining set, and the decisions compared with the corresponding ones\\nderived from the original event frequencies in the initial data set.\\nMore specifically, for each deleted pair (v,n) and each verb v'that occurred with n in the initial data either at least twice as\\nfrequently or at most half as frequently as v, we compared the\\nsign of \\n\\n\\nwith that of \\n\\n\\nfor the initial data set. The error rate for each\\nmodel is simply the proportion of sign disagreements in the selected\\n(v,n,v') triples. Figure\\n  shows the error rates for each model for all the selected (v,n,v') (all) and for just those exceptional\\ntriples in which the log frequency ratio of (n,v) and (n,v')differs from the log marginal frequency ratio of v and v'. In\\nother words, the exceptional cases are those in which predictions\\nbased just on the marginal frequencies, which the initial one-cluster\\nmodel represents, would be consistently wrong.\\n\\n\\nHere too we see some overtraining for the largest models considered,\\nalthough not for the exceptional verbs.\\n\\n\\n\\n  CONCLUSIONS \\n\\nWe have demonstrated that a general divisive clustering procedure for\\nprobability distributions can be used to group words according to\\ntheir participation in particular grammatical relations with other\\nwords. The resulting clusters are intuitively informative, and can be\\nused to construct class-based word coocurrence models with substantial\\npredictive power.\\n\\n\\nWhile the clusters derived by the proposed method seem in many cases\\nsemantically significant, this intuition needs to be grounded in a\\nmore rigorous assessment. In addition to predictive power evaluations\\nof the kind we have already carried out, it might be worth comparing\\nautomatically-derived clusters with human judgements in a suitable\\nexperimental setting.\\n\\n\\nMoving further in the direction of class-based language models, we\\nplan to consider additional distributional relations (for instance,\\nadjective-noun) and apply the results of clustering to the grouping of\\nlexical associations in lexicalized grammar frameworks such as\\nstochastic lexicalized tree-adjoining grammars\\n . \\n\\n\\n  ACKNOWLEDGMENTS \\n\\n We would like to thank Don Hindle for making\\navailable the 1988 Associated Press verb-object data set, the Fidditch\\nparser and a verb-object structure filter, Mats Rooth for selecting\\nthe objects of ``fire'' data set and many discussions, David Yarowsky\\nfor help with his stemming and concordancing tools, and Ido Dagan for\\nsuggesting ways of testing cluster models.\\n\\nBibliography \\n\\nPeter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and\\n  Robert L. Mercer.\\n1990.\\nClass-based n-gram models of natural language.\\nIn Proceedings of the IBM Natural Language ITL, pages\\n  283-298, Paris, France, March.\\n\\n\\nKenneth W. Church and William A. Gale.\\n1991.\\nA comparison of the enhanced Good-Turing and deleted estimation\\n  methods for estimating probabilities of English bigrams.\\nComputer Speech and Language, 5:19-54.\\n\\n\\nKenneth W. Church.\\n1988.\\nA stochastic parts program and noun phrase parser for unrestricted\\n  text.\\nIn Proceedings of the Second Conference on Applied Natural\\n  Language Processing, pages 136-143, Austin, Texas. Association for\\n  Computational Linguistics, Morristown, New Jersey.\\n\\n\\nThomas M. Cover and Joy A. Thomas.\\n1991.\\nElements of Information Theory.\\nWiley-Interscience, New York, New York.\\n\\n\\nIdo Dagan, Shaul Markus, and Shaul Markovitch.\\n1992.\\nContextual word similarity and the estimation of sparse lexical\\n  relations.\\nSubmitted for publication.\\n\\n\\nA. P. Dempster, N. M. Laird, and D. B. Rubin.\\n1977.\\nMaximum likelihood from incomplete data via the EM algorithm.\\nJournal of the Royal Statistical Society, Series B,\\n  39(1):1-38.\\n\\n\\nRichard O. Duda and Peter E. Hart.\\n1973.\\nPattern Classification and Scene Analysis.\\nWiley-Interscience, New York, New York.\\n\\n\\nDonald Hindle.\\n1990.\\nNoun classification from predicate-argument structures.\\nIn 28th Annual Meeting of the Association for Computational\\n  Linguistics, pages 268-275, Pittsburgh, Pennsylvania. Association for\\n  Computational Linguistics, Morristown, New Jersey.\\n\\n\\nDonald Hindle.\\n1993.\\nA parser for text corpora.\\nIn B.T.S. Atkins and A. Zampoli, editors, Computational\\n  Approaches to the Lexicon. Oxford University Press, Oxford, England.\\nTo appear.\\n\\n\\nPhilip Resnik.\\n1992.\\nWordNet and distributional analysis: A class-based approach to\\n  lexical discovery.\\nIn AAAI Workshop on Statistically-Based\\n  Natural-Language-Processing Techniques, San Jose, California, July.\\n\\n\\nKenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox.\\n1990.\\nStatistical mechanics and phase transitions in clustering.\\nPhysical Review Letters, 65(8):945-948.\\n\\n\\nYves Schabes.\\n1992.\\nStochastic lexicalized tree-adjoining grammars.\\nIn Proceeedings of the 14th International Conference on\\n  Computational Linguistics, Nantes, France.\\n\\n\\nDavid Yarowsky.\\n1992.\\nPersonal communication.\\n\\nFootnotes\\n\\n  A more formal discussion will appear in our paper\\nDistributional Clustering, in preparation.\\n  As usual in clustering models\\n , we assume that the model distribution and the empirical distribution are interchangeable at the solution of the\\nparameter estimation equations, since the model is assumed to be able\\nto represent correctly the data at that solution point. In practice,\\nthe data may not come exactly from the chosen model class, but\\nthe model obtained by solving the\\nestimation equations may still be the closest one to the data.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nWe describe and experimentally evaluate a method for automatically\\nclustering words according to their distribution in particular\\nsyntactic contexts. Deterministic annealing is used to find lowest\\ndistortion sets of clusters. As the annealing parameter increases,\\nexisting clusters become unstable and subdivide, yielding a\\nhierarchical ``soft'' clustering of the data. Clusters are used as the\\nbasis for class models of word coocurrence, and the models evaluated\\nwith respect to held-out test data.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nThis paper describes part of our ongoing investigation on the use of\\nformal deduction in linear logic to explicate the relationship between\\nsyntactic analyses in Lexical-Functional Grammar (LFG) and semantic\\ninterpretations. The use of formal deduction in semantic\\ninterpretation was implicit in deductive systems for categorial syntax\\n , and has been made explicit through applications of the Curry-Howard parallelism between proofs and terms\\nin more recent work on categorial semantics\\n ,, labeled deductive  systems  and flexible categorial systems  . Accounts of the syntax-semantics interface in the categorial tradition require that syntactic and\\nsemantic analyses be formalized in parallel algebraic structures of\\nsimilar signatures, based on generalized application and abstraction\\n(or residuation) operators, and structure-preserving relations between\\nthem.  Those accounts therefore force the adoption of categorial\\nsyntactic analyses, with their strong dependence on phrase structure\\nand linear order.\\n\\n\\n In contrast, our approach uses linear logic  to represent the connection between two dissimilar levels of\\nrepresentation, LFG f-structures and their semantic interpretations.\\nF-structures provide a crosslinguistically uniform representation of\\nsyntactic information relevant to semantic interpretation that\\nabstracts away from the details of phrase structure and linear order\\nin particular languages. This generality is in part achieved by using\\ngrammatical functions rather than functor-argument relations to\\nrepresent syntactic predicate-argument relationships.  As\\n notes, however, the flatter,\\nunordered, grammatical function structure of LFG does not fit well\\nwith traditional semantic compositionality, based on functional\\nabstraction and application, which mandates a rigid order of semantic\\ncomposition.  We are thus forced to use a more relaxed form of\\ncompositionality, in which, as in more traditional ones, the semantics\\nof each lexical entry in a sentence is used exactly once in\\ninterpretation, but without imposing a rigid order of composition. It\\nturns out that linear logic offers exactly what is required for a\\ncalculus of semantic composition for LFG, in that it can represent\\ndirectly the constraints on the creation and use of semantic units in\\nsentence interpretation without forcing a particular hierarchical\\norder of composition except as required by the properties of\\nparticular lexical entries.\\n\\n\\nWe have shown previously that the linear-logic formalization of the\\nsyntax-semantics interface for LFG provides simple and general\\nanalyses of modification, functional completeness and coherence, and\\n complex predicate formation ,.  In the present paper, the analysis is extended to the interpretation of\\nquantified noun phrases.  After an overview of the approach, we\\npresent our analysis of the compositional properties of quantifiers,\\nand we conclude by showing that the analysis correctly accounts for\\nscope ambiguity and its interactions with bound anaphora.\\n\\n\\n  LFG and Linear Logic \\n  Syntactic Framework \\n\\nLFG assumes two syntactic levels of representation: constituent\\nstructure (c-structure) represents phrasal dominance and\\nprecedence relations, while functional structure (f-structure) represents syntactic predicate-argument structure.\\nFor example, the f-structure for sentence  is given\\nin .\\nAs illustrated, a functional structure consists of a collection\\nof attributes, such as , , and , whose values can, in\\nturn, be other functional structures.  The following annotated\\nphrase-structure rules can generate the f-structure in .\\n\\nThese two phrase structure rules do not encode semantic\\ninformation; they specify only how grammatical functions such as\\n are expressed in English.  The f-structure metavariables\\n and  refer, respectively, to the f-structure of the mother\\nof the current node and to the f-structure of the current node\\n .  The annotations on the S rule indicate, then, that the f-structure for the S has a  attribute\\nwhose value is the f-structure for the NP daughter, and that the\\nf-structure for the S is the same as the one for the VP daughter.  The\\nrelation between the nodes of the c-structure and the f-structure for\\nthe sentence  is expressed by means of arrows in (1):\\n\\n\\n  Lexically-Specified Semantics \\n\\nUnlike phrase structure rules, lexical entries specify semantic\\nas well as syntactic information.  Here are the lexical entries\\nfor the words in the sentence:\\nJust like phrase structure rules, lexical entries are\\ninstantiated for a particular utterance.  The metavariable \\nin a\\nlexical entry represents the f-structure of the c-structure mother of\\n(an instance of) the entry in a c-structure.  The syntactic information\\ngiven in lexical entries consists of equality statements about the\\nf-structure, while the semantic information consists of assertions\\nabout how the meaning of the f-structure participates in various\\nsemantic relations.\\n\\n\\nThe semantic information in a lexical entry, which we will call the\\nsemantic contribution of the entry, is a linear-logic formula\\nthat constrains the association between semantic structures\\nprojected from the f-structures mentioned in the lexical entry\\n , and their semantic interpretations.  The semantic projection function \\nmaps an\\nf-structure to a semantic structure encoding information about its\\nmeaning, in the same way as the functional projection function maps c-structure nodes to the associated f-structures.  The\\nassociation between \\nand a meaning P is represented by the\\natomic formula \\n\\n,\\nwhere \\nis an otherwise\\nuninterpreted binary predicate symbol.  (In fact, we use not one but a\\nfamily of relations \\n\\n\\nindexed by the semantic type of\\nthe intended second argument, although for simplicity we will omit the\\ntype subscript whenever it is determinable from context.)  We will\\noften informally say that P is f's meaning without referring to\\nthe role of the semantic structure \\nin \\n\\n.\\nWe will see, however, that f-structures and their semantic\\nprojections must be distinguished, because in general semantic\\nprojections carry more information than just the association to the\\nmeaning for the corresponding f-structure.\\n\\n\\nWe can now explain the semantic contributions in .\\nIf a particular occurrence of `Bill' in a sentence is\\nassociated with f-structure f, the syntactic constraint in the\\nlexical entry Bill will be instantiated as\\n\\n\\nand the semantic constraint will be\\ninstantiated as \\n\\n,\\nrepresenting the association between \\nand the\\nconstant \\nrepresenting its meaning.\\n\\n\\nThe semantic contribution of the appointed entry is more\\ncomplex, as\\nit relates the meanings of the subject and object of a clause\\nto the clause's meaning. Specifically, if f is the\\nf-structure for a clause with predicate ()\\n`appoint', the\\nsemantic contribution asserts that if f's subject \\n\\n\\nhas\\nmeaning X and (linear conjunction )\\nf's object\\n\\n\\nhas meaning Y, then (linear implication\\n)\\nf has meaning \\n\\n. \\n\\n  Logical Representation of Semantic Compositionality \\n\\nIn the semantic contribution for appointed in ,\\nthe linear-logic connectives of multiplicative conjunction and linear implication \\nare used to specify how the meaning\\nof a clause headed by the verb is composed from the meanings of the\\narguments of the verb. For the moment, we can think of the linear\\nconnectives as playing the same role as the analogous classical\\nconnectives conjunction and implication, but we will soon see that the\\nspecific properties of the linear connectives are essential to\\nguarantee that lexical entries bring into the interpretation process\\nall and only the information provided by the corresponding words.  The\\nsemantic contribution of appointed asserts that if the subject\\nof a clause with main verb appointed means X and its object\\nmeans Y, then the whole clause means \\n\\n.\\nThe\\nsemantic contribution can thus be thought of as a linear definite\\nclause, with the variables X and Y playing the same role as Prolog\\nvariables.\\n\\n\\nIt is worth noting that the form of the semantic contribution of\\nappointed parallels the type \\n\\n\\nwhich, in\\nits curried form \\n\\n\\n\\n\\n  Meaning and glue \\n\\nOur approach shares the order-independence of representations of\\nsemantic information by attribute-value matrices\\n ,,, while still allowing a well-defined treatment of variable binding and scope.\\nWe do this by distinguishing (1) a language of meanings and (2) a\\nlanguage for assembling meanings or glue language.\\n\\n\\nThe language of meanings could be that of any appropriate logic, for\\n instance Montague's intensional logic .  The glue language, described below, is a fragment of linear logic.  The\\nsemantic contribution of each lexical entry is represented by a\\nlinear-logic formula that can be understood as instructions in the\\nglue language for combining the meanings of the lexical entry's\\nsyntactic arguments into the meaning of the f-structure headed by the\\nentry.  Glue formulas may also be contributed by some syntactic\\nconstructions, when properties of a construction as a whole and not\\njust of its lexical elements are responsible for the interpretation of\\nthe construction; these cases include the semantics of relative\\nclauses.  We will not discuss construction-specific interpretation\\nrules in this paper.\\n\\n\\n Appendix  gives further details on the syntax of the meaning and glue languages used in this paper.\\n\\n\\n  Linear logic \\n\\n As we have just outlined, we\\nuse deduction in linear logic to assign meanings to sentences,\\nstarting from information about their functional structure and about\\nthe semantics of the words they contain.  An approach based on linear\\nlogic, which crucially allows premises to commute, appears to be more\\ncompatible with the shallow and relatively free-form functional\\nstructure than are compositional approaches, which rely on deeply\\nnested binary-branching immediate dominance relationships.  As noted\\nabove, the use of linear logic as the system for assembling meanings\\npermits a uniform treatment of a range of natural language phenomena\\ndescribed by , including modification, completeness\\n and coherence, and complex predicate formation.\\n\\n\\nAn important motivation for using linear logic is that it allows us to\\nto capture directly the intuition that lexical items and phrases each\\ncontribute exactly once to the meaning of a sentence.  As noted by\\n,\\nTranslation rules in Montague semantics have the property that the\\ntranslation of each component of a complex expression occurs exactly\\nonce in the translation of the whole.  ...That is to say, we do\\nnot want the set S [of semantic representations of a phrase] to\\ncontain all meaningful expressions of IL which can be built up\\nfrom the elements of S, but only those which use each element exactly\\nonce.\\n\\n\\nIn our terms, the semantic contributions of the constituents\\nof a sentence are not context-independent assertions that may be used\\nor not in the derivation of the meaning of the sentence depending on\\nthe course of the derivation. Instead, the semantic contributions are\\noccurrences of information which are generated and used exactly\\nonce.  For example, the formula \\n\\n\\ncan be thought of as providing one occurrence of the meaning\\n\\nassociated to the semantic projection \\n\\n.\\nThat\\nmeaning must be consumed exactly once (for example, by appointed in\\n) in the derivation of a meaning of the entire utterance.\\n\\n\\nIt is this ``resource-sensitivity'' of natural language semantics--an\\nexpression is used exactly once in a semantic derivation--that linear\\nlogic can model. The basic insight underlying linear logic is that\\nlogical formulas are resources that are produced and consumed in\\nthe deduction process.  This gives rise to a resource-sensitive notion\\nof implication, the linear implication :\\nthe formula \\n\\n\\ncan be thought of as an action that can consume (one\\ncopy of) A to produce (one copy of) B. Thus, the formula \\n\\n\\nlinearlyentails B.  It does not entail \\n\\n\\n(because the deduction consumes A), and it does not entail\\n\\n\\n(because the linear implication is also\\nconsumed in doing the deduction).  This resource-sensitivity not only\\ndisallows arbitrary duplication of formulas, but also disallows\\narbitrary deletion of formulas. Thus the linear multiplicative\\nconjunction \\nis sensitive to the multiplicity of formulas: \\n\\n\\nis not equivalent to A (the former has two copies of the\\nformula A).  For example, the formula \\n\\n\\nlinearly entails \\n\\n\\n(there is still one Aleft over) but does not entail B (there must still be one\\nA present).  In this way, linear logic checks that a formula is used\\nonce and only once in a deduction, enforcing the requirement that\\neach component of an utterance contributes exactly once to the\\nassembly of the utterance's meaning.\\n\\n\\nTo handle quantification, our glue language needs to be only a\\nfragment of higher-order linear logic, the tensor fragment, that\\nis closed under conjunction, universal quantification, and implication\\n(with at most one level of nesting of implication in antecedents).  In\\nfact, all but the determiner lexical entries are in the first-order\\nsubset of this fragment.  This fragment arises from transferring to\\nlinear logic the ideas underlying the concurrent constraint\\nprogramming scheme of . An explicit formulation\\nfor the higher-order version of the linear concurrent constraint\\nprogramming scheme is given in\\n.  A nice tutorial introduction\\nto linear logic itself may be found in\\n; see also .\\n\\n\\n  Relationship with Categorial Syntax and Semantics \\n\\nAs suggested above, there are interesting connections between our\\napproach and various systems of categorial syntax and semantics. The\\nLambek calculus\\n , introduced as a logic of syntactic combination, turns out to be a fragment of noncommutative\\nmultiplicative linear logic.  If permutation is added to Lambek's\\nsystem, its left- and right-implication connectives (\\nand\\n/) collapse into a single implication connective with behavior\\nidentical to .\\nThis undirected version of the Lambek calculus\\nwas developed by\\nvan Benthem  to account\\nfor the semantic combination possibilities of phrase meanings.\\n\\n\\nThose systems and related ones\\n ,, were developed as calculi of syntactic/semantic types, with propositional\\nformulas representing syntactic categories or semantic types. Given\\nthe types for the lexical items in a sentence as assumptions, the\\nsentence is syntactically well-formed in the Lambek calculus if the\\ntype of the sentence can be derived from the assumptions arranged as\\nan ordered list. Furthermore, the Curry-Howard isomorphism between\\n proofs and terms  allows the extraction of a term representing the meaning of the sentence from the proof that the\\n sentence is well-formed . However, the Lambek calculus and its variants carry with them a particular view of\\nsyntactic structure that is not obviously compatible with the flatter\\nf-structures proposed by LFG.\\n\\n\\nOn the other hand, categorial semantics in the undirected Lambek\\ncalculus and other related commutative calculi provides an analysis of\\nthe possibilities of meaning combination independently of the\\nsyntactic realizations of those meanings, but does not provide a\\nmechanism for relating semantic combination possibilities to\\nthe corresponding syntactic combination possibilities.\\n\\n\\nIn more recent work, multidimensional and labeled deductive systems\\n , have been proposed as refinements of the Lambek systems that are able to represent\\nsynchronized derivations involving multiple levels of representation,\\nfor instance a level of head-dependent representations and a level of\\nsyntactic functor-argument representations. However, these systems do\\nnot yet seem able to represent the connection between a flat syntactic\\nrepresentation in terms of grammatical functions and a\\nfunction-argument semantic representation. As far as we can see, the\\nproblem in those systems is that at the type level it is not possible\\nto express the link between particular syntactic structures\\n(f-structures in our case) and particular contributions to\\nmeaning. The extraction of meanings from derivations following the\\nCurry-Howard isomorphism that is standard in categorial systems\\ndemands that the order of syntactic combination coincide with the\\norder of semantic combination so that functor-argument relations at\\nthe syntactic and semantic level are properly aligned.\\n\\n\\nThus, while the ``propositional skeleton'' of an analysis in our\\nsystem can be seen as a close relative of the corresponding categorial\\nsemantics derivation in the undirected Lambek calculus, the\\nfirst-order part of our analysis (notably the f, g, and h in the\\nexample above) explicitly carries the connection between f-structures\\nand their contributions to meaning. In this way, we can take advantage\\nof the principled description of potential meaning combinations of\\ncategorial semantics without losing track of the constraints imposed\\nby syntax on the possible combinations of those meanings.\\n\\n\\n\\n\\n\\n  Quantification \\n\\nOur treatment of quantification, and in particular of quantifier scope\\nambiguity and of the interactions between scope and bound anaphora,\\nfollows the approach of Pereira\\n.  It turns out, however, that\\nthe linear-logic formulation is simpler and easier to justify than\\nthe earlier analysis, which used an intuitionistic type assignment\\nlogic.\\n\\n\\nThe basic idea for the analysis can be seen as a logical\\ncounterpart at the glue level of the standard type\\nassignment for generalized quantifiers\\n . The generalized quantifier meaning of a natural language determiner has the following type, a\\nfunction from two properties, the quantifier's restriction and scope,\\nto a proposition:\\n\\n At the semantic glue level, we can understand\\nthat type as follows. For any determiner, if for arbitrary x we can\\nconstruct a meaning R x for the quantifier's restriction, and again\\nfor arbitrary x we can construct a meaning S x for the\\nquantifier's scope, where R and S are properties (functions from\\nentities to propositions), then we can construct the meaning Q R Sfor the whole sentence containing the determiner, where Q is the\\nmeaning of the determiner.  In the following we\\nwill notate Q R S meaning more perspicuously as \\n\\nQ(z,Rz,Sz).\\n\\n\\nAssume that we have determined the following semantic structures:\\n\\n\\nfor the restriction (a common noun phrase),\\n\\n\\nfor its implicit argument, \\n\\n\\nfor the scope\\nof quantification, and \\n\\n\\nfor the grammatical function\\nfilled by the quantified noun phrase.  Then the foregoing analysis can\\nbe represented in linear logic by the following schematic formula:\\n  Given the equivalence between \\n\\n\\nand \\n\\n,\\nthe propositional part of \\nparallels the generalized quantifier type .\\n\\n\\nIn addition to providing a semantic type assignment for determiners,\\n uses glue language quantification to express how\\nthe meanings of the restriction and scope of quantification are\\ndetermined and combined into the meaning of the quantified clause.\\nThe condition \\n\\n\\n\\n    Quantified noun phrase meanings\\n\\n\\nWe first demonstrate how the semantic contribution of a quantified\\nnoun phrase such as every voter is derived.  The following\\nannotated phrase structure rule is necessary:\\n\\n\\n\\nThis rule states that the determiner Det and noun N each contribute to\\nthe f-structure for the NP.  Lexical specifications ensure that the\\nnoun contributes the  attribute and its value, and the\\ndeterminer contributes the  attribute and its value.  The\\nf-structure for the noun phrase every voter is:\\nThe semantic contributions of common nouns and determiners were\\ndescribed in the previous section.\\n\\n\\nGiven those entries, the semantic contributions of every and\\nvoter in  are\\n\\n\\n\\n\\n\\nFrom these two premises, the semantic contribution for every voter follows:\\n\\n\\n\\n\\n\\nThe propositional part of this contribution corresponds to the\\nstandard type for noun phrase meanings, \\n\\n.\\nInformally, the whole contribution can be read as follows: if by\\ngiving the arbitrary meaning x of type e to the argument position\\nfilled by the noun phrase we can derive the meaning S x of type tfor the semantic structure scope of quantification H, then\\nS can be the property that the noun phrase meaning requires as its\\nscope, yielding the meaning \\n\\n\\nfor\\nH. The quantified noun phrase can thus be seen as providing\\ntwo contributions to an interpretation: locally, a referential\\nimport x, which must be discharged when the scope of quantification\\nis established; and globally, a quantificational import of type\\n\\n,\\nwhich is applied to the meaning of\\nthe scope of quantification to obtain a quantified proposition.\\n\\n\\n  Simple example of quantification \\n\\nBefore we look at quantifier scope ambiguity and interactions between\\nscope and bound anaphora, we demonstrate the basic operation of our\\nproposed representation of the semantic contribution of a determiner.\\nWe use the  following sentence with a single quantifier and no\\nscope ambiguities:\\nThe premises for the derivation are the semantic\\ncontributions for Bill and convinced together with the\\ncontribution derived above for the quantified noun phrase every\\nvoter:\\n\\n\\n\\n\\n\\nGiving the name bill-convinced to the formula\\n\\n\\n\\n\\n\\nwe have the derivation:\\n\\n\\n\\n\\n\\nNo derivation of a different formula \\n\\n\\nis\\npossible.  The formula bill-convinced represents the semantics of\\nthe scope of the determiner `every'. The derivable formula \\n\\n\\n\\n\\n\\ncould at\\nfirst sight be considered another possible, but erroneous,\\nscope. However, the type subscripting of the  relation used in\\nthe determiner lexical entry requires the scope to represent a\\ndependency of a proposition on an individual, while this formula\\nrepresents the dependency of an individual on an individual\\n(itself). Therefore, it does not provide a valid scope for the\\nquantifier.\\n\\n\\n  Quantifier scope ambiguities \\n\\nWhen a sentence contains more than one quantifier, scope ambiguities\\nare of course possible. In our system, those ambiguities will appear\\nas alternative successful derivations. We will take as our example the\\n sentence \\nWe can derive semantic contributions\\nfor every candidate and a manager in the way shown in\\n Section .  Further derivations proceed from those contributions together with the contribution of appointed:\\n\\n\\n\\n\\n\\nAs of yet, we have not made any commitment about the scopes\\nof the quantifiers; the 's have not been instantiated.\\nScope ambiguities are manifested in two different ways in our system:\\nthrough the choice of different semantic structures H,\\ncorresponding to different syntactic choices for where to scope the\\nquantifier, or through different relative orders of quantifiers that\\nscope at the same point.  For this example, the second case is\\nrelevant, and we must now make a choice to proceed. The two possible\\nchoices correspond to two equivalent rewritings of appointed:\\n\\n\\n\\n\\n\\nThese two equivalent forms correspond to the two possible\\nways of ``currying'' a two-argument function  \\n\\n\\nas one-argument functions: \\n\\n\\n\\n\\n\\n\\n\\n\\nWe select a manager to take\\nnarrower scope by using universal instantiation and\\ntransitivity of implication to combine the\\nfirst form with a-manager to yield\\n\\n\\n\\n\\n\\nWe have thus the following derivation\\n\\n\\n\\n\\n\\nof the \\n\\n\\nreading of ae.\\n\\n\\nAlternatively, we could have chosen every candidate to take\\nnarrow scope, by combining the second equivalent form of appointed\\nwith every-candidate to produce:\\n\\n\\n\\n\\n\\nThis gives the derivation\\n\\n\\n\\n\\n\\nfor the \\n\\n\\nreading. These are the only two possible\\noutcomes of the derivation of a meaning for ae, as required. We\\nhave used our implementation to verify that no other outcomes are\\npossible, since manual verification would be rather laborious.\\n\\n\\n    Constraints on quantifier scoping\\n\\n\\n Sentence () contains two quantifiers and therefore might be expected to show a two-way ambiguity analogous to the one described\\nin the previous section:\\n\\n\\n Every candidate appointed an admirer of his. \\n\\n\\nHowever, no such ambiguity is found if the pronoun his\\nis taken to corefer with the subject every candidate. In this\\ncase, only one reading is available, in which an admirer of his\\ntakes narrow scope.  Intuitively, this noun phrase may not take wider\\nscope than the quantifier every candidate, on which its\\nrestriction depends.\\n\\n\\nAs we will soon see, the lack of a wide scope a reading follows\\nautomatically from our formulation of the semantic contributions of\\nquantifiers without further stipulation. In Pereira's earlier work on\\n deductive interpretation ,, the same result was achieved through constraints on the relative scopes of\\nglue-level universal quantifiers representing the dependencies between\\nmeanings of clauses and the meanings of their arguments. Here,\\nalthough universal quantifiers are used to support the extraction of\\nproperties representing the meanings of the restriction and scope (the\\nvariables R and S in the determiner lexical entries), the blocking\\nof the unwanted reading follows from the propositional structure of\\nthe glue formulas, specifically the nested linear implications. This\\nis more satisfactory, since it does not reduce the problem of proper\\nquantifier scoping in the object language to the same problem in the\\nmetalanguage.\\n\\n\\nThe lexical entry for admirer is:\\n\\nHere, admirer is a relational noun\\ntaking as its oblique argument a phrase with prepositional marker of, as indicated in the f-structure by the attribute\\nOF.  The semantic contribution for a relational noun\\nhas, as expected, the same propositional form as the binary relation\\ntype \\n\\n:\\none argument is the admirer, and the\\nother argument is the admiree.\\n\\n\\nWe assume that the semantic projection for the antecedent of the\\npronoun his has been determined by some separate mechanism and\\nrecorded as the \\nattribute of the pronoun's semantic\\n projection. The semantic contribution of the pronoun is, then, a formula that consumes the meaning of its antecedent and then\\nreintroduces that meaning, simultaneously assigning it to its own\\nsemantic projection:\\n\\nIn other words, the semantic contribution of a pronoun\\ncopies the meaning X of its antecedent as the meaning of the pronoun\\nitself.  Since the left-hand side of the linear implication\\n``consumes'' the antecedent meaning, it must be reinstated in the\\nconsequent of the implication.\\n\\n\\n The f-structure for example () is, then: \\n\\nWe will begin by illustrating the derivation of the meaning of an\\nadmirer of his, starting from the following premises:\\n\\n\\n\\n\\n\\nFirst, we rewrite admirer into the equivalent form\\n\\n\\n\\n\\n\\nWe can use this formula to rewrite the\\nthe second conjunct in the consequent of his,\\nyielding\\n\\n\\n\\n\\n\\nIn turn, the second conjunct in the consequent of admirer-of-his\\nmatches the first conjunct in the antecedent of a given\\nappropriate variable substitutions, allowing us to derive\\n\\n\\n\\n\\n\\nAt this point the other formulas available are:\\n\\n\\n\\n\\n\\nWe have thus the meanings of the two quantified noun phrases.  The\\nantecedent implication of every-candidate has an atomic\\nconclusion and hence cannot be satisfied by\\nan-admirer-of-his, which has a conjunctive conclusion.\\nTherefore, the only possible move is to combine appointed\\nand an-admirer-of-his. We do this by first\\nputting appointed in the equivalent form\\n\\n\\n\\n\\n\\nAfter universal instantiation of Z with X, this\\ncan be used to rewrite the first conjunct in the consequent of an-admirer-of-his to derive\\n\\n\\n\\n\\n\\nUniversal instantiation of H and S together with modus\\nponens with the two conjuncts in the consequent as premises yield\\n\\n\\n\\n\\n\\nFinally, this formula can be combined with every-candidate\\nto give the meaning of the whole sentence:\\n\\n\\n\\n\\n\\nIn fact, this is the only derivable conclusion, showing that our\\nanalysis blocks those putative scopings in which variables occur\\noutside the scope of their binders.\\n\\n\\n\\n  Conclusion \\n\\nOur approach exploits the f-structure of LFG for syntactic information\\nneeded to guide semantic composition, and also exploits the\\nresource-sensitive properties of linear logic to express the semantic\\ncomposition requirements of natural language.  The use of linear logic\\nas the glue language in a deductive semantic framework allows a natural\\ntreatment of quantification which automatically gives the right\\nresults for quantifier scope ambiguities and interactions with bound\\nanaphora.\\n\\n\\nThe analyses discussed here show that our linear-logic encoding of\\nsemantic compositionality captures the interpretation constraints\\nbetween quantified noun phrases, their scopes and bound\\nanaphora. The same basic facts are also accounted for in\\nother recent treatments of compositionality, in particular categorial\\nanalyses with discontinuous constituency connectives\\n .  However, we show elsewhere   that our approach has advantages over those accounts, in that certain available\\nreadings of sentences with intensional verbs and quantified noun\\nphrases that current categorial analyses cannot derive are readily\\nproduced in our analysis.\\n\\n\\nRecently,  independently proposed a\\nmultidimensional categorial system with types indexed so as to keep\\ntrack of the syntax-semantic connections that we represent with\\n.\\nUsing proof net techniques due to\\n and ,\\nhe maps categorial formulas to first-order clauses similar to our\\nsemantic contributions, except that the formulas arising from\\ndeterminers lack the embedded implication. Oehrle's system models\\nquantifier scope ambiguities in a way similar to ours, but it is not\\nclear that it can account correctly for the interactions with\\nanaphora, given the lack of implication embedding in the clausal\\nrepresentation used.  A full comparison of the two systems is left for\\nfuture work.\\n\\n\\n  Acknowledgments \\n\\nWe thank Johan van Benthem, Bob Carpenter, Jan van Eijck, Angie\\nHinrichs, David Israel, Ron Kaplan, John Maxwell, Michael Moortgat,\\nJohn Nerbonne, Stanley Peters, Henriette de Swart and an anonymous\\nreviewer for discussions and comments.  They are not responsible\\nfor any remaining errors, and we doubt that they will endorse all our\\nanalyses and conclusions, but we are sure that the end result is much\\nimproved for their help.\\n\\n\\n    Syntax of the Meaning and Glue Languages\\n\\n\\nThe meaning language is based on Montague's\\nintensional higher-order logic. In fact, in the present paper we just\\nuse an extensional fragment with the following syntax:\\n\\n\\n\\n\\n\\nTerms are typed in the usual way;\\nlogical connectives such as every and a are\\nrepresented by constants of appropriate type.\\nFor readability, we will often ``uncurry'' \\n\\n\\nas\\n\\n.\\nNote that we allow variables in the glue\\nlanguage to range over meaning terms.\\n\\n\\nThe glue language refers to three kinds of terms: meaning terms,\\nf-structures, and semantic or -structures. f- and\\n-structures are feature structures in correspondence (through\\nprojections) with constituent structure. Conceptually, feature\\nstructures are just functions which, when applied to attributes (a set\\nof constants), return constants or other feature structures.  In the\\nfollowing we let A range over some pre-specified set of attributes.\\n\\n\\n\\n\\n\\nGlue-language formulas are built up using linear connectives from\\natomic formulas of the form \\n\\n,\\nwhose intended\\ninterpretation is that the meaning associated with -structure\\nS is denoted by term M of type .\\nWe omit the type subscript\\n\\nwhen it can be determined from context.\\n\\n\\n\\n\\nBibliography \\n\\nAlsina, Alex.\\n1993.\\nPredicate Composition: A Theory of Syntactic Function\\n  Alternations.\\nPh.D. thesis, Stanford University.\\n\\n\\nBarwise, Jon and Robin Cooper.\\n1981.\\nGeneralized quantifiers and natural language.\\nLinguistics and Philosophy, 4:159-219.\\n\\n\\nBresnan, Joan and Jonni M. Kanerva.\\n1989.\\nLocative inversion in Chichewa: A case study of factorization\\n  in grammar.\\nLinguistic Inquiry, 20(1):1-50.\\nAlso in E. Wehrli and T. Stowell, eds., Syntax and Semantics 26:\\n  Syntax and the Lexicon. New York: Academic Press.\\n\\n\\nButt, Miriam.\\n1993.\\nThe Structure of Complex Predicates.\\nPh.D. thesis, Stanford University.\\n\\n\\nDalrymple, Mary.\\n1993.\\nThe Syntax of Anaphoric Binding.\\nNumber 36 in CSLI Lecture Notes. Center for the Study of Language and\\n  Information.\\n\\n\\nDalrymple, Mary, John Lamping, and Vijay Saraswat.\\n1993.\\nLFG semantics via constraints.\\nIn Proceedings of the Sixth Meeting of the European ACL,\\n  University of Utrecht, April. European Chapter of the Association for\\n  Computational Linguistics.\\n\\n\\nDalrymple, Mary, Angie Hinrichs, John Lamping, and Vijay Saraswat.\\n1993.\\nThe resource logic of complex predicate interpretation.\\nIn Proceedings of the 1993 Republic of China Computational\\n  Linguistics Conference (ROCLING), Hsitou National Park, Taiwan, September.\\n  Computational Linguistics Society of R.O.C.\\n\\n\\nDalrymple, Mary, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat.\\n1994.\\nIntensional verbs without type-raising or lexical ambiguity.\\nIn Conference on Information-Oriented Approaches to Logic,\\n  Language and Computation, Moraga, California. Saint Mary's College.\\nTo appear.\\n\\n\\nFenstad, Jens Erik, Per-Kristian Halvorsen, Tore Langholm, and Johan van\\n  Benthem.\\n1987.\\nSituations, Language and Logic.\\nD. Reidel, Dordrecht.\\n\\n\\nGamut, L. T. F.\\n1991.\\nLogic, Language, and Meaning, volume 2: Intensional Logic and\\n  Logical Grammar.\\nThe University of Chicago Press, Chicago.\\n\\n\\nGirard, J.-Y.\\n1987.\\nLinear logic.\\nTheoretical Computer Science, 45:1-102.\\n\\n\\nHalvorsen, Per-Kristian.\\n1983.\\nSemantics for Lexical-Functional Grammar.\\nLinguistic Inquiry, 14(4):567-615.\\n\\n\\nHalvorsen, Per-Kristian.\\n1988.\\nSituation Semantics and semantic interpretation in\\n  constraint-based grammars.\\nIn Proceedings of the International Conference on Fifth\\n  Generation Computer Systems, FGCS-88, pages 471-478, Tokyo, Japan,\\n  November.\\nAlso published as CSLI Technical Report CSLI-TR-101, Stanford\\n  University, 1987.\\n\\n\\nHalvorsen, Per-Kristian and Ronald M. Kaplan.\\n1988.\\nProjections and semantic description in Lexical-Functional\\n  Grammar.\\nIn Proceedings of the International Conference on Fifth\\n  Generation Computer Systems, pages 1116-1122, Tokyo, Japan. Institute for\\n  New Generation Systems.\\n\\n\\nHendriks, Herman.\\n1993.\\nStudied Flexibility: Categories and Types in Syntax and\\n  Semantics.\\nILLC dissertation series 1993--5, University of Amsterdam,\\n  Amsterdam, Holland.\\n\\n\\nHepple, Mark.\\n1990.\\nThe Grammar and Processing of Order and Dependency: a Categorial\\n  Approach.\\nPh.D. thesis, University of Edinburgh.\\n\\n\\nHoward, W.A.\\n1980.\\nThe formulae-as-types notion of construction.\\nIn J.P. Seldin and J.R. Hindley, editors, To H.B. Curry: Essays\\n  on Combinatory Logic, Lambda Calculus and Formalism. Academic Press, London,\\n  England, pages 479-490.\\n\\n\\nKaplan, Ronald M.\\n1987.\\nThree seductions of computational psycholinguistics.\\nIn Peter Whitelock, Harold Somers, Paul Bennett, Rod Johnson, and\\n  Mary McGee Wood, editors, Linguistic Theory and Computer Applications.\\n  Academic Press, London, pages 149-188.\\n\\n\\nKaplan, Ronald M. and Joan Bresnan.\\n1982.\\nLexical-Functional Grammar: A formal system for grammatical\\n  representation.\\nIn Joan Bresnan, editor, The Mental Representation of\\n  Grammatical Relations. The MIT Press, Cambridge, MA, pages 173-281.\\n\\n\\nKlein, Ewan and Ivan A. Sag.\\n1985.\\nType-driven translation.\\nLinguistics and Philosophy, 8:163-201.\\n\\n\\nLambek, Joachim.\\n1958.\\nThe mathematics of sentence structure.\\nAmerican Mathematical Monthly, 65:154-170.\\n\\n\\nMiller, Dale A.\\n1990.\\nA logic programming language with lambda abstraction, function\\n  variables and simple unification.\\nIn Peter Schroeder-Heister, editor, Extensions of Logic\\n  Programming, Lecture Notes in Artificial Intelligence. Springer-Verlag.\\n\\n\\nMontague, Richard.\\n1974.\\nThe proper treatment of quantification in ordinary English.\\nIn Richmond H. Thomason, editor, Formal Philosophy. Yale\\n  University Press, New Haven, Connecticut.\\n\\n\\nMoortgat, Michael.\\n1988.\\nCategorial Investigations: Logical and Linguistic Aspects of the\\n  Lambek Calculus.\\nPh.D. thesis, University of Amsterdam, Amsterdam, The Netherlands,\\n  October.\\n\\n\\nMoortgat, Michael.\\n1992a.\\nGeneralized quantifiers and discontinuous type constructors.\\nIn W. Sijtsma and H. van Horck, editors, Discontinuous\\n  Constituency. Mouton de Gruyter, Berlin, Germany.\\nTo appear.\\n\\n\\nMoortgat, Michael.\\n1992b.\\nLabelled deductive systems for categorial theorem proving.\\nIn P. Dekker and M. Stokhof, editors, Proceedings of the Eighth\\n  Amsterdam Colloquium, pages 403-423, Amsterdam. Institute for Logic,\\n  Language and Computation.\\n\\n\\nMorrill, Glyn.\\n1990.\\nIntensionality and boundedness.\\nLinguistics and Philosophy, 13(6):699-726.\\n\\n\\nMorrill, Glyn V.\\n1993.\\nType Logical Grammar: Categorial Logic of Signs.\\nStudies in Linguistics and Philosophy. Kluwer Academic Publishers,\\n  Dordrecht, Holland.\\nTo appear.\\n\\n\\nNeale, Stephen.\\n1990.\\nDescriptions.\\nThe MIT Press, Cambridge, MA.\\n\\n\\nOehrle, Richard T.\\n1993.\\nString-based categorial type systems.\\nWorkshop ``Structure of Linguistic Inference: Categorial and\\n  Unification-Based Approaches,'' European Summer School in Logic, Language and\\n  Information, Lisbon, Portugal.\\n\\n\\nPereira, Fernando C. N.\\n1990.\\nCategorial semantics and scoping.\\nComputational Linguistics, 16(1):1-10.\\n\\n\\nPereira, Fernando C. N.\\n1991.\\nSemantic interpretation as higher-order deduction.\\nIn Jan van Eijck, editor, Logics in AI: European Workshop\\n  JELIA'90, pages 78-96, Amsterdam, Holland. Springer-Verlag.\\n\\n\\nPollard, Carl and Ivan A. Sag.\\n1987.\\nInformation-Based Syntax and Semantics, Volume I.\\nNumber 13 in CSLI Lecture Notes. Center for the Study of Language and\\n  Information, Stanford University.\\n\\n\\nPollard, Carl and Ivan A. Sag.\\n1993.\\nHead-Driven Phrase Structure Grammar.\\nThe University of Chicago Press, Chicago.\\n\\n\\nRoorda, Dirk.\\n1991.\\nResource Logics: Proof-theoretical Investigations.\\nPh.D. thesis, University of Amsterdam.\\n\\n\\nSaraswat, Vijay A.\\n1989.\\nConcurrent Constraint Programming Languages.\\nPh.D. thesis, Carnegie-Mellon University.\\nReprinted by MIT Press, Doctoral Dissertation Award and Logic\\n  Programming Series, 1993.\\n\\n\\nSaraswat, Vijay A.\\n1993.\\nA brief introduction to linear concurrent constraint programming.\\nTechnical report, Xerox Palo Alto Research Center, April.\\n\\n\\nSaraswat, Vijay A. and Patrick Lincoln.\\n1992.\\nHigher-order, linear concurrent constraint programming.\\nTechnical report, Xerox Palo Alto Research Center, August.\\n\\n\\nScedrov, Andre.\\n1993.\\nA brief guide to linear logic.\\nIn G. Rozenberg and A. Salomaa, editors, Current Trends in\\n  Theoretical Computer Science, pages 377-394. World Scientific Publishing\\n  Co.\\n\\n\\nvan Benthem, Johan.\\n1986.\\nCategorial grammar and lambda calculus.\\nIn D. Skordev, editor, Mathematical Logic and its Application.\\n  Plenum Press, New York, New York, pages 39-60.\\n\\n\\nvan Benthem, Johan.\\n1988.\\nThe Lambek calculus.\\nIn Richard T. Oehrle, Emmon Bach, and Deirdre Wheeler, editors,   Categorial Grammars and Natural Language Structures. D. Reidel, Dordrecht,\\n  pages 35-68.\\n\\n\\nvan Benthem, Johan.\\n1991.\\nLanguage in Action: Categories, Lambdas and Dynamic Logic.\\nNorth-Holland, Amsterdam.\\n\\nFootnotes\\n\\n  Xerox Palo\\nAlto Research Center Technical Report ISTL-NLTT-1993-06-01. To appear\\nin Quantifiers, Deduction, and Context, ed. Makoto Kanazawa,\\nChristopher J. Pin, and Henriette de Swart.  Stanford,\\nCalifornia: Center for the Study of Language and Information, 1994.\\n  Xerox PARC, Palo Alto, California.\\n  ATT Bell Laboratories, Murray Hill, New Jersey.\\n  In fact, we believe\\nthat the correct\\ntreatment of the relation between a verb and its arguments requires\\nthe use of mapping principles specifying the relation between\\nthe array of semantic arguments required by a verb and their possible\\nsyntactic realizations\\n ,,.  A verb like appoint, for example, might specify that one of its arguments is an agent and the other is a theme.  Mapping principles would then specify\\nthat agents can be realized as subjects and themes as objects.\\n\\n\\nHere we make the simplifying assumption that the arguments of verbs have\\nalready been linked to syntactic functions and that this linking is\\nrepresented in the lexicon, since for the examples we will discuss this\\nassumption is innocuous.  However, in the case of complex predicates\\nthis assumption produces incorrect results, as shown by\\n.  Mapping principles are very naturally incorporated\\ninto the framework discussed here; see\\n and \\nfor discussion and illustration.\\n  ``An f-structure is locally complete if\\nand only if it contains all the governable grammatical functions that\\nits predicate governs.  An f-structure is complete if and only\\nif all its subsidiary f-structures are locally complete. An\\nf-structure is locally coherent if and only if all the\\ngovernable grammatical functions that it contains are governed by a\\nlocal predicate.  An f-structure is coherent if and only if\\nall its subsidiary f-structures are locally coherent.''\\n , pages 211-212]. \\n  In order to allow for apparent scope ambiguities, we\\nadopt a scoping analysis of indefinites, as proposed, for example, by\\n.\\n  The determination of appropriate values for\\n\\nrequires a more more detailed analysis of other linguistic\\nconstraints on anaphora resolution, which would need further\\nprojections to give information about, for example, discourse\\nrelations and salience.\\n discusses in detail LFG analyses of\\nanaphoric binding.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nThe relationship between Lexical-Functional Grammar (LFG) functional structures (f-structures) for sentences and their semantic\\ninterpretations can be expressed directly in a fragment of linear\\nlogic in a way that explains correctly the constrained interactions\\nbetween quantifier scope ambiguity and bound anaphora.\\nThe use of a deductive framework to account for the compositional\\nproperties of quantifying expressions in natural language obviates the\\nneed for additional mechanisms, such as Cooper storage, to represent\\nthe different scopes that a quantifier might take.  Instead, the\\nsemantic contribution of a quantifier is recorded as an ordinary\\nlogical formula, one whose use in a proof will establish the scope of\\nthe quantifier.  The properties of linear logic ensure that each\\nquantifier is scoped exactly once.\\nOur analysis of quantifier scope can be seen as a recasting of\\n Pereira's analysis , which was expressed in higher-order intuitionistic logic.  But our use of LFG and linear\\nlogic provides a much more direct and computationally more\\nflexible interpretation mechanism for at least the same range of\\nphenomena.  We have developed a preliminary Prolog implementation\\nof the linear deductions described in this work.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nThe resource-based approach to semantic composition in\\nLexical-Functional Grammar (LFG) obtains the interpretation for a\\nphrase via a logical deduction, beginning with the\\n interpretations of its parts as premises .  The resource-sensitive system of linear logic is used to\\ncompute meanings in accordance with relationships manifest in LFG\\nf-structures.  The properties of the system ensure that\\nmeanings are used exactly once, allowing coherence and completeness conditions on f-structures\\n  to be maintained. \\n\\n\\nHowever, there are cases where a single constituent appears to\\nyield more than one contribution to the meaning of an utterance.\\nThis is most obvious in, but is not limited to, sentences\\n involving coordination.  In example (), for instance, NAFTA is the object of two different verbs:\\n\\n\\nBill supported, and Hillary opposed, NAFTA.\\n   Since the hallmark of the linear logic approach is to ensure that\\nf-structure contributions are utilized exactly once in a\\nderivation, such constructions would at first glance appear to be\\nproblematic for the approach.\\n\\n\\nWe argue that the resource sharing that is commonly manifest in\\nthe treatment of coordination in other approaches is\\nappropriately handled by exploiting the structure-sharing in LFG\\nf-structures. We refine our previous analysis to account\\nfor cases where an f-structure is reached by multiple\\npaths from an enclosing f-structure.\\n\\n\\nDalrymple et al.  provides an account of LFG\\nsemantics that represents the meaning of lexical items with\\nlinear logic formulas.  These formulas manipulate basic\\nassertions of the form \\n\\n,\\nfor f-structures f and meaning logic terms M.  Here\\n\\nis a mapping, the semantic projection, that\\nrelates f-structures to semantic structures. To\\ndistinguish between multiple paths entering an\\nf-structure, we now take \\nto map from sets of\\npaths in f-structures to semantic structures.  Further,\\nthe paths between f-structures are made available in the\\nsemantic space as resources.  This makes it possible for the\\nsemantic formulas to exploit information about the multiple paths\\ninto an f-structure in order to account for the multiple\\nuses of the f-structure's semantic contribution.  The\\nresulting system is sufficiently restricted in cases where other\\napproaches overgenerate; the very property of\\nresource-sensitivity for which resource sharing appears to be\\nproblematic actually provides explanatory advantages over systems\\nthat more freely replicate resources during derivation.\\n\\n\\n In Section , we review previous approaches to the semantics of coordination and argument sharing, and make note of some\\nof their drawbacks.  We describe the revised semantic framework in\\n Section , and work through several examples of non-constituent coordination (specifically, right-node raising) in\\n Section .  We discuss examples involving intensional  verbs in Section . \\n\\n\\n    Previous Work\\n\\n  Combinatory Categorial Grammar \\n\\nSteedman\\n,\\nworking in the framework of Combinatory Categorial Grammar (CCG),\\npresents what is probably the most adequate analysis of\\nnon-constituent coordination to date.  As noted by Steedman and\\ndiscussed by Oehrle , the addition of the rule\\nof function composition to the inventory of syntactic rules in\\nCategorial Grammar enables the formation of constituents with\\nright-peripheral gaps, providing a basis for a clean treatment of\\ncases of right node raising as exemplified by sentence\\n ().  Such examples are handled by a coordination schema which allows like categories to be conjoined, shown in\\n (). \\n\\n\\nCoordination: X  CONJ X \\n\\n\\nX\\n  This schema gives rise to various actual rules whose semantics depends\\non the number of arguments that the shared material takes.  For the\\ncases of RNR considered here, the rule has the form shown in\\n (). (coordination)\\n X/Y:F   CONJ:   X/Y:G  \\n\\nX/Y:\\n\\n\\n   The contraction from \\n\\n\\nand \\n\\n\\nto \\n\\n\\nin this rule allows for the single argument to be utilized\\ntwice.\\n\\n\\nAs noted by Hudson , however, not all\\nexamples of RNR involve coordinate structures:\\n\\n\\nCitizens who support, paraded against politicians who\\n oppose, two trade bills.  Obviously, such cases fall outside of the purview of the coordination\\nschema.  An analysis for this sentence is available in the CCG\\nframework by the addition of the xsubstitute combinator\\n(Steedman, p.c.), as defined in Steedman\\n.\\n([xsubstitute)\\nY/Z:G (X\\n\\nY)/Z:F \\n\\n\\nX/Z: \\n\\n\\nThe use of this combinator assimilates cases of noncoordinate RNR to\\ncases involving parasitic gaps.\\n\\n\\n While this approach has some drawbacks, we do not offer a competing analysis of  the syntax of sentences like (   ) here.  Rather, we seek an analysis of RNR (and of resource sharing in general) that is\\nuniform in the semantics; such a treatment isn't available in CCG\\nbecause of its tight integration between syntax and semantics.\\n\\n\\n    Partee and Rooth\\n\\n\\nPerhaps the most influential and widely-adopted semantic treatment of\\ncoordination is the approach of Partee and Rooth\\n.\\nThey propose a generalized conjunction scheme in which conjuncts of\\nthe same type can be combined.  As is the case with Steedman's\\noperators, contraction inherent in the schema allows for a single\\nshared argument to be distributed as an argument of each conjunct.\\nType-lifting is allowed to produce like types when necessary; the\\ncombination of the coordination scheme and type-lifting can have the\\neffect of `copying' an argument of higher type, such as a quantifier\\nin the case of coordinated intensional verbs.  They propose a\\n`processing strategy' requiring that expressions are interpreted at\\nthe lowest possible type, with type-raising taking place only where\\nnecessary.\\n\\n\\nTo illustrate, Partee and Rooth assume that extensional verbs such as\\nfind are entered in the lexicon with basic type \\n\\n,\\nwhereas intensional verbs like want, which require a quantifier as an argument, have type \\n\\n(ignoring intensionality).  Two\\nextensional verbs such as find and support are\\ncoordinated at their basic types:\\n\\n\\nfind and support (type \\n\\n):\\n\\n\\n\\n\\nTwo intensional verbs such as want and seek are also\\ncoordinated at their basic (higher) types:\\n\\n\\nwant and seek (type \\n\\n):\\n\\n\\n\\n\\nThe argument to this expression is a quantified NP.  When an\\nintensional and an extensional verb are coordinated, the extensional\\nverb must be type-raised to promote it to the type of the intensional\\nverb:\\n\\n\\nwant and find (type \\n\\n):\\n\\n\\n\\n\\nAgain, this leads to the desired result.  However, an unwelcome\\nconsequence of this approach, which appears to have gone unnoticed in\\nthe literature, arises in cases in which more than two verbs are\\nconjoined.  If an intensional verb is coordinated with more than one\\nextensional verb, a copy of the quantifier will be\\ndistributed to each verb in the coordinate structure.  For instance,\\n in (), two extensional verbs and an intensional verb are coordinated.\\n  want, find, and support: \\n\\n\\n\\n\\nApplication of this expression to a quantifier results in two\\nquantifiers being scoped separately over the extensional verbs.  This\\nis the wrong result; in a sentence such as Hillary wanted, found,\\nand supported two candidates, the desired result is where one\\nquantifier scopes over both extensional verbs (that is, Hillary found and\\nsupported the same two candidates), just as in the case where all the\\nverbs are extensional.  Further, there does not seem to be an obvious\\nway to modify the Partee and Rooth proposal so as to produce the\\ncorrect result, the problem being that the ability to copy\\nquantifiers inherent in their schema is too unrestricted.\\n\\n\\nA second problem with the account is that, as with Steedman's\\ncoordination schema, Partee and Rooth's type-raising strategy only\\napplies to coordinate structures.  However, the need to type-raise\\nextends to cases not involving coordination, as in sentence\\n (). \\n\\n\\n Citizens who seek, paraded against politicians who have, a decent health insurance policy.\\n\\n\\nWe will present an analysis that preserves the intuition underlying Partee\\nand Rooth's processing strategy, but that predicts and generates the\\n correct reading for cases such as ().  Furthermore, the account applies equally to examples not involving coordination, as is\\n the case in sentence (). \\n\\n\\n\\n    LFG and Linear Logic\\n\\n\\nLFG assumes two syntactic levels of representation: constituent\\n structure (c-structure) encodes phrasal dominance and precedence relations, and functional structure (f-structure)\\nencodes syntactic predicate-argument structure.  The f-structure for\\n sentence () is given in (): \\n\\n\\nLexical entries specify syntactic constraints on f-structures as well\\nas semantic information:\\n\\n\\nSemantic information is expressed in (1) a meaning\\nlanguage and (2) a language for assembling meanings, or glue\\nlanguage.  The meaning language could be that of any appropriate\\nlogic; for present purposes, higher-order logic will suffice.\\nExpressions of the meaning language (such as Bill) appear on the\\nright side of the meaning relation .\\n\\n\\nThe glue language is the tensor fragment of linear\\n logic . The semantic contribution of each lexical entry, which we will refer to as a meaning constructor,\\nis a linear-logic formula consisting of instructions in the glue\\nlanguage for combining the meanings of the lexical entry's syntactic\\narguments to obtain the meaning of the f-structure headed by the\\nentry.  For instance, the meaning constructor for the verb supported is a glue language formula paraphrasable as: ``If my \\nSUBJ means X and ()\\nmy  OBJ means Y, then\\n(\\n\\n)\\nmy sentence means \\n\\n''.\\n\\n\\nIn the system described in Dalrymple et al. ,\\nthe \\nrelation associates expressions in the meaning\\nlanguage with f-structures.  As a result, each\\nf-structure contributed a single meaning constructor as a\\nresource to be used in a derivation.  Because linear logic does\\nnot have any form of logical contraction (as is inherent in the\\napproaches discussed earlier), cases where resources are shared\\nappear to be problematic in this framework.  Intuitively,\\nhowever, the need for the multiple use of an f-structure\\nmeaning results not from the appearance of a particular lexical\\nitem (e.g., a conjunction) or a particular syntactic construction\\n(e.g., parasitic gap constructions), but instead results from\\nmultiple paths to it from within the f-structure that\\ncontains it, where structure sharing is motivated on syntactic\\ngrounds.  We therefore revise the earlier framework to model what\\nwe will term occurrences of f-structures as\\nresources explicitly in the logic.\\n\\n\\nF-structures can mathematically be regarded as (finite)\\nfunctions from a set of attributes to a set of atomic values,\\nsemantic forms and (recursively) f-structures. We will\\nidentify an occurrence of an f-structure with a path (from\\nthe root) to that occurrence; sets of occurrences of an\\nf-structure can therefore be identified with path sets in\\nthe f-structure. We take, then, the domain of the projection to be path sets in the root f-structure.  Only\\nthose path sets S are considered which satisfy the property\\nthat the extensions of each path in S are identical.  Therefore\\nthe f-structure reached by each of these paths is\\nidentical. Hence from a path set S, we can read off an\\nf-structure Sf.  In the examples discussed in Dalrymple et\\nal.  there is a one-to-one correspondence\\nbetween the set of path sets S and the set of\\nf-structures Sf picked out by such path sets, so the\\ntwo methods yield the same predictions for those cases.\\n\\n\\nRelations between path sets are represented explicitly as\\nresources in the logic by R-relations.  R-relations\\nare represented as three-place predicates of the form\\n\\n\\nwhich indicate that (the path set) Gappears at the end of a path P (of length 1) extending (the\\npath set) F. That is, the f-structure  Gf appears at\\nthe end of the singleton path P in the f-structure\\nFf.  For example, the f-structure given in\\n () results in two R-relations: \\n\\n\\n\\n\\n\\nBecause f and g represent path sets entering an\\nf-structure that they label, R-relation (i)\\nindicates that the set of paths \\n\\n\\n\\n\\n    Examples\\n\\n  RNR with Coordination \\n\\nFirst we consider the derivation of the basic case of right-node raising\\n (RNR) illustrated in sentence (), repeated in  (). \\n\\n\\n ) is shown in  (   ). \\n\\n\\n\\n\\n\\nThe meaning constructors contributed by the lexical items are as\\nfollows:\\n\\n\\n\\n\\n\\nHere, we treat and as a binary relation.  This suffices for this\\nexample, but in general we will have to allow for cases where more\\nthan two constituents are conjoined.  Therefore, a second meaning\\nconstructor and2 is also contributed by the appearance of and, prefixed with the linear logic operator `!', so that it may be\\nused as many times as necessary (and possibly not at all, as is the\\ncase in this example).\\n\\n\\nThe R-relations resulting from the feature-value relationships\\n manifest in the f-structure in () are: \\n\\n\\n\\n\\n\\nThere are several equivalent derivation orders; here we step through\\n one.  Using the meanings for Bill, supported, Hillary, and opposed, R-relations (iii) and (v), and\\nAxiom I, we can derive meanings for Bill supported and\\n Hillary opposed in the fashion described in Section : \\n\\n\\n\\n\\n\\nWe combine the antecedents and consequents of the foregoing formulae to yield:\\n\\n\\n\\n\\n\\nConsuming the meaning of and and R-relations (i) and (ii), and\\nusing Axiom I, we derive:\\n\\n\\n\\n\\n\\nUsing Axiom I and  R-relations (iv) and (vi), the following\\nimplication can be derived:\\n\\n\\n\\n\\n\\nUsing these last two formulae, by transitivity we obtain:\\n\\n\\n\\n\\n\\nFinally, consuming the contribution of NAFTA, by universal\\ninstantiation and modus\\nponens we obtain a meaning for the whole sentence:\\n\\n\\n\\n\\n\\nAt this stage, all accountable resources have been consumed, and\\nthe deduction is complete.\\n\\n\\n  RNR with Coordination and Quantified NPs \\n\\n We now consider sentence (), where a quantified NP is shared. \\n\\n\\n\\n\\n\\nPartee and Rooth  observe,\\nand we agree, that the quantifier in such cases only scopes once,\\nresulting in the reading where Bill supported and Hillary opposed the\\n same two bills.  Our analysis predicts this fact in the same way as Partee and Rooth's analysis\\ndoes.\\n\\n\\nThe meanings contributed by the lexical items and f-structure\\ndependencies are the same as in the previous example, except for that\\nof the object NP.  Following Dalrymple et\\nal. , the meaning derived using the\\ncontributions from an f-structure h for two trade bills\\nis:\\n\\n\\n\\n\\n\\nThe derivation is just as before, up until the final step, where we\\nhave derived the formula labeled\\nbill-supported-and-hillary-opposed2.  This formula matches the\\nantecedent of the quantified NP meaning, so by universal instantiation\\nand modus ponens we derive:\\n\\n\\n\\n\\n\\nWith this derivation, there is only one quantifier meaning which\\nscopes over the meaning of the coordinated material.  A result where\\nthe quantifier meaning appears twice, scoping over each conjunct\\nseparately, is not available with the rules we have given thus far; we\\n return to this point in Section . \\n\\n\\nThe analysis readily extends to cases of noncoordinate RNR such as\\n example (), repeated as example  (). \\n  In our analysis, the f-structure for two trade bills is\\nresource-shared as the object of the two verbs, just as it is in the\\ncoordinated case.\\n\\n\\nSpace limitations preclude our going through the derivation; however,\\nit is straightforward given the semantic contributions of the lexical\\nitems and R-relations.  The fact that there is no coordination\\ninvolved has no bearing on the result, since the semantics of\\nresource-sharing is distinct from that of coordination in our\\nanalysis.  As previously noted, this separation is not possible in\\nCCG because of the tight integration between syntax\\nand semantics.  In LFG, the syntax/semantics interface is more loosely\\ncoupled, affording the flexibility to handle coordinated and\\nnon-coordinated cases of RNR uniformly in the semantics.  This also\\nallows for our semantics of coordination not to require schemas nor\\nentities of polymorphic type; our meaning of and is type \\n\\n.\\n\\n\\n\\n    Intensional Verbs\\n\\n\\nWe now return to consider cases involving intensional verbs.\\n The preferred reading for sentence (), in which only one quantifier scopes over the two extensional predicates, is\\nshown below:\\n\\n\\n\\n\\n\\n\\n\\n\\n The f-structure for example () is given in  (). \\n\\n\\nThe meaning constructors for the lexical items are given in Figure\\n .  Recall that a second meaning constructor and2 is introduced by and in order to handle cases where there are more\\nthan two conjuncts; this contribution will be used once in the\\n derivation of the meaning for sentence (). The following R-relations result from the f-structural\\nrelationships:\\n\\n\\n\\n\\n\\nFollowing the analysis given in Dalrymple et al. , the lexical entry for want takes a\\nquantified NP as an argument.  This requires that the quantified NP\\nmeaning be duplicated, since otherwise no readings result.  We provide\\na special rule for duplicating quantified NPs when necessary:\\n\\n\\n\\nIn the interest of space, again we only show a few steps of the derivation.\\nCombining the meanings for Hillary, found, supported, and and,\\nAxiom I, and R-relations (ii), (iii),\\n(v), (vi), (viii), and (ix), we can derive:\\n\\n\\n\\n\\n\\nWe duplicate the meaning of two candidates using QNP\\nDuplication, and combine one\\ncopy with the foregoing formula to yield:\\n\\n\\n\\n\\n\\nWe then combine the other meaning of two candidates with the\\nmeanings of Hillary and\\nwanted, and using Axiom I and R-relations (i), (iv), and\\n(vii) we obtain:\\n\\n\\n\\n\\n\\nFinally, using and2 with the two foregoing formulae, we deduce\\nthe desired result:\\n\\n\\n\\n\\n\\nWe can now specify a Partee and Rooth style processing strategy, which\\nis to prefer readings which require the least use of QNP duplication.\\nThis strategy predicts the readings generated for the examples in\\n Section .  It also predicts the desired reading for  sentence (), since that reading requires two quantifiers.  While the reading generated by Partee and Rooth is\\nderivable, it requires three quantifiers and thus uses QNP duplication\\ntwice, which is less preferred than the reading requiring two\\nquantifiers which uses QNP duplication once.  Also, it allows some\\nflexibility in cases where pragmatics strongly suggests that\\nquantifiers are copied and distributed for multiple extensional verbs;\\nunlike the Partee and Rooth account, this would apply equally to the\\ncase where there are also intensional verbs and the case where there\\nare not.  Finally, our account readily applies to cases of intensional\\n verbs without coordination as in example (), since it applies more generally to cases of resource sharing.\\n\\n\\n    Conclusions and Future Work\\n\\n\\nWe have given an account of resource sharing in the syntax/semantics\\ninterface of LFG.  The multiple use of semantic contributions results\\nfrom viewing dependencies in f-structures as resources; in this way\\nthe one-to-one correspondence between f-structure relations and\\nmeanings is maintained.  The resulting account does not suffer from\\novergeneration inherent in other approaches, and applies equally to\\ncases of resource sharing that do not involve coordination.\\nFurthermore, it lends itself readily to an extension for the\\nintensional verb case that has advantages over the widely-assumed\\naccount of Partee and Rooth .\\n\\n\\nHere we have separated the issue of arriving at the appropriate\\nf-structure in the syntax from the issue of deriving the correct\\nsemantics from the f-structure.  We have argued that this is the\\ncorrect distinction to make, and have given a treatment of the second\\nissue.  A treatment of the first issue will be articulated in a future\\nforum.\\n\\n\\n  Acknowledgements \\n\\nWe would like to thank Sam Bayer, John Maxwell, Fernando Pereira, Dick\\nOehrle, Stuart Shieber, and especially Ron Kaplan for helpful\\ndiscussion and comments.  The first author was supported in part by\\nNational Science Foundation Grant IRI-9009018, National Science\\nFoundation Grant IRI-9350192, and a grant from the Xerox Corporation.\\n\\nBibliography \\n\\nMary Dalrymple, Angie Hinrichs, John Lamping, and Vijay Saraswat.\\n1993a.\\nThe resource logic of complex predicate interpretation.\\nIn Proceedings of the 1993 Republic of China Computational\\n  Linguistics Conference (ROCLING), Hsitou National Park, Taiwan, September.\\n  Computational Linguistics Society of R.O.C.\\n\\n\\nMary Dalrymple, John Lamping, and Vijay Saraswat.\\n1993b.\\nLFG semantics via constraints.\\nIn Proceedings of the Sixth Meeting of the European ACL,\\n  University of Utrecht, April. European Chapter of the Association for\\n  Computational Linguistics.\\n\\n\\nMary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat.\\n1994a.\\nA deductive account of quantification in LFG.\\nIn Makoto Kanazawa, Christopher J. Pin, and Henriette\\n  de Swart, editors, Quantifiers, Deduction, and Context. Center for the\\n  Study of Language and Information, Stanford, California.\\nTo appear.\\n\\n\\nMary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat.\\n1994b.\\nIntensional verbs without type-raising or lexical ambiguity.\\nIn Conference on Information-Oriented Approaches to Logic,\\n  Language and Computation, Moraga, California. Saint Mary's College.\\n\\n\\nJ.-Y. Girard.\\n1987.\\nLinear logic.\\nTheoretical Computer Science, 45:1-102.\\n\\n\\nHerman Hendriks.\\n1993.\\nStudied Flexibility: Categories and Types in Syntax and\\n  Semantics.\\nILLC dissertation series 1993--5, University of Amsterdam,\\n  Amsterdam, Holland.\\n\\n\\nRichard A Hudson.\\n1976.\\nConjunction reduction, gapping, and right-node raising.\\nLanguage, 52(3):535-562.\\n\\n\\nRonald M. Kaplan and Joan Bresnan.\\n1982.\\nLexical-Functional Grammar: A formal system for grammatical\\n  representation.\\nIn Joan Bresnan, editor, The Mental Representation of\\n  Grammatical Relations, pages 173-281. The MIT Press, Cambridge, MA.\\n\\n\\nRichard T. Oehrle.\\n1990.\\nCategorial frameworks, coordination, and extraction.\\nIn Aaron Halpern, editor, Proceedings of the Ninth West Coast\\n  Conference on Formal Linguistics, pages 411-425.\\n\\n\\nBarbara Partee and Mats Rooth.\\n1983.\\nGeneralized conjunction and type ambiguity.\\nIn Bauerle, Schwarze, and von Stechow, editors, Meaning, Use,\\n  and Interpretation of Language, pages 361-383. de Gruyter.\\n\\n\\nMark J. Steedman.\\n1985.\\nDependency and coordination in the grammar of Dutch and English.\\nLanguage, 61:523-568.\\n\\n\\nMark J. Steedman.\\n1987.\\nCombinatory grammars and parasitic gaps.\\nNatural Language and Linguistic Theory, 5:403-439.\\n\\n\\nMark J. Steedman.\\n1989.\\nConstituency and coordination in a combinatory grammar.\\nIn Mark Baltin and Anthony Kroch, editors, Alternative\\n  Conceptions of Phrase Structure, pages 201-231. Chicago University Press.\\n\\n\\nMark J. Steedman.\\n1990.\\nGapping as constituent coordination.\\nLinguistics and Philosophy, 13(2):207-263.\\n\\nFootnotes\\n\\n   kehler@das.harvard.edu\\n   {dalrymple,lamping,saraswat}@parc.xerox.com\\n  We find two problems\\nwith the approach as it stands.  First, the intuition that one gap is\\n `parasitic' upon the other in cases like () is not strong, whereas the CCG analysis suggests an asymmetry between the two\\ngaps.  Second, the combinator appears to cause overgeneration.  While\\n it allows sentence (), it also allows sentence (b), where two trade bills is analyzed as the object of both\\nverbs:\\n[(b)]*Politicians who oppose, paraded\\nagainst, two trade bills.\\n  For discussion of c-structure\\nand its relation to f-structure, see, for example, Kaplan and Bresnan\\n.\\n  We\\ntreat the  CONJ features as unordered, as they are in the\\nf-structure set.\\n  In the interest of space, we will skip some intermediate\\nsteps in the derivation.\\n  We therefore disagree with Hendricks\\n, who claims that such sentences\\nreadily allow a reading involving four trade bills.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe argue that the resource sharing that is commonly manifest\\n  in semantic accounts of coordination is instead appropriately\\n  handled in terms of structure-sharing in LFG\\n  f-structures.  We provide an extension to the previous\\n   account of LFG semantics  according to which   dependencies between f-structures are viewed as\\n  resources; as a result a one-to-one correspondence between uses\\n  of f-structures and meanings is maintained.  The\\n  resulting system is sufficiently restricted in cases where\\n  other approaches overgenerate; the very property of\\n  resource-sensitivity for which resource sharing appears to be\\n  problematic actually provides explanatory advantages over\\n  systems that more freely replicate resources during derivation.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nIn the past years, multiple inheritance has been\\nincreasingly used for the  description of natural languages.\\nSome examples are the work in Head-driven Phrase Structure Grammar\\n( HPSG) on the structure of the lexicon\\n ,,  semantic sorts for selectional restrictions , derivational morphology\\n ,,  and the syntax of English relative clauses .  As an example of this type of analysis, figure  shows a recent  HPSG description of English clauses.\\n\\n\\nIn the example, relative\\nclauses are cross-classified according to two ``dimensions'';\\non the one hand according to the Phrase-Type (Headed or Non-Headed),\\nand on the other hand according to the Clause-Type (Interrogative,\\nDeclarative, or Relative). The choices within one dimension are\\nmutually exclusive: no structure can be described as both Headed and\\nNon-Headed, or as more than one of {Int, Decl, Rel}. However, a\\nstructure can be assigned types from different dimensions, without\\nthe need for a subtype that inherits from both dimensions.\\n\\n\\n In  section , we present a concept of typing, which provides direct support for\\nmulti-dimensional inheritance, and compare it to the type hierarchies\\nin Carpenter's typed feature logic.\\n Section  shows that unification in multi-dimensional inheritance hierarchies can be implemented\\nefficiently as unification of a Prolog term representation of\\n the types. Section  applies multi-dimensional inheritance to the problem of systemic classification.\\n\\n\\n   Multi-Dimensional Inheritance\\n\\n\\nWe follow the Carpenter's formalisation of typed feature logic, but\\nmodify the conception of the type hierarchy. In Carpenter's logic\\n , the type hierarchy is required to be a bounded complete partial\\norder ( BCPO), which means that any two types which do\\nhave a common subtype\\nmust have a unique most general common subtype. A type hierarchy\\n as in figure  fails this requirement because the types Headed-Ph and Rel have two common subtypes, but none of them\\nis more general than the other. In order to make the type\\nhierarchy a  BCPO, additional types must be introduced,\\n resulting in a hierarchy like the one in figure . In Carpenter's system, every feature structure has only one unique\\nmost general type, so that it is not possible to assign a feature\\nstructure two types neither of which subsumes the other unless\\nthey have a common subtype.\\n\\n\\n Carpenter  describes a conjunctive type construction by which\\n a type hierarchy like the one in figure  can be converted into a bounded complete partial order like in figure\\n . Once this is done, efficient algorithms for the  calculation of greatest lower bounds can be used . We argue that such a conjunctive type construction is neither necessary\\nfor theoretical reasons nor for reasons of efficient implementation.\\n\\n\\nIn our system, a feature structure can have different types\\nas long as they are chosen from different dimensions. Our syntax\\n for subtype declarations, given in (), combines information about subtyping and disjointness. All the Yi are subtypes of X,\\nand all Yi are disjoint.\\n\\n\\n\\n\\n\\nMulti-dimensional inheritance\\narises in the case where there is more\\nthan one declaration with the same supertype on the left-hand side,\\n as in  (). \\n\\n\\nX  >  [Y_{1.1},...,Y_{1.n}]. \\\\\\\\\\n\\\\vdots  \\\\vdots    \\\\vdots       \\\\nonumber \\\\\\\\\\nX  >  [Y_{m.1},\\\\ldots,Y_{m.k}]. \\\\nonumber\\n\\\\end{eqnarray} -->\\n\\n\\nInstead of writing a separate subtype declaration for each dimension,\\nmultiple dimensions are conventionally connected with the product\\n operator *, as in declaration (), which is equivalent  to the declarations in (). \\n\\n\\n\\n\\n\\nMultiple inheritance is the case where some Y type occurs\\nin the right-hand side of more than one type declaration. In this case,\\nthe type has several supertypes, which must be chosen from different\\ndimensions in order to be consistent with each other.\\n\\n\\nWe now turn to the semantics of type declarations.\\nThe denotation of each type is a subset of the domain. The semantics\\n of the type declaration in () is given by the axioms (), which states that the denotation of any of the Yi is a subset of the\\n denotation of X, and (), which states that all the Yiare disjoint. No additional axioms are needed in case of multi-dimensional inheritance.\\n\\n\\n\\\\forall Y_i ([ \\\\hspace{-.5mm} [Y_i ] \\\\hspace{-.5mm} ] \\\\subseteq [ \\\\hspace{-.5mm} [X ] \\\\hspace{-.5mm} ])\\\\\\\\\\n\\\\forall Y_i \\\\forall Y_j (Y_i \\\\neq Y_j \\\\Rightarrow\\n   [ \\\\hspace{-.5mm} [Y_i ] \\\\hspace{-.5mm} ] \\\\cap [ \\\\hspace{-.5mm} [Y_j ] \\\\hspace{-.5mm} ] = \\\\emptyset)\\n\\\\end{eqnarray} -->\\n\\n\\nOur system has an open-world semantics for type hierarchies.\\nA feature structure can be described by two types from different hierarchies,\\nbut there need not be a common subtype of these two types. This\\n is in contrast with a system like ALE  with a closed-world semantics\\nwhere the conjunction of two types is inconsistent\\nunless one subsumes the other or they have a common subtype.\\n\\n\\nOur notion of feature typing and appropriateness is based on\\nCarpenter's feature logic. Every feature is introduced for a\\nunique most general type, and is appropriate for all subtypes\\nof that type. In case of multiple inheritance, a type can\\ninherit different features from its supertypes in different dimensions.\\nA difference arises with type restrictions for feature values.\\nIn Carpenter's system, the value of a feature has one type as\\nthe type restriction, whereas in our system, the type restriction\\ncan be a conjunction of types from different dimensions.\\n\\n\\n In our system, the type hierarchy from figure  can be expressed  directly with the declarations given in figure . \\n\\n\\nOur notion of typing is similar to the one adopted\\nin the Comprehensive Unification Formalism ( CUF\\n ), in that it adopts an open-world semantics, and two types are\\nconsidered as consistent unless they are explicitly declared to\\nbe disjoint.\\nHowever,  CUF allows to state more general type axioms\\nusing the full power of propositional logic. In our system, the\\ntype axioms are restricted to subtyping (which corresponds to\\nimplication) and disjointness. This restriction allows the\\nefficient compilation of multi-dimensional inheritance hierarchies\\nto Prolog terms, which will be described in the next section.\\n\\n\\n   Compilation into a Prolog Term Representation\\n\\n\\nMulti-dimensional type hierarchies have the favourable property that\\nthe types can be compiled to a Prolog term representation. With this\\nrepresentation, Prolog's built-in term unification is all that is\\nrequired to compute the conjunction of two types. The Prolog term\\nrepresentation given here builds on and extends the representation\\n introduced by Mellish , and used  in the Core Langauge Engine  and the ALEP grammar  development system ,. \\n\\n\\nWe start out by describing how the translation of\\ntype hierarchies into Prolog terms works, and then give an\\nexample. The translation to terms must be able to handle different\\ndimensions of typing, mutually exclusive choices in a dimension,\\nsubtyping, multiple inheritance, features, and equality.\\n\\n\\nDifferent dimensions:\\nEach dimension occupies a different\\nargument position in the resulting term representation, so that\\ninformation\\n       from different dimensions can be combined by unification.\\n\\n\\nMutually exclusive types:\\nMutually exclusive types in the same\\n      dimension have different functors at the same argument position,\\n      so that their unification fails.\\n\\n\\nSubtype:\\nThe term which corresponds to the subtype is a further\\n      instantiation of the term corresponding to its supertype.\\n\\n\\nMultiple inheritance:\\nThe term which corresponds to the subtype\\n       is a further instantiation of the unification of the terms which\\n       correspond to its supertypes.\\n\\n\\nFeature:\\nThe term representation has an argument position\\n       for each feature\\n       introduced for a type. If a feature is introduced for a subtype,\\n       then an argument position is provided in that argument which\\n       further instantiates the supertype.\\n  Equality:\\nIn order to be able to distinguish structures that\\n       are identical from those which just happen to have the same\\n       value (i.e. their term representation is instantiated to the\\n       same ground term), an extra variable is introduced in the term\\n       representation\\n       (preventing instantiation to a ground term),\\n       which is only equal for two structures if they have been made\\n       identical by unification.\\n\\n\\nGiven this kind of representation, two typed feature structures can\\nbe unified simply by unification of the corresponding Prolog terms.\\n In figure , we provide an encoding of the types in the  hierarchy from figure  by Prolog terms. For simplicity, we leave out the argument position used to establish the equality\\nof feature structures, and any argument positions used to encode\\nfeatures. Note that the type Su-Wh-Rel makes a choice in both dimensions.\\n\\n\\nIf the type Su-Wh-Rel has any subtypes, a choice must be made which of\\nthe two occurences of su_wh_rel in the Prolog terms should\\nget argument positions for carrying this information.\\n  We always choose the leftmost occurence in a term for representing\\nsubtypes (and features). Further occurences then only serve to\\nmake a choice in a particular dimension of the hierarchy, and for that\\npurpose, an\\natom which is distinct from other terms that can occur as alternatives\\nin the same dimension is sufficient.\\n\\n\\n   Application to Systemic Classification\\n\\n\\nIn this section, we apply the conception of multi-dimensional\\ninheritance to systemic classification networks, which have been\\n discussed in . \\n\\n\\nSystemic classification networks\\nare an interesting formalism for such an encoding because they\\n offer considerable expressive power. Figure  shows a systemic classification network for\\n English pronouns taken from . \\n\\n\\n Figure  shows the connectives, and the translation of the first three in our system. The\\nfinal connective (disjunctive entry condition) has no simple\\ntranslation. This is not surprising, given the complexity\\n analysis by Brew  who shows systemic classification to be NP-hard by giving an encoding of the 3SAT-problem in a\\nsystemic network with disjunctive entry conditions.\\n\\n\\nWe treat disjunctive entry conditions by introduction of new\\ntypes into the hierarchy. For each type X at the right-hand side\\nof a disjunctive entry condition, we introduce two new types,\\nX' for the original type, and \\nfor its negation. These\\npairs of new types are introduced in different dimensions at\\nthe top of the choice system containing the disjunctive\\nentry conditions. All types Y which have subtypes\\nthat are on the left-hand side of the disjuntive entry condition\\nfor X become subtypes of X', and all other types become\\nsubtypes of .\\nIn effect, this is an expansion of the\\ndisjunctive entry conditions to disjunctive normal form.\\n\\n\\nIn the worst case, this method can lead to translations which\\nare exponentially larger than the original classification network,\\nas for the 3SAT problem.\\nWe show by an example that this need not be the case in practice\\nby converting the classification of pronouns\\n given in figure  into our system (figure ). \\n\\n\\nIn this case, our translation is more efficient than the\\n``brute-force translation''\\n described in  for networks with disjunctive entry conditions. In Winograd's systemic network, there are 54 possibilities\\n(6 for interrogative pronouns, 36 for personal pronouns, and\\n4 for demonstrative pronouns), resulting in a brute-force\\ntranslation with 55 arguments.\\n\\n\\nIn contrast, in our type system, the Prolog term translation has at\\nmost 8 nodes in the worst case, as in the following term which\\nencodes the subjective case masculine (third person singular)\\npersonal pronoun.\\n\\n\\n\\n\\n\\n  Conclusion \\n\\nWe have presented a concept of inheritance which provides direct support\\nfor current linguistic descriptions making use of\\n``cross-classification'', and can be compiled into an efficient Prolog\\nterm representation.\\n\\n\\nGiven the need for multiple dimensions in lingistic descriptions, we\\nbelieve that multi-dimensional type hierarchies will remain important\\neven when their compilation into Prolog terms is not needed any longer\\nbecause unification of typed feature terms will be built-in in future logic\\nprogramming languages.\\n\\n\\nFor the time being, however, the combination of multi-dimensional\\ninheritance and compilation into Prolog terms appears to give both the\\nefficiency and the expressive power needed to develop larger-scale\\ngrammars and lexicons, and use existing Prolog-based technology\\n(DCG parsers, left-corner, head-corner, or chart parsers,\\nsemantic-head driven or tabular generators) to build\\nNLP systems. Such an approach can benefit from all the advantages\\nof modern Prolog compilers (indexing, coroutining facilities,\\nmodule systems etc.) that would need considerable effort to\\nduplicate in a dedicated grammar formalism.\\n\\n\\nThe multi-dimensional inheritance described in this paper is\\n implemented in the system ProFIT , which translates programs containing\\ntyped feature terms to ordinary Prolog programs.\\n\\nBibliography \\n\\nHassan At-Kaci and Patrick Lincoln.\\n LIFE, a natural language for natural language.\\nT. A. Informations, 30(1-2):37 - 67, 1989.\\n\\n\\nHassan At-Kaci and Roger Nasr.\\n LOGIN: A logical programming language with built-in\\n  inheritance.\\nIn Proceedings of the 13th ACM Symposium an Principles of\\n  Programming Languages, pages 219-228, St. Petersburg, Florida, 1986.\\n\\n\\nHassan At-Kaci and Andreas Podelski.\\nTowards a meaning of  LIFE.\\nTechnical Report 11, DEC Paris Research Laboratory, Paris, June 1991.\\n\\n\\nH. Alshawi, D. J. Arnold, R. Backofen, D. M. Carter, J. Lindop, K. Netter,\\n  J. Tsujii, and H. Uszkoreit.\\nEurotra 6/1: Rule formalism and virtual machine design study. final\\n  report.\\nTechnical report, SRI International, Cambridge, 1991.\\n\\n\\nHiyan Alshawi, editor.\\nThe Core Language Engine.\\nMIT Press, 1991.\\n\\n\\nBIM-SEMA.\\nALEP System Documentation: The ALEP Linguistic Subsystem,\\n  Version 1.0.\\nCommission of the European Communities, March 1993.\\n\\n\\nChris Brew.\\nSystemic classification and its efficiency.\\nComputational Linguistics, 17(4):375 - 408, 1991.\\n\\n\\nBob Carpenter.\\nThe logic of typed feature structures.\\nCambridge Tracts in Theoretical Computer Science. Cambridge\\n  University Press, Cambridge, 1992.\\n\\n\\nBob Carpenter.\\nALE Version : User Manual.\\nUniversity of Pittsburgh, 1993.\\n\\n\\nMichael Covington.\\nGULP 2.0: an extension of Prolog for unification-based grammar.\\nTechnical Report AI-1989-01, Advanced Computational Methods Center,\\n  University of Georgia, 1989.\\n\\n\\nJochen Drre and Michael Dorna.\\nCUF - A formalism for linguistic knowledge representation.\\nIn Jochen Drre, editor, Computational Aspects of\\n  Constraint-Based Linguistic Description. Deliverable R1.2.A. DYANA-2 -\\n  ESPRIT Basic Research Project 6852, 1993.\\n\\n\\nGregor Erbach.\\nProFIT User's Manual - Version 1.05.\\nUniversitt des Saarlandes, Saarbrcken, July 1994.\\n\\n\\nDan Flickinger and John Nerbonne.\\nInheritance and complementation: a case study of easy adjectives and\\n  related nouns.\\nComputational Linguistics, 18(3):269 - 309, 1992.\\n\\n\\nSusan Beth Hirsh.\\nP-PATR: A compiler for unification-based grammars.\\nMaster's thesis, Stanford University, Stanford, CA, December 1986.\\n\\n\\nHans-Ulrich Krieger and John Nerbonne.\\nFeature-based inheritance networks for computational lexicons.\\nIn Ted Briscoe, Anne Copestake, and Valeria de Paiva, editors,   Default Inheritance within Unification-Based Approaches to the Lexicon.\\n  Cambridge University Press, 1992.\\n\\n\\nChristopher S. Mellish.\\nImplementing systemic classification by unification.\\nComputational Linguistics, 14(1):40-51, 1988.\\n\\n\\nChristopher S. Mellish.\\nTerm-encodable description spaces.\\nIn D. R. Brough, editor, Logic Programming: New Frontiers,\\n  pages 189 - 207. Intellect, Oxford, 1992.\\n\\n\\nM. Andrew Moshier.\\nA rational reconstruction of the domain of feature structures.\\nUniversitt des Saarlandes, Saarbrcken, 1994.\\n\\n\\nCarl J. Pollard and Ivan A. Sag.\\nInformation-Based Syntax and Semantics. Volume 1: Fundamentals.\\nCSLI Lecture Notes. Center for the Study of Language and Information,\\n  Stanford, CA, 1987.\\n\\n\\nSusanne Riehemann.\\nWord formation in lexical type hierarchies: A case study of   bar-adjectives in german.\\nMaster's thesis, University of Tbingen, Department of\\n  Linguistics, 1992.\\n\\n\\nIvan Sag.\\nRelative clauses: A multiple inheritance analysis.\\nStanford, 1994.\\n\\n\\nAndreas P. Schter.\\nCompiling feature structures into terms: A case study in Prolog.\\nTechnical Report RP-55, University of Edinburgh, Centre for Cognitive\\n  Science, 1993.\\n\\n\\nTerry Winograd.\\nLanguage as a Cognitive Process, volume I: Syntax.\\nAddison Wesley, Reading, MA, 1983.\\n\\nFootnotes\\n\\n  This work was supported by the Deutsche Forschungsgemeinschaft,\\nSpecial Research Division 314 ``Artificial Intelligence - Knowledge-Based\\nSystems'' through project N3 ``Bidirectional Linguistic Deduction'' (BiLD),\\nthe Commission of the European Communities through the project\\nLRE-61-061 ``Reusable Grammatical Resources,'' and Cray Systems.\\nPart of the work was done during a visit at the Human Communication\\nResearch Centre, University of Edinburgh.\\nI would like to thank the members of the Language Technology Group at\\nHCRC, Chris Brew, Suresh Manandhar, Drew Moshier\\nand Hans Uszkoreit for their comments.\\n  This figure is taken from a slide presented by Ivan Sag\\n          at the 1993 EACL meeting in Utrecht. The use of horizontal\\n          lines in the hierarchy to indicate a split into several\\n          dimensions has been adopted from the book on the Core\\n           Language Engine . \\n  A similar point holds for the type system of the logic\\n          programming language  LIFE\\n           ,, with the           difference that the unification of two types need not\\n          have a unique result;\\n          the unification of the types Headed-Ph\\n          and Rel would create a choice point and\\n          produce the two alternative solutions\\n          Su-Wh-Rel and That-Rel.\\n  We ignore the case where one of the supertypes subsumes the\\n          other since such declarations are redundant.\\n  The compilation of feature structures to Prolog terms has\\n           been described in ,,,           but these works assume untyped feature structures. One may\\n          wonder what the difference is between features and multiple\\n          typing dimensions, since they have very similar term\\n          representations. A technical answer would be that equality\\n          constraints (coreferences) can be stated over feature values,\\n          but not over type dimensions. Work by Moshier\\n            on a rational           reconstruction of typed feature structures in domain theory\\n          approaches the question from a more fundamental perspective.\\n  From the correctness of the translation, it would be no\\n          problem representing the subtypes in both occurences, but\\n          such an encoding is clearly redundant.\\n  The ``brute-force translation'' works by encoding n possibilities\\n          into a term with n+1 arguments, whose first and last\\n          argument are different (either instantiated to different atoms\\n          or related by an inequality constraint). For encoding a value\\n          that excludes the n[th] possilibity, the n[th] and\\n          n+1[st] argument position are unified. If a combination of\\n          values excludes all possibilities, then all arguments are\\n          unified with each other, including the first and the last,\\n          which are different, so that the unification fails.\\n          There is limited use for this type\\n          of translation to avoid creation of choice points in\\n          disjunctions with a small number of possible values. This\\n          can be regarded as a Prolog implementation of finite domains.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nIn this paper, we present an alternative approach to multiple inheritance\\nfor typed feature structures.\\nIn our\\napproach, a feature structure can be associated with several types coming\\nfrom different hierarchies (dimensions). In case of multiple inheritance, a\\ntype\\nhas supertypes from different hierarchies. We contrast this approach\\nwith approaches based on a single type hierarchy where a feature structure\\nhas only one unique most general type, and multiple inheritance\\ninvolves computation of greatest lower bounds in the hierarchy.\\nThe proposed approach supports current linguistic\\nanalyses in constraint-based formalisms like HPSG, inheritance in\\nthe lexicon, and knowledge representation for NLP systems. Finally,\\nwe show that multi-dimensional inheritance hierarchies can be\\ncompiled into a Prolog term representation, which allows to compute\\nthe conjunction of two types efficiently by Prolog term unification.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nIn this paper, we present a new method for characterizing the\\ninterpretive possibilities generated by elliptical constructions in\\nnatural language.  Unlike previous analyses, which postulate ambiguity\\nof interpretation or derivation in the full clause source of the\\nellipsis, our analysis requires no such hidden ambiguity.  For\\nexample, the ambiguity typically characterized as enabling ``strict''\\nversus ``sloppy'' readings of elliptical constructions does not arise\\nfrom a corresponding ambiguity as to whether the pronoun in the\\nantecedent clause is given a strict or sloppy interpretation; instead,\\nthe ambiguity follows from the process of interpreting the elided\\nphrase on the basis of its unambiguous antecedent.  Further, the\\nanalysis follows relatively directly from an abstract statement of the\\nellipsis interpretation problem and applies to the interpretation of a\\nwide variety of elliptical constructions, including VP ellipsis, ``do\\nso'' and ``do it'' anaphora, gapping, stripping, and related\\nconstructions involving recovery of implicit relations such as\\n``only'' modification and cleft constructions.  It predicts correctly\\na wide range of interactions between ellipsis and other semantic\\nphenomena such as quantifier scope and bound anaphora.  Finally,\\nalthough the analysis itself is stated nonprocedurally, it admits of a\\ndirect computational method for generating interpretations.\\n\\n\\nThe analysis we present is intended to characterize the semantics of\\nconstructions involving ellipsis.  Many interesting issues arise\\nregarding the syntax of ellipsis, and we will touch on some of these\\nissues; our main goal, though, is to characterize a method for\\nellipsis interpretation.\\n\\n\\n  Basics \\n  An Abstract Statement of the Ellipsis Problem \\n\\nWe can provide an abstract and reasonably theory-neutral\\ncharacterization of ellipsis phenomena and their interpretation as\\nfollows.  An elliptical construction involves two phrases (usually\\nclauses) that are parallel in structure in some sense.  The antecedent\\nor source clause is complete, whereas the target clause is\\nmissing (or contains only vestiges of) material found\\novertly in the source.  As a concrete example, which we will use as\\nour primary source of data in the paper, consider the verb phrase (VP)\\nellipsis phenomenon, as in (1).\\n\\n\\n  Dan likes golf, and George does too. \\n\\n\\nThe sentence is interpreted as meaning that Dan and George\\nboth like golf.  The source clause, `Dan likes golf', parallels the\\ntarget `George does too', with the subjects `Dan' and `George' being\\nparallel elements, and the VP of the target sentence being vestigially\\n represented by the target phrase `does too'. \\n\\n\\nGiven this abstract view of ellipsis, the problem of ellipsis\\ninterpretation is just to recover a property of (or relation over) the\\nparallel element (respectively, elements) in the target that the missing or\\nvestigial material stands proxy for.  Of course, this property is not\\narbitrary.  We know that the application of the property or relation\\nto the parallel elements in the source constitutes the interpretation\\n of the source clause.  In example () above, we know then that the property P being predicated of George in the second\\nsentence is such that when it is predicated of Dan, it means that Dan\\nlikes golf.  We might state this equationally as follows:\\n\\n\\n\\n\\n\\nA possible value for P in this equation is the\\nproperty represented by the lambda term \\n\\n.\\nPredicating this property of George, we have\\n\\n\\nwhich reduces to\\n\\n.\\n\\n\\nIn general, then, the abstract problem of ellipsis can be stated as\\nthe problem of recovering solutions to the equation\\n\\n\\n\\n\\nwhere s1 through sn are the interpretations of the parallel\\nelements of the source, and s is the interpretation of the source\\nitself.  (The determination of the parallelism itself is a separate\\nproblem, about which more later.)  Once P is determined,\\n\\n\\nserves as the interpretation of the target,\\nwhere t1 through tn are the interpretations of the corresponding\\nparallel elements of the target.\\n\\n\\nNot only is this an abstract characterization of the ellipsis problem,\\nit is essentially the entire analysis proposed in this paper.  It\\nconstitutes an analysis because the equational statement of the\\nproblem, together with some reasonable assumptions, determines\\nrigorously the sets of interpretations for target clauses, which\\ninterpretations, we will see, correspond to the actual possible\\ninterpretations of the target.\\n\\n\\n  Previous Analyses of Ellipsis \\n\\nIt is important that ellipsis analyses (including the equational one\\noutlined above) allow for ambiguity in the target clause, that is, for\\na set of relations to be made available by the source clause.  The\\navailability of multiple relations is attested in various phenomena in\\nwhich the target clause has multiple readings; it can be seen most\\nclearly in the distinction between strict and sloppy readings.  In the\\nsentence\\n\\n\\n Dan likes his wife, and George does too. \\n\\n\\n(under the reading in which the pronoun `his' refers to Dan)\\nthe property predicated of George might be the property of liking\\nDan's wife or the property of liking one's own wife.  In lambda\\nnotation, these two properties are given by\\n\\n\\n\\n\\n\\nand\\n\\n\\n\\n\\n\\ncorresponding to the strict and sloppy readings of the\\n sentence, respectively. As we will see, the possibility of several available properties arises in\\nother cases as well.\\n\\n\\nMost previous analyses of ellipsis have allowed for the possibility of\\nmultiple available properties by arranging for the source clause to be\\nambiguous as to what property it makes available.  That is, in an\\nindividual instance, the source clause is interpreted in one of\\nseveral possible ways, leading to the use of a particular property in\\nthe interpretation; the property used in the target clause is\\nidentical to the corresponding property in the source clause.  Dahl\\n was the first to draw a distinction between\\napproaches that place the ambiguity in the source clause (which he\\ncalled strict identity approaches) and those that place the\\nambiguity in the process of recovering a property or relation for the\\ntarget (which he called sloppy or non-strict identity\\napproaches).  So as not to confuse the terminology here with that of\\nstrict and sloppy readings, we will call the former kind of analysis\\nan identity-of-relations analysis, as opposed to a non-identity\\nanalysis.\\n\\n\\nUnder an identity-of-relations analysis, ambiguity of interpretation\\nin a target clause comes about because the source clause is ambiguous.\\nHowever, only a single relation is available from the source clause in\\nany given instance.  The relation that the source clause makes\\navailable is, in most previous work, that associated with its VP,\\nthough we emphasize that this is not a necessary condition for an\\nidentity-of-relations analysis, nor do we believe it is a tenable\\nstance.\\n\\n\\nThe multiplicity of interpretations for the elided phrase in an\\nidentity-of-relations analysis may arise in various ways.  Purely\\n interpretive analyses , allow for multiple semantic interpretations arising from an unambiguous syntactic\\nanalysis of the source clause.  Partially interpretive analyses\\ninvolve either copying the syntactic tree from the source clause to\\nthe target but requiring identical semantic interpretations for the\\n two VPs , or deleting the phrase structure of the second tree under the constraint of identical interpretation\\n .  Finally, syntactic analyses may also allow the semantic ambiguity to arise from underspecification in the copied\\n constituent . \\n\\n\\nThe solutions therefore form a continuum, the ambiguity arising at more\\nor less superficial levels.  All of the analyses, however, share a\\nreliance on semantic ambiguity in the source clause.\\n\\n\\nOur solution to the question of what properties are made available can\\nbe seen as lying at the far end of this continuum.  We eschew not only\\nsyntactic ambiguity in the source clause, but semantic ambiguity\\narising from any source, as a generator of the multiple readings of\\nelliptical constructions.  Instead, multiple solutions come about as a\\nnatural result of directly stating the definition of the relation to\\nbe applied in the target clause.\\n\\n\\n  The New Analysis and an Example \\n\\nAs described earlier, the problem of extracting a relation from the\\nsource clause can be stated equationally as\\n\\n\\n\\n\\n\\nIn cases of VP ellipsis where the subjects of the source\\nand target are parallel, the equation is simply\\n\\n\\nP(s1) = s\\n\\n\\nwhere s1 is the interpretation of the subject of the source\\nclause.  By solving this equation for the unknown, P, we generate\\nthe relation (or relations, if multiple solutions exist) that the\\nresolution of the ellipsis requires.\\n\\n\\n Huet's higher-order unification algorithm  provides a means of completely enumerating representations of the\\nsolutions of such equations, under assumptions whose detailed\\n discussion we defer to Section . \\n\\n\\nAs an example of the use of equations to state a problem in ellipsis\\n resolution, consider () above, repeated here: \\n\\n\\nDan likes golf, and George does too.\\n\\n\\nRecall that `Dan' and `George' are the parallel elements in\\nthis example, and the semantic interpretation of `Dan likes golf' is\\n\\n\\n\\n\\n\\n\\nWe have underlined what we will call primary\\noccurrences of the parallel element's interpretation, for reasons to be\\nclarified later.  In this case, the single occurrence of dan is\\nprimary.  Any other occurrences will be referred to as secondary.\\n\\n\\nTo form an interpretation for the second conjunct, we require a\\nproperty P that, when applied to the interpretation of the subject\\nof the first conjunct, will yield the interpretation of the first\\nconjunct as a whole.  This property will serve to generate the\\ninterpretation of the target clause.  It will be applied to the\\ninterpretation of the parallel element, that of the subject `George',\\nin the second clause.\\n\\n\\nWe can state this requirement directly with the following equation, an\\n instance of the more general equation (): \\n\\n\\n\\n\\n\\n\\nThe latter term is the interpretation for the source sentence;\\nthe equation requires P to be a property that, when predicated of\\nthe subject interpretation dan, yields the first term.\\n\\n\\nA solution for an equation can be represented by a substitution of\\nvalues for the free variables in the equation that makes both sides of\\nthe equation identical. For example, the following two alternative\\n substitutions solve equation (): \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe first substitution will be disregarded because it leaves\\na primary occurrence in the result.  This constraint requiring abstraction of primary occurrences comes about because the parallel\\nelement in the target clause must play the primary role in the meaning\\nof the target.  We will have more to say about this in\\n Section , especially as regards the distinction between primary and secondary occurrences.  Given this\\nconstraint, the only remaining value for P is the property\\n\\n.\\nWe can now use this function as the\\ninterpretation of the elided VP in (1); with P(george) as the\\ninterpretation of the target clause, this gives the following\\n semantics for the sentence as a whole: \\n\\n\\n\\n\\n\\n\\nIn summary, our analysis of the abstract problem of ellipsis\\nresolution, that is, generating appropriate properties to be used in\\ninterpreting the target clause, is to state the problem equationally\\nbased on the parallel structure in the two clauses and to solve the\\nequation using higher-order unification (under the constraint\\nrequiring abstraction of primary occurrences).  The properties that\\nare generated as solutions to the equation are predicated of the\\nparallel elements in the target clause to generate the target clause\\ninterpretation.\\n\\n\\n  Strict and Sloppy Readings \\n\\nAs seen in the preceding example, the equation stating an ellipsis\\ninterpretation problem may have several alternative solutions, which\\nthe higher-order unification algorithm will generate.  Specifically,\\nwhen there are multiple occurrences of some subterm in a term,\\nmultiple alternative substitutions for the relation formed by\\n abstracting out that subterm will exist. Consider sentence (   ), repeated here: \\n\\n\\n Dan likes his wife, and George does too.   \\n\\n\\nLet us assume the following interpretation for the first\\nconjunct:\\n\\n\\n\\n\\n\\n\\nThe first occurrence of dan, which arises directly\\nfrom the parallel element, the subject `Dan', is primary; the\\noccurrence arising from the pronoun, which is not a parallel element,\\nis secondary.  Solution of the equation\\n\\n\\nby higher-order unification\\nyields four ways of forming a property by abstracting out the\\nsemantics of the subject.  The possible values for P are\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAgain, the first two solutions fail the constraint on\\nabstraction of primary occurrences.  Either of the other two remaining\\nproperties yields a possible interpretation of the target clause.  The\\nfirst gives rise to what has been called the strict reading of the\\nsecond conjunct, while the second gives rise to the sloppy reading:\\n\\n\\n\\n\\n\\n\\n(George likes Dan's wife.)\\n\\n\\n\\n\\n\\n\\n(George likes George's wife.)\\n\\n\\n  Constraints on Relation Formation \\n\\nAs illustrated in the foregoing examples, we use the distinction\\nbetween primary and secondary occurrences of a term to constrain the\\nacceptable solutions of an ellipsis interpretation equation.  We\\ndefine a primary occurrence as an occurrence of a subexpression in the\\nsemantic form directly associated with one of the parallel elements in\\nthe source clause; we assume that the notion of ``directly\\nassociated'' is sufficiently well-defined for the purposes at hand.\\nWe then require that the solution process preserve the primacy of\\noccurrences, with the consequence that solutions must abstract over\\nall primary occurrences in the source. In other words: Solutions\\nmust not include primary occurrences.  In\\n example (), this constraint removes from consideration the first two putative solutions\\n in (). \\n\\n\\nIf the constraint were not in force, the following readings would be\\nproduced for `... and George does too':\\n\\n\\n\\n... and Dan likes Dan's wife.\\n\\n\\n\\n... and Dan likes George's wife.\\n\\n\\nThese are just the readings where the parallelism between\\nthe clauses has been disregarded.  Thus, the constraint is a reflex of\\nthe inherent parallelism in elliptical constructions.\\n\\n\\nThe existence of this constraint means, not surprisingly, that it is\\nnecessary to retain a connection between the syntactic and the\\nsemantic representation of the source sentence.  By maintaining this\\nconnection, we can ensure that the solutions produced by higher-order\\nunification satisfy the constraint that parallelism must be maintained\\n by abstracting out of parallel positions. \\n\\n\\n  Constraints on Parallelism \\n\\nOne of the distinguishing features of our analysis is that the\\nellipsis resolution problem is separated into two subtasks: a prior\\ndetermination of the parallel structure of source and target, and\\nconsequent formation of the implicit relation to be used in the\\ntarget.  We have been addressing the latter subtask primarily, and\\nwill continue to do so, but we digress to mention some perhaps obvious\\nfacts about the parallelism determination that might get lost in the\\nsequel.\\n\\n\\nThe task of determining the parallel structure of two\\nclauses is far more subtle, and less syntactic, than a cursory\\nexamination exposes.  (We discuss this issue further in\\n Section .)  For this reason, the division of the ellipsis problem into two parts--separating parallelism determination\\nfrom relation formation--allows a simpler description of relation\\nformation and a more appropriate characterization of the problem of\\ndetermination of parallelism.  Nonetheless, this paper does not\\nprovide a theory of parallelism; previous attempts have been far too\\nrestrictive, limiting themselves to purely syntactic criteria.  The\\nwide range of possibilities for parallelism described in\\n Section  indicate that the process is not a purely linguistic one.  As an extreme example, we describe in\\n Section  cases of parallelism with no linguistic source whatsoever.\\n\\n\\nOur emphasis in this paper on issues in relation\\nformation and our liberal view of parallelism determination should\\nnot, of course, be taken to imply that no constraints apply to the\\ntask of determining parallelism.  For example, parallelism must\\n respect stativeness of verbs () and  pleonasticity of noun phrases (). \\n\\n\\n Dan likes golf and George is too.\\n\\nIt is raining and George is too.\\n\\n\\nDepth of embedding imposes constraints as well.\\n\\n\\n\\nThe mayor of Washington left, and New York did too.\\n\\n\\n(We ignore the nonsensical reading in which the city of New\\nYork was the agent of the leaving action.)  Similarly, the sentence\\n\\n\\nIt is obvious that Dan is happy, and George is too.\\n\\n\\n(pointed out to us by Mats Rooth) can only be interpreted as\\nmeaning that George is happy, not that it is obvious that George is\\nhappy.  If the obviousness is included, the parallelism would have to\\nhold between `George' and `it', not `Dan'.\\n\\n\\nSuch constraints hold not only in VP ellipsis, but also in gapping,\\nstripping, comparative deletion and other elliptical constructions.\\nThus, not all constraints on readings of elliptical sentences follow\\nfrom the relation formation issues that are the primary topic of this\\npaper.\\n\\n\\nFurthermore, there are syntactic constraints that apply differentially\\nto different ellipsis constructions.  Even among constructions eliding\\nVPs, such as the ``do'', ``do so'', and ``do too'' variants, syntactic\\ndistinctions can be found.  For instance, as noted by \\n, these variants differ in their\\ngrammaticality in antecedent-contained-ellipsis contexts:\\n\\n\\nJohn greeted every person that Bill did.\\n* John greeted every person that Bill did so.\\n? John greeted every person that Bill did too.\\n\\n\\nThese fine syntactic distinctions are not addressed by the\\npresent analysis, which attempts to make clear only the space of\\nsemantic interpretations.\\n\\n\\nOf course, not all elements in the target clause must be analyzed as\\nparallel to some element in the source.  For instance, adverbial\\nphrases can be viewed as modifying the target directly.  This\\n possibility is exemplified by the following sentence: \\n\\n\\nJim couldn't open the door, but Polly did with her blowtorch.\\n\\n\\nNo empty adverbial modifier need be posited in the source\\nclause; the instrumental modifies the target sentence directly.  (Such\\nelements can be made parallel in other cases, however; see\\n Section  for examples.) \\n\\n\\n    Formal Semantic Background\\n\\n\\nWe outline here the formal machinery underlying the semantic\\nanalyses used in this paper.\\n\\n\\nMeanings of phrases are to be represented by terms of a typed\\nhigher-order system with lambda abstraction. Since we are not\\nconcerned here with intensional phenomena, we will just need the basic\\ntypes e (entities) and t (truth values). Two type constructors\\nwill be used: \\n\\n\\nto form the type \\n\\n\\nof\\nfunctions mapping arguments of type T to results of type T', and\\n\\nto form the type \\n\\n\\nof pairs \\n\\n\\nsuch that thas type T and t' has type T'.\\n\\n\\nWe will use various elementary concepts from the lambda calculus,\\nspecifically the notions of free and bound occurrences of\\nvariables, and of substitution of a lambda term for a variable.  We\\nwill notate the substitution of term N for all free occurrences of\\nx in M by \\n\\n.\\nWe require, as is typical, that\\nsubstitution rename bound variables in M appropriately to avoid\\ncapture of free variables in N.  The reader is urged to refer to\\nHindley and Seldin HindleySeldin:Lambda for precise definitions\\nof these notions.\\n\\n\\nWe have proposed codifying the ellipsis interpretation problem using\\nexpressions equating terms that represent phrase meanings. What counts\\nas a solution to such an equation depends crucially on what notion of\\nequality between terms we are considering, or, in other words, on when\\nwe consider that two terms denote the same semantic object. One\\nsalient notion of equality is that of \\n\\n\\n interconvertibility , which captures formally the intuitive notion that two terms can represent the same\\n``recipe'' for calculating a function.  Specifically, two terms are\\nconsidered equal if one can be converted to the other by repeated\\napplication of the following rules and their inverses:\\n\\nconversion:\\nconvert \\n\\n\\nto\\n\\n,\\nthat is, the names of bound variables\\nare immaterial to their meaning. The two terms are said to be alphabetic variants.\\n\\nconversion:\\nconvert \\n\\n\\nto\\n\\n.\\nThis represents formally the operation of applying a\\nfunction to an argument.\\n\\nconversion:\\nconvert \\n\\n\\nto M when x does not occur free in M.\\n\\n  Interpretation of Ellipsis Resolution  Equations \\n\\nWe have claimed that the meaning of\\n\\n\\n Dan likes golf, and George does too. \\n\\n\\nis\\n\\n\\n\\n\\n\\n\\nwhere the equation\\n\\n\\n\\n\\n\\n\\nmust be satisfied.  It is not obvious that the equation in\\n0 is semantically interpretable, as opposed to being a recipe for\\ninvoking a formal procedure, higher-order unification, with no\\nunderlying meaning in and of itself.  Clearly, the combined meaning of\\n-1 and 0 is not equivalent to\\n\\n\\n\\n\\n\\nThis would merely require that P be such that it is true\\nof Dan if Dan likes golf.  Since the first conjunct states that Dan\\ndoes in fact like golf, P need only be a true property of Dan.  The\\nentire formula then requires only that George possess some property,\\nany property, that Dan possesses, which would give an incorrect\\ninterpretation for the target sentence.  The equation is not to be\\ninterpreted, then, as codenotation in a model.\\n\\n\\nInstead, we want ellipsis to be more content-independent, in that the\\nproperty should be such that the equation holds whether or not Dan\\nhappens to like golf.  It should be independent of the particulars of\\na given model, that is, it should hold in any model in a suitable\\nclass of models.  But we have to be careful about the choice of model\\nclass.  Even necessary truths should not codenote over the class of\\nmodels in which the ellipsis equation is interpreted.  Otherwise, the\\nsentence\\n\\n\\nEvery square has four sides, and every rhombus does too.\\n\\n\\nwould be subject to an interpretation where what is\\npredicated of every rhombus is some property of squares that is true\\nof them in whatever models assign squares four sides.  But these, ex hypothesi, include all the models, so any property true of all\\nsquares (such as having four equal angles) would do.  The sentence\\nmight mean, then, that every rhombus has four equal angles.  Of\\ncourse, it does not.  Similarly, logical tautologies must not be\\n valid; sentence () does not, for example, mean that George likes golf and either it is raining or it is not raining, even\\nthough this is logically equivalent to George liking golf.\\n\\n\\nThe class of models, then, in which the ellipsis equation is held as\\nvalid is very weak.  In fact, for higher-order unification to be an\\nappropriate procedure for determining valid instances, the valid\\nequations must be exactly those whose two sides are \\n\\n-interconvertible.  Higher-order unification finds the most\\ngeneral substitutions of terms for the free variables in an equation\\nthat make the equation valid.\\n\\n\\nFriedman:Equality demonstrates that such equalities are\\nexactly those that hold in all extensional models for the typed lambda\\ncalculus (without interpreted constants), and also exactly those that\\nhold in any model consisting of all the higher-order functions over\\nsome infinite base set, with application interpreted as function\\napplication.  Although the logic that we presuppose is augmented with\\na full first-order quantificational logic (and presumably, for\\nintensional phenomena, would be an intensional logic), recall that the\\ntautologies of this logic are not required to hold for the purposes of\\ninterpreting the ellipsis equations; the symbols of the logic\\n(,\\n,\\netc.) can be viewed as uninterpreted function\\nsymbols of the appropriate type.  The only structure that the model\\nmanifests is, then, the structure arising from the categorial or type\\nstructure of the language, together with the reasonable requirement of\\nextensionality.\\n\\n\\nThus, the semantic invariants in ellipsis resolution are those that\\nfollow from the type structure--the function-argument\\nrelationships--of natural language, and not from any contingent or\\neven necessary truths.  This accords with intuition, in that the\\nfelicity of ellipsis does not depend on the meaning of the words in\\nthe source sentence (though the elided property does), but does depend\\non their type structure.  An ellipsis equation is not merely a recipe\\nfor a syntactic process.  It has a meaning, but the meaning must be\\ntaken in a different, and much more profligate, model than that of the\\ninterpretation of the sentence itself.  The ellipsis equation reflects\\nsemantic facts about the sentence, but just at the gross level of\\nfunction-argument structure.\\n\\n\\nThere is one remaining problem, however, for this view of ellipsis\\nequations--the issue of primary occurrences.  The primary occurrence\\nnotation serves to couple the ellipsis equations to the choice of\\nparallel elements, and provides a way of forcing abstraction over the\\nmeanings of the parallel elements.  Intuitively, the distinction\\nbetween primary and secondary occurrences is clear: a primary\\noccurrence corresponds to a distinguished semantic role in the\\nsituations described by the source and target clauses.  At present,\\nhowever, we have no way of making precise the intuitions that led to\\nthe primary occurrence notation; that is, we know of no semantic\\ncorrelate to the equational system with primary occurrence notation\\nunder the related constraint on abstraction.  It remains for future\\nwork to reconstruct the semantical foundations of this variant of\\nhigher-order unification.  Although we have some ideas as to how such\\na reconstruction might proceed, it is premature to discuss them here.\\n\\n\\n  Higher-order unification \\n\\nThe unification problem for terms in a logical system is the problem\\nof finding substitutions for the free variables of two terms t and\\nt' that make the terms equal.  Such a substitution is called a unifier of t and t'. A unifier \\nis more general\\nthan another unifier \\nif there is a nontrivial substitution\\n\\nsuch that \\n\\n,\\nwhere \\nis\\nthe result of applying substitution \\nto term t. Informally,\\na more general unifier will leave more free variables, or make fewer\\nvariable identifications, than a less general one. Unifiers that are\\nmost general represent the solutions of a unification problem in their\\nsimplest form, since any less general unifier (solution) can be\\nobtained from the output of a most general one by additional\\nsubstitutions of terms for free variables. As is well known, the\\nunification problem for first-order terms, and the related unification\\nproblem for certain kinds of feature graphs, admit of unique most\\ngeneral unifiers (up to variable renaming). However, this is not the\\ncase for higher-order unification, in which variables can range over\\nfunctions of arbitrary order rather than just over [first-order]\\nindividuals. This multiplicity of unifiers corresponds to the\\npossibility of multiple alternative interpretations for elided\\nmaterial.\\n\\n\\nHuet's higher-order unification algorithm enumerates the unifiers of\\nhigher-order terms in a typed -calculus of the kind we are\\nusing. Because higher-order unification is in general undecidable,\\ngiven two terms, the algorithm will either stop without finding any\\nunifiers (the terms are not unifiable), generate successive [most\\ngeneral] unifiers (possibly without end), or run forever without\\nproducing any unifiers.  This computational property of higher-order\\nunification has not been problematic on the cases we have examined\\nthat are engendered by ellipsis resolution, however, and there are\\nseveral reasons why this might be so.\\n\\n\\nFirst, many of the equations arising in ellipsis resolution fall under\\nthe subcase called second-order matching.  In the second-order\\nsubcase of unification, variables range only over individuals and\\nfirst-order functions.  The simpler matching problem occurs when the\\nsubstitution need be applied only to one of the terms, that is, \\n\\n.\\nHuet and Lang HuetLang:SecondOrder use\\nsecond-order matching as a way of applying program transformations in\\na manner reminiscent of the method used for ellipsis interpretation in\\nthis paper.  Second-order matching is, fortunately, decidable, and\\nHuet and Lang provide an algorithm, which is an adaptation of Huet's\\nmore general algorithm for the subcase of interest.\\n\\n\\nFurthermore, many of the equations we will be interested in solving\\nare of the schematic form\\n\\n\\n\\n\\n\\nwhere the si are all ground, that is, contain no free\\nvariables.  The special case of second-order matching engendered by\\ninstances of this schema is computationally even more tractable.\\nThere are only a finite number of unifiers and these can be simply\\nconstructed as follows: Construct a term s from s0 by replacing\\nzero or more instances of the si by xi.  For each such s,\\nconstruct a possible binding for P given by \\n\\n.\\nClearly, there are at most 2[c] such unifiers\\nwhere c is the number of occurrences of the si in s0, and\\nthese can be enumerated efficiently (in time linear in the output\\nlength).\\n\\n\\nFinally, although certain phenomena require use of the more general\\nhigher-order unification (as the examples in\\n Section ), the bulk of the cases considered in this paper rely on the ground subcase of second-order matching, and\\nare therefore less computationally problematic.  Of course, even the\\nhigher-order cases we consider may turn out to fall into a\\ncomputationally reasonable subclass; further inquiry in this area\\nwould be useful.\\n\\n\\n    Interpretation of quantification and long-distance\\ndependencies\\n\\n\\nBefore characterizing the interaction between our analysis of ellipsis\\nand various other semantic phenomena, we must first lay out an\\napproach to semantic interpretation--quantifier scoping in\\nparticular--in which to couch the discussion.  For the most part, the\\nparticulars of the method for characterizing quantifier scoping are\\nrelatively unimportant; the analysis could be stated in terms of\\n Cooper storage, say, or even quantifier raising. We will use here a variation of a method for interpreting quantifier scoping and\\nlong-distance dependencies developed by Pereira\\nPereira:SemComp.  For those readers unfamiliar with this\\nmethod, we provide some examples later in this section.\\n\\n\\nIn general, the interpretation of a phrase will have the form\\n\\n\\nwhere \\nis a set of assumptions\\nanalogous to a quantifier store in the Cooper storage method\\n  and m is a matrix term in which free variables introduced by the assumptions in \\nmay occur.\\n\\n\\nThe assumptions used for quantifier scoping are triples of the form\\n\\n\\nwhere q is a determiner meaning, x is a\\nfree variable, and p is a term of type t in which x is\\n free. The assumption  \\n\\nis said\\nto introduce variable x.  A quantified noun phrase is\\ninterpreted as a variable introduced by an assumption whose first\\ncomponent is the meaning of the noun phrase's determiner and whose third\\ncomponent represents the meaning of the noun phrase's nominal.\\n\\n\\nInstead of the usual generalized quantifier type \\n\\n\\nfor determiner meanings,\\nwe will use the pair quantifier type \\n\\n.\\nIn other words, the meaning of a determiner takes a\\nfunction from entities to pairs of truth values and yields a\\ntruth value. For example, the meaning of every will be the\\nfunction that assigns `true' just to those functions that take each\\nentity to a pair whose second component is true whenever its first\\ncomponent is.\\n\\n\\nOur somewhat unusual type for determiner meanings, which is needed in\\nour analysis of antecedent-contained ellipsis\\n (Section ) can be understood as a lambda calculus implementation of some aspects of Discourse Representation\\n Theory (DRT) ,. Specifically, the interpretation of\\n\\n\\n John greeted every person. \\n\\n\\nwill be for us\\n\\n\\n\\n\\n\\n\\nwhich we will abbreviate as\\n\\n\\n\\n\\n\\nThis interpretation can be directly related to the discourse\\nrepresentation structure (DRS) for the same sentence:\\n\\n\\nThe discourse marker x corresponds to the variable to be abstracted,\\nthe left inner DRS to the first element of the pair in\\n () (the quantifier restriction), the right inner DRS to the second element of the pair (the quantifier scope) and the arrow to\\nthe determiner meaning every. The referential connection established\\nby the discourse marker x in the DRS is simulated in our analysis by\\nthe simultaneous abstraction of the variable x over both the\\nrestriction and the scope of the quantifier.\\n\\n\\nIt is straightforward to show that there is a one-to-one\\ncorrespondence between Barwise-Cooper generalized quantifiers and pair\\nquantifiers.  Indeed, such a correspondence is established by two\\nfunctionals, ,\\nmapping pair quantifiers to generalized\\nquantifiers, and ,\\nmapping generalized quantifiers to pair\\nquantifiers, satisfying \\n\\n\\nand \\n\\n.\\n\\nand \\nare defined as:\\n\\n\\n\\n\\n\\nwhere \\n\\n\\nand \\n\\n.\\n\\n\\nFor a derivation to be considered complete, all assumptions must be\\ndischarged. We will exemplify this process with\\n sentence (). \\n\\n\\nThe quantified noun phrase `every person' is given the interpretation\\n\\n\\n\\n\\n\\nthat is, the meaning of the noun phrase is x under the\\nassumption to the left of the .\\n\\n\\nThe verb meaning \\n\\n\\napplied to the\\nNP meaning yields a VP meaning, still under the above\\nassumption:\\n\\n\\n\\n\\n\\nApplication of this VP meaning to the subject meaning\\n\\nresults in this sentence meaning:\\n\\n\\n\\n\\n\\nDischarging the quantifier assumption involves applying the quantifier\\nevery to the result \\n\\n\\nof abstracting x over the pair consisting of\\nthe restriction and the scope of the quantifier. The resulting\\ninterpretation is \\n\\n\\n\\n\\n\\nthat is, a\\nsentence meaning free of undischarged assumptions. When several\\nquantifier assumptions are introduced, there is the option of\\ndischarging them in several different orders, leading to alternative\\nquantifier scopings for the sentence.\\n\\n\\nIn Pereira's original system, the treatment of quantifier assumptions\\nis semantically justified by showing how such derivations are\\nconvenient shorthand for derivations in the Curry system of semantic\\ncombination containing only the operations of functional application\\nand abstraction. In the present variant, we could carry out a similar\\njustification in a system that would include pairing in addition to\\nfunctional application and composition\\n ,. \\n\\n\\nThe treatment of the semantics of long-distance dependencies is\\nhandled similarly by introducing and discharging assumptions.\\nAgain, the form of these introduction and discharge rules could be\\njustified on the basis of functional application and abstraction.  As\\nan example of a derivation involving a long-distance dependency, we\\nconsider the example\\n\\n\\nJohn greeted every person that arrived.\\n\\n\\nThe trace in the subject position of the relative clause can\\nbe thought of as introducing a bind assumption for a new variable\\n\\n\\n\\n\\n\\nwhich serves as the argument to \\n\\n\\n(the\\ninterpretation of `arrived'):\\n\\n\\n\\n\\n\\nThe relative clause being completed, we can now discharge\\nthe bind assumption.  We do so by forming a higher-order predicate by\\nabstracting the matrix, conjoined with a place-holder for the modified\\nnominal, and abstracted by the bound variable.\\n\\n\\n\\n\\n\\nThis relative clause meaning serves as a function over the\\nnominal meaning \\n\\n.\\n\\n\\n\\n\\n\\nFinally, the rule for combining a quantifier every with a\\npredicate forms a quantifier assumption over a new variable.\\n\\n\\n\\n\\n\\nFrom here, the derivation continues as before, yielding the sentence\\nmeaning (before the quantifier is discharged)\\n\\n\\n\\n\\n\\nand the scoped meaning is\\n\\n\\n\\n\\n\\nThis completes the background information on the formal semantics we\\nwill presume in the remainder of the paper.\\n\\n\\n\\n\\n    Interactions with Other Phenomena\\n\\n\\nThe approach to ellipsis resolution that is advocated here displays\\ndifferences from previous approaches in its handling of various\\nphenomena.  We will discuss how our analysis differs from\\nidentity-of-relations analyses in general, and certain particular\\ninstances thereof, by briefly examining the predictions of the\\nanalyses with respect to the following phenomena:\\n\\n\\nNon-constituent abstractions:\\nThere are many cases in which\\nthe relation constructed from the source clause does not correspond in\\nany straightforward fashion to the interpretation of some syntactic\\nconstituent: for example, when it must take more than one argument.\\nFor instance, the tense and aspect as well as the subject are\\nabstracted in the sentence\\nDan is running for president, and George did last term.\\nExamples demonstrate other nonstandard abstractions as well. Such\\ncases are especially problematic for identity-of-relations analyses in\\nwhich the relation  is necessarily associated with some\\nconstituent such as the VP in the source clause.\\n\\n\\nMultiple property extraction:\\nIn some cases, a single sentence\\nserves as the antecedent for two subsequent instances of ellipsis\\ninvolving different parallel elements:\\nJohn finished reading the poem before Bill did, and the short story too.\\nThis sentence has a reading on which John finished reading both the\\npoem and the short story before Bill finished reading the poem.  This\\nis problematic for identity-of-relations analyses in which only a\\nsingle property is available in any given instance for the\\ninterpretation of subsequent elided phrases.\\n\\n\\nCascaded ellipsis:\\nAnalyses differ as to what readings are\\npredicted for sentences containing multiple elliptical clauses in\\nwhich the interpretation of one elided constituent depends partially\\nor entirely on the interpretation of another elided constituent.  An\\nexample is:\\nJohn realizes that he is a fool, but Bill does not, even though\\nhis wife does.\\n\\n\\nInteraction with quantifier scoping:\\nAs is well known, the\\nambiguities following from varying quantifier scope possibilities\\ninteract with ellipsis resolution possibilities.  For instance, in the\\nsentence\\nJohn greeted every person when Bill did.\\ntwo readings are possible, depending on whether the universal quantifier\\nhas wide scope over both the main and subordinate clause, or\\nquantifies separately in each clause.  But in\\nJohn greeted every person that Bill did.\\nonly a wide scope reading is available.\\n\\n\\nWe discuss each of these phenomena below, and demonstrate that our\\napproach constructs appropriate solutions.  In the succeeding section,\\nwe discuss in detail an example sentence which illustrates differences\\namong a number of analyses of ellipsis that have been proposed in the\\npast.  Finally, we turn to problematic cases for this and other\\nanalyses.\\n\\n    Non-Constituent Abstractions\\n\\n\\nThere are many instances in which the interpretation of elided phrases\\ndoes not correspond to the interpretation of a syntactic constituent\\nin the source clause.  The most obvious cases include the elliptical\\nconstructions of gapping or stripping.  But VP ellipsis provides\\nexamples as well.  For instance, there are cases in which a deeply\\nembedded constituent induces a sloppy reading; in other cases,\\nrelations are formed with multiple parallel elements in the source and\\ntarget clause; in still other cases, as discussed in\\n Section  below, the parallelism between the elements in the source and target clause is not syntactic, but\\nsemantically or pragmatically induced.  These cases are problematic\\nfor identity-of-relations approaches in general, since such approaches\\nwould have to make available a very large number of different semantic\\nanalyses for each source clause, some of them otherwise unmotivated,\\nto allow for all of the possible interpretations that might need to be\\nprovided for subsequent ellipsis.  They are particularly problematic\\nfor identity-of-relations analyses in which the interpretation\\nprovided by the source clause corresponds to the translation of a\\nsyntactic constituent in the source.\\n\\n  Sloppy readings with embedded antecedents \\n\\nThe primary argument given by Reinhart Reinhart:Anaph for\\nthe distinction between bound variable and referential pronouns is the\\nrequirement that bound variable pronouns must be c-commanded by their\\nantecedents.  She uses this requirement to predict that the following\\nexample has only a strict reading:\\n\\n\\n People from LA adore it and so do people from NY.  [Reinhart's (17a), page 150]\\n\\n\\nReinhart proposes a requirement that a pronoun must be\\nc-commanded by its antecedent if the antecedent is a quantifier;\\nfurther, she claims that a pronoun giving rise to a sloppy\\ninterpretation must be c-commanded by its antecedent.  For Reinhart,\\nthen, the availability of a sloppy reading correlates with the\\npossibility of a bound-variable interpretation of a pronoun, and she\\nrequires a c-command relation for this interpretation to be possible.\\nThis restriction simplifies the task of an identity-of-relations\\nanalysis because it reduces the number of cases in which a sloppy\\nreading is available.  An analysis postulating ambiguity of pronoun\\ninterpretation for only this restricted set of cases seems\\nmethodologically more plausible.\\n\\n\\nHowever, Reinhart herself  notes certain\\ncounterexamples to this correlation, cases where a sloppy reading is\\navailable even when c-command does not hold:\\n\\n\\nFelixi's mother thinks hei's a genius and so does\\nSiegfried's mother.  [Reinhart's (8a)]\\n\\n\\nWe'll discuss Rosai's problems with heri parents and\\nSonya's problems too.  [Reinhart's (8b)]\\n\\n\\nWescoat:StrictSloppy notes a number of more extreme cases of\\nsloppy readings involving non-c-commanding, embedded constituent\\nantecedents, such as:\\n\\n\\n The policeman who arrested John failed to read him his rights, and so did the one who arrested Bill.\\n\\n\\nThe person who introduced Mary to John would not give her his phone\\nnumber, nor would the person who introduced Sue to\\nBill.\\n\\n\\nWescoat claims, and we agree, that sloppy readings are\\npossible with these sentences; that is, that the following readings\\nare available--perhaps even preferred--for them:\\n\\n\\nThe policeman who arrested John failed to read John John's\\nrights, and the one who arrested Bill failed to read Bill Bill's rights.\\n\\n\\nThe person who introduced Mary to John would not give Mary John's\\nphone number, and the person who introduced Sue to Bill would not give\\nSue Bill's phone number.\\n\\n\\nHirschberg and Ward\\nHirschbergWard:StrictSloppy have obtained experimental\\nevidence consistent with these examples that the c-command criterion\\nposited by Reinhart and counterexemplified by Wescoat is not a general\\nrequirement for sloppy readings.  For example, their data show that as\\nmany as one-third of a group of  test subjects preferred\\nthe sloppy reading of\\nPeople from Los Angeles think it's a scary place to\\nlive, and so do people from New York.  [Hirschberg and Ward's (31)]\\n which parallels sentence () closely. \\n\\n\\nOn our analysis, barring any stipulated prohibition, there is no\\nobstacle to forming relations abstracted over arbitrarily deeply\\nembedded positions.  In skeletal form, the analysis of such an example\\nwould proceed as follows.  Suppose the source clause of\\n () is interpreted as \\n\\n\\n\\n\\n\\nwhere pwi(x,y) is the person\\nwho introduced x to y, and phone(x) is x's phone number. The\\nparallel elements in the construction are, respectively, `the person\\nwho introduced Mary to John' and `the person who introduced Sue to\\nBill'; `Mary' and `Sue'; `John' and `Bill'.  Thus, the appropriate\\nequation to solve is\\n\\n\\n\\n\\n\\nThe sloppy reading is engendered by the following unifying\\nsubstitution for P:\\n\\n\\n\\n\\n\\n\\nwhich, when applied to the interpretations of the parallel\\nelements in the target, yields the target interpretation\\n\\n\\n\\n\\n\\n Note that the recovered relation () is not a relation corresponding to a conventional interpretation for the\\nVP--or any other constituent--in the source clause.  On an\\nidentity-of-relations analysis, such relations would be available only\\nby virtue of their use in the derivation of the source clause\\ninterpretation.  This would necessitate the postulation of wild\\nambiguity in the source clause, one derivation for each possible case\\nof subsequent ellipsis.\\n\\n\\n    Non-subject abstraction\\n\\n\\nThere exist many cases of multiple parallel elements in the source and\\ntarget clause; it is very common for ellipsis to involve relations\\nformed by abstraction of elements other than the interpretation of the\\nsubject noun phrase.\\n\\n\\nFor example, the tense and aspect of the target clause might differ\\nfrom that in the source clause:\\n\\n\\nDan is running for president, and George did last term.\\n\\n\\nThe mood can also differ:\\n\\n\\n``I want to leave.''  ``Well, do.''\\n\\n\\n``Eat your dinner.''  ``I did.''\\n\\n\\nThe two clauses may differ in polarity:\\n\\n\\n Dan didn't leave, but George did. \\n\\n\\nThese examples show that relations of varying arity must be\\navailable as interpretations for elided phrases.  The consequence of\\nthis for theories where the relation available for interpretation of\\nsubsequent ellipsis must be available in the source sentence is,\\nagain, that every sentence which can be the antecedent for subsequent\\nellipsis must be many ways ambiguous; an interpretation must be\\navailable for each relation that might be needed to interpret ellipsis\\nin subsequent discourse.\\n\\n\\nOn the equational analysis, however, such ambiguity is not required.\\nA single interpretation for the source clause can give rise to any\\nrequired interpretation for the target, since there is no inherent\\nrestriction as to the number or nature of the parallel elements\\ninvolved in the ellipsis.\\n\\n\\n Take, for instance, the sentence in ().  Here, the parallel elements are `Dan' and `George' (the subjects), and the\\npositive and negative polarities, represented semantically as\\n operators pos and neg. Thus, the equation \\n\\n\\n\\n\\n\\ncan be solved yielding\\n\\n\\n\\n\\n\\nand applied to the target parallel elements:\\n\\n\\n\\n\\n\\nOf course, gapping and stripping provide abundant examples of\\nnon-subject abstraction involving other arguments or modifiers;\\n provides this example, which\\ninvolves non-subject parallel elements and has both a strict and a\\nsloppy reading:\\n\\n\\nYou can keep Rosa in her room for the whole afternoon,\\nbut not Zelda. [Reinhart's (18c)]\\n\\n\\n also discusses examples\\ninvolving both subject and non-subject parallelism:\\n\\n\\nMaxwell killed the judge with a silver hammer, and I'd like to do the\\nsame thing to that cop, with a cudgel. [Jackendoff's (6.196)]\\n\\n\\nFred hung Tessie up in a tree and poured paint on her,\\nbut I bet he wouldn't do it to Sue with glue. [Jackendoff's (6.197)]\\n\\n\\n\\n  Multiple Property Abstraction \\n\\nA difficulty in any identity-of-relations analysis, which makes\\navailable only one interpretation for subsequent clauses exhibiting\\nellipsis, is seen when a single sentence is the antecedent for the\\nellipsis of two different noun phrases.  Consider the following:\\n\\n\\nJohn finished reading the poem before Bill did, and the short story too.\\n\\n\\nThis sentence has a reading on which John finished reading\\nboth the poem and the short story before Bill finished reading the\\npoem.  On this reading, the source for both elliptical clauses is the\\nsame clause, `John finished reading the poem.'  To produce a relation\\nwhich can be the interpretation for the elided VP whose subject is\\nBill, the interpretation for the sentence `John finished reading the\\npoem' must be derived as:\\n\\n\\n\\n\\n\\nso as to make available the property \\n\\n.\\nSimilarly, an interpretation\\nmust also be produced for the second conjunct `and the short story too'.\\nOn the desired reading, the interpretation for `John finished reading\\nthe poem' must be derived as:\\n\\n\\n\\n\\n\\nUnder an identity-of-relations analysis, the source clause is\\ndeemed ambiguous between the two derivations. They do not\\nsimultaneously exist in a given analysis; only one or the other may be\\nchosen.  Thus, the reading noted above would not be generable.  On the\\nother hand, an analysis such as ours allows for the formation of two\\ndifferent relations from the semantic representation of the first\\nsentence; the representation of the first sentence does not constrain\\nthe possibilities for construction of such relations.  The next\\nsection provides an example of a similar problem and its derivation in\\nour framework.\\n\\n\\n  Cascaded Ellipsis \\n\\nWe use the term ``cascaded ellipsis'' to refer to cases of multiple\\nellipsis in which one of the elided constituents depends on another\\nelided constituent for its interpretation.  Analyses dependent on an\\nidentity-of-relations approach generally make available fewer\\nreadings in cascaded ellipsis cases than the analysis presented here;\\nwe believe that the greater number of readings available with our\\nanalysis is in fact warranted.\\n\\n\\nDahl:SI provides the\\nfollowing example (Dahl's (12), an English paraphrase of Scheibe's\\n(58a) Scheibe:Problem):\\n\\n\\n John realizes that he is a fool, but Bill does not, even though his wife does.\\n\\n\\nDahl claims that this sentence has, among other readings,\\nthe following one:\\n\\n\\nJohn realizes that John is a fool but\\n\\n\\nBill does not realize that Bill is a fool, even though\\n\\n\\nBill's wife realizes that Bill is a fool\\n\\n\\n discusses this example; his claim\\nis that this reading is not available for this sentence.  We disagree,\\nand find the reading acceptable.\\n\\n\\nOn our analysis, this reading is\\nreadily available.  Assume that the interpretation for `John realizes\\nthat he is a fool' on the reading under discussion is:\\n\\n\\n\\n\\n\\nThis sentence serves as the antecedent for the elided phrase\\nin the second conjunct, `Bill does not'.  `Bill' and `John' are\\nparallel elements; for the reading under discussion, second-order\\nmatching solves the equation\\n\\n\\n\\n\\n\\nproducing, among others, the following property\\n(corresponding to the sloppy option):\\n\\n\\n\\n\\n\\nwhich is applied\\nto `Bill'.  The interpretation for the second conjunct as a whole is,\\n then, the following: \\n\\n\\n\\n\\n\\nWe assume that the second clause may serve as an antecedent\\nfor the elided portion of the third conjunct.  The parallel elements\\n are `Bill' and `his wife'; the ellipsis equation is \\n\\n\\n\\n\\n\\nOn the reading under discussion, the\\nstrict option is chosen; the property Q applied to the\\ninterpretation of `his wife' is:\\n\\n\\n\\n\\n\\nThe resulting interpretation for the third conjunct is:\\n\\n\\n\\n\\n\\nAlthough we have described the derivation of the meaning for this\\nexample in terms of temporal ordering (we resolve the first ellipsis,\\nusing its result to resolve the second), it is important to note that\\nthe analysis is truly order-free.  In essence, we merely set up two\\nequations in two unknowns and solve them using unification.  The\\nresult, as is typical with declarative, equational methods, does not\\ndepend on solving the equations in a particular order.\\n\\n\\nUnder an identity-of-relations analysis, such as Sag's, the existence\\nof this reading is problematic, as he notes.  The problem is that\\nthere are conflicting requirements on the form of the semantic\\nrepresentation of the second clause.\\n\\n\\nSag obtains strict and sloppy readings under ellipsis by optionally\\napplying a rule that replaces the interpretation of a pronoun (which\\nhas an invariant referent and induces a strict reading) by a\\nlambda-bound variable (inducing a sloppy reading).  The representation\\n Sag provides for the first two conjuncts is: \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCrucially, the interpretation for the pronoun `he' which is\\nreconstructed in the second conjunct is represented by a bound\\nvariable.\\n\\n\\nIn contrast, for the reading under discussion, the representation for\\nthe second and third conjuncts is:\\n\\n\\n\\n\\n\\n\\n\\n\\nThe strict reading is only available when the option\\nof replacing the pronoun interpretation with a lambda-bound variable\\nis not taken.  These conflicting requirements make it impossible for\\nSag's analysis--or any identity-of-relations analysis, where the\\ndifference between a strict and a sloppy reading corresponds to a\\ndifference in the form of the semantic representation--to obtain the\\nreading for this sentence that we (and Dahl) assume exists.\\n\\n\\n  Interactions with Quantifier Scope \\n\\n As described in Section , quantifier scope is generated through a mechanism of discharging of assumptions introduced\\nin the course of a derivation.  The interaction between quantifier\\nscoping and ellipsis, then, will simply involve the relative\\nderivational order of discharging such quantifier assumptions and\\nresolving elliptical constructs.  That is, when we set up the\\nappropriate instance of the schematic equation \\n\\n,\\ns is the meaning of the source clause, possibly\\nunder one or more undischarged assumptions.\\n\\n    Quantification and antecedent-contained ellipsis\\n\\n\\nAs an example, we consider the two sentences given in\\n (). \\n\\n\\n  John greeted every person when Bill did. John greeted every person that Bill did.\\n\\n\\nAs noted by Sag Sag:PhD, the quantifier scope\\npossibilities differ for these two sentences: whereas\\n () admits of two readings, () allows only one.  Both Sag and Williams Williams:DLF provide\\nanalyses for these semantic intuitions.\\n\\n\\n We will take the source of the ellipsis in () to be the clause `John greeted every person', and the target to be `Bill\\n did'.  The derivation of interpretations for () proceeds as follows.  The source clause is interpreted under a\\nquantifier assumption generated by the subject NP.  (See\\n Section  for the derivation.)\\n\\n\\n\\n\\n\\n\\nWe might discharge the assumption at this point, but we\\nchoose not to in this first scenario.  Consequently, the\\ninterpretation of the full sentence is\\n\\n\\n\\n\\n\\nwhere P is constrained equationally by virtue of the\\ninterpretation of the source clause:\\n\\n\\n\\n\\n\\nThis equation has a single (most general) solution\\n\\n\\n\\n\\n\\nIt is this value for P that we will apply to bill.\\nThus, the interpretation of the full sentence, with ellipsis resolved\\nis\\n\\n\\n\\n\\n\\nThe assumption may now be discharged, yielding the full\\ninterpretation\\n\\n\\n\\n\\n\\nThis interpretation corresponds to a necessarily\\ndistributive reading, the `individual' reading, in which each person\\nis simultaneously greeted by John and Bill.\\n\\n\\nAlternatively, the assumption can be discharged before the ellipsis is\\nresolved.  Under this scenario, the interpretation of the source clause\\nis\\n\\n\\n\\n\\n\\nThe full\\nsentence, then, is interpreted as\\n\\n\\n\\n\\n\\nwhere, again, the interpretation of the source clause is\\nused to constrain the property P:\\n\\n\\n\\n\\n\\nThe single value for Pis\\n\\n\\n\\n\\n\\nleading to the final interpretation\\n\\n\\n\\n\\n\\nThis interpretation yields a `group' reading paraphrasable\\nas `John greeted every person when Bill greeted every person.'  The\\ntwo derivations, then, correspond to just the interpretations noted by\\n Sag. \\n\\n\\n Now, we turn to the superficially similar sentence (). We take the source clause to be the entire sentence, and the target to\\nbe, again, `Bill did'.  The interpretation of the sentence before\\nresolution of ellipsis and discharge of assumptions is\\n\\n\\n\\n\\n\\n (Again, Section  records a derivation for a similar clause.)\\n\\n\\nAs before, we will resolve the ellipsis by finding solutions for the\\nequation\\n\\n\\n\\n\\n\\nwhose solution assigns\\nP the relation \\n\\n.\\nWith this substitution,\\nthe interpretation is\\n\\n\\n\\n\\n\\nwhich, discharging the assumption, reduces to\\n\\n\\n\\n\\n\\nAlternatively, we might attempt to discharge the assumption before\\nresolving the ellipsis.  Recall the starting point for the previous\\nderivation:\\n\\n\\n\\n\\n\\nDischarging the assumption yields\\n\\n\\n\\n\\n\\nResolving the ellipsis, then, involves finding solutions\\nto the equation\\n\\n\\n\\n\\n\\nNotice that the variable P appears on both sides of this\\nequation.  For this reason, the derivation runs into problems, for\\nthere simply are no solutions to this equation; no unifier exists for\\nthis pair of typed terms.   Thus, the derivation fails at this\\npoint; the sentence has only the `individual' reading, in agreement\\nwith our judgments.\\n\\n\\nThe computational reflex of the above lack of a solution is a\\nviolation of the so-called ``occurs check'' in the unification\\nalgorithm. This check prevents the construction of unifiers in which\\nthe substitution for a variable contains that variable. In other\\nwords, the occurs check blocks the creation of a circularity (a value\\nfor P containing P itself). It is interesting to note that this is\\nformally analogous to the syntactic argumentation which Williams\\nWilliams:DLF uses to eliminate such readings.\\n\\n\\nIn sum, the ellipsis characterization described here allows for\\nantecedent-contained ellipsis, and predicts correctly the interactions\\nwith quantifier scope.\\n\\n\\n    Quantification parallelism\\n\\n\\nAnother implication of the account of quantifier scope given above is\\nthat in cases where the source clause exhibits quantifier scoping\\nambiguities, if the quantifiers are separately quantified in the two\\n clauses (i.e., as in the `group' reading for ()) the scopes in the two clauses must be the same.  For instance, in the\\nsentence\\n\\n\\nJohn gave every student a test, and Bill did too.\\n\\n\\nwe predict (correctly, we take it) no reading of the form\\n\\n\\n\\n\\n\\nwhere the two quantifiers take different scopes in the two\\nclauses.   Consider the possible orderings of ellipsis resolution and\\ndischarging of quantifier assumptions.  If ellipsis resolution occurs\\nbefore some of the quantifiers have been discharged, these quantifiers\\nwill scope over both clauses.  Thus, the only way for both\\nquantifiers to scope separately is for ellipsis resolution to occur\\nafter both quantifier assumptions are discharged.  But in that case,\\nthe ellipsis equation will include the quantifier order manifest in\\nthe source clause interpretation, and this will be carried over to the\\ntarget interpretation.\\n\\n\\n    Quantification and type raising\\n\\n\\nIn general, the semantic types of parallel elements must be identical.\\nIn the case of a quantified NP parallel to a non-quantified NP, this\\nimplies that the type of the non-quantified NP must be raised to that\\nof the quantified type.  As an example, we consider the sentence\\n\\n\\nEvery student revised his paper, and then Bill did.\\n\\n\\nThis sentence is ambiguous, depending on whether Bill\\nrevises his own paper or each student's paper.\\n\\n\\nAs usual, the ellipsis resolution can occur before or after the\\nquantifier assumption is discharged.  If the quantifier is discharged\\nfirst, we have the meaning given in 1 for the first clause.\\n\\n\\n\\n\\n\\n\\nTo resolve the ellipsis, we need to set up an equation\\ninvolving the interpretation of the parallel element in the source,\\n``every student''. However, for this purpose we cannot directly use\\nthe stored noun phrase interpretation given in 1.\\n\\n\\n\\n\\n\\n\\nThis interpretation only makes sense as part of the\\nderivation of the meaning of the whole source clause. Instead, we\\ncalculate the contribution of ``every student'' to the meaning of the\\nsource clause by examining the effect of applying it to an arbitrary\\nproperty S.  Combining S with 0, we obtain first the\\ninterpretation in 1.\\n\\n\\n\\n\\n\\n\\nThe assumption can then be discharged to yield the sentence\\nmeaning in 1.\\n\\n\\n\\nevery(x, student(x), S(x))\\n\\n\\nThe contribution of ``every student'' is thus to map an\\n arbitrary property S to the term in (), that is, the contribution is the function given in 1.\\n\\n\\n\\n\\n\\n\\nIt is easy to show that this is equivalent to the usual\\ngeneralized quantifier meaning of ``every student''.\\n\\n\\nTo resolve the ellipsis, then, we set up the equation in 1\\n involving the meaning () of the parallel element in the source, ``every student'':\\n\\n\\n\\n\\n\\n\\nThis equation has the solution\\n\\n\\n\\n\\n\\nThe type of the noun phrase meaning\\n\\n\\nis\\n\\n.\\nThe meaning for ``Bill'' must be\\ntype-raised to \\n\\n\\nfor type consistency.  The value for\\nP, when applied to the type-raised meaning for ``Bill'', yields the\\ntarget meaning\\n\\n\\n\\n\\n\\naccording to which Bill revises his own paper.\\n\\n\\nOn the other hand, if ellipsis resolution occurs first, the derivation\\nyields\\n\\n\\n\\n\\n\\njust before resolution.  The equation\\n\\n\\n\\n\\n\\nadmits of a strict interpretation for P:\\n\\n\\n\\n\\n\\nThe meaning for the conjoined sentence before ellipsis\\nresolution\\n\\n\\n\\n\\n\\nreduces, after ellipsis resolution, to\\n\\n\\n\\n\\n\\nwhich, following discharging of the quantifier assumption, becomes\\n\\n\\n\\n\\n\\nOn this reading, Bill revises each student's paper after the\\nstudent revises it.  A sloppy reading, on which Bill revises his own\\npaper, is generable in this way as well; in this particular case,\\nthough, it is logically equivalent to the reading described above, in\\nwhich type-lifting of `Bill' is involved.\\n\\n\\n\\n  Other Phenomena \\n\\nWe defer discussion of several other important interactions of\\nellipsis and scoping phenomena to a companion paper in preparation.\\nIn that paper we intend to discuss, in addition to a more detailed\\nexplication of the mainstream quantifier cases:\\n\\n\\nScope ambiguities with indefinites:\\nIndefinites give rise to\\nseveral readings under ellipsis, depending on their scope and the\\nrelative order of ellipsis resolution and discharge of the indefinite.\\nThis sentence, for example, has three readings:\\n\\n\\nJohn lost a book he owned, and so did Bill.\\n\\n\\nOn the first reading, John and Bill lost the same book; on the second\\nreading, John and Bill each lost one of John's books, possibly\\ndistinct; and on the third reading Bill lost one of his own books.\\n\\n\\nDe dicto/de re ambiguities:\\nSimilar ambiguities are found in\\nsentences with opaque verbs.  This sentence has three readings:\\n\\n\\nBill wants to read a good book and John does too.\\n\\n\\nWhere the first conjunct has a de dicto reading, the second conjunct\\ndoes also; where the first conjunct has a de re reading, however,\\nthere are two readings for the sentence as a whole, depending on\\nwhether or not Bill and John want to read the same book.\\n\\n\\n`Canadian flag' examples:\\nHirshbuhler:Ellipsis\\ndiscusses examples such as the following:\\n\\n\\nA Canadian flag was hanging in front of each window, and an American\\none was too.\\n\\n\\nExamples such as these illustrate that the subject of the source\\nclause need not take widest scope in VP ellipsis.  On our analysis,\\n these examples, like those in Section , involve matching of higher than second order.\\n\\n\\n\\n  A Comparison of Approaches \\n\\nIn order to more fully explicate the differences between our approach\\nto ellipsis resolution and other approaches, we analyze a single\\nexample in detail from the perspective of several previous proposals.\\nRather than make the comparison in the respective notations of the\\noriginal proposals, we normalize those notations by using lambda\\nterms uniformly.  When the analyses are viewed in this way, several\\napparently different analyses are seen to generate the same set of\\nreadings in the same manner, despite their having originally been\\nstated in differing notations.\\n\\n\\nWe will use the following example, discussed at length by Gawron and\\nPeters GP:Anaph:\\n\\n\\n John revised his paper before the teacher did, and Bill did too. \\n\\n\\nWe follow Gawron and Peters in directing attention to the\\nreading of the first conjunct where `John' and `his' corefer, and of\\nthe second elliptical clause where its source is the entire previous\\nsentence.  The following six readings exhaust the set of readings\\ngenerated by any of the analyses (including our own) that we discuss.\\nWe present them with paraphrases for reference.\\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nEach person revised his own paper.\\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nEach person revised John's paper.\\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nJohn and then the teacher revised John's paper; Bill and then\\nthe teacher revised Bill's paper. \\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nJohn and Bill both revised John's paper before the teacher revised the\\nteacher's paper.\\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nJohn and Bill revised their own papers before the teacher\\nrevised John's paper.\\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nJohn and then the teacher revised John's paper; Bill revised John's\\npaper before the teacher revised Bill's paper.  \\n\\n\\nAs we will see, the equational analysis that we propose in\\nthis paper is the most profligate of the analyses, potentially\\ngenerating all six of these readings, though restrictions might\\neliminate certain of these.\\n\\n  Zero-Reading Analyses \\n\\nThe strictest version of an identity-of-relations analysis\\nrequires that a lambda term used in the derivation of the meaning of\\nthe source clause be used in the derivation of the target clause\\nmeaning (either by copying or deletion under identity).  Under such\\nan analysis, the pertinent level of semantic representation of the\\nsource clause to use in the target clause derivation is that before\\nbeta reduction has occurred, as beta reduction eliminates the\\nfunction-typed lambda terms.  For the sentence `John revised his\\npaper', the unreduced meaning representation is one of:\\n\\n\\n\\n\\n\\nor\\n\\n\\n\\n\\n\\ncorresponding to the strict and sloppy readings,\\nrespectively.  In forming the meaning of the first target clause\\n`before the teacher did', we use whichever term P is made\\navailable by the first clause, generating \\n\\nP(teacher).  An issue\\nremains as to how the two clause meanings are then combined to form a\\nsingle sentence meaning.  The most natural method, direct\\n coordination, would yield (for the sloppy reading): \\n\\n\\n\\n\\n\\ncorresponding to a syntactic analysis under which the\\nadverbial clause is attached at the S level.  However, this is not\\nitself of the form appropriate for being the source of a later\\nellipsis, that is, a function applied to the subject meaning.  Thus,\\nunder this analysis, the second ellipsis, `and Bill did too', would be\\nuninterpretable.  (Recall that we are ignoring the readings in which\\nthe source for the second ellipsis is merely `John revised his paper'.\\nSuch a reading would be possible, although it would be strict or\\nsloppy dependent on the interpretation  of the other two\\nclauses.)\\n\\n\\n  Two-Reading Analyses \\n\\nThe second ellipsis is not, of course, uninterpretable, so we attempt\\nto design a meaning representation for its source that is of the\\nappropriate form.  A first method is to place the meaning of the\\n`before' clause within the meaning of its source VP:\\n\\n\\n\\n\\n\\n(We will discuss a second method in\\n Section .)  The exact mechanism for constructing such a reading is not specified here.  It may seem especially problematic,\\nas the source and target terms are not alphabetic variants in this\\nnotation.  Nonetheless, if this were the meaning representation of the\\nsource of the second ellipsis, it would allow for a sloppy reading of\\nthe target; this would correspond to the sloppy\\n reading ().  The strict variant \\n\\n\\n\\n\\n\\n would yield reading (). \\n\\n\\nAn alternative method of subordinating the `before' clause meaning\\nmaintains the alphabetic variance property of the two clauses.  If we\\nassume, counterintuitively, that the before operator takes a\\nproperty and a truth value to a property (that is, it is of type\\n\\n), we can\\nform the meaning representation\\n\\n\\n\\n\\n\\nand similarly for the strict case.  Again, the mechanism is\\n a bit mysterious (though less so) and readings ()  and () would be generated for the sentence as a whole. \\n\\n\\nThe analyses of Sag:PhD and Williams:DLF, although\\nthey do not consider cases such as these, might be reasonably viewed\\nas generating these readings in much this way.  Similarly, the\\nanalysis presented by Roberts:PhD and phrased in terms of\\nDRT generates these two readings (as discussed by Gawron and Peters).\\n\\n\\n    Three-Reading Analyses\\n\\n\\nThe representation for the source of the second ellipsis might be\\nextrapolated along different lines.  In particular, the interpretation\\nfor each verb phrase, including the compound one, might be given by an\\novert abstraction.  This would correspond to a syntactic analysis\\nunder which the adverbial clause is adjoined to the main clause verb\\nphrase, with one verb phrase meaning appearing as a subconstituent of\\n the other.  For the sloppy reading (), we would have \\n\\n\\n\\n\\n\\n and for the strict () \\n\\n\\n\\n\\n\\nThis representation has the benefit of being uniform,\\npreserving alphabetic variance, and assigning a more attractive type\\nto before.  It also allows for a third reading when used as the\\n source for the second elliptical clause.  The second argument to revise might be--in addition to j (the strict option) or x (the locally sloppy option)--the reabstracted subject meaning u.  This leads to a globally sloppy meaning (). \\n\\n\\n\\n\\n\\nThis analysis, which generates three readings for the example\\nsentence, is essentially the analysis developed by Gawron and Peters\\nwithin a situation-theoretic framework. (The final reading corresponds\\nto the second argument of revise being ``absorbed'' by the\\nlambda operator.)  Our transliteration makes clear that, for this case\\nat least, situation-semantics machinery is not necessary to yield the\\nreadings in question; an extrapolation of Sag's or Williams's analyses\\nmight achieve the same result.  Of course, other aspects of the Gawron\\nand Peters analysis depend intrinsically on the situation-theoretic\\nfoundation.\\n\\n\\n  Six-Reading Analyses \\n\\nThe three readings provided by the Gawron and Peters analysis seem to\\nexhaust the possibilities for an identity-of-relations approach.\\nOur analysis produces six readings for the example sentence.\\n\\n\\nLet us examine in detail how one of the readings for this sentence,\\n () above, is obtained under the equational analysis of ellipsis.  Assume that the semantics for `John revised his paper' is:\\n\\n\\n\\n\\n\\nThe first conjoined sentence then will have the meaning\\n\\n\\n\\n\\n\\nunder the constraint\\n\\n\\n\\n\\n\\nThe second elliptical clause takes its source to be the whole first\\nconjunction.  Thus, its interpretation will be\\n\\n\\n\\n\\n\\nunder the constraint\\n\\n\\n\\n\\n\\nThese two equations in two unknowns (P and Q) are solved, as\\nusual, by higher-order unification; we will take the equation for Pfirst.  One solution is to take the strict reading for P,\\n\\n\\n\\n\\n\\nleading to the following interpretation for the second equation:\\n\\n\\n\\n\\n\\nThis equation, in turn, has a solution\\n\\n\\n\\n\\n\\nThe semantics for this reading of the sentence as a whole is:\\n\\n\\n\\n\\n\\nOur analysis allows for readings that are missing under the analyses\\ndiscussed above because it is not an identity-of-relations analysis;\\ninterpretation of ellipsis does not involve copying the interpretation\\nof a constituent in the source.\\n\\n\\n  Five-Reading Analyses \\n\\n In Section , we discuss a restriction on unifiers that uniformly eliminates certain readings of elliptical\\nclauses.  This restriction, when applied to the example at hand,\\n eliminates reading ().  Thus, our analysis, strictly speaking, generates only five of the six combinatorially possible\\nreadings of the sentence.\\n\\n\\n  Four-Reading Analyses \\n\\nAn unpublished analysis attributed to Hans Kamp (personal\\ncommunication to Mark Gawron and Stanley Peters, cited by Gawron and\\nPeters GP:Anaph) and couched in DRT assigns four readings\\nto the sentence, and does so by eliminating the identity-of-relations\\nassumption.  In Kamp's analysis, as in our own, ambiguities between\\nstrict and sloppy readings do not arise from ambiguity in the source\\nclause; the source has only a single interpretation.  Essentially,\\nKamp makes a copy of the discourse representation structure of the\\nsource, and then imposes constraints identifying the participants in\\nthe source and target copies. These constraints must be applied in a\\nsymmetric manner.  If a sloppy interpretation constraint applies to\\none copied discourse entity, it must apply to all; similarly for a\\nstrict interpretation constraint.  Gawron and Peters mention a\\npossible extension to Kamp's analysis that allows for the generation\\nof all six of the readings listed above by relaxing the symmetry\\nrequirement.  We refer the reader to the discussion by Gawron and\\nPeters for a fuller description of Kamp's proposal.\\n\\n\\nInsofar as Kamp's analysis can be fleshed out, his analysis and ours\\nmake the same predictions as to the class of readings available in\\ncases of cascaded ellipsis.  Readings missing under other analyses are\\navailable for our analysis and his.  The particular syntactic\\noperations that Kamp (under Gawron and Peters's reconstruction)\\npresupposes have no particular foundation other than efficacy.  Our\\nanalysis can be seen as providing an argument for the operational view\\nimplicit in Kamp's analysis based on the underlying equational\\ncharacterization of elliptical constructions.  This equational\\nfoundation, as we have seen, articulates with other semantic phenomena\\nin ways not appreciated in the previous research.\\n\\n\\n  Summary \\n\\nIn summary, each analysis differs in the number of analyses that are\\npredicted for the given sentence.  Here is a scorecard.\\n\\n\\nThe single reading that our method derives that remains underived by\\n others is that in ().  Clearly, this reading is difficult, if not impossible, to dig out (although plausibility\\nconsiderations play a large role here).  However, we have seen\\nexamples that demonstrate that the reason for its absence in the other\\nanalyses is faulty.  For instance, its elimination on the basis of an\\nidentity-of-relations analysis, as Roberts or Gawron and Peters would\\nhave it, has repeatedly been seen to be too strong.\\n\\n\\nRather, the pertinent distinction in differentiating the first four readings\\nfrom the last two is that the resolution of the second elliptical\\nconstruction in the last two readings must treat the parallel\\nstructures that ellipsis applies to in a non-parallel fashion.  We\\nconjecture that such non-parallel cases are highly dispreferred, if\\nnot disallowed entirely.\\n\\n\\nIn order to test this hypothesis, we can try to construct an example\\nwhich pragmatically favors this dispreferred reading to see whether it\\nis obtainable.  The following sentence is parallel in structure to\\n sentence (): \\nDewey announced his victory after the newspapers did, but so did Truman.\\n\\n\\nA reasonable, historically accurate reading\\nfor this sentence may be represented as:\\n\\n\\n\\n'\\n\\n'\\n\\n'\\n\\n'\\n\\n\\n\\n\\nThat is, Dewey claimed victory for himself after the newspapers\\nannounced Dewey's victory, but Truman also claimed victory after Dewey\\nwas announced the winner by the newspapers.  This reading is parallel\\n to reading () described above. \\n\\n\\nOpinions differ as to the acceptability of this reading.  One's\\nopinion in this case can be seen as a litmus determining whether\\nparallelism of the sort violated here is required, or merely\\npreferred.\\n\\n\\n\\n  Problematic Cases \\n\\nThe following issues are problematic for most analyses of ellipsis\\ninterpretation.  We present them, along with our conjectured\\nsolutions, to codify the range of phenomena that analyses of ellipsis\\nmight account for and to provide a preliminary guess as to their\\npossible solutions in our framework.\\n\\n    Non-Syntactic Parallelism\\n\\n\\nOur analysis of ellipsis resolution presupposes identification of the\\nsource of the ellipsis and the parallel structuring of the source and\\ntarget.  This division of labor between identification of parallelism\\nand resolution of ellipsis is purposeful, as the factors involved in\\nthe solution of the two problems are quite different.  Although\\ndetermining the parallelism may seem to be a purely syntactic\\noperation, much like the matching that goes on at the semantic level,\\nthis similarity is illusory.  Cases of semantic or pragmatic\\nparallelism also exist.  These cases are particularly\\nproblematic for theories of ellipsis in which the interpretation of an\\nelided phrase is presumed to correspond to the interpretation of some\\nsyntactic constituent in the source clause, as is the case in most\\nidentity-of-relations analyses.\\n\\n  Semantic parallelism \\n\\nExamples of ellipsis exist in which the parallelism is between the\\n``logical subject'' (i.e., passive agent) in the source clause and the\\nsurface subject in the target clause:\\n\\n\\nA lot of this material can be presented in a fairly informal\\nand accessible fashion, and often I do.  \\n\\n\\nIt should be noted, as Dummett does, \\n(example due to Ivan Sag)\\n\\n\\nSimilar examples involving ``so-anaphora'' are also\\nfound:\\n\\n\\nThe formalisms are thus more aptly referred to as information- or\\nconstraint-based rather than unification-based, and we will do so\\nhere. \\n\\n\\nIt is possible that this result can be derived from some\\nindependent principle, but I know of no theory that does so.\\n\\n\\n\\nExamples of this type are ubiquitous, but seem\\nto be confined pragmatically to cases where the source clause states a\\ngeneral fact or rule, and the target clause provides a specific\\ninstance of this fact or rule.\\n\\n\\nExamples of passive/active parallelism are not confined\\nto those in which the source and target appear in the main clause.  In\\nthe following example (due to ),\\nparallelism of the heads of the main clause subjects forces a\\nparallelism of arguments of the modifying relative clauses; John, the object of the relative clause in the source sentence, is\\nparallel to Bill, the subject of the relative clause in the\\ntarget sentence:\\n\\n\\nThe policeman who arrested John failed to read him his rights, and\\nso, for that matter, did the one Bill got collared by.\\n\\n\\nOur analysis does not require that the property provided as the\\ninterpretation for the elided portion of the target clause in examples\\nlike those above correspond to the interpretation of any constituent\\nin the source clause.  It is not clear that there is any analysis\\navailable for examples of this sort within a theory in which the\\ninterpretation for elided phrases must be that of some constituent in\\nthe source clause, as is the case in most identity-of-relations\\nanalyses.\\n\\n\\nOther cases of semantic/thematic parallelism can also\\narise; sentence 1 is from instructions on a bottle of Agree\\nshampoo:\\n\\n\\nAvoid getting shampoo in eyes--if it does, flush\\nthoroughly with water.\\n\\n\\nSyntactically, the parallel elements are the object of the\\nsource clause and the subject of the target; thematically, these\\nelements are the ``theme'' arguments of the intransitive/causative\\nget verb pair.\\n\\n\\nOther combinations of logical-subject/surface-subject parallelism do\\nnot seem to arise.  The following examples, where the parallel\\nelements are intended to be the surface subject in the source clause\\nand the logical subject in the target, are ungrammatical:\\n\\n\\nDummett notes, as it should be, \\n\\n\\nWe will refer to the formalisms as information- or\\nconstraint-based, as they more aptly are.\\n\\n\\nHowever, the following sentence (due to Peter Sells) has a\\nsimilar structure, yet seems to be more acceptable:\\n\\n\\nJohn completed the assignment faster than it ever had been in the\\nhistory of the school.\\n\\n\\nTo the extent that this sentence is grammatical, it\\nillustrates that either the source or target clause can contain a\\nlogical subject which is parallel to a surface subject in the other\\nclause.  Although examples such as these are often\\nrestricted in their distribution, they demonstrate that the\\nparallelism between elements in the source and target clause need not\\nbe confined to surface syntactic parallelism.\\n\\n\\n  Pragmatic parallelism \\n\\nMore extreme cases exist in which arbitrary information may need to be\\nbrought to bear to determine the appropriate parallelism between\\nsource and target.   cites some such\\ncases:\\n\\n\\n Irv and Mary want to dance together but Mary can't since her husband is here.\\n\\n\\nMary wants to go to Spain and Fred wants to go to Peru, but because\\nof limited resources, only one of them can.\\n\\n\\nRecovery of the pertinent properties in these\\nsentences requires nonlinguistic knowledge concerning social norms and\\n economic processes.  In example (), for instance, the implicit property that only one of Mary and Fred can\\nhave is to do what he or she wants to do.  Other attested\\n examples include: \\n\\n\\nFortunately, the first person to die in 1990 and the\\nfirst couple to file for divorce in 1990 were allowed to do so\\nanonymously.  []\\n\\n\\nAmid applause at the Congress of the   Russian\\nFederation (RSFSR), Mr. Yeltsin put forward a bill setting Russian law\\nabove the law of the Soviet Union - something Mr. Gorbachev, as\\nSoviet president, declared unconstitutional when Estonia, Latvia and\\nLithuania did it last year.  []\\n\\n\\nSentences of this sort illustrate that, to a greater or lesser extent,\\nrelations involved in the resolution of anaphoric processes such as\\nellipsis can be made available contextually.  Identity-of-relations\\nanalyses allow for only the simplest cases of resolution of elided\\nconstituents, since the only mechanism that is available to provide an\\ninterpretation for the target is that of copying an interpretation\\nfrom the source.  Our approach goes beyond identity-of-relations\\nanalyses by allowing for the construction of new relations on the\\nbasis of old ones; the use of unification to construct relations is,\\nas we have seen, more powerful and more flexible than copying.\\n\\n\\nEven for cases of what  call ``surface\\nanaphora'', such as verb phrase ellipsis, resolution is possible to\\nrelations that are pragmatically determined or influenced, as the\\n examples in () show.  To interpret these examples, not only the apparatus we introduce for constructing new\\nrelations but also pragmatic knowledge must be brought to bear.\\n\\n\\n\\n  Further Constraints on Relation Formation \\n\\nCases in which sloppy but not strict readings are available might seem\\nto be problematic for an analysis like the one presented here.  Below\\nwe will examine cases of control and reflexivization in which\\nexclusively sloppy readings are available.  Solutions will be proposed\\nwhich do not involve constraining the process by which relations are\\nformed as interpretations for elided constituents.\\n\\n\\nHowever, there are other cases in which readings are unexpectedly\\nunavailable; these cases generally involve multiple occurrences of\\npronouns whose antecedent is a parallel element in the source clause.\\nIt seems that a constraint is necessary on the possibilities for\\nforming relations in cases such as these.\\n\\n  Obligatory sloppy readings \\n  Control \\n\\nIn general, only sloppy readings are available for sentences involving\\ncontrol.  The following sentence is not ambiguous:\\n\\n\\nJohn tried to run, and Bill did too.\\n\\n\\nThere is no reading according to which Bill tries to bring\\nit about that John runs.\\n\\n\\nChierchia , noting facts of\\nthis type, proposes that the semantic type of a controlled verb phrase\\nis a property rather than a proposition.  That is, for a sentence like\\n``John tried to run'', the correct semantic representation would be\\n(a) and not (b):\\n\\n\\n[a.] \\n\\n\\n\\n\\n[b.] \\n\\ntry(john,run(john))\\n\\n\\nIf Chierchia's hypothesis is correct, the lack of a strict\\nreading is predicted; in the representation in (a), there is only a\\nsingle occurrence of ``john'', and a strict reading is impossible to\\nproduce.\\n\\n\\nHowever, there are reasons to doubt the adequacy of an analysis like\\nthis one.  First, anaphors whose antecedent is the subject of a\\ncontrolled VP can give rise to both a sloppy and a strict reading\\nunder ellipsis.  Consider this sentence:\\n\\n\\n John tried to kill himself before Bill did. \\n\\n\\nSince the reflexive ``himself'' cannot be bound to a higher\\nclause subject, its antecedent must be the subject of ``kill''.  On the\\nproperty analysis, the interpretation for the first conjunct would\\nthen be:\\n\\n\\n\\n\\n\\n\\nHowever, we find this sentence to have two readings, corresponding to\\n the following paraphrases: \\n\\n\\n[a.] John tried to kill himself before Bill tried to kill himself.\\n\\n\\n[b.] John tried to kill himself before Bill tried to kill John.\\n\\n\\nThe second of these readings would not be obtainable\\n given () as the source clause meaning. \\n\\n\\n demonstrates another problem for Chierchia's\\nproperty analysis.  The data she cites bear on Chierchia's claim that\\nthere is a correlation between the syntactic form and the semantic\\ntype of complements: that complements of type VP are properties, while\\ncomplements of type S' are propositions.  Chierchia predicts that\\nthere would be no case in which an S' complement gives rise to only\\na sloppy reading.\\n\\n\\nZec discusses cases of obligatory control in Serbo-Croatian, showing\\nthat there are cases of obligatory control into verbal complements\\nthat are of the syntactic category S' rather than VP.  For example,\\nthe verb pokusati ``try'' takes an S' complement, and\\nyet only a sloppy reading is possible in the following sentence\\n :\\n\\n\\n PetarPetar jeAux\\npokusaotried\\ndathat\\npostanebecome\\npredsednikpresident\\naand\\ntoit\\njeAux\\npokusalatried\\nitoo\\nMarijaMarija\\n``Petar tried to become president and Marija tried it too.''\\n\\n\\nThis sentence means that Marija tried to bring it about that\\nMarija (not Petar) become president.\\n\\n\\nThere seem to be two possibilities with regard to the Serbo-Croatian\\ndata.  The first is to deny that there is a necessary correlation\\nbetween the syntactic type of the complement and its semantic type.\\nThe claim would be in that case that, although the syntactic type of\\nthe complement of ``try'' is an S' in Serbo-Croatian, semantically\\nit is a property.  In that case, the lack of a strict reading would be\\npredicted for the Serbo-Croatian case, just as it is for the English\\n case; the problem illustrated by example () would remain, however.\\n\\n\\nAnother option, and the one that Zec takes, is to posit an obligatory\\ncoreference relation between the subject of ``try'' and the subject of\\nits complement clause; this relation would presumably be induced by\\nthe control verb.  If this option is taken, it would presumably force\\nthe abstraction of both arguments at the same time under second-order\\nmatching.\\n\\n\\nIn our terms, the obligatory coreference relation and the obligatory\\nsloppy readings Zec discusses would mean that controlled occurrences\\nare primary in both Serbo-Croatian and English.  Thus,\\n in (), the interpretation would be \\n\\n\\n\\n\\n\\nwhich, when taken to be a source for ellipsis, would\\ngenerate only two solutions to the equation\\n\\n\\n\\n\\n\\nmanifesting a sloppy reading for the controlled subject\\noccurrence and either a strict or a sloppy reading for the reflexive\\noccurrence, as required.\\n\\n\\n  Reflexivization \\n\\n present a number of cases of reflexivization\\n in which only sloppy readings are available.  The Dutch reflexive zich  is such a case :\\n\\n\\nZijShe\\nverdedigdedefended\\nzichherself\\nbeterbetter\\ndanthan\\nPeterPeter\\n``She defended herself better than Peter.''\\n\\n\\nSells et al. characterize reflexive constructions involving\\nonly sloppy readings as ``closed predicate'' constructions.  They\\ndiscuss only examples in which the reflexive appears in object\\nposition, with its antecedent being the subject of the same clause.\\n\\n\\nWe might assume, then, that for the examples they discuss, the\\npresence of the reflexive correlates with the operation of a semantic\\nrelation-reducing rule, one which semantically ``intransitivizes'' the\\nverb.  In the Dutch case, then, the presence of zich signals a\\nchange in the meaning of the verb from the meaning in (a) to the one\\nin (b):\\n\\n\\n[a.] \\n\\n\\n\\n\\n[b.] \\n\\n\\n\\n\\nA solution of this type would work for all cases of\\nobligatory sloppy readings for reflexives that are described by Sells\\net al., since they consider only cases where the reflexive and its\\nantecedent are arguments of the same predicate, in which a\\nrelation-reducing operation can apply.\\n\\n\\nHowever, this solution would be inappropriate in cases where the\\nreflexive and its antecedent are clearly arguments of different\\npredicates, where a relation-reducing operation cannot apply.\\nAlthough such cases are difficult to find, it may be that the\\nSerbo-Croatian reflexive sebe (genitive svoje) is such a\\ncase.\\n\\n\\nThe following sentence has only a sloppy reading (Draga Zec, personal\\ncommunication):\\n\\n\\nPetarPetar\\njeAux\\nsakriohid\\nstoone\\nhiljadahundred\\ndolaradollars\\nispodunderneath\\nsvojeself's\\nkucehouse\\naand\\ntothat\\njeAux\\nuciniodid\\nialso\\nPavlePavle\\n``Petar hid one hundred dollars underneath self's house, and Paul did\\n(that) too.''\\n\\n\\nThe only reading available for this sentence is that Paul\\nhid one hundred dollars under his own house.\\n\\n\\nWe postulate that the reflexive sebe in Serbo-Croatian engenders\\nprimary as opposed to secondary occurrences, which would then be\\nsubject to the primary occurrence constraint. This would also be true\\nof the English reflexive for those speakers who find that strict\\nreadings with reflexives are unacceptable.\\n\\n\\nIn short, a variety of syntactic constructions give rise to multiple\\nprimary occurrences of parallel elements: control, both of the type\\nseen in English and of the type seen in Serbo-Croatian, and\\nreflexivization in some dialects of English and in Serbo-Croatian.\\n\\n\\n\\n    Antecedent-anaphor constraints\\n\\n  Missing readings with multiple occurrences of anaphora \\n\\nIn cases where there are two pronouns coreferent with the\\nparallel element in the source, one might expect that each pronoun\\nwould give rise to either a strict or a sloppy reading, giving a\\ntotal of four readings for the target clause.  This does not seem to\\nbe the case, however; one of the readings is systematically missing.\\n\\n\\n notes that the following sentence, with two\\noccurrences of pronouns in the source clause, has only three and not\\nfour interpretations:\\n\\n\\n [a.] Bill believed that he loved his wife, and Harry did too. \\n\\n\\n[b.] Harry believed that Bill loved Bill's wife.\\n\\n\\n[b''.] Harry believed that Harry loved Harry's wife.\\n\\n\\n[b''.] Harry believed that Harry loved Bill's wife.\\n\\n\\n[b'''.]  Harry believed that Bill loved Harry's wife.\\n\\n\\nIn this case, the missing reading corresponds to the following unifier\\n for (): \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOther examples illustrate a similar phenomenon.\\n observes that this sentence has only three\\nreadings, not four:\\n\\n\\n [a.] Edith said that finding her husband nude had upset her, and Martha did too.\\n\\n\\n[b.] Martha said that finding Martha's husband nude had upset\\nMartha.\\n\\n\\n[b'.] Martha said that finding Edith's husband nude had upset\\nEdith.\\n\\n\\n[b''.] Martha said that finding Edith's husband nude had upset\\nMartha.\\n\\n\\n[b'''.]  Martha said that finding Martha's husband nude\\nhad upset Edith.\\n\\n\\nThe interpretation paraphrased in 0b''' is missing.\\nThe unifier for P for the missing reading is:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Examples () and () illustrate a  constraint on relation formation.  In example (), the position corresponding to the pronoun ``his'' may not be\\nabstracted unless the position corresponding to ``he'' is also\\nabstracted.  The constraint does not seem to correlate with linear\\n order of the pronouns, though; in example () the position corresponding to the rightmost pronoun may not be abstracted\\nunless the position corresponding to the leftmost pronoun is also\\n abstracted.  The reverse is true in example (), however, where it is the position corresponding to the rightmost pronoun that\\nmust be extracted.\\n\\n\\nAlthough these examples show that the proper generalization about the\\nordering between pronominal positions does not have to do with linear\\norder, they are consistent with the hypothesis that the ordering\\ncorrelates with depth of syntactic embedding.  In both\\n example () and example (), if the position corresponding to a more deeply embedded pronoun is abstracted\\nover, the position corresponding to a less deeply embedded pronoun\\nmust also be abstracted over.\\n\\n\\n However, example () shows that the ordering between positions may not be dependent on syntactic facts at all.\\n Recall that sentence (), repeated here, was not associated  with the reading in (), paraphrased  in (): \\n\\n\\n John revised his paper before the teacher did, and Bill did too. \\n\\n\\n#150\\n\\t\\t\\n\\n\\n\\n\\n\\nJohn and then the teacher revised John's paper; Bill revised John's\\npaper before the teacher revised Bill's paper.\\n\\n\\nOn the excluded reading, the source clause is ``John revised\\nhis paper before the teacher did'', and the target clause is ``Bill\\ndid too''.  ``John'' and ``Bill'' are the parallel elements.  The\\nunifier for P for this reading is:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThese various missing readings can be captured by positing a linking\\nrelationship between the semantics of pronouns and that of their\\nantecedents, and generalizing it to include the relation between the\\nsemantics of terms induced by ellipsis and that of their source\\nparallel element.  Under a suitable definition of this generalized\\nantecedent linking, all of the cases here can be captured by requiring\\nthat if an occurrence is abstracted over, so must its generalized\\nantecedent.\\n\\n\\n  Apparent Syntactic Constraints \\n\\nFinally, we turn to some simple examples that seem to lack any\\nreadings whatsoever.  Consider the following examples, where ``Mary''\\n is taken to be the antecedent of ``she'': \\n\\n\\n\\nJohn gave Mary everything she did.\\n\\n\\n\\nJohn likes Mary, and she does too.\\n\\n\\nThese judgments would follow from an analysis on which\\nsyntactic structure is copied from the source to the target, as the\\nsentences with copies in place violate constraints on binding.\\n\\n\\n\\nJohn gave Mary everything shei gave Maryi.\\n\\n\\n\\nJohn likes Mary, and shei likes Maryi too.\\n\\n\\nHowever, a simple copying analysis faces problems in accounting for\\ngrammatical examples of similar structure:\\n\\n\\nJohn got to Suei's apartment before shei did.\\n\\n\\nJohn voted for Suei because shei told him to.\\n\\n\\nOn a copying analysis, these examples would be incorrectly predicted\\nto be ungrammatical, just as their copied versions are:\\n\\n\\n*John got to Suei's apartment before shei got to Suei's\\napartment.\\n\\n\\n*John voted for Suei because shei told him to vote for Suei.\\n\\n\\nThe resolution of this puzzle remains an open\\nquestion, as does its incorporation in the present analysis; for\\ndiscussion of the problem, see .\\n\\n\\nAnother example that seems to argue for a quite superficial analysis\\nof ellipsis is the following, due to Yoshihisa Kitagawa (personal\\ncommunication to Peter Sells):\\n\\n\\nJohn thinks that Mary will revise his paper before Bill\\nwill.\\n\\n\\non the reading in which Mary revises John's paper and Bill\\nrevises his own paper.  We find the intuition questionable, but it is\\nclearly problematic for our (and many others') analysis if the reading\\nis deemed to be available.\\n\\n\\nFinally, examples which seem to argue for the presence of a gap in the\\nellipsis site include the following:\\n\\n\\n*John met everyone that Peter wondered when he could.\\n , p. 511] \\n\\n\\n *Tom visited everyone who told Sue where to. , p. 218] \\n\\n\\nOn the assumption that long-distance dependencies are\\nsyntactically constrained and that subjacency violations involve an\\nimproper syntactic relation between a filler and a gap, these examples\\nindicate that the ellipsis site contains a gap at syntactic structure.\\n\\n\\n\\n\\n\\n  Conclusion \\n\\nThe underlying idea in the analysis of ellipsis that we have presented\\nhere--namely, the construction of higher-order equations on the basis\\nof parallel structures, and their solution by unification--has been\\nexemplified primarily by the verb-phrase ellipsis construction.\\nHowever, many other elliptical phenomena and related phenomena subject\\nto multiple readings akin to the strict and sloppy readings discussed\\nhere may be analyzed using the same techniques.  The ambiguities in\\ncleft sentences such as\\n\\n\\nIt is Dan who loves his wife.\\n\\n\\nand interpretation of ``only'' with respect to its focus, as in\\n\\n\\nOnly Dan loves his wife.\\n\\n\\nas well as more standard elliptical phenomena such as\\nstripping and comparative deletion can be analyzed in this way as\\nwell, making a broad range of predictions as to the space of possible\\nreadings and their interaction with other semantic phenomena.  It\\nremains for future work to test these potential applications more\\nfully.\\n\\n\\nWe adduce three advantages of the analysis of elliptical constructions\\npresented here over previous alternatives.  First, it is in certain\\nrespects simpler, in that it requires no postulation of otherwise\\nunmotivated ambiguities in the source clause.  Second, it is more\\naccurate in its predictions, especially in allowing readings\\ndisallowed in identity-of-relations analyses.  Third, it is\\nmethodologically preferable in that the analysis follows directly from\\na semantic statement of the ellipsis problem with little stipulation.\\nThe operation on which it relies, higher-order unification, is\\nsemantically sound in that the results it produces are determined by\\nthe meanings of phrases directly rather than by the form of the\\nrepresentations encoding those meanings, as operations of deletion or\\ncopying of portions of such representations are.\\n\\n\\n  Acknowledgements \\n\\nWe would like to thank the following people for helpful discussion:\\nHiyan Alshawi, Sam Bayer, Joan Bresnan, Mark Gawron, Kris Halvorsen,\\nDan Hardt, Julia Hirschberg, David Israel, Mark Johnson, Ron Kaplan,\\nLauri Karttunen, Shalom Lappin, Richard Larson, Peter Ludlow, John\\nMaxwell, Richard Oehrle, Stanley Peters, Hub Prust, Steve Pulman, Mats\\nRooth, Ivan Sag, Peter Sells, Gregory Ward, Michael Wescoat, Annie\\nZaenen, Draga Zec, and two anonymous reviewers.  The written comments\\nalone that we received prior to publication ran to well over 50 pages,\\nlonger, in fact, than the paper itself.  We regret that we could not\\ninclude discussion of all the important issues that they raised.\\n\\n\\n=\\n\\nBibliography \\n\\nJohan van Benthem.\\n1989.\\nCategorial grammar and type theory.\\nJournal of Philosophical Logic.\\nTo appear.\\n\\n\\nGennaro Chierchia.\\n1983.\\nOutline of a semantic theory of (obligatory) control.\\nIn Michael Barlow, Daniel Flickinger, and Michael Wescoat, editors,\\n  Proceedings of the West Coast Conference on Formal Linguistics 2, pages\\n  19-31. Stanford Linguistics Association, Stanford University.\\n\\n\\nGennaro Chierchia.\\n1984.\\nAnaphoric properties of infinitives and gerunds.\\nIn Mark Cobler, Susannah MacKaye, and Michael Wescoat, editors,\\n  Proceedings of the West Coast Conference on Formal Linguistics 3, pages\\n  28-39. Stanford Linguistics Association, Stanford University.\\n\\n\\nGennaro Chierchia.\\n1988.\\nDynamic generalized quantifiers and donkey anaphora.\\nIn M. Krifka, editor, Proceedings of the 1988 Tbingen\\n  Conference. Seminar fr Natrliche-Sprachiche Systeme der\\n  Universitt Tbingen, November.\\n\\n\\nRobin Cooper.\\n1983.\\nQuantification and Syntactic Theory, volume 21 of Synthese\\n  Language Library.\\nD. Reidel, Dordrecht.\\n\\n\\nsten Dahl.\\n1972.\\nOn so-called `sloppy identity'.\\nIn Gothenburg Papers in Theoretical Linguistics, volume 11.\\n  University of Gteborg.\\n\\n\\nsten Dahl.\\n1974.\\nHow to open a sentence: Abstraction in natural language.\\nIn Logical Grammar Reports, No. 12. University of\\n  Gteborg.\\n\\n\\nRobert Fiengo and Robert May.\\n1990.\\nAnaphora and ellipsis.\\n MS, City University of New York and University of California,\\n  Irvine.\\n\\n\\nHarvey Friedman.\\n1975.\\nEquality between functionals.\\nIn R. Parikh, editor, Lecture Notes in Mathematics 453, pages\\n  22-37. Springer-Verlag, Berlin, Germany.\\n\\n\\nMark Gawron and Stanley Peters.\\n1990.\\nAnaphora and quantification in Situation Semantics.\\nCSLI/University of Chicago Press, Stanford University.\\nCSLI Lecture Notes, Number 19.\\n\\n\\nGerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and Ivan A. Sag.\\n1985.\\nGeneralized Phrase Structure Grammar.\\nHarvard University Press, Cambridge, MA.\\n\\n\\nJ. Groenendijk and M. Stockhof.\\n1987.\\nDynamic Montague Grammar.\\nPaper presented at the Workshop on Discourse Representation Theory,\\n  Stuttgart, West Germany, December.\\n\\n\\nIsabelle Hak.\\n1985.\\nThe Syntax of Operators.\\nPh.D. thesis, MIT.\\n\\n\\nIsabelle Hak.\\n1987.\\nBound VPs that need to be.\\nLinguistics and Philosophy, 10:503-530.\\n\\n\\nJorge Hankamer and Ivan A. Sag.\\n1976.\\nDeep and surface anaphora.\\nLinguistic Inquiry, 7(3):391-428.\\n\\n\\nIrene Heim.\\n1982.\\nThe Semantics of Definite and Indefinite Noun Phrases.\\nPh.D. thesis, University of Massachusetts-Amherst.\\n\\n\\nLars Hellan.\\n1988.\\nAnaphora in Norwegian and the Theory of Grammar.\\nForis Publications, Dordrecht.\\n\\n\\nJ. Roger Hindley and Jonathon P. Seldin.\\n1986.\\nIntroduction to Combinators and -Calculus.\\nCambridge University Press, Cambridge, England.\\n\\n\\nJulia Hirschberg and Gregory Ward.\\n1991.\\nAccent and bound anaphora.\\nCognitive Linguistics.\\nTo appear.\\n\\n\\nPaul Hirshbhler.\\n1982.\\nVP deletion and across-the-board quantifier scope.\\nIn James Pustejovsky and Peter Sells, editors, Proceedings of\\n  NELS 12. GLSA, University of Massachusetts-Amherst.\\n\\n\\nJerry R. Hobbs and Stuart M. Shieber.\\n1987.\\nAn algorithm for generating quantifier scopings.\\nComputational Linguistics, 13:47-63.\\n\\n\\nGrard Huet and Bernard Lang.\\n1978.\\nProving and applying program transformations expressed with\\n  second-order patterns.\\nActa Informatica, 11(1):31-55.\\n\\n\\nGrard Huet.\\n1975.\\nA unification algorithm for typed \\n\\n-calculus.\\nTheoretical Computer Science, 1:27-57.\\n\\n\\nRay S. Jackendoff.\\n1972.\\nSemantic Interpretation in Generative Grammar.\\nMIT Press, Cambridge, MA.\\n\\n\\nHans Kamp.\\n1981.\\nA theory of truth and semantic representation.\\nIn Jeroen Groenendijk, Theo Janssen, and Martin Stokhof, editors,\\n  Formal Methods in the Study of Language, pages 277-321, Amsterdam.\\n  Mathematical Centre.\\n\\n\\nYoshihisa Kitagawa.\\n1991.\\nCopying identity.\\nNatural Language and Linguistic Theory.\\nTo appear.\\n\\n\\nJoachim Lambek.\\n1980.\\nFrom -calculus to cartesian closed categories.\\nIn J.P. Seldin and J.R. Hindley, editors, To H.B. Curry: Essays\\n  on Combinatory Logic, Lambda Calculus and Formalism, pages 375-402.\\n  Academic Press, London.\\n\\n\\nShalom Lappin.\\n1984.\\nVP anaphora, quantifier scope, and logical form.\\nLinguistic Analysis, 13(4):273-315.\\n\\n\\nFernando C. N. Pereira.\\n1990.\\nCategorial semantics and scoping.\\nComputational Linguistics, 16(1):1-10.\\n\\n\\nS. G. Pulman.\\n1988.\\nA contextual reasoning and cooperative response framework for the\\n  Core Language Engine.\\nInternal report, SRI Cambridge, Cambridge, England.\\n\\n\\nTanya Reinhart.\\n1983.\\nAnaphora and Semantic Interpretation.\\nUniversity of Chicago Press, Chicago.\\n\\n\\nCraige Roberts.\\n1987.\\nModal Subordination, Anaphora, and Distributivity.\\nPh.D. thesis, University of Massachusetts-Amherst.\\n\\n\\nIvan A. Sag.\\n1976.\\nDeletion and Logical Form.\\nPh.D. thesis, MIT.\\n\\n\\nTraugott Scheibe.\\n1973.\\nZum Problem der grammatisch relevanten Identitt.\\nIn F. Kiefer and N. Ruwet, editors, Generative Grammar in\\n  Europe, pages 482-527. D. Reidel Publishing Company, Dordrecht.\\n\\n\\nLen Schubert and F J. Pelletier.\\n1982.\\nFrom English to logic: Context-free computation of `conventional'\\n  logical translations.\\nAmerican Journal of Computational Linguistics, 10:165-176.\\nReprinted in Grosz et al., 1986.\\n\\n\\nPeter Sells, Annie Zaenen, and Draga Zec.\\n1987.\\nReflexivization variation: Relations between syntax, semantics, and\\n  lexical structure.\\nIn Masayo Iida, Stephen Wechsler, and Draga Zec, editors,   Working Papers in Grammatical Theory and Discourse Structure, pages\\n  169-238. CSLI/University of Chicago Press, Stanford University.\\nCSLI Lecture Notes, Number 11.\\n\\n\\nMark J. Steedman.\\n1990.\\nGapping as constituent coordination.\\nLinguistics and Philosophy, 13(2):207-263.\\n\\n\\nBonnie Lynn Webber.\\n1978.\\nA Formal Approach to Discourse Anaphora.\\nPh.D. thesis, Harvard University.\\n\\n\\nMichael Wescoat.\\n1989.\\nSloppy readings with embedded antecedents.\\n MS, Stanford University.\\n\\n\\nEdwin Williams.\\n1977.\\nDiscourse and logical form.\\nLinguistic Inquiry, 8(1):101-139.\\n\\n\\nDraga Zec.\\n1987.\\nOn obligatory control in clausal complements.\\nIn Masayo Iida, Stephen Wechsler, and Draga Zec, editors,   Working Papers in Grammatical Theory and Discourse Structure, pages\\n  139-168. CSLI/University of Chicago Press, Stanford University.\\nCSLI Lecture Notes, Number 11.\\n\\n\\n\\n\\nFootnotes\\n\\n  We emphasize\\nthat although the bulk of the examples in this paper involve VP\\nellipsis, the techniques can be applied to the semantic interpretation\\nof many other elliptical constructions.  This is in part because we do\\nnot restrict the notion of parallel elements to the subjects of the\\nsource and target clauses (see\\n Section ).  Indeed, the parallelism between the clauses need not even be purely syntactic (see\\n Section ).  The generality of this ellipsis resolution method is one of its primary advantages.\\n   For brevity, we represent the semantic interpretation of `Dan's wife' in this case as\\n\\n;\\nthe important thing to note about this representation\\nis that the semantics of the pronoun `his' is identical to that of its\\nantecedent, `Dan'.  Any other notation that has this property would do\\nas well here.  In particular, a treatment of possessives as\\nintroducing bound variables along the lines of the quantifier\\nassumptions described in\\n Section  is possible and perhaps preferable.  Notations such as \\n\\n\\ncan be thought\\nof as abbreviatory of such analyses.  We only use such notations for\\ncases in which the space of readings generated is unaffected.\\n  The use\\nof higher-order unification to resolve elliptical constructions has\\nbeen independently noted by other researchers.  Pulman and Milward\\nimplemented a prototype system that handled simple cases of VP\\n ellipsis and gapping along these lines .  Pareschi and Steedman's ``left conjunct decomposition'' operation, which is\\nused for the parsing of gapping constructions, bears a certain\\nresemblance to higher-order unification as well\\n . \\n  At this point, we can\\nignore the distinction between primary and secondary occurrences.\\n  Since multiple\\noccurrences of the same proper name do not necessarily co-denote, we\\nwill use different constants as the representations of the denotata of\\nsuch occurrences.  For instance, the interpretation of the sentence\\n\\n\\n[(a)]Dan likes Dan's wife.\\n\\n\\nshould be \\n\\n\\nwhere dan1 and dan2 are separate constants that only\\ncontingently co-denote.  As a consequence, an ellipsis like\\n\\n\\n[(b)]Dan likes Dan's wife, and Bill does too.\\n\\n\\nhas only a strict reading, which accords with conventional\\nwisdom.\\n  Solutions involving\\nvacuous abstraction, such as\\n\\n\\n\\n\\n\\nare ruled out where\\nnecessary as special cases of this more general constraint.\\nA direct prohibition against vacuous abstraction might be\\ntoo strong, since verb phrase ellipsis is possible even in cases where\\nthe subject of the source clause is pleonastic and makes no\\nsemantic contribution (examples due to Ivan Sag):\\n\\n\\nJohn said it would rain, and it did.\\nJohn said there would be trouble, and there was.\\n\\n\\nSuppose the interpretation of the former example (ignoring\\ntense and aspect as usual) were\\n\\n\\n\\n\\n\\nThen the\\nsecond-order matching problem induced by the ellipsis would be\\n\\n\\n\\n\\n\\n(We use the symbol \\nfor specifying the\\ninterpretation of pleonastic elements, following \\n.)  The requirement of abstraction of primary\\noccurrences would still allow a binding for P involving vacuous\\nabstraction, namely \\n\\n.\\nThe elliptical clause would\\ntherefore be interpreted as rain.\\n  We are\\nindebted to an anonymous reviewer for this example.\\n  The\\nincorporation of an ellipsis analysis such as ours into a\\ntransformational framework is quite conceivable, merely requiring the\\nability to form abstractions over the syntactic objects representing\\nsemantic construals of sentences, that is, LF trees.  The intrinsic\\nportion of the analysis is its use of an equational framework for\\ndeclaratively characterizing ellipsis resolution, not its use of\\nparticular logics for the representation of meanings.  Nonetheless,\\nthe use of typed lambda calculus allows us to directly state our\\nanalysis with a minimum of extraneous machinery.\\n  Formally similar store elements have been used in\\nquantifier scoping systems such as those of Schubert and Pelletier\\nSchubertPelletier82 and Hobbs and Shieber\\n.\\n  Dynamic Montague\\n grammar , provides a more complex and possibly more general approach to incorporating\\nsome of the aspects of DRT into a compositional framework.\\n  See\\n Footnote  for a discussion of the status of the function symbols pwi and phone.\\n  Other analyses of polarity, for\\ninstance as an implicit argument of the predicate, will yield similar\\nresults.\\n  For simplicity of exposition, we ignore\\nthe fact that the polarities of the two sentences differ.  See\\n Section  for a discussion of this issue. \\n  As the\\nsemantic roles for `Bill' are parasitic on those for `John', we let\\nthe primary occurrences of `Bill' be determined by those of `John' in\\nthe source sentence.\\n  Sag uses the\\nnotation \\n\\n\\nto represent the application of a VP\\nmeaning \\n\\n\\nto the subject meaning a.  The scope of\\nthe negation operator is the entire predication.\\n   proposes a combination of\\nCooper storage and copying for interpreting sentences such as\\n ().  His scheme differs substantively from ours in that he copies both matrix and store from the storage counterpart to\\ngive the interpretation of the elided material.  However, such a\\nscheme fails to preserve the scope parallelism noted in\\n Section . \\n  Here and\\nelsewhere, we uniformly rename bound variables apart for clarity.\\n  We are indebted to James McCawley and\\nBonnie Webber for bringing these examples to our attention.\\n  Additional readings arise if the\\nsource clause for the ellipsis is taken to be not the matrix ``John\\ntried to kill himself'', but the VP complement ``to kill himself'':\\n[(a)]\\nJohn tried to kill himself before Bill killed himself.\\n[(b)]\\nJohn tried to kill himself before Bill killed John.\\n\\n\\nSimilar comments apply to these examples.\\n  Examples of this\\ntype are due to .\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe present a new method for characterizing the interpretive\\npossibilities generated by elliptical constructions in natural\\nlanguage.  Unlike previous analyses, which postulate ambiguity of\\ninterpretation or derivation in the full clause source of the\\nellipsis, our analysis requires no such hidden ambiguity.  Further,\\nthe analysis follows relatively directly from an abstract statement of\\nthe ellipsis interpretation problem.  It predicts correctly a wide\\nrange of interactions between ellipsis and other semantic phenomena\\nsuch as quantifier scope and bound anaphora.  Finally, although the\\nanalysis itself is stated nonprocedurally, it admits of a direct\\ncomputational method for generating interpretations.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nThere are thousands, if not millions, of application programs in\\neveryday use that automatically generate texts; but probably fewer than\\nten of these programs use the linguistic and knowledge-based techniques that\\nhave been studied by the Natural-Language Generation (NLG) community.\\nThe other 99.9% of systems use programs that simply manipulate character\\nstrings, in a way that uses little, if any, linguistic knowledge.\\nFor lack of a better name, I will call this the `template' approach.\\n\\n\\nIn order for NLG technology to make it out of the lab and into everyday\\nfielded application systems, the NLG community will need to prove\\nthat there are at least some niches\\nwhere using linguistic/AI approaches in a text-generation system\\nprovides real commercial advantages, such as reducing the effort required\\nto build (or maintain) the system, or improving the effectiveness of the\\ngenerated texts.\\nDetermining under what conditions and in what aspects NLG techniques\\nare `better' than character-string\\nmanipulation is of utmost importance to the applied NLG community, and\\nshould also be of interest to the research community;\\nif nothing else,\\nresearch funding for NLG is likely to increase if there are\\na large number of successful\\nfielded systems that use NLG technology.\\n\\n\\nIn this paper, I will use the term automatic text generation (ATG)\\nto refer to any computer program\\nthat automatically produces texts from some input\\ndata, regardless of whether NLG or template technology is used internally.\\nThe topic of this paper is thus when is NLG `appropriate technology'\\nfor building ATG systems, and when should simpler approaches be used.\\nMy goal is not to provide a definitive\\nanswer to this question, because I don't think we (currently) know enough to be\\nable to do this, but rather to present the issues, summarize\\ncomments made by other people, and present some opinions of my own.  Hopefully\\nthis will help start a discussion within the community about this very\\nimportant but (so far) somewhat ignored issue.\\n\\n\\n    Template systems\\n\\n\\nAll ATG systems are, of course, simply computer programs that run on some\\ninput data and produce an output (the text) from this data.\\nNon-linguistic (`template') text-generation is done via manipulating\\ncharacter strings; the user writes\\na program which includes statements such as `include XXXXX\\nif condition Y is true, and YYYYY otherwise.'  This program\\ncan be written directly in a programming language such as Lisp or C++,\\nor it can be specified via a `mail-merge' system which allows conditional\\ntexts (eg, Microsoft Word).  The key difference between\\nthis approach and NLG is that all manipulation is done at the\\ncharacter string level; there is no attempt to represent the text in any\\ndeeper way, at either the syntactic or `text-planning' level.\\n\\n\\nTo the best of my knowledge, most programming languages and mail-merge\\nenvironments provide very little, if any, support for manipulating texts\\nin even the simplest `linguistic' manner.\\nFor programming languages, the most sophisticated\\nfeature that I am aware of is the ~P construct in the Lisp\\nformat function, which will do some simple pluralizations\\n(eg, win vs. wins,\\nor try vs. tries) depending on whether a numeric\\nparameter is one or not.\\n\\n\\nMail-merge systems can have slightly more sophisticated capabilities, such as\\nautomatically capitalizing\\nan inserted word if it is the first word of a sentence.  However, even\\nsomething as simple\\nas changing pronouns according to gender\\nneeds to be explicitly programmed.\\nSome mail-merge systems are integrated with grammar checkers that might in\\ntheory be able to handle some low-level syntactic problems such as\\nverb agreement, a vs. an, and elimination of multiple\\ncommas; however, current grammar checkers may not be robust enough to be able\\nto do this in a reliable fashion.\\n\\n  Example: Apple Balloon Help \\n\\nA simple example of template-based generation is\\nthe Apple Macintosh Balloon Help system.  It can produce texts such as\\nThis is the kind of item displayed at left.  This shows that test data\\nis a(n) Microsoft Word document.\\nand\\nThis is a folder -- a place to store related files.  Folders can contain\\nfiles and other folders.\\n\\n\\nThe icon is dimmed because the folder is open.\\nIn the first text, test data and Microsoft Word were inserted into\\ntemplate slots for `filename' and `application program.'  Note the use of\\na(n); even this simple type of agreement is not done in the\\nBalloon Help system.\\nIn the second\\ntext, the last sentence (The icon is dimmed because the folder is open)\\nonly appears when the mouse is positioned over an open folder; just the\\nfirst two sentences will appear if the mouse is positioned\\nover a closed folder.  This is an example of conditional text.\\n\\n\\n    Example: Employee Appraiser and Performance Now\\n\\n\\nTwo more sophisticated `non-linguistic' automatic text-generation systems are\\nAustin-Haynes Employee Appraiser and\\nKnowledgePoint's Performance Now.  Both of these systems help managers\\nwrite appraisals of employees (eg, for justifying salary increases).\\nEach system provides a set of general evaluation topics, such\\nas Communication, which are broken up into more specific subtopics,\\nsuch as Communicates ideas verbally.  Managers give employees rankings\\non each of these subtopics, and the system then composes a complete appraisal,\\nwhich the manager can edit in a word processor.\\n\\n\\nIn Employee Appraiser, when the manager chooses a subtopic rating, the system\\nretrieves an appropriate paragraph from a library and does some simple\\nlinguistic processing.  In particular, the manager can specify whether the\\nemployee is male or female, and whether the report should be written in\\nsecond or third person.\\nThis affects\\nthe pronouns used in the text (eg, he, she, or\\nyou), and also verb conjugation (eg, you do vs.\\nhe does).  This is the most sophisticated syntactic processing\\nthat I am aware of in a `template' system.\\n\\n\\nPerformance Now does less syntactic processing (it does not allow the manager\\nto choose between second and third person; only third-person is possible),\\nbut it does do some simple sentence planning.  In particular,\\nPerformance Now combines all the information about a particular\\nhigh-level topic (such as Communication) into a single\\nparagraph, and this requires the system to use conjunctions and pronouns,\\nand to add initial conjuncts (eg, Furthermore) to sentences.\\n\\n\\nAn example output of Performance Now is\\nBert does not display the verbal communication skills required, and his\\nwritten communications fall short of the quality needed.  Additionally,\\nhe does not exhibit the listening and comprehension skills necessary\\nfor satisfactory performance of his job.\\nThe system has composed this from three separate phrases retrieved from its\\nlibrary.  The first two phrases are combined with and to produce\\nthe first sentence above.  The third phrase is left as a separate sentence,\\nbut the conjunct Additionally is added to it, and the subject is\\npronominalized.  The system also orders phrases by putting the most\\npositive ones first, and most negative ones last (this is not shown in the\\nabove example).\\n\\n\\nPerformance Now performs the most sophisticated sentence-planning of any\\n`template' system that I am aware of; indeed, one might argue that Performance\\nNow is doing enough linguistic processing that it really should be regarded as\\na (simple) NLG system.  KnowledgePoint's marketing literature in fact\\nstresses their `IntelliText[TM]' technology, which ``generates clear,\\nlogical sentences and modifies them to work together as if you wrote them\\nyourself''.  This is the only mass-market system I am aware of which advertises\\nNLG-like abilities as part of its competitive advantage.\\n\\n\\n\\n  Advantages of NLG \\n\\nMany advantages of NLG over templates have been described in the literature.\\nIn this section, I try to summarize these arguments, paying special\\nattention to those arguments\\nthat seem important to the success of current applied NLG systems.\\n\\n  Maintainability \\n\\nOne reason for using NLG is maintainability; template-based generators\\ncan be difficult to modify according to changing user needs.\\nThis has been a real factor in the success of the FoG weather-report\\n generation system, for example.  To quote , \\nExperience has shown that [template-based weather report generators are]\\ndifficult to maintain.  This has hampered the testing and implementation\\nof the software and has made it difficult to update the program for changing\\nuser requirements.  This is a critical factor.  Although the Canadian\\ntextual forecast products fall into several common broad categories (marine\\nforecasts, public forecasts, and so on), each category contains many\\nregional variations.  Also, content, structure, and terminology tend to vary\\nwith time, albeit slowly.  To succeed, a system must address variations\\nbetween forecast types, variations between geographical regions in a forecast\\ntype, and gradually changing requirements.\\nMaking even a slight-change to the output of a template-based generator\\nmay require a large amount of recoding (of programs) and rewriting\\n(of templates); in contrast, such a change may be straightforward to make in\\nlinguistically-based system.  To take one simple example,\\nif a user wishes to change a text-generation system so that dates are\\nalways at the beginning of sentences (eg,\\nIn 1995, a severe winter is expected instead of\\nA severe winter is expected in 1995), this can easily be done\\nwith almost any linguistically-based NLG system.  With a template system,\\nin contrast, making this change may require rewriting a large number of\\n template fragments. \\n\\n\\nThere is an interesting analogy with expert systems here.  Early expert\\nsystems, such as the R1/XCON system used to configure computers at Digital\\n Equipment Corporation , were partially justified on the grounds that they were easier to maintain and modify than `conventional' programs\\nthat performed the same task.  It subsequently became clear, however\\n (eg, ), that maintaining expert systems, although still perhaps easier than maintaining conventional programs that performed similar\\ntasks, was not as simple as the initial enthusiasts had thought it would be.\\nWe as yet have little data on how easy/difficult it really is to maintain\\n fielded NLG systems;  is the only paper I am aware of that discusses the maintenance of fielded NLG systems.\\n\\n\\n  Improved Text Quality \\n\\nAnother advantage of NLG-based systems is that they can produce\\nhigher-quality output.  It is useful when\\ndiscussing output quality to distinguish between aspects of quality that\\narise from the three different processing\\n stages used in most applied NLG systems : content/text planning, sentence planning, and syntactic realization.\\n\\n  Content Planning \\n\\nThe ability to vary the information content of a text in a fine-grained\\nand flexible way may be the most important `quality' enhancement\\nof all; it allows NLG-based systems to include whatever information is deemed\\nimportant in a text, and leave out unimportant information.  This has\\nbeen especially important in letter-generation systems\\n (eg, ,) which have been one of the most popular NLG applications to date.\\n To quote , \\nAn automated form-letter system originally formed the core of this\\norganization's correspondence facility.  Because its letters must specifically\\ndiscuss different kinds of financial transactions, the system has grown\\nto include close to 1,000 different form letters to address the simplest\\ndivisions of common problems.  However, in practice most of these letters\\nare never used: Customer service representatives, working under pressure\\nto handle as many cases as quickly as possible, tend to use 10 to 20\\nletters that are close enough to describing the client's situation rather\\nthan take the time to discriminate between slight variations within the\\nform library.  When a client's situation even slightly varies from these\\nforms or encompasses a combination of topics addressed in separate form\\nletters, a new letter must be composed by hand if the client is to be\\nconvinced that s/he has received individual attention.  Form-letter systems\\nmight come cheap, but they don't always stay that way, and the quality of\\noutput for any particular situation can never be very high.\\nIn other words, if a text-generation system has to be able to generate texts\\nthat are appropriate for many different kinds of situations, it may be\\ndifficult to use as well as build; and there may be a strong argument for\\nbuilding a system which uses knowledge-based techniques to represent the\\ndesired content of the output, and then generates an appropriate textual\\npresentation of this context.\\n\\n\\n    Sentence Planning\\n\\n\\nMost applied NLG systems have a sentence planning module that handles\\naggregation, referring-expression generation, sentence formation, and\\n lexicalization . Performing these tasks well can greatly enhance\\nthe readability of a text.  Consider, for example, the difference between\\nThe house is white.  The house is large.  The house is owned by\\nJohn.  The house is on Sullivan Street.  The house is next to the elementary\\nschool\\nand\\nJohn owns a large white house on Sullivan Street.\\nIt is next to the school.\\n This is an example of aggregation . Aggregation is mentioned as one of the most important benefits\\n provided by the PLANDOC system . PLANDOC summarizes the history\\nof an engineer using a simulation package, and generates text such as\\nThis refinement activated  DLC for CSA 2111 in 1995 Q3, for CSAs 2112 and 2113\\nin 1995 Q4, and for CSAs 2114, 2115, and 2116 in 1996 Q1.\\nIf each of the activations was expressed by a separate sentence, the above\\nmessage would require six separate sentences, and would be much longer.\\n\\n\\nAn interesting open question is how much sentence planning can be done\\nwithout having a `proper' syntactic representation of the text.\\n The Performance Now (Section ) system demonstrates that some simple aggregation can be done even if phrases are represented\\nas character strings,\\nbut it seems doubtful whether more sophisticated aggregation (eg, ellipsis\\nor relative-clause introduction) is possible\\nwithout a syntactic representation of phrases.\\n\\n\\n  Syntactic Realization \\n\\nTexts that are comprehensible but ungrammatical can be\\nannoying to readers, and it may be expensive (in terms of programming effort)\\nto set up a template system to correctly handle agreement, morphology,\\npunctuation reduction, and other `low-level' phenomena.\\nIt is straightforward, in contrast, for an NLG system to handle\\nsuch phenomena.\\n\\n\\nInterestingly enough, however, I am not aware of any applied NLG\\nsystem whose success is primarily based on better syntactic (or morphological)\\n processing.  Systems such as FoG  and PLANDOC   do possess sophisticated syntactic realization systems, but I do not believe that their success derives from the fact that they\\ncan get agreement or morphology right.  Template systems such as\\nEmployee Appraiser are, after all, able to handle some agreement phenomena.\\nParticularly if a developer is only concerned with relatively\\nstraightforward phenomena (eg, noun pluralization, or noun-verb agreement), it\\nmay be easier for him or her to `hack' something together that\\nappropriately manipulates character strings, instead of trying to build\\nexplicit syntactic structures that can be processed by an NLG system.\\n\\n\\nAlso, in many cases texts can be phrased in a manner which minimizes the\\nneed for syntactic adjustment.  For example, problems will occur with the\\ntemplate N iterations were run  when N is 1; these problems can\\nbe avoided, however, by changing the text to Number of iterations\\nrun: N.\\n\\n\\nA good syntactic module may of course be needed to support a sophisticated\\ncontent determination or sentence planning module.  For example, as mentioned\\nin the previous section, proper syntactic representation is probably needed\\nfor many kinds of aggregation.\\nBy itself, however, good syntactic processing\\nmay not provide much `competitive advantage' to an NLG system.\\n\\n\\n\\n    Other advantages\\n\\n\\nTwo other advantages of NLG that may be important in some cases are\\nmultilingual output and guaranteed conformance\\nto standards.  Multilingual output can of course be achieved with templates;\\nmany error-message systems, for example, are localized to other languages\\nsimply by inserting a new set of format strings.  The quality of texts\\ngenerated by this approach is not high, but this may be acceptable in some\\ncircumstances.\\n\\n\\nAt the other extreme, multilingual output could also be achieved by building\\nseveral separate systems, one for each target language.  Such a system\\nwould be expensive to construct and might prove difficult to maintain, however.\\n\\n\\n The FoG weather-report generation system  probably owes some of its success\\nto the fact that it can produce texts in both French and English.  Weather\\nreports in Canada must be produced in both French and English, and if reports\\nare first written in one language and than translated into the other, there\\nmay be a significant delay before the translated reports are available\\n(even a one-hour delay can be significant for a 24-hour weather forecast).\\nThe FoG system enables the forecaster to simultaneously produce both English\\nand French versions of the forecast, thus eliminating\\n this delay. \\n\\n\\nThe final advantage of NLG I'd like to mention is guaranteed conformance\\nto document standards, including writing standards such as AECMA Simplified\\n English ,  and content standards such as DoD-2167A . In many domains it is essential that documents conform to such standards,\\nand rules such as `sentences should not be longer than 20 words' \\nor `sentences should not contain more than three sequential nouns' (from AECMA\\nSimplified English) may be easier to enforce in an NLG system, which\\ncan paraphrase or reword texts to meet such constraints.\\nAn NLG system could,\\nfor example, take sentence-length constraints into account when\\nmaking aggregation decisions.\\nI do not know\\nof any current applied NLG system for which standards conformance is an issue,\\nbut this may become important in future applications.\\n\\n\\n\\n  Advantages of Templates \\n\\nTemplates, of course, also have advantages over NLG.  The most basic of these\\nis probably that NLG systems\\ncannot generate text unless they have a representation of the information\\nthat the text is supposed to communicate; and in the great majority of\\ntoday's application systems, such representations do not exist.\\n\\n\\nFor instance, suppose a scientific program wishes to inform the user that\\nN iterations of an algorithm were performed.\\nIn principle, NLG techniques could\\nbe used to improve the handling of special cases such as N=0 and N=1, so that\\nthe system could produce\\nNo iterations were performed\\n1 iteration was performed\\n2 iterations were performed\\nHowever, doing this with NLG techniques\\nwould require the system to have either a declarative\\nrepresentation of concepts such as algorithms and iterations; or a syntactic\\nrepresentation of the sentence N iterations were performed.\\nNeither of these is likely to exist in a scientific program, and few\\nscientific programmers would bother putting them in.  Instead, such programmers\\nwould either accept low-level syntactic problems (eg, the output\\n1 iteration were performed);\\nuse an alternate formulation\\nthat did not suffer from this problem (eg, number of iterations\\nperformed: 1); or write special code to produce the appropriate output\\nwhen N is 0 or 1.\\n\\n\\nThis is perhaps an extreme case, but it illustrates the point that switching\\nto NLG will be expensive if the application does not already have a\\ndeclarative domain knowledge base and/or syntactic representations of output\\ntext, and no one is going to pay this cost if the resultant improvement in\\ntext quality (or system maintainability) is not perceived as significantly\\nenhancing the usefulness of the application.  Since knowledge-based\\napplication systems are still rare, and even the ones that do exist often\\ndo not have all the information that an NLG system would need, it may be\\nthe case that NLG is not the most\\nappropriate technology for many current text-generation applications.\\n\\n\\nBesides the above problem, NLG also suffers from generic problems that\\nare common to all new technologies.\\nThere are very few people who can build NLG systems,\\ncompared to the millions of programmers who can build template systems;\\nthere is also very little awareness of what NLG can (and cannot) do among\\nmost developers of application systems.\\nAdditionally, there is very little in the\\nway of reusable NLG resources (software, grammars, lexicons, etc), which\\nmeans that most NLG developers still have to more or less start from scratch.\\nFinally, the fact that NLG is an experimental technology means that\\nconservative developers may want to avoid using it.  As mentioned above,\\nthese problems are common to all new technologies, and will evaporate\\nwith time if NLG proves to be a truly useful and valuable technology.\\n\\n\\n    Hybrid Systems\\n\\n\\nIt is of course possible to build ATG systems that use both NLG and template\\ntechniques.  To date, two variants of this have been particularly common:\\n\\nSystems that embed NLG-generated fragments into a template slot,\\nor that insert canned phrases into an NLG-generated matrix sentence.\\n The IDAS system , for example, could insert generated referring expressions\\ninto templates such as Carefully remove the wrapping\\nmaterial from X, and also could insert canned phrasal modifiers such as\\nusing both hands into an otherwise generated sentence.\\n\\nSystems that use NLG techniques for `high-level' operations such as\\ncontent planning, but templates for low-level realization\\n (eg, ). \\n\\n\\nThe basic goal of such systems is to use NLG where it\\nreally `adds value', and to use simpler approaches where NLG is not needed\\nor would be too expensive.  The decision on where NLG should be used can\\n be based on cost-benefit analysis . \\n\\n\\nReal-world decisions about where NLG should be used in a hybrid system\\nare likely to be based on practical criteria as well as theoretical ones.\\nNLG modules that are slow, error-prone, written in unusual programming\\nlanguages, and/or difficult to maintain will\\nof course not get used much in real applications.  But also, even a\\nwell-engineered module is unlikely to get used if it does not fit into\\nthe way the developer wishes to build his or her system, or give the developer\\nsufficient control over the system's output.  For example, a system that\\nreserves the right to reorder sentences based on some rhetorical model\\nmay be unacceptable to a developer who insists that the sentences must appear\\nin a specific order that he or she thinks is best.\\n\\n\\nAnother way of saying this is that NLG shouldn't `get in the way'.\\nDevelopers will use NLG modules and techniques if NLG helps them produce the\\nkind of texts they want to produce; if an NLG system is seen not as a helpful\\ntool but as something that needs to be worked around, it will not be used.\\nNLG should also only be used when it clearly increases maintainability, text\\nreadability, or some other important attribute of the target application\\nsystem.  If a certain portion of the output text never varies, for example,\\nit would be silly to generate it with NLG, and much more sensible to\\nsimply use canned text for this portion of the document.\\n\\n\\nI believe, by the way, that most current hybrid systems\\nuse `real NLG' in content-determination and perhaps sentence-planning,\\nand use template techniques mainly in syntactic realization.  This may\\nsimply be a coincidence, but it may also suggest\\nthat much of the real `value-added' of many NLG systems may be\\nin the high-level processing, not in ensuring correct syntax.\\n\\n\\n  Making NLG More Useful \\n\\nThere are several areas where I think more academic research could help\\nimprove the advantages of NLG-based text\\n generators over template-based systems. \\n\\n\\nAggregation:\\n As mentioned in Section , aggregation (eg, clause\\ncombining, ellipsis, conjunction reduction) is something that\\nseems to significantly add to text quality in many\\ncircumstances.  Yet, there has been surprisingly little research on this\\nvery important and interesting topic.\\nStandards Conformance:\\nBeing able to generate text that is guaranteed\\nto conform to a writing or content standard could be a big selling point of\\n NLG in some circumstances (Section ). Currently, however, it is only possible to\\nenforce `low-level' syntactic and lexical standards; I think it would\\nbe very interesting to examine how higher level standards, such as\\n`only one topic per paragraph', might be enforced.\\nMultilinguality:\\nMultilingual output is another feature that could be\\na strong selling point for NLG in many circumstances\\n (Section ).  But although many multilingual NLG systems have been built, surprisingly little research has\\nbeen done on the principles underlying multilinguality.\\nFor example, where can language\\nindependent modules be used in a multilingual system, and where is this\\nimpossible?\\nMultimodality:\\nReal-world documents include diagrams, tables, and other\\ngraphics as well as text;\\nand real-world documents also use visual formatting,\\nsuch as font changes and bulletized lists, within texts.  Additionally,\\non-line documents often include hypertext links.  A system that generates\\ndocuments is going to be much more useful if it can\\ncombine text and graphics, use appropriate visual formatting, and insert\\nhypertext links into online documents.\\nHybrid Systems:\\nIt seems safe to predict that many fielded NLG systems,\\nat least in the near term, will use a hybrid approach, ie, they will use both\\ntemplate and NLG technology.  But little is known about how this should best\\nbe done; if we want to insert a generated referring expression into a template\\nslot, for example, what constraints does the template need to satisfy in order\\nfor this to produce correct output?\\nAnd how should `templates' used by an\\nNLG system be authored; can we develop a nice authoring environment which\\nenforces any necessary constraints in an intuitive manner?\\nModifying Generated Text:\\nAll real-world NLG systems that I am aware of allow the human user to\\nmodify the generated texts.  But there are many ways of doing this, and\\nit is unclear which is best.  Should the generated text simply be dumped into\\n a conventional word-processor, as done in ICG ? Or should users make changes with a structured editor, perhaps using the\\n `linguistic spreadsheet' idea proposed by ? Or is it best to ask\\nthe user to edit a conceptual representation, as done in\\n? Examples:\\nIn many cases, adding examples to a text can greatly increase\\nits usefulness.\\nBut this is another research topic that has been barely\\n scratched; Mittal's thesis  and subsequent work is the only research in this area that I am aware of.\\nWhat principles govern the creation of good examples, and how can\\na system generate examples that not only communicate the target information,\\nbut also are consistent with the user's general world knowledge?\\nKnowledge acquisition:\\nThis is not really an `NLG' topic, but it is\\nvery important to the success of NLG systems, which usually are\\nknowledge intensive.  Anything that makes it easier to build knowledge bases\\nwill probably make it easier to build NLG systems.\\n\\n\\n  Conclusion \\n\\nAs NLG technology begins to move out of the lab and into real applications,\\nthe NLG community needs to begin thinking not just about how to generally\\nimprove our understanding of this research area, but also about questions\\nsuch as (a) what advantages NLG\\noffers over simpler approaches; (b) under what circumstances\\nusing NLG `adds value' to real-world systems; and (c) where further advances\\nin NLG could really increase the usefulness of applied NLG systems.\\nIt will probably be many years before we can confidently provide answers\\nto these questions, but an important step on this path would be to start\\nmore explicitly discussing and exploring these issues within\\nthe community; I can only hope that the presentation in this paper will at\\nleast in a small way encourage people to start thinking more\\nabout these issues.\\n\\nBibliography \\n\\nAECMA.\\nA guide for the preparation of aircraft maintenance documentation in\\n  the international aerospace maintenance language, 1986.\\nAvailable from BDC Publishing Services, Slack Lane, Derby, UK.\\n\\n\\nB. Buchanan, J. Moore, D. Forsythe, G. Carenini, and S. Ohlsson.\\nUsing medical informatics for explanation in a clinical setting.\\nTechnical Report 93-16, Intelligent Systems Laboratory, University of\\n  Pittsburgh, 1994.\\n\\n\\nJose Coch and Raphael David.\\nRepresenting knowledge for planning multisentential text.\\nIn Proceedings of the Fourth Conference on Applied Natural\\n  Language Processing (ANLP-1994), pages 203-204, 1994.\\n\\n\\nHercules Dalianis and Eduard Hovy.\\nAggregation in natural language generation.\\nIn Proceedings of the Fourth European Workshop on Natural\\n  Language Generation, pages 67-78, 1993.\\n\\n\\nMilitary Standard DoD-Std-2167A: Defense System Software Development, 1988.\\n\\n\\nEli Goldberg, Norbert Driedger, and Richard Kittredge.\\nUsing natural-language processing to produce weather forecasts.\\nIEEE Expert, 9(2):45-53, 1994.\\n\\n\\nGerard Kempen, Gert Anbeek, Peter Desain, Leo Konst, and Koenraad DeSmedt.\\nAuthor environments: Fifth generation text processors.\\nIn Directorate General XIII, European Commission, editor,   ESPRIT'86 Results and Achievements, pages 365-372. Elsevier, 1986.\\n\\n\\nRichard Kittredge, Eli Goldberg, Myunghee Kim, and Alain Polgure.\\nSublanguage engineering in the FOG system.\\nIn Proceedings of the Fourth Conference on Applied Natural\\n  Language Processing (ANLP-1994), pages 215-216, 1994.\\n\\n\\nJohn McDermott.\\nR1: A rule-based configurer of computer systems.\\nArtificial Intelligence, 19:39-88, 1982.\\n\\n\\nKathleen McKeown, Karen Kukich, and James Shaw.\\nPractical issues in automatic document generation.\\nIn Proceedings of the Fourth Conference on Applied\\n  Natural-Language Processing (ANLP-1994), pages 7-14, 1994.\\n\\n\\nVibhu Mittal.\\nGenerating natural language descriptions with integrated text and\\n  examples.\\nResearch Report ISI/RR-93-392, Information Sciences Institute,\\n  University of Southern California, Marina del Rey, California, 1993.\\n\\n\\nEhud Reiter.\\nHas a consensus NL Generation architecture appeared, and is it\\n  psycholinguistically plausible?\\nIn Proceedings of the Seventh International Workshop on Natural\\n  Language Generation (INLGW-1994), pages 163-170, 1994.\\n\\n\\nEhud Reiter and Chris Mellish.\\nOptimising the costs and benefits of natural language generation.\\nIn Proceedings of the 13th International Joint Conference on\\n  Artificial Intelligence (IJCAI-1993), volume 2, pages 1164-1169, 1993.\\n\\n\\nEhud Reiter, Chris Mellish, and John Levine.\\nAutomatic generation of technical documentation.\\nApplied Artificial Intelligence, 9, 1995.\\nForthcoming.\\n\\n\\nElliot Soloway, Judy Bachant, and Keith Jensen.\\nAssessing the maintainability of XCON-in-RIME: Coping with the\\n  problems of a very large rule-base.\\nIn Proceedings of the Sixth National Conference on Artificial\\n  Intelligence (AAAI-1987), volume 2, pages 824-829, 1987.\\n\\n\\nStephen Springer, Paul Buta, and Thomas Wolf.\\nAutomatic letter composition for customer service.\\nIn Reid Smith and Carlisle Scott, editors, Innovative\\n  Applications of Artificial Intelligence 3 (Proceedings of CAIA-1991). AAAI\\n  Press, 1991.\\n\\nFootnotes\\n\\n  After 1 August 1995, Dr. Reiter's address will be\\nDepartment of Computing Science, University of Aberdeen,\\nKing's College, Aberdeen AB9 2UE, BRITAIN.\\nHis email address will be ereiter@csd.abdn.ac.uk\\n  This assumes that the system\\nuses a large number of templates.  If only a small set of templates is needed\\nto generate the system's texts, maintaining them is unlikely to be a problem.\\n  Also, a forecaster who uses FoG is not dependent on a\\nthird-party to translate his or her forecasts; this\\nfeeling of `more control' may be a significant plus to some users.\\n  This list has been heavily\\ninfluenced by discussions with Chris Mellish.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nOne of the most important questions in applied NLG is what benefits\\n(or `value-added', in business-speak) NLG technology offers over\\ntemplate-based approaches.  Despite the importance of this question\\nto the applied NLG community, however, it has not been discussed\\nmuch in the research NLG community, which I think is a pity.  In this paper,\\nI try to summarize the issues involved and recap current thinking on this\\ntopic.  My goal is not to answer this question (I don't think we know enough\\nto be able to do so), but rather to increase the visibility of this issue\\nin the research community, in the hope of getting some input and ideas\\non this very important question.  I conclude with a list of specific\\nresearch areas I would like to see more work in, because I think they would\\nincrease the `value-added' of NLG over templates.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nThe Data-Oriented Parsing (DOP) model has a short, interesting, and\\ncontroversial history.  It was introduced by Remko Scha \\n, and was then studied by Rens Bod.\\nUnfortunately, Bod  was not able to find an\\nefficient exact algorithm for parsing using the model; however he did\\ndiscover and implement Monte Carlo approximations.  He tested these\\nalgorithms on a cleaned up version of the ATIS corpus, and achieved\\nsome very exciting results, reportedly getting 96% of his test set\\nexactly correct, a huge improvement over previous results.  For\\ninstance, Bod  compares these results to Schabes\\n, in which, for short sentences, 30% of the\\nsentences have no crossing brackets (a much easier measure than exact\\nmatch).  Thus, Bod achieves an extraordinary 8-fold error rate\\nreduction.\\n\\n\\nNot surprisingly, other researchers attempted to duplicate these\\nresults, but due to a lack of details of the parsing algorithm in his\\npublications, these other researchers were not able to confirm the\\nresults (Magerman, Lafferty, personal communication).  Even Bod's\\n thesis  does not contain enough information to replicate his results.\\n\\n\\nParsing using the DOP model is especially difficult.  The model can be\\nsummarized as a special kind of Stochastic Tree Substitution Grammar\\n(STSG): given a bracketed, labelled training corpus, let every subtree of that corpus be an elementary tree, with a\\nprobability proportional to the number of occurrences of that subtree\\nin the training corpus.  Unfortunately, the number of trees is in\\ngeneral exponential in the size of the training corpus trees,\\nproducing an unwieldy grammar.\\n\\n\\nIn this paper, we introduce a reduction of the DOP model to an exactly\\nequivalent Probabilistic Context Free Grammar (PCFG) that is linear in\\nthe number of nodes in the training data.  Next, we present an\\nalgorithm for parsing, which returns the parse that is expected to\\nhave the largest number of correct constituents.  We use the\\nreduction and algorithm to parse held out test data, comparing these\\nresults to a replication of Pereira and Schabes  on\\nthe same data.  These results are disappointing: the PCFG\\nimplementation of the DOP model performs about the same as the Pereira\\nand Schabes method.  We present an analysis of the runtime of our\\nalgorithm and Bod's.  Finally, we analyze Bod's data, showing that\\nsome of the difference between our performance and his is due to a\\nfortuitous choice of test data.\\n\\n\\nThis paper contains the first published replication of the full DOP\\nmodel, i.e. using a parser which sums over derivations.  It also\\ncontains algorithms implementing the model with significantly fewer\\nresources than previously needed.  Furthermore, for the first time,\\nthe DOP model is compared on the same data to a competing model.\\n\\n\\n    Previous Research\\n\\n\\nThe DOP model itself is extremely simple and can be described as\\nfollows: for every sentence in a parsed training corpus, extract every\\nsubtree.  In general, the number of subtrees will be very large,\\ntypically exponential in sentence length.  Now, use these trees to\\nform a Stochastic Tree Substitution Grammar (STSG).  There are two\\nways to define a STSG: either as a Stochastic Tree Adjoining Grammar\\n  restricted to substitution operations, or as an extended PCFG in which entire trees may occur on the right hand side,\\ninstead of just strings of terminals and non-terminals.\\n\\n\\n Given the tree of Figure , we can use the DOP model  to convert it into the STSG of Figure .  The numbers in parentheses represent the probabilities.  These trees can\\nbe combined in various ways to parse sentences.\\n\\n\\nIn theory, the DOP model has several advantages over other models.\\nUnlike a PCFG, the use of trees allows capturing large contexts,\\nmaking the model more sensitive.  Since every subtree is included,\\neven trivial ones corresponding to rules in a PCFG, novel sentences\\nwith unseen contexts can still be parsed.\\n\\n\\nUnfortunately, the number of subtrees is huge; therefore Bod randomly\\nsamples 5% of the subtrees, throwing away the rest.  This\\nsignificantly speeds up  parsing.\\n\\n\\nThere are two existing ways to parse using the DOP model.  First, one\\ncan find the most probable derivation.  That is, there can be\\nmany ways a given sentence could be derived from the STSG.  Using the\\nmost probable derivation criterion, one simply finds the most probable\\n way that a sentence could be produced.  Figure  shows a simple example STSG.  For the string x x, what is the most\\nprobable derivation?  The parse tree\\nx\\n1A\\nx\\n1CD\\n2S\\nhas probability \\n\\n\\nof being generated by the\\ntrivial derivation containing a single tree.  This tree corresponds to\\nthe most probable derivation of x x.\\n\\n\\nOne could try to find the most probable parse tree.  For a given\\nsentence and a given parse tree, there are many different derivations\\nthat could lead to that parse tree.  The probability of the parse\\ntree is the sum of the probabilities of the derivations.  Given our\\nexample, there are two different ways to generate the parse tree\\nx\\n1E\\nx\\n1B\\n2S\\neach with probability \\n\\n,\\nso that the parse tree\\nhas probability \\n\\n.\\nThis parse tree is most probable.\\n\\n\\nBod  shows how to approximate this most probable parse\\nusing a Monte Carlo algorithm.  The algorithm randomly samples\\npossible derivations, then finds the tree with the most sampled\\nderivations.  Bod shows that the most probable parse yields better\\nperformance than the most probable derivation on the exact match\\ncriterion.\\n\\n\\nKhalil Sima'an  implemented a version\\nof the DOP model, which parses efficiently by limiting the number of\\ntrees used and by using an efficient most probable derivation model.\\nHis experiments differed from ours and Bod's in many ways, including\\nhis use of a different version of the ATIS corpus; the use of word\\nstrings, rather than part of speech strings; and the fact that he did\\nnot parse sentences containing unknown words, effectively throwing out\\nthe most difficult sentences.  Furthermore, Sima'an limited the number\\nof substitution sites for his trees, effectively using a subset of the\\nDOP model.\\n\\n\\n    Reduction of DOP to PCFG\\n\\n\\nUnfortunately, Bod's reduction to a STSG is extremely expensive, even\\nwhen throwing away 95% of the grammar.  Fortunately, it is possible\\nto find an equivalent PCFG that contains exactly eight PCFG rules for\\neach node in the training data; thus it is O(n).  Because this\\nreduction is so much smaller, we do not discard any of the grammar\\nwhen using it.  The PCFG is equivalent in two senses: first it\\ngenerates the same strings with the same probabilities; second, using\\nan isomorphism defined below, it generates the same trees with the\\nsame probabilities, although one must sum over several PCFG trees for\\neach STSG tree.\\n\\n\\nTo show this reduction and equivalence, we must first define some\\nterminology.  We assign every node in every tree a unique number,\\nwhich we will call its address.  Let A@k denote the node at address\\nk, where A is the non-terminal labeling that node.  We will need\\nto create one new non-terminal for each node in the training data.  We\\nwill call this non-terminal Ak.  We will call non-terminals of this\\nform ``interior'' non-terminals, and the original non-terminals in the\\nparse trees ``exterior''.\\n\\n\\nLet aj represent the number of subtrees headed by the node A@j.\\nLet a represent the number of subtrees headed by nodes with\\nnon-terminal A, that is \\n\\n.\\n\\n\\nConsider a node A@j of the form:\\nB@k AA\\nC@l AA\\n2A@j\\nHow many subtrees does it have?  Consider first the possibilities on\\nthe left branch.  There are bk non-trivial subtrees headed by\\nB@k, and there is also the trivial case where the left node is\\nsimply B.  Thus there are bk+1 different possibilities on the\\nleft branch.  Similarly, for the right branch there are cl+1possibilities.  We can create a subtree by choosing any possible left\\nsubtree and any possible right subtree.  Thus, there are\\n\\naj=(bk+1)(cl+1) possible subtrees headed by A@j.  In our\\n example tree of Figure , both noun phrases have exactly one subtree: \\n\\nnp4=np2=1; the verb phrase has 2 subtrees:\\nvp3=2; and the sentence has 6: s1=6.  These numbers correspond to\\n the number of subtrees in Figure . \\n\\n\\nWe will call a PCFG subderivation isomorphic to a STSG tree if the\\nsubderivation begins with an external non-terminal, uses internal\\nnon-terminals for intermediate steps, and ends with external\\nnon-terminals.  For instance, consider the tree\\nPN\\nPN\\n2NP\\nV\\nNP\\n2VP\\n2S \\n\\n\\n taken from Figure .  The following PCFG subderivation is isomorphic: \\n\\n.\\nWe say that a PCFG derivation is\\nisomorphic to a STSG derivation if there is a corresponding PCFG\\nsubderivation for every step in the STSG derivation.\\n\\n\\nWe will give a simple small PCFG with the following surprising\\nproperty: for every subtree in the training corpus headed by A, the\\ngrammar will generate an isomorphic subderivation with probability\\n1/a.  In other words, rather than using the large, explicit STSG, we\\ncan use this small PCFG that generates isomorphic derivations, with\\nidentical probabilities.\\n\\n\\nThe construction is as follows.  For a node such as\\nB@k AA\\nC@l AA\\n2A@j\\nwe will generate the following eight\\nPCFG rules, where the number in parentheses following a rule is its\\nprobability.\\n\\n\\n\\n\\n\\nWe will show that subderivations headed by A with external\\nnon-terminals at the roots and leaves, internal non-terminals\\nelsewhere have probability 1/a.  Subderivations headed by Aj with\\nexternal non-terminals only at the leaves, internal non-terminals\\nelsewhere, have probability 1/aj.  The proof is by\\ninduction on the depth of the trees.\\n\\n\\nFor trees of depth 1, there are two cases:\\nB AA\\nC AA\\n2A\\nB AA\\nC AA\\n2A@j\\nTrivially, these trees have the required probabilities.\\n\\n\\nNow, assume that the theorem is true for trees of depth n or less.\\nWe show that it holds for trees of depth n+1.  There are eight\\ncases, one for each of the eight rules.  We show two of them.  Let\\n\\n\\nrepresent a\\ntree of at most depth n with external leaves, headed by B@k, and\\nwith internal intermediate non-terminals.  Then, for trees such as\\n\\n\\nAA\\n\\n\\nAA\\n2A@j\\nthe probability of the tree is \\n\\n.\\nSimilarly, for another case, trees headed by\\nB@k AA\\nC AA\\n2A\\nthe probability of the tree is\\n\\n.\\nThe other six cases follow\\ntrivially with similar reasoning.\\n\\n\\nWe call a PCFG derivation isomorphic to a STSG derivation if for every\\nsubstitution in the STSG there is a corresponding subderivation in the\\n PCFG.  Figure  contains an example of isomorphic derivations, using two subtrees in the STSG and four productions in\\nthe PCFG.\\n\\n\\nWe call a PCFG tree isomorphic to a STSG tree if they are identical\\nwhen internal non-terminals are changed to external non-terminals.\\nOur main theorem is that this construction produces PCFG trees\\nisomorphic to the STSG trees with equal probability.  If every subtree\\nin the training corpus occurred exactly once, this would be trivial to\\nprove.  For every STSG subderivation, there would be an isomorphic\\nPCFG subderivation, with equal probability.  Thus for every STSG\\nderivation, there would be an isomorphic PCFG derivation, with equal\\nprobability.  Thus every STSG tree would be produced by the PCFG with\\nequal probability.\\n\\n\\nHowever, it is extremely likely that some subtrees, especially\\ntrivial ones like\\nNP AA\\nVP AA\\n2S\\nwill occur repeatedly.    \\n\\n\\nIf the STSG formalism were modified slightly, so that trees could\\noccur multiple times, then our relationship could be made one to one.\\nConsider a modified form of the DOP model, in which when subtrees\\noccurred multiple times in the training corpus, their counts were not\\nmerged: both identical trees are added to the grammar.  Each of these\\ntrees will have a lower probability than if their counts were merged.\\nThis would change the probabilities of the derivations; however the\\nprobabilities of parse trees would not change, since there would be\\ncorrespondingly more derivations for each tree.  Now, the desired one\\nto one relationship holds: for every derivation in the new STSG there\\nis an isomorphic derivation in the PCFG with equal probability.  Thus,\\nsumming over all derivations of a tree in the STSG yields the same\\nprobability as summing over all the isomorphic derivations in the\\nPCFG.  Thus, every STSG tree would be produced by the PCFG with equal\\nprobability.\\n\\n\\nIt follows trivially from this that no extra trees are produced by the\\nPCFG.  Since the total probability of the trees produced by the STSG\\nis 1, and the PCFG produces these trees with the same probability, no\\nprobability is ``left over'' for any other trees.\\n\\n\\n    Parsing Algorithm\\n\\n\\nThere are several different evaluation metrics one could use for\\nfinding the best parse.  In the section covering previous research, we\\nconsidered the most probable derivation and the most probable parse\\ntree.  There is one more metric we could consider.  If our performance\\nevaluation were based on the number of constituents correct, using\\nmeasures similar to the crossing brackets measure, we would want the\\nparse tree that was most likely to have the largest number of correct\\nconstituents.  With this criterion and the example grammar of Figure\\nx\\n1A\\nx\\n1BD\\n2S\\nThe probability that the S constituent is correct is 1.0, while\\nthe probability that the A constituent is correct is \\n\\n,\\nand the probability that the B constituent is correct is\\n\\n.\\nThus, this tree has on average 2 constituents\\ncorrect.  All other trees will have fewer constituents correct on\\naverage.  We call the best parse tree under this criterion the Maximum Constituents Parse.  Notice that this parse tree cannot even\\nbe produced by the grammar: each of its constituents is good,\\nbut it is not necessarily good when considered as a full tree.\\n\\n\\nBod  shows that the most probable\\nderivation does not perform as well as the most probable parse for the\\nDOP model, getting 65% exact match for the most probable derivation,\\nversus 96% correct for the most probable parse.  This is not\\nsurprising, since each parse tree can be derived by many different\\nderivations; the most probable parse criterion takes all possible\\nderivations into account.  Similarly, the Maximum Constituents Parse\\nis also derived from the sum of many different derivations.\\nFurthermore, although the Maximum Constituents Parse should not do as\\nwell on the exact match criterion, it should perform even better on\\nthe percent constituents correct criterion.  We have previously\\nperformed a detailed comparison between the most likely parse, and the\\nMaximum Constituents Parse for Probabilistic Context Free Grammars\\n ; we showed that the two have very similar performance on a broad range of measures, with at most a 10%\\ndifference in error rate (i.e., a change from 10% error rate to 9%\\nerror rate.)  We therefore think that it is reasonable to use a\\nMaximum Constituents Parser to parse the DOP model.\\n\\n\\nThe parsing algorithm is a variation on the\\nInside-Outside algorithm, developed by Baker \\nand discussed in detail by Lari and Young .  However,\\nwhile the Inside-Outside algorithm is a grammar re-estimation\\nalgorithm, the algorithm presented here is just a parsing algorithm.\\nIt is closely related to a similar algorithm used for Hidden Markov\\n Models  for finding the most likely state at each time.  However, unlike in the HMM case where the algorithm produces a\\nsimple state sequence, in the PCFG case a parse tree is produced,\\nresulting in additional constraints.\\n\\n\\nA formal derivation of a very similar algorithm is given elsewhere\\n ; only the intuition is given here.  The algorithm can be summarized as follows.  First, for each potential constituent,\\nwhere a constituent is a non-terminal, a start position, and an end\\nposition, find the probability that that constituent is in the parse.\\nAfter that, put the most likely constituents together to form a parse\\ntree, using dynamic programming.\\n\\n\\nThe probability that a potential constituent occurs in the correct\\nparse tree, \\n\\n,\\nwill\\nbe called \\n\\ng(s, t, X).  In words, it is the probability that, given\\nthe sentence w1...wn, a symbol X generates ws...wt.  We can\\ncompute this probability using elements of the Inside-Outside\\nalgorithm.  First, compute the inside probabilities, \\n\\n.\\nSecond, compute the outside probabilities,\\n\\n.\\nThird, compute the matrix \\n\\ng(s, t, X):\\n\\n\\n\\n\\n\\nOnce the matrix \\n\\ng(s, t, X) is computed, a dynamic programming\\nalgorithm can be used to determine the best parse, in the sense of\\nmaximizing the number of constituents expected correct.\\n Figure  shows pseudocode for a simplified form of this algorithm.\\n\\n\\nFor a grammar with g nonterminals and training data of size T, the\\nrun time of the algorithm is \\n\\nO(Tn[2] + gn[3] + n[3]) since there are\\ntwo layers of outer loops, each with run time at most n, and inner\\nloops, over addresses (training data), nonterminals and n.  However,\\nthis is dominated by the computation of the Inside and Outside\\nprobabilities, which takes time O(rn[3]), for a grammar with rrules.  Since there are eight rules for every node in the training\\ndata, this is O(Tn[3]).\\n\\n\\nBy modifying the algorithm slightly to record the actual split used at each\\nnode, we can recover the best parse.  The entry maxc[1, n]\\ncontains the expected number of correct constituents, given the model.\\n\\n\\n    Experimental Results and Discussion\\n\\n\\nWe are grateful to Bod for supplying the data that he used for\\n his experiments ,,.  The original ATIS data from the Penn Tree Bank, version 0.5, is very noisy; it is\\ndifficult to even automatically read this data, due to inconsistencies\\nbetween files.  Researchers are thus left with the difficult decision\\nas to how to clean the data.  For this paper, we conducted two sets of\\n experiments: one using a minimally cleaned set of data,   making our results comparable to previous\\nresults; the other using the ATIS data prepared by Bod, which\\ncontained much more significant revisions.\\n\\n\\nTen data sets were constructed by randomly splitting\\nminimally edited ATIS (Hemphill et al., 1990)  sentences into a 700\\nsentence training set, and 88 sentence test set, then discarding\\nsentences of length ] 30.  For each of the ten sets, both the DOP\\nalgorithm outlined here and the grammar induction experiment of\\nPereira and Schabes were run.  Crossing brackets, zero crossing\\nbrackets, and the paired differences are presented in Table\\n .  All sentences output by the parser were made binary branching (see the section covering analysis of Bod's data),\\nsince otherwise the crossing brackets measures are meaningless\\n .  A few sentences were not parsable; these were assigned right branching period high structure, a good heuristic\\n . \\n\\n\\nWe also ran experiments using Bod's data, 75 sentence test sets, and\\nno limit on sentence length.  However, while Bod provided us\\nwith his data, he did not provide us with the split into test and\\ntraining that he used; as before we used ten random splits.  The\\n results are disappointing, as shown in Table . They are noticeably worse than those of Bod, and again very comparable\\nto those of Pereira and Schabes.  Whereas Bod reported 96% exact\\nmatch, we got only 86% using the less restrictive zero crossing\\nbrackets criterion.  It is not clear what exactly accounts for these\\n differences. It is also noteworthy that the results are much better on Bod's data\\nthan on the minimally edited data: crossing brackets rates of 96% and\\n97% on Bod's data versus 90% on minimally edited data.  Thus it\\nappears that part of Bod's extraordinary performance can be explained\\nby the fact that his data is much cleaner than the data used by other\\nresearchers.\\n\\n\\nDOP does do slightly better on most measures.  We performed a\\nstatistical analysis using a t-test on the paired differences\\nbetween DOP and Pereira and Schabes performance on each run.  On the\\nminimally edited ATIS data, the differences were statistically\\ninsignificant, while on Bod's data the differences were statistically\\nsignificant beyond the 98'th percentile.  Our technique for finding\\nstatistical significance is more strenuous than most: we assume that\\nsince all test sentences were parsed with the same training data, all\\nresults of a single run are correlated.  Thus we compare paired\\ndifferences of entire runs, rather than of sentences or constituents.\\nThis makes it harder to achieve statistical significance.\\n\\n\\nNotice also the minimum and maximum columns of the ``DOP-PS'' lines,\\nconstructed by finding for each of the paired runs the difference\\nbetween the DOP and the Pereira and Schabes algorithms.  Notice that\\nthe minimum is usually negative, and the maximum is usually positive,\\nmeaning that on some tests DOP did worse than Pereira and Schabes and\\non some it did better.  It is important to run multiple tests,\\nespecially with small test sets like these, in order to avoid\\nmisleading results.\\n\\n\\n    Timing Analysis\\n\\n\\nIn this section, we examine the empirical runtime of our algorithm,\\nand analyze Bod's.\\nWe also note that Bod's algorithm\\nwill probably be particularly inefficient on longer sentences.\\n\\n\\nIt takes about 6 seconds per sentence to run our algorithm on an HP\\n9000/715, versus 3.5 hours  to run Bod's algorithm on a Sparc 2\\n .  Factoring in that the HP is roughly four times faster than the Sparc,\\nthe new algorithm is about 500 times faster.  Of course, some of this\\ndifference may be due to differences in implementation, so this\\nestimate is fairly rough.\\n\\n\\nFurthermore, we believe Bod's analysis of his parsing algorithm is\\nflawed.  Letting G represent grammar size, and \\nrepresent\\nmaximum estimation error, Bod correctly analyzes his runtime as\\n\\n.\\nHowever, Bod then neglects analysis of this\\n\\n\\nterm, assuming that it is constant.  Thus he concludes\\nthat his algorithm runs in polynomial time.  However, for his\\nalgorithm to have some reasonable chance of finding the most probable\\nparse, the number of times he must sample his data is at least\\ninversely proportional to the conditional probability of that parse.\\nFor instance, if the maximum probability parse had probability 1/50,\\nthen he would need to sample at least 50 times to be reasonably sure\\nof finding that parse.\\n\\n\\nNow, we note that the conditional probability of the most probable\\nparse tree will in general decline exponentially with sentence length.\\nWe assume that the number of ambiguities in a sentence will increase\\nlinearly with sentence length; if a five word sentence has on average\\none ambiguity, then a ten word sentence will have two, etc.  A linear\\nincrease in ambiguity will lead to an exponential decrease in\\nprobability of the most probable parse.\\n\\n\\nSince the probability of the most probable parse decreases\\nexponentially in sentence length, the number of random samples needed\\nto find this most probable parse increases exponentially in sentence\\nlength.  Thus, when using the Monte Carlo algorithm, one is left with\\nthe uncomfortable choice of exponentially decreasing the probability\\nof finding the most probable parse, or exponentially increasing the\\nruntime.\\n\\n\\nWe admit that this is a somewhat informal argument.  Still, the Monte\\nCarlo algorithm has never been tested on sentences longer than those\\nin the ATIS corpus; there is good reason to believe the algorithm will\\nnot work as well on longer sentences.  Note that our algorithm has\\ntrue runtime O(Tn[3]), as shown previously.\\n\\n\\n    Analysis of Bod's Data\\n\\n\\nIn the DOP model, a sentence cannot be given an exactly correct parse\\nunless all productions in the correct parse occur in the training set.\\nThus, we can get an upper bound on performance by examining the test\\ncorpus and finding which parse trees could not be generated using only\\nproductions in the training corpus.  Unfortunately, while Bod provided\\nus with his data, he did not specify which sentences were test and\\nwhich were training.  We can however find an upper bound on average\\ncase performance, as well as an upper bound on the probability that\\nany particular level of performance could be achieved.\\n\\n\\nBod randomly split his corpus into test and training.  According to\\n his thesis , only one of his 75 test sentences had a correct parse which could not be generated from the training\\ndata.  This turns out to be very surprising.  An analysis of Bod's\\ndata shows that at least some of the difference in performance between\\nhis results and ours must be due to an extraordinarily fortuitous\\nchoice of test data.  It would be very interesting to see how our\\nalgorithm performed on Bod's split into test and training, but he has\\nnot provided us with this split.  Bod did examine versions of DOP that\\nsmoothed, allowing productions which did not occur in the training\\nset; however his reference to coverage is with respect to a version\\nwhich does no smoothing.\\n\\n\\nIn order to perform our analysis, we must determine certain details of\\nBod's parser which affect the probability of having most sentences\\ncorrectly parsable.  When using a chart parser, as Bod did, three\\nproblematic cases must be handled: \\nproductions, unary\\nproductions, and n-ary (n ] 2) productions.  The first two kinds\\nof productions can be handled with a probabilistic chart parser, but\\nlarge and difficult matrix manipulations are required\\n ; these manipulations would be especially difficult given the size of Bod's grammar.  Examining Bod's data, we find he\\nremoved \\nproductions.  We also assume that Bod made the same\\nchoice we did and eliminated unary productions, given the difficulty\\nof correctly parsing them.  Bod himself does not know which technique\\nhe used for n-ary productions, since the chart parser he used was\\nwritten by a third party (Bod, personal communication).\\n\\n\\nThe n-ary productions can be parsed in a straightforward manner, by\\nconverting them to binary branching form; however, there are at least\\nthree different ways to convert them, as illustrated in Table\\n .  In method ``Correct'', the n-ary branching productions are converted in such a way that no overgeneration is\\nintroduced.  A set of special non-terminals is added, one for each\\npartial right hand side.  In method ``Continued'', a single new\\nnon-terminal is introduced for each original non-terminal.  Because\\nthese non-terminals occur in multiple contexts, some overgeneration is\\nintroduced.  However, this overgeneration is constrained, so that\\nelements that tend to occur only at the beginning, middle, or end of\\nthe right hand side of a production cannot occur somewhere else.  If\\nthe ``Simple'' method is used, then no new non-terminals are\\nintroduced; using this method, it is not possible to recover the\\nn-ary branching structure from the resulting parse tree, and\\nsignificant overgeneration occurs.\\n\\n\\n Table  shows the undergeneration probabilities for each of these possible techniques for handling unary productions and\\n n-ary productions. The first number in each column is the probability that\\na sentence in the training data will have a production that occurs\\nnowhere else.  The second number is the probability that a test set of\\n75 sentences drawn from this database will have one ungeneratable\\nsentence: \\n\\n 75p[74](1-p). \\n\\n\\nThe table is arranged from least generous to most generous: in the\\nupper left hand corner is a technique Bod might reasonably have used;\\nin that case, the probability of getting the test set he described is\\nless than one in a million.  In the lower right corner we give Bod the\\nabsolute maximum benefit of the doubt: we assume he used a parser\\ncapable of parsing unary branching productions, that he used a very\\novergenerating grammar, and that he used a loose definition of ``Exact\\nMatch.''  Even in this case, there is only about a 1.5% chance of\\ngetting the test set Bod describes.\\n\\n\\n  Conclusion \\n\\nWe have given efficient techniques for parsing the DOP model.  These\\nresults are significant since the DOP model has perhaps the best\\nreported parsing accuracy; previously the full DOP model had not been\\nreplicated due to the difficulty and computational complexity of the\\nexisting algorithms.  We have also shown that previous results were\\npartially due to an unlikely choice of test data, and partially due to\\nthe heavy cleaning of the data, which reduced the difficulty of the\\ntask.\\n\\n\\nOf course, this research raises as many questions as it answers.  Were\\nprevious results due only to the choice of test data, or are the\\ndifferences in implementation partly responsible?  In that case, there\\nis significant future work required to understand which differences\\naccount for Bod's exceptional performance.  This will be complicated\\nby the fact that sufficient details of Bod's implementation are not\\navailable.\\n\\n\\nThis research also shows the importance of testing on more than one\\nsmall test set, as well as the importance of not making cross-corpus\\ncomparisons; if a new corpus is required, then previous algorithms\\nshould be duplicated for comparison.\\n\\nBibliography \\n\\nJ.K. Baker.\\n1979.\\nTrainable grammars for speech recognition.\\nIn Proceedings of the Spring Conference of the Acoustical\\n  Society of America, pages 547-550, Boston, MA, June.\\n\\n\\nRens Bod.\\n1992.\\nMathematical properties of the data oriented parsing model.\\nPaper presented at the Third Meeting on Mathematics of Language\\n  (MOL3), Austin Texas.\\n\\n\\nRens Bod.\\n1993a.\\nData-oriented parsing as a general framework for stochastic language\\n  processing.\\nIn K. Sikkel and A. Nijholt, editors, Parsing Natural Language.\\n  Twente, The Netherlands.\\n\\n\\nRens Bod.\\n1993b.\\nMonte Carlo parsing.\\nIn Proceedings Third International Workshop on Parsing\\n  Technologies, Tilburg/Durbury.\\n\\n\\nRens Bod.\\n1993c.\\nUsing an annotated corpus as a stochastic grammar.\\nIn Proceedings of the Sixth Conference of the European Chapter\\n  of the ACL, pages 37-44.\\n\\n\\nRens Bod.\\n1995a.\\nEnriching Linguistics with Statistics: Performance Models of\\n  Natural Language.\\nUniversity of Amsterdam ILLC Dissertation Series 1995-14. Academische\\n  Pers, Amsterdam.\\n\\n\\nRens Bod.\\n1995b.\\nThe problem of computing the most probable tree in data-oriented\\n  parsing and stochastic tree grammars.\\nIn Proceedings of the Seventh Conference of the European Chapter\\n  of the ACL.\\n\\n\\nEric Brill.\\n1993.\\nA Corpus-Based Approach to Language Learning.\\nPh.D. thesis, University of Pennsylvania.\\n\\n\\nJoshua Goodman.\\n1996.\\nParsing algorithms and metrics.\\nIn Proceedings of the 34th Annual Meeting of the ACL.\\nTo appear.\\n\\n\\nCharles T. Hemphill, John J. Godfrey, and George R. Doddington.\\n1990.\\nThe ATIS spoken language systems pilot corpus.\\nIn DARPA Speech and Natural Language Workshop, Hidden Valley,\\n  Pennsylvania, June. Morgan Kaufmann.\\n\\n\\nK. Lari and S.J. Young.\\n1990.\\nThe estimation of stochastic context-free grammars using the\\n  inside-outside algorithm.\\nComputer Speech and Language, 4:35-56.\\n\\n\\nDavid Magerman.\\n1994.\\nNatural Language Parsing as Statistical Pattern Recognition.\\nPh.D. thesis, Stanford University University, February.\\n\\n\\nFernando Pereira and Yves Schabes.\\n1992.\\nInside-Outside reestimation from partially bracketed corpora.\\nIn Proceedings of the 30th Annual Meeting of the ACL, pages\\n  128-135, Newark, Delaware.\\n\\n\\nL.R. Rabiner.\\n1989.\\nA tutorial on hidden Markov models and selected applications in\\n  speech recognition.\\nProceedings of the IEEE, 77(2), February.\\n\\n\\nR. Scha.\\n1990.\\nLanguage theory and language technology; competence and performance.\\nIn Q.A.M. de Kort and G.L.J. Leerdam, editors,   Computertoepassingen in de Neerlandistiek. Landelijke Vereniging van\\n  Neerlandici (LVVN-jaarboek), Almere.\\nIn Dutch.\\n\\n\\nYves Schabes, Michal Roth, and Randy Osborne.\\n1993.\\nParsing the Wall Street Journal with the Inside-Outside\\n  algorithm.\\nIn Proceedings of the Sixth Conference of the European Chapter\\n  of the ACL, pages 341-347.\\n\\n\\nY. Schabes.\\n1992.\\nStochastic lexicalized tree-adjoining grammars.\\nIn Proceedings of the 14th International Conference on\\n  Computational Linguistics.\\n\\n\\nKhalil Sima'an.\\n1996.\\nEfficient disambiguation by means of stochastic tree substitution\\n  grammars.\\nIn R. Mitkov and N. Nicolov, editors, Recent Advances in NLP\\n  1995, volume 136 of Current Issues in Linguistic Theory. John\\n  Benjamins, Amsterdam.\\n\\n\\nAndreas Stolcke.\\n1993.\\nAn efficient probabilistic context-free parsing algorithm that\\n  computes prefix probabilities.\\nTechnical Report TR-93-065, International Computer Science Institute,\\n  Berkeley, CA.\\n\\nFootnotes\\n\\n  A diff\\nfile between the original ATIS data and the cleaned up version, in a\\nform usable by the ``ed'' program, is available by anonymous FTP from\\nftp://ftp.das.harvard.edu/pub/goodman/atis-ed/ ti_tb.par-ed and\\nti_tb.pos-ed.  Note that the number of changes made was small.  The\\ndiff files sum to 457 bytes, versus 269,339 bytes for the original\\nfiles, or less than 0.2%.\\n  Ideally, we would exactly reproduce these\\nexperiments using Bod's algorithm.  Unfortunately, it was not possible\\nto get a full specification of the algorithm.\\n  A perl script for analyzing Bod's data\\nis available by anonymous FTP from\\nftp://ftp.das.harvard.edu/pub/goodman/analyze.perl\\n  Actually, this is a slight\\noverestimate for a few reasons, including the fact that the 75\\nsentences are drawn without replacement.  Also, consider a sentence\\nwith a production that occurs only in one other sentence in the\\ncorpus; there is some probability that both sentences will end up in\\nthe test data, causing both to be ungeneratable.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nExcellent results have been reported for Data-Oriented Parsing (DOP)\\n of natural language texts .  Unfortunately, existing algorithms are both computationally intensive and difficult to\\nimplement.  Previous algorithms are expensive due to two factors: the\\nexponential number of rules that must be generated and the use of a\\nMonte Carlo parsing algorithm.  In this paper we solve the first\\nproblem by a novel reduction of the DOP model to a small, equivalent\\nprobabilistic context-free grammar.  We solve the second problem by a\\nnovel deterministic parsing strategy that maximizes the expected\\nnumber of correct constituents, rather than the probability of a\\ncorrect parse tree.  Using the optimizations, experiments yield a 97%\\ncrossing brackets rate and 88% zero crossing brackets rate.  This\\ndiffers significantly from the results reported by Bod, and is\\ncomparable to results from a duplication of Pereira and Schabes's\\n experiment on the same data.  We show that Bod's\\nresults are at least partially due to an extremely fortuitous choice\\nof test data, and partially due to using cleaner data than other\\nresearchers.\\n\\n\"],\n",
              " ['\\n\\n  Introduction \\n\\nIn this paper, we argue that Higher-Order Unification (HOU) provides\\na linguistically adequate tool for modeling the semantics of focus.\\n Building up on , we develop a unification-based analysis of focus which we show favourably compares\\nwith two prominent theories of focus, Rooth\\'s Alternative Semantics\\nand Krifka\\'s Structured Meanings theory.  For data which is generally\\nviewed as a test-bed for focus theory (utterances with multiple focus\\noperators and second occurrence expressions), we show that contrary to\\nRooth\\'s and Krifka\\'s theories, the HOU treatment yields a transparent\\nanalysis while avoiding under- and over-generation.\\n\\n\\n    Focus theory\\n\\n\\nFocus is a much debated notion. In this paper, we assume a simplified\\nversion of Jackendoff\\'s definition: a focus is the semantic\\nvalue of a prosodically prominent element. We take the\\nidentification of prosodically prominent elements as given.\\n\\n\\nTo set the stage for this paper, we will briefly review the folklore,\\ni.e. the main issues of focus theory. It is commonly agreed that focus\\ntriggers the formation of an additional semantic value which we will\\ncall the Focus Semantic Value (FSV).  The name and definition of\\nthe FSV varies from author to author:\\n Jackendoff  calls it the   presuppositional set, Rooth  the   Alternative Set and Krifka  the   Ground. In this paper, we assume a definition of the FSV which is in essence Rooth\\'s Alternative set, that is, the set of semantic\\nobjects obtained by making an appropriate substitution in the focus\\nposition. For instance, the FSV of (1a) is defined as (1b), the\\nset of properties of the form like-ing y where y is an\\nindividual (in what follows, focus is indicated using upper-case; we\\nalso follow Montague\\'s convention that for any type ,\\nis the set of objects of type \\nand \\nis the set of\\nwffs of type ).\\n\\n\\nIt is also usually agreed that certain linguistic elements   associate with focus in that the meaning of the utterance\\ncontaining these elements varies depending on the choice of focus. For\\ninstance in (2a-b), the focus operator only associates with\\nfocus so that the difference in focus between (2a) and (2b) induces a\\ndifference in meaning between the two utterances: in a world where Jon\\nintroduced Paul to Mary and Sarah, and no other introduction takes\\nplace, (2a) is necessarily false whilst (2b) is true.\\n\\n\\nTo model this ``association-with-focus\" phenomenon, the semantics of\\nassociating-elements (e.g. focus operators, quantificational adverbs)\\nis made contingent on the FSV which itself, varies with the choice of\\nfocus. The following example illustrates this. Suppose that the\\nmeaning of only is determined by the following rule: \\n\\n\\nwhere NP\\', VP\\' represent the meaning of NP and VP respectively, and\\nFSV stands for the focus semantic value of the VP. As we have seen\\nabove, the FSV of (1a) is (1b), hence by the above semantic for   only, the semantics of (1a) is:\\n\\n\\nIntuitively, the only property\\nof the form like-ing y that holds of Jon is the property\\nof like-ing Mary. \\n\\n\\n    The basic analysis\\n\\n\\nFor computing the Focus Semantic Value, we propose to use\\nHigher-Order Unification. More specifically, given (part of) an\\nutterance U with semantic representation Sem and foci \\n\\n,\\nwe require that the following equation, the ground\\n  equation, be solved:\\n\\n\\n\\n\\n\\nAssuming the typed -calculus as our semantic representation\\nlanguage, this equation can be solved by Huet\\'s algorithm (cf.\\n ), thus assigning a value to Gd. On the basis of this value, we can then define the FSV, written ,\\nas follows:\\n\\n\\n\\n\\n\\nAs mentioned before, this yields a focus semantic value which is in\\n essence Rooth\\'s Alternative Set. \\n\\n\\n Finally, we assume as in , that foci are stored and discharged non-deterministically as the need arises, thus contributing to the definition of the ground equation. Furthermore, equations are set up at the level at which there are needed e.g. at the VP level in the case of a pre-verbal focus operator.  \\n\\n\\nTo illustrate the workings of our approach, we now run through a\\nsimple example. Consider (1a). To determine the meaning of only\\n  likes MARY, the FSV of the VP must be known. Hence the\\nfollowing equation must be solved:\\n\\n\\n\\n\\n\\n By HOU, the value of Gd is then: \\n\\n\\n\\n\\n\\nAnd by definition (3.1), the FSV is:\\n\\n\\n\\n\\n\\nAssuming the semantic of only given above, the semantic\\nrepresentation of (1a) is then:\\n\\n\\n\\n\\n\\nIn short, we obtain a reading similar to that of Rooth, the difference\\nbeing in the way the FSV is determined: by HOU in our approach, by\\nmeans of a semantic definition in Rooth\\'s.\\n\\n\\n    Linguistic applications\\n\\n\\nIn this section, we show that the HOU approach favourably compares\\nwith Rooth\\'s and Krifka\\'s analysis in that it correctly generates\\ninterpretations which these two theories fail to yield. As we\\nshall see, the main reason for this is that the HOU approach makes\\nminimal assumptions about the role syntax plays in determining the\\nFSV. In particular, it relies neither on the use of Quantifier\\nRaising, nor on the assumption of a rule-to-rule definition of\\nthe FSV. In this way, it avoids some of the pitfalls these\\ntheories encounter.\\n\\n\\nWe begin by a brief summary of Rooth\\'s and Krifka\\'s theories and\\nstress the properties relevant for the present discussion.\\nWe then confront the three theories with the data.\\n\\n    Two alternative theories of focus\\n\\n  Rooth\\'s Alternative Semantics \\n\\nIn Rooth\\'s approach, the FSV is defined by recursion on the\\ntruth-conditional structure which is itself derived from LF\\n(i.e. Logical Form, the Government and Binding level of semantic\\nrepresentation). Focus is then seen as introducing a free variable\\nwhose value is determined by the current context and is furthermore\\nconstrained to be an element or a subset of the FSV. For our purpose,\\nthe following characteristics are particularly important:\\n\\n\\nGiven Rooth\\'s definition of the Alternative Set, a focus\\n  operator associates with any focus occurring in its\\n  scope.\\nAny NP may be subject to Quantifier Raising. Importantly, this includes focused NPs.\\nQuantifier Raising may not apply to quantifiers occurring in a scope-island.\\n\\n\\nNote that Rooth\\'s approach critically relies on quantifier raising as\\na means of moving a focused NP out of the scope of a focus operator.\\nHowever this only applies if the focus NP is not embedded in a scope\\nisland.\\n\\n\\n  Krifka\\'s Structured Meanings \\n\\nKrifka\\'s approach defines a rule-to-rule semantics which assigns to\\nany syntactic constituent, a meaning which can be either a\\n-term or a structured meaning, i.e. a tuple of the\\nform \\n\\n\\nwhere Gd is Krifka\\'s\\nFocus Semantic Value and F is a (possibly complex) focus. \\n\\n\\nFor our purpose, an important characteristic of Krifka\\'s approach is\\nthe tight syntax/semantic interaction it presupposes. In particular,\\nthe theory requires that a focus operator combines with a syntactic\\nconstituent C whose structured semantics \\n\\n\\nprovides the focus (F) this operator associates with. In\\nother words, the right-adjacent sibling of a focus operator must\\ncontain all and only the foci this operator associates with.  As we\\nshall later see, some of the data does not seem to square with this\\nassumption.\\n\\n\\n\\n    Multiple Focus Operators\\n\\n\\n Utterances with multiple focus operators are known pathological cases of focus theory:\\n\\n\\nIn the given context, the preferred reading of (3b) can be glossed as\\nfollows: it is also the case for SUE2, that Jon only1 read the\\n  letters she sent to PAUL1 - i.e. Jon didn\\'t read the letters she2  sent to e.g.  Peter. In other words, the preferred reading is that\\nalso2 associates with SUE2 and only1 with   PAUL1.\\n\\n  The HOU analysis \\n\\nUnder the HOU approach, (3b) is analysed as follows. First, the\\nmeaning of only1 read the letters that SUE2 sent to\\n  PAUL1 is derived. To determine the FSV of the VP, the ground\\nequation (4b) must be solved for which (4c) is a solution. Applying\\n the semantics of only given in section , the semantics  of (4a) is then as given in (4d). \\n\\n\\n\\n\\n\\nAnalysis then proceeds further and the ground equation \\n\\n\\n\\n\\n\\nmust be solved to determine the meaning of also2 only1 read the letters that\\n  SUE2 sent to PAUL1. A possible solution for G[2] is \\n\\n\\n\\n\\n\\nAssuming the  following semantics for \\n\\n\\n\\n\\n\\n\\n\\nwe obtain the desired reading\\n\\n\\n\\n\\n\\n  Comparison with Rooth and Krifka \\n\\n As mentioned in section , under the Alternative Semantics approach, a focus operator necessarily associates with any focus\\noccurring in its scope. Furthermore in (3b), the scope of   only1 is the whole VP read the letters that SUE2 sent to\\n  PAUL1.  Hence, if no quantifier raising occurs, only1\\nassociates with both SUE2 and PAUL1. Thus in order\\nto generate the desired reading, SUE2 must be moved out of\\nthe scope of only1. However, since the NP the letters\\n  that SUE2 sent to PAUL1 is a scope island, quantifier raising\\nis impossible. Hence, the desired reading cannot be\\n generated. \\n\\n\\nRecall that in the Structured Meanings approach, the right-sibling of\\na focus operator must contain all and only the focus this operator\\n associates with (cf. section ). Hence, to generate the desired reading in (3b), there must exist a syntactic constituent\\nwhich is right-adjacent to only1 and which contains PAUL1\\n but not SUE2; similarly, there must exist a syntactic constituent which is right-adjacent to also and which contains SUE2 but not\\nPAUL1.  Given standard assumptions about syntax, such\\nconstituents do not exist so that the desired interpretation cannot be\\ngenerated.\\n\\n\\n\\n    Second Occurrence Expressions\\n\\n\\nWe call second occurrence expressions (SOE) utterances which partially\\nor completely repeat a previous utterance. Typical cases of SOEs are:\\ncorrections (5a), echo-sentences (5b) and variants (5c).\\n\\n\\nAn important property of SOEs is that the repeated material is   deaccented, that is, it is characterised by an important reduction\\n in pitch, amplitude and duration (cf. ).  On the other hand, all three theories of focus considered here are based on the\\nassumption that focus is prosodically marked and thus, identifiable.\\nHence, the question arises of whether these theories can account for\\nSOEs.\\n\\n  The HOU analysis \\n\\nOur proposal is to analyse SOEs as involving a deaccented anaphor\\nwhich consists of the repeated material, and is subject to the\\ncondition that its semantic representation must unify with the\\nsemantic representation of its antecedent.\\n\\n\\nThis is modeled as follows. Let SSem and TSem be the semantic\\nrepresentation of the source (i.e. antecedent) and target (i.e.\\nanaphoric) clause respectively, and \\n\\n,\\n\\n\\n be the target and source parallel elements, then the interpretation of an SOE must respect the following equations:\\n\\n\\n\\n\\n\\nIntuitively, these two equations require that target and source clause\\nshare a common semantics An, the semantics of the deaccented\\nanaphor.\\n\\n\\nGiven this proposal, the analysis of (5a) involves three equations:\\n\\n\\n\\n\\n\\nSince neither Gd nor Focus are initially given, the third equation\\n above is untyped and cannot be solved by Huet\\'s algorithm. In that situation, we can either assume some delaying mechanism or some extension of\\nHuet\\'s algorithm that can cope with type variables (cf.\\n ,).  Resolution of the first equation yields the following solution:\\n\\n\\n\\n\\n\\nBy applying An to p, the left-hand side of the second equation is\\nthen determined so that the second equation becomes  \\n\\n\\n\\n\\n\\nand the value of Gd is identified as being\\n\\n\\n\\n\\n\\n(Note further, that the third equation can now be solved thus yielding\\nthe value m for the focus F.)  That is, the HOU approach to\\nSOEs allows us to correctly capture that fact that an SOE can inherit\\nits FSV from its source clause (by unification). In\\n , we show in more detail how the analysis accounts for the interaction of focus with anaphora and definiteness in the\\ncase of a particular instantiation of SOEs, namely corrections.\\n\\n\\n  Comparison with Rooth and Krifka \\n\\nUnder the Alternative Semantics approach, SOEs are captured as\\nfollows. It is assumed that the quantification domain of focus\\noperators is a variable whose value is contextually determined. In the\\nstandard case (i.e. the case where the focus is prosodically marked),\\nthis quantification domain of focus operators is usually identified\\nwith the FSV of the VP.  However, in the SOE cases, the assumption is\\nthat the quantification domain of focus operators is identified with\\nthe FSV of the source clause. Thus in (5a), the quantification of\\nonly in the second clause is identified with the FSV of the\\npreceding utterance i.e. the set of properties of the form   like-ing somebody.\\n\\n\\nBut now, consider the following example:\\n\\n\\nClearly, this dialog is ill-formed in that (6b) is no appropriate\\ncorrection for (6a). However, under the Alternative Semantics\\napproach, it will not be ruled out since the FSV of (6a) provides an\\nappropriate quantification domain for the focus operator in (6b): as\\nrequired by the semantic of pre-verbal only, it is a set of\\nproperties whose elements can be identified with the VP semantic value\\n\\n.\\nHence although Rooth\\'s approach captures some\\ncases of SOEs, it does not seem to provide an adequate\\ncharacterisation of the phenomena at hand.\\n\\n\\nThe Structured Meanings proposal distinguishes between proper-\\nand quasi-SOEs. Proper-SOEs involve an exact repetition of\\nsome previous linguistic material, and are analysed as involving an\\nanaphor which is constrained by the restriction that it be a segmental\\ncopy of its antecedent.  For instance, the semantics of only\\n  likes Mary in (5b) is not determined by the semantics of its parts\\nbut is instead identified with the semantic value of its antecedent\\nonly likes MARY in (5a). In contrast, quasi-SOEs only involve\\nsemantic equivalence between repeating and repeated material\\n(for instance, in a quasi-SOE a repeated element may be\\npronominalised). Krifka claims that quasi-SOEs have prosodically\\nmarked foci and thus do not raise any specific difficulty.\\n\\n\\nHowever this theory faces a number of methodological and empirical\\ndifficulties. First, it is non-compositional because the meaning of\\nthe deaccented material in proper-SOEs is solely defined by the\\nmeaning of its antecedent (rather than the meaning of its parts).\\nSecond, the prosodic data is rather unclear: the assumption that\\nquasi-SOE contains a prosodically marked focus is a moot point (cf.\\n ) and if it proves to be false, the analysis fails to account for quasi-SOEs. Third, it is counterintuitive in that it\\nhandles separately two classes of data (i.e.  quasi- and\\nproper-SOEs) which naturally belong together. Indeed, the HOU\\napproach can be shown to provide a uniform treatment of quasi- and\\n proper-SOEs (cf. ). \\n\\n\\n\\n\\n    Formal properties of the HOU approach\\n\\n\\nThe unification problem can be stated as follows: Given two terms of a\\nlogic M and N, is there a substitution, ,\\nof terms\\nfor variables that will make the two terms identical (i.e.\\n(M) = (N))?\\n\\n\\nIt is well-known that for Higher-Order Logic (e.g. the typed\\n-calculus) the space of solutions can be infinite and\\nfurthermore, the HOU problem is only semi-decidable so that the\\nunification algorithm need not terminate for unsolvable problems.\\n\\n\\nFortunately, in our case we are not interested in general unification,\\nbut we can use the fact that our formulae belong to very restricted\\nsyntactic subclasses, for which much better results are known. In\\nparticular, the fact that free variables only occur on the left hand\\nside of our equations reduces the problem of finding solutions to\\nhigher-order matching, of which decidability has been proven for the\\n subclass of third-order formulae  and is conjectured for the general case. This class, (intuitively allowing\\nonly nesting functions as arguments up to depth two) covers all of our\\nexamples in this paper.  For a discussion of other subclasses of\\nformulae, where higher-order unification is computationally feasible\\n see . \\n\\n\\n    Conclusion\\n\\n\\nIn this paper, we have argued that Higher-Order Unification provides\\nan adequate tool for computing Focus Semantic Values. To this end, we\\nhave considered data which is viewed as a test-bed for focus theory\\nand shown that, whilst existing theories either under-generate,\\nover-generate or are methodologically unsatisfactory, the HOU\\napproach yields a simple and transparent analysis. There appear to be\\ntwo main reasons for this.\\n\\n\\nFirst, the HOU analysis makes minimal assumptions about the role\\nsyntax is called to play in determining the FSV. It is defined on a\\npurely semantic level in the sense that unification operates on\\nsemantic representations, and relies neither on quantifier raising,\\nnor on a rule-to-rule definition of the FSV. As we have seen, this\\ntype of approach is a plausible way to avoid under-generation.\\n\\n\\nSecond, the HOU approach permits an equational analysis which can\\nnaturally be further constrained by additional equations. The interest\\nof such an approach was illustrated in our treatment of SOEs which we\\ncharacterise as involving two phenomena: the computation of an FSV,\\nand the resolution of a deaccented anaphor. Not only did we show that\\nthis analysis is methodologically and empirically sound, we also\\nshowed that it finds a natural realisation in the equational framework\\nof HOU: each linguistic phenomena is characterised by some equation(s)\\nand the equations may mutually constrain each other. For instance, in\\nthe case of SOEs, we saw that the equations characterising the\\ndeaccented anaphor help determine the unidentified FSV of the\\nutterance containing the unmarked focus.\\n\\n\\nClearly, our approach extends to cases of adverbial quantification.\\nFor lack of space we could not develop the theory here; let us just\\n point out that von Fintel\\'s criticism  of semantic approaches to focus, also applies to Krifka\\'s Structured\\nMeanings analysis, but not to the HOU approach presented here. Von\\nFintel points out that in certain cases of adverbial quantification, a\\nfocus operator associates with an unmarked focus and does   not associate with a marked focus occurring in its scope - as\\nshould be clear from this article, this is unproblematic for our\\nanalysis.\\n\\n\\nOf course, there are still many open issues. First, how does the\\nproposed analysis interact with quantification? Second, how does it\\nextend to a dynamic semantics (e.g. Discourse Representation Theory)?\\n\\n\\n  Acknowledgments \\n\\nThe work reported in this paper was funded by the Deutsche\\nForschungsgemeinschaft (DFG) in Sonderforschungsbereich SFB-378,\\nProject C2 (LISA).\\n\\nBibliography \\n\\nChristine Bartels.\\n1995.\\nSecond occurrence test.\\nMs.\\n\\n\\nMary Dalrymple, Stuart Shieber, and Fernando Pereira.\\n1991.\\nEllipsis and higher-order-unification.\\nLinguistics and Philosophy, 14:399-452.\\n\\n\\nDaniel Dougherty.\\n1993.\\nHigher-order unification using combinators.\\nTheoretical Computer Science B, 114(2):273-298.\\n\\n\\nGilles Dowek.\\n1992.\\nThird order matching is decidable.\\nIn Proc. LICS-7, pages 2-10, IEEE.\\n\\n\\nClaire Gardent and Michael Kohlhase.\\n1996.\\nHigher-order coloured unification and natural language semantics.\\nIn Proc. ACL96, Santa Cruz, USA.\\nforthcoming.\\n\\n\\nClaire Gardent, Michael Kohlhase and Noor van Leusen.\\n1996.\\nCorrections and higher-order unification.\\nCLAUS report 77, University of Saarland.\\n\\n\\nGrard Huet.\\n1995.\\nA unification algorithm for typed -calculus.\\nTheoretical Computer Science 1, pages 27-57.\\n\\n\\nUlrich Hustadt.\\n1991.\\nA complete transformation system for polymorphic higher-order\\n  unification.\\nTechnical Report MPI-I-91-228, MPI Informatik, Saarbrcken, Germany.\\n\\n\\nDieter Hutter and Michael Kohlhase.\\n1995.\\nA coloured version of the -calculus.\\nSEKI-Report SR-95-05, Universitt des Saarlandes.\\n\\n\\nRay S. Jackendoff.\\n1972.\\nSemantic Interpretation in Generative Grammar.\\nThe MIT Press.\\n\\n\\nAngelika Kratzer.\\n1991.\\nThe representation of focus.\\nIn Arnim van Stechow and Dieter Wunderlich, editors, Semantik:\\n  Ein internationales Handbuch der zeitgenoessischen Forschung. Berlin: Walter\\n  de Gruyter.\\n\\n\\nManfred Krifka.\\n1992.\\nA compositional semantics for multiple focus constructions.\\nIn Joachim Jacobs, editor, Informationsstruktur and Grammatik.\\nSonderheft 4.\\n\\n\\nChristian Prehofer.\\n1994.\\nDecidable higher-order unification problems.\\nIn Alan Bundy, editor, Proc. CADE94, LNAI, pages 635-649, Nancy, France.\\n\\n\\nSteve G. Pulman.\\n1995.\\nHigher-order unification and the interpretation of focus.\\nPaper submitted for publication.\\n\\n\\nMats Rooth.\\n1992.\\nA theory of focus interpretation.\\nNatural Language Semantics, pages 75-116.\\n\\n\\nKai von Fintel.\\n1995.\\nA minimal theory of adverbial quantification.\\nUnpublished draft Ms. MIT, Cambridge, March.\\n\\nFootnotes\\n\\n  Though in fact, our\\n  definition is more syntactic than Rooth. In Rooth\\'s approach, the\\n  FSV definition is purely semantic whereas in our approach the FSV is\\n  indirectly defined by solving equations and the value thus obtained\\n  (i.e. the value of Gd) is a term, that is, a syntactic\\n  object. Hence, our FSV can be more accurately compared to Kratzer\\'s\\n  presupposition skeleton,.  This means that our approach\\n  inherits the advantages of Kratzer\\'s approach (cf.\\n   ). In particular, it adequately captures the   interaction of focus with VP ellipsis as illustrated by Kratzer\\'s\\n  notorious example: I only went to TANGLEWOOD because you\\n    did.\\n    Unification   yields another possible value of Gd, namely \\n\\n.\\nIn what follows, we assume a restriction similar to the\\n   DSP\\'s Primary Occurrence Restriction \\'s:   the occurrence directly associated with the focus is a primary\\n  occurrence and any solution containing a primary occurrence is\\n  discarded as linguistically invalid. For instance, m is a primary\\n  occurrence in the equation \\n\\n\\nso that the\\n  solution \\n\\n\\nis invalid. For a\\n  formal treatment of DSP\\'s Primary Occurrence Restriction and a\\n  discussion of how it can be extended to focus, see\\n   . \\n  The subscripts\\n  indicates which operators associate with which focus. There are\\n  there for clarity only, and have no theoretical import.\\n  For clarity, we have\\n  simplified the semantic representation of (3b); nothing hinges on\\n  this.\\n  This point is independently noted in\\n   . \\n  This is a simplification: the constituent\\n  may in fact contain SUE2 but this focused NP should already\\n  have been bound by some focus operator so that the focus of the\\n  whole constituent only includes PAUL1. Since no focus operators\\n  occur in this constituent, it follows that such constituent does not\\n  exist.\\n  As in\\n   , we take the identification of parallel   elements as given - for the moment.\\n  Even\\n   though this is not explicitly stated, Pulman\\'s analysis  faces a similar problem.\\n\\n\\n\\n\\n\\n',\n",
              "  \"\\n\\nPulman has shown that Higher-Order Unification (HOU) can\\nbe used to model the interpretation of focus. In this\\npaper, we extend the unification-based approach to cases which are\\noften seen as a test-bed for focus theory: utterances with multiple\\nfocus operators and second occurrence expressions. We then show that the\\nresulting analysis favourably compares with two prominent theories of\\nfocus (namely, Rooth's Alternative Semantics and Krifka's Structured\\nMeanings theory) in that it correctly generates interpretations which\\nthese alternative theories cannot yield.  Finally, we discuss the\\nformal properties of the approach and argue that even though HOU need\\nnot terminate, for the class of unification-problems dealt with in\\nthis paper, HOU avoids this shortcoming and is in fact computationally\\ntractable.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nIn collaborative consultation dialogues, the consultant and the\\nexecuting agent collaborate on developing a plan to achieve the\\nexecuting agent's domain goal. Since agents are autonomous and\\nheterogeneous, it is inevitable that conflicts in their beliefs arise\\nduring the planning process. In such cases, collaborative agents\\n should attempt to square away  the conflicts by engaging in collaborative negotiation to determine what should\\nconstitute their shared plan of actions and shared beliefs.\\nCollaborative negotiation differs from non-collaborative negotiation\\nand argumentation mainly in the attitude of the participants,\\nsince collaborative agents are not self-centered, but act in a way as\\nto benefit the agents as a group. Thus, when facing a conflict, a\\ncollaborative agent should not automatically reject a belief with\\nwhich she does not agree; instead, she should evaluate the belief and\\nthe evidence provided to her and adopt the belief if the evidence is\\nconvincing.  On the other hand, if the evaluation indicates that the\\nagent should maintain her original belief, she should attempt to\\nprovide sufficient justification to convince the other agent to adopt\\nthis belief if the belief is relevant to the task at hand.\\n\\n\\nThis paper presents a model for engaging in collaborative negotiation\\nto resolve conflicts in agents' beliefs about domain knowledge. Our\\nmodel 1) detects conflicts in beliefs and initiates a negotiation\\nsubdialogue only when the conflict is relevant to the current task, 2)\\nselects the most effective aspect to address in its pursuit of\\nconflict resolution when multiple conflicts exist, 3) selects\\nappropriate evidence to justify the system's proposed modification of\\nthe user's beliefs, and 4) captures the negotiation process in a\\nrecursive Propose-Evaluate-Modify cycle of actions, thus\\nenabling the system to handle embedded negotiation subdialogues.\\n\\n\\n  Related Work \\n\\nResearchers have studied the analysis and generation of arguments\\n ,,,,,; however, agents engaging in argumentative dialogues are solely\\ninterested in winning an argument and thus exhibit different behavior\\nfrom collaborative agents. Sidner sid_aaaiws92,sid_aaai94\\nformulated an artificial language for modeling collaborative discourse\\nusing proposal/acceptance and proposal/rejection sequences; however,\\nher work is descriptive and does not specify response generation\\nstrategies for agents involved in collaborative interactions.\\n\\n\\nWebber and Joshi web_jos_coling82 have noted the\\nimportance of a cooperative system providing support for its\\nresponses. They identified strategies that a system can adopt in\\njustifying its beliefs; however, they did not specify the criteria\\nunder which each of these strategies should be selected. Walker\\nwal_coling94 described a method of determining when to\\ninclude optional warrants to justify a claim based on factors such as\\ncommunication cost, inference cost, and cost of memory\\nretrieval. However, her model focuses on determining when to include\\ninformationally redundant utterances, whereas our model determines\\nwhether or not justification is needed for a claim to be convincing\\nand, if so, selects appropriate evidence from the system's private\\nbeliefs to support the claim.\\n\\n\\n Caswey et al. , introduced the idea  of utilizing a belief revision mechanism  to predict whether a set of evidence is sufficient to change a user's existing\\nbelief and to generate responses for information retrieval dialogues\\nin a library domain. They argued that in the library dialogues they\\nanalyzed, ``in no cases does negotiation extend beyond the initial\\n belief conflict and its immediate resolution.'' .  However, our analysis of naturally-occurring\\n consultation dialogues , shows that in other domains conflict resolution does extend beyond a single exchange of\\nconflicting beliefs; therefore we employ a recursive model for\\ncollaboration that captures extended negotiation and represents the\\nstructure of the discourse.  Furthermore, their system deals with a\\nsingle conflict, while our model selects a focus in its pursuit of\\nconflict resolution when multiple conflicts arise. In addition, we\\nprovide a process for selecting among multiple possible pieces of\\nevidence.\\n\\n\\n  Features of Collaborative Negotiation \\n\\nCollaborative negotiation occurs when conflicts arise among agents\\n developing a shared plan during collaborative planning. A collaborative agent is driven by the goal of developing a\\nplan that best satisfies the interests of all the agents as a group,\\ninstead of one that maximizes his own interest. This results in\\nseveral distinctive features of collaborative negotiation: 1) A\\ncollaborative agent does not insist on winning an argument, and may\\nchange his beliefs if another agent presents convincing justification\\nfor an opposing belief. This differentiates collaborative negotiation\\nfrom argumentation\\n ,,,. 2) Agents involved in collaborative negotiation are open and honest with one\\nanother; they will not deliberately present false information to other\\nagents, present information in such a way as to mislead the other\\nagents, or strategically hold back information from other agents for\\nlater use. This distinguishes collaborative negotiation from\\nnon-collaborative negotiation such as labor negotiation\\n . 3) Collaborative agents are interested in others' beliefs in order to decide whether to revise their own beliefs so as\\n to come to agreement . Although agents involved in argumentation and non-collaborative negotiation take other agents'\\nbeliefs into consideration, they do so mainly to find weak points in\\ntheir opponents' beliefs and attack them to win the argument.\\n\\n\\nIn our earlier work, we built on Sidner's proposal/acceptance and\\n proposal/rejection sequences  and developed a model  that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions . This model views collaborative planning as agent A proposing a set of\\nactions and beliefs to be incorporated into the shared plan being\\ndeveloped, agent B evaluating the proposal to determine whether\\nor not he accepts the proposal and, if not, agent B proposing a set of\\nmodifications to A's original proposal. The proposed\\nmodifications will again be evaluated by A, and if conflicts arise,\\nshe may propose modifications to B's previously proposed\\nmodifications, resulting in a recursive process.  However, our\\nresearch did not specify, in cases where multiple conflicts arise, how\\nan agent should identify which part of an unaccepted proposal to\\naddress or how to select evidence to support the proposed\\nmodification. This paper extends that work by incorporating into the\\nmodification process a strategy to determine the aspect of the\\nproposal that the agent will address in her pursuit of conflict\\nresolution, as well as a means of selecting appropriate evidence to\\njustify the need for such modification.\\n\\n\\n  Response Generation in Collaborative Negotiation \\n\\nIn order to capture the agents' intentions conveyed by their\\nutterances, our model of collaborative negotiation utilizes an\\nenhanced version of the dialogue model described in\\n  to represent the current status of the interaction. The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for\\nthe user's later execution, the problem-solving level which\\ncontains the actions being performed to construct the domain plan, the\\nbelief level which consists of the mutual beliefs pursued during\\nthe planning process in order to further the problem-solving\\nintentions, and the discourse level which contains the\\ncommunicative actions initiated to achieve the mutual beliefs\\n . This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for\\nengaging in collaborative negotiations.\\n\\n    Evaluating Proposed Beliefs\\n\\n\\nOur system maintains a set of beliefs about the domain and about the\\nuser's beliefs. Associated with each belief is a strength that\\nrepresents the agent's confidence in holding that belief. We model the\\nstrength of a belief using endorsements, which are explicit\\nrecords of factors that affect one's certainty in a hypothesis\\n , following ,. Our endorsements are based on the semantics of the utterance used to\\nconvey a belief, the level of expertise of the agent conveying the\\nbelief, stereotypical knowledge, etc.\\n\\n\\nThe belief level of the dialogue model consists of mutual beliefs\\nproposed by the agents' discourse actions. When an agent proposes a\\nnew belief and gives (optional) supporting evidence for it, this set\\nof proposed beliefs is represented as a belief tree, where the belief\\nrepresented by a child node is intended to support that represented by\\nits parent. The root nodes of these belief trees (top-level beliefs)\\ncontribute to problem-solving actions and thus affect the domain plan\\nbeing developed. Given a set of newly proposed beliefs, the system\\nmust decide whether to accept the proposal or to initiate a\\nnegotiation dialogue to resolve conflicts.  The evaluation of proposed\\nbeliefs starts at the leaf nodes of the proposed belief trees since\\nacceptance of a piece of proposed evidence may affect acceptance of\\nthe parent belief it is intended to support. The process continues\\nuntil the top-level proposed beliefs are evaluated. Conflict\\nresolution strategies are invoked only if the top-level proposed\\nbeliefs are not accepted because if collaborative agents agree on a\\nbelief relevant to the domain plan being constructed, it is irrelevant\\nwhether they agree on the evidence for that belief\\n . \\n\\n\\nIn determining whether to accept a proposed belief or evidential\\nrelationship, the evaluator first constructs an evidence set\\ncontaining the system's evidence that supports or attacks _bel and\\nthe evidence accepted by the system that was proposed by the user as\\nsupport for _bel. Each piece of evidence contains a belief _beli, and an evidential relationship supports(_beli,_bel). Following Walker's weakest link\\n assumption  the strength of the evidence is the weaker of the strength of the belief and the strength of the\\nevidential relationship. The evaluator then employs a simplified\\n version of Galliers' belief revision mechanism, to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of\\nevidence strongly outweighs that of the other, the decision to accept\\nor reject _bel is easily made. However, if the difference in their\\nstrengths does not exceed a pre-determined threshold, the evaluator\\nhas insufficient information to determine whether to adopt _bel and\\ntherefore will initiate an information-sharing subdialogue\\n  to share information with the user so that each of them can knowledgably re-evaluate the user's original proposal. If,\\nduring information-sharing, the user provides convincing support for a\\nbelief whose negation is held by the system, the system may adopt the\\nbelief after the re-evaluation process, thus resolving the conflict\\nwithout negotiation.\\n\\n    Example\\n\\n\\nTo illustrate the evaluation of proposed beliefs, consider the\\nfollowing utterances:\\nS:S:\\n   #2U:\\n\\n\\n   to (5) #1\\n  I think Dr. Smith is teaching AI next semester. \\n\\n\\n   to (6) #2\\n  Dr. Smith is not teaching AI. \\n\\n\\n   to (7)\\n  He is going on sabbatical next year. \\n\\n\\n Figure  shows the belief and discourse levels of  the dialogue model that captures utterances () and  (). The belief evaluation process will start with the belief at the leaf node of the proposed belief tree, On-Sabbatical(Smith,next year)). The system will first gather its\\nevidence pertaining to the belief, which includes 1) a warranted\\n belief that Dr. Smith has postponed his sabbatical until 1997 (Postponed-Sabbatical(Smith,1997)), 2) a warranted belief that\\nDr. Smith postponing his sabbatical until 1997 supports the belief\\nthat he is not going on sabbatical next year (supports(Postponed-Sabbatical(Smith,1997),\\nOn-Sabbatical(Smith,next year)), 3) a strong belief that\\nDr. Smith will not be a visitor at IBM next year (visitor(Smith, IBM, next year)), and 4) a warranted belief\\nthat Dr. Smith not being a visitor at IBM next year supports the\\nbelief that he is not going on sabbatical next year (supports(visitor(Smith, IBM, next year),\\nOn-Sabbatical(Smith, next year)), perhaps because Dr. Smith\\nhas expressed his desire to spend his sabbatical only at IBM). The\\nbelief revision mechanism will then be invoked to determine the\\nsystem's belief about On-Sabbatical(Smith, next year) based on\\nthe system's own evidence and the user's statement. Since beliefs (1)\\nand (2) above constitute a warranted piece of evidence against the\\nproposed belief and beliefs (3) and (4) constitute a strong piece of\\nevidence against it, the system will not accept On-Sabbatical(Smith, next year).\\n\\n\\nThe system believes that being on sabbatical implies a faculty member\\nis not teaching any courses; thus the proposed evidential relationship\\nwill be accepted.  However, the system will not accept the top-level\\nproposed belief, Teaches(Smith, AI), since the system has\\na prior belief to the contrary (as expressed in utterance (1)) and the\\nonly evidence provided by the user was an implication whose antecedent\\nwas not accepted.\\n\\n\\n\\n  Modifying Unaccepted Proposals \\n\\nThe collaborative planning principle in\\n , suggests that ``conversants must provide evidence of a detected discrepancy in belief as soon as\\npossible.'' Thus, once an agent detects a relevant conflict, she must\\nnotify the other agent of the conflict and initiate a negotiation\\nsubdialogue to resolve it -- to do otherwise is to fail in her\\nresponsibility as a collaborative agent. We capture the attempt to\\nresolve a conflict with the problem-solving action Modify-Proposal, whose goal is to modify the proposal to a form that\\nwill potentially be accepted by both agents. When applied to belief\\nmodification, Modify-Proposal has two specializations: Correct-Node, for when a proposed belief is not accepted, and Correct-Relation, for when a proposed evidential relationship is not\\n accepted. Figure  shows the problem-solving  recipes for Correct-Node and its subaction, Modify-Node, that is responsible for the actual modification of the proposal. The\\n applicability conditions  of Correct-Node specify that the action can only be invoked when _s1 believes that _node is not acceptable while\\n_s2 believes that it is (when _s1 and _s2 disagree about the\\nproposed belief represented by _node). However, since this is a\\ncollaborative interaction, the actual modification can only be\\nperformed when both _s1 and _s2 believe that _node is not\\nacceptable -- that is, the conflict between _s1 and _s2 must have\\nbeen resolved. This is captured by the applicability condition and\\nprecondition of Modify-Node. The attempt to satisfy the\\nprecondition causes the system to post as a mutual belief to be\\nachieved the belief that _node is not acceptable, leading the system\\nto adopt discourse actions to change _s2's beliefs, thus initiating a\\n collaborative negotiation subdialogue. \\n\\n  Selecting the Focus of Modification \\n\\nWhen multiple conflicts arise between the system and the user\\nregarding the user's proposal, the system must identify the aspect of\\nthe proposal on which it should focus in its pursuit of conflict\\nresolution. For example, in the case where Correct-Node is\\nselected as the specialization of Modify-Proposal, the system\\nmust determine how the parameter _node in Correct-Node should\\nbe instantiated. The goal of the modification process is to resolve\\nthe agents' conflicts regarding the unaccepted top-level proposed\\nbeliefs. For each such belief, the system could provide evidence\\nagainst the belief itself, address the unaccepted evidence proposed by\\nthe user to eliminate the user's justification for the belief, or\\nboth. Since collaborative agents are expected to engage in effective\\nand efficient dialogues, the system should address the unaccepted\\nbelief that it predicts will most quickly resolve the top-level\\nconflict. Therefore, for each unaccepted top-level belief, our process\\nfor selecting the focus of modification involves two steps:\\nidentifying a candidate foci tree from the proposed belief tree, and\\nselecting a focus from the candidate foci tree using the heuristic\\n``attack the belief(s) that will most likely resolve the conflict\\nabout the top-level belief.''  A candidate foci tree contains the\\npieces of evidence in a proposed belief tree which, if disbelieved by\\nthe user, might change the user's view of the unaccepted top-level\\nproposed belief (the root node of that belief tree). It is identified\\nby performing a depth-first search on the proposed belief tree. When a\\nnode is visited, both the belief and the evidential relationship\\nbetween it and its parent are examined. If both the belief and\\nrelationship were accepted by the evaluator, the search on the current\\nbranch will terminate, since once the system accepts a belief, it is\\nirrelevant whether it accepts the user's support for that belief\\n . Otherwise, this piece of evidence will be included in the candidate foci tree and the system will continue to\\nsearch through the evidence in the belief tree proposed as support for\\nthe unaccepted belief and/or evidential relationship.\\n\\n\\nOnce a candidate foci tree is identified, the system should select the\\nfocus of modification based on the likelihood of each choice changing\\n the user's belief about the top-level belief. Figure  shows our algorithm for this selection process. Given an unaccepted\\nbelief (_bel) and the beliefs proposed to support it, Select-Focus-Modification will annotate _bel with 1) its focus of\\nmodification (_bel.focus), which contains a set of beliefs (_bel\\nand/or its descendents) which, if disbelieved by the user, are\\npredicted to cause him to disbelieve _bel, and 2) the system's\\nevidence against _bel itself (_bel.s-attack).\\n\\n\\nSelect-Focus-Modification determines whether to attack _bel's\\nsupporting evidence separately, thereby eliminating the user's reasons\\nfor holding _bel, to attack _bel itself, or both.  However, in\\nevaluating the effectiveness of attacking the proposed evidence for\\n_bel, the system must determine whether or not it is possible to\\nsuccessfully refute a piece of evidence (i.e., whether or not the\\nsystem believes that sufficient evidence is available to convince the\\nuser that a piece of proposed evidence is invalid), and if so, whether\\nit is more effective to attack the evidence itself or its\\nsupport. Thus the algorithm recursively applies itself to the evidence\\nproposed as support for _bel which was not accepted by the system\\n (step ). In this recursive process, the algorithm annotates each unaccepted belief or evidential relationship proposed\\nto support _bel with its focus of modification (_beli.focus) and\\nthe system's evidence against it (_beli.s-attack). _beli.focus\\ncontains the beliefs selected to be addressed in order to change the\\nuser's belief about _beli, and its value will be nil if the system\\npredicts that insufficient evidence is available to change the user's\\nbelief about _beli.\\n\\n\\n Based on the information obtained in step , Select-Focus-Modification decides whether to attack the evidence  proposed to support _bel, or _bel itself (step ). Its preference is to address the unaccepted evidence, because McKeown's\\nfocusing rules suggest that continuing a newly introduced topic (about\\nwhich there is more to be said) is preferable to returning to a\\n previous topic . Thus the algorithm first considers whether or not attacking the user's support for _bel is sufficient to\\n convince him of _bel (step ). It does so by gathering (in cand-set) evidence proposed by the user as direct\\nsupport for _bel but which was not accepted by the system and which\\nthe system predicts it can successfully refute (i.e., _beli.focus\\nis not nil). The algorithm then hypothesizes that the user has changed\\nhis mind about each belief in cand-set and predicts how this\\n will affect the user's belief about _bel (step ). If the user is predicted to accept _bel under this hypothesis,\\nthe algorithm invokes Select-Min-Set to select a minimum subset\\nof cand-set as the unaccepted beliefs that it would actually\\npursue, and the focus of modification (_bel.focus) will be the union\\nof the focus for each of the beliefs in this minimum subset.\\n\\n\\nIf attacking the evidence for _bel does not appear to be sufficient\\nto convince the user of _bel, the algorithm checks whether\\ndirectly attacking _bel will accomplish this goal. If providing\\nevidence directly against _bel is predicted to be successful, then\\n the focus of modification is _bel itself (step ). If directly attacking _bel is also predicted to fail, the algorithm\\nconsiders the effect of attacking both _bel and its unaccepted\\nproposed evidence by combining the previous two prediction processes\\n (step ). If the combined evidence is still predicted to fail, the system does not have sufficient evidence to change the\\nuser's view of _bel; thus, the focus of modification for _bel is nil\\n (step ). and     of the algorithm invoke a function, Predict, that makes use of the belief revision mechanism\\n  discussed in Section  to predict the user's acceptance or unacceptance of _bel based on the system's\\nknowledge of the user's beliefs and the evidence that could be\\n presented to him . The result of Select-Focus-Modification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user's belief about\\nthe unaccepted top-level belief.  Thus, the negations of these beliefs\\nwill be posted by the system as mutual beliefs to be achieved in order\\nto perform the Modify actions.\\n\\n\\n  Selecting Justification for a Claim \\n\\nStudies in communication and social psychology have shown that\\nevidence improves the persuasiveness of a message\\n ,,,. Research on the quantity of evidence indicates that there is no\\noptimal amount of evidence, but that the use of high-quality evidence\\n is consistent with persuasive effects . On the other  hand, Grice's maxim of quantity  specifies that one should not contribute more information than is\\n required. Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an\\nintended mutual belief.\\n\\n\\nTo convince the user of a belief, _bel, our system selects\\nappropriate justification by identifying beliefs that could be used to\\nsupport _bel and applying filtering heuristics to them. The system\\nmust first determine whether justification for _bel is needed by\\npredicting whether or not merely informing the user of _bel will be\\nsufficient to convince him of _bel. If so, no justification will be\\npresented. If justification is predicted to be necessary, the system\\nwill first construct the justification chains that could be used to\\nsupport _bel. For each piece of evidence that could be used to\\ndirectly support _bel, the system first predicts whether the user\\nwill accept the evidence without justification. If the user is\\npredicted not to accept a piece of evidence (evidi), the system\\nwill augment the evidence to be presented to the user by posting\\nevidi as a mutual belief to be achieved, and selecting propositions\\nthat could serve as justification for it. This results in a recursive\\nprocess that returns a chain of belief justifications that could be\\nused to support _bel.\\n\\n\\nOnce a set of beliefs forming justification chains is identified, the\\nsystem must then select from this set those belief chains which, when\\npresented to the user, are predicted to convince the user of\\n_bel. Our system will first construct a singleton set for each such\\njustification chain and select the sets containing justification\\nwhich, when presented, is predicted to convince the user of _bel.  If\\nno single justification chain is predicted to be sufficient to change\\nthe user's beliefs, new sets will be constructed by combining the\\nsingle justification chains, and the selection process is repeated.\\nThis will produce a set of possible candidate justification chains,\\nand three heuristics will then be applied to select from among\\nthem. The first heuristic prefers evidence in which the system is most\\nconfident since high-quality evidence produces more attitude change\\n than any other evidence form .  Furthermore, the system can better justify a belief in which it has high confidence\\nshould the user not accept it. The second heuristic prefers evidence\\nthat is novel to the user, since studies have shown that evidence is\\nmost persuasive if it is previously unknown to the hearer\\n ,.  The third heuristic is based on Grice's maxim of quantity and prefers justification chains that contain the\\nfewest beliefs.\\n\\n\\n  Example \\n\\n After the evaluation of the dialogue model in Figure , Modify-Proposal is invoked because the top-level proposed belief is not accepted.  In selecting the focus of modification, the system will\\nfirst identify the candidate foci tree and then invoke the Select-Focus-Modification algorithm on the belief at the root node of\\nthe candidate foci tree. The candidate foci tree will be identical to\\n the proposed belief tree in Figure  since both the top-level proposed belief and its proposed evidence were rejected\\nduring the evaluation process. This indicates that the focus of\\nmodification could be either Teaches(Smith,AI) or On-Sabbatical(Smith, next year) (since the evidential relationship\\nbetween them was accepted).  When Select-Focus-Modification is\\napplied to Teaches(Smith,AI), the algorithm will first be\\nrecursively invoked on On-Sabbatical(Smith, next year) to\\ndetermine the focus for modifying the child belief (step 3.1 in\\n Figure ). Since the system has two pieces of evidence against On-Sabbatical(Smith, next year), 1) a warranted piece of\\nevidence containing Postponed-Sabbatical(Smith,1997) and supports(Postponed-Sabbatical(Smith,1997),On-Sabbatical(Smith,\\nnext year)), and 2) a strong piece of evidence containing visitor(Smith,IBM,next year) and supports(visitor(Smith,IBM,next\\nyear),On-Sabbatical(Smith,next year)), the evidence is\\npredicted to be sufficient to change the user's belief in On-Sabbatical(Smith,next year), and hence Teaches(Smith,AI); thus, the focus of modification will be\\nOn-Sabbatical(Smith,next year). The Correct-Node\\nspecialization of Modify-Proposal will be invoked since the\\nfocus of modification is a belief, and in order to satisfy the\\n precondition of Modify-Node (Figure ), MB(S,U, On-Sabbatical(Smith,next year)) will be posted as a mutual\\nbelief to be achieved.\\n\\n\\nSince the user has a warranted belief in On-Sabbatical(Smith,next\\nyear) (indicated by the semantic form of utterance\\n ()), the system will predict that merely informing the user of the intended mutual belief is not sufficient to change his\\nbelief; therefore it will select justification from the two available\\npieces of evidence supporting On-Sabbatical(Smith,next\\nyear) presented earlier.  The system will predict that either piece\\nof evidence combined with the proposed mutual belief is sufficient to\\nchange the user's belief; thus, the filtering heuristics are applied.\\nThe first heuristic will cause the system to select Postponed-Sabbatical(Smith, 1997) and supports(Postponed-Sabbatical(Smith, 1997),On-Sabbatical(Smith,\\nnext year)) as support, since it is the evidence in which the system\\nis more confident.\\n\\n\\n The system will try to establish the mutual beliefs as an attempt to satisfy the precondition of Modify-Node. This will cause the system to invoke Inform discourse actions to generate the following utterances:\\n\\n\\nS:S:\\n   #2\\n\\n\\n   to (8) #1\\n  Dr. Smith is not going on sabbatical next year.\\n\\n\\n   to (9)\\n  He postponed his sabbatical until 1997. \\n\\n\\nIf the user accepts the system's utterances, thus satisfying\\nthe precondition that the conflict be resolved, Modify-Node can\\nbe performed and changes made to the original proposed beliefs.\\nOtherwise, the user may propose modifications to the system's proposed\\nmodifications, resulting in an embedded negotiation subdialogue.\\n\\n\\n\\n\\n  Conclusion \\n\\nThis paper has presented a computational strategy for engaging in\\ncollaborative negotiation to square away conflicts in agents'\\nbeliefs. The model captures features specific to collaborative\\nnegotiation. It also supports effective and efficient dialogues by\\nidentifying the focus of modification based on its predicted success\\nin resolving the conflict about the top-level belief and by using\\nheuristics motivated by research in social psychology to select a set\\nof evidence to justify the proposed modification of beliefs.\\nFurthermore, by capturing collaborative negotiation in a cycle of Propose-Evaluate-Modify actions, the evaluation and modification\\nprocesses can be applied recursively to capture embedded negotiation\\nsubdialogues.\\n\\n\\n  Acknowledgments \\n\\nDiscussions with Candy Sidner, Stephanie Elzer, and Kathy McCoy have\\nbeen very helpful in the development of this work. Comments from the\\nanonymous reviewers have also been very useful in preparing the final\\nversion of this paper.\\n\\nBibliography \\n\\nJames Allen.\\n1991.\\nDiscourse structure in the TRAINS project.\\nIn Darpa Speech and Natural Language Workshop.\\n\\n\\nLawrence Birnbaum, Margot Flowers, and Rod McGuire.\\n1980.\\nTowards an AI model of argumentation.\\nIn Proceedings of the National Conference on Artificial\\n  Intelligence, pages 313-315.\\n\\n\\nAlison Cawsey, Julia Galliers, Brian Logan, Steven Reece, and Karen\\n  Sparck Jones.\\n1993.\\nRevising beliefs and intentions: A unified framework for agent\\n  interaction.\\nIn The Ninth Biennial Conference of the Society for the Study of\\n  Artificial Intelligence and Simulation of Behaviour, pages 130-139.\\n\\n\\nJennifer Chu-Carroll and Sandra Carberry.\\n1994.\\nA plan-based model for response generation in collaborative\\n  task-oriented dialogues.\\nIn Proceedings of the Twelfth National Conference on Artificial\\n  Intelligence, pages 799-805.\\n\\n\\nJennifer Chu-Carroll and Sandra Carberry.\\n1995.\\nGenerating information-sharing subdialogues in expert-user\\n  consultation.\\nIn Proceedings of the 14th International Joint Conference on\\n  Artificial Intelligence.\\nTo appear.\\n\\n\\nJennifer Chu-Carroll.\\n1995.\\nA Plan-Based Model for Response Generation in Collaborative\\n  Consultation Dialogues.\\nPh.D. thesis, University of Delaware.\\nForthcoming.\\n\\n\\nPaul R. Cohen.\\n1985.\\nHeuristic Reasoning about Uncertainty: An Artificial\\n  Intelligence Approach.\\nPitman Publishing Company.\\n\\n\\nRobin Cohen.\\n1987.\\nAnalyzing the structure of argumentative discourse.\\nComputational Linguistcis, 13(1-2):11-24, January-June.\\n\\n\\nColumbia University Transcripts.\\n1985.\\nTranscripts derived from audiotape conversations made at Columbia\\n  University, New York, NY.\\nProvided by Kathleen McKeown.\\n\\n\\nJulia R. Galliers.\\n1992.\\nAutonomous belief revision and communication.\\nIn Gardenfors, editor, Belief Revision. Cambridge University\\n  Press.\\n\\n\\nH. Paul Grice.\\n1975.\\nLogic and conversation.\\nIn Peter Cole and Jerry L. Morgan, editors, Syntax and Semantics\\n  3: Speech Acts, pages 41-58. Academic Press, Inc., New York.\\n\\n\\nBarbara J. Grosz and Candace L. Sidner.\\n1990.\\nPlans for discourse.\\nIn Cohen, Morgan, and Pollack, editors, Intentions in\\n  Communication, chapter 20, pages 417-444. MIT Press.\\n\\n\\nDale Hample.\\n1985.\\nRefinements on the cognitive model of argument: Concreteness,\\n  involvement and group scores.\\nThe Western Journal of Speech Communication, 49:267-285.\\n\\n\\nAravind K. Joshi.\\n1982.\\nMutual beliefs in question-answer systems.\\nIn N.V. Smith, editor, Mutual Knowledge, chapter 4, pages\\n  181-197. Academic Press.\\n\\n\\nLynn Lambert and Sandra Carberry.\\n1991.\\nA tripartite plan-based model of dialogue.\\nIn Proceedings of the 29th Annual Meeting of the Association for\\n  Computational Linguistics, pages 47-54.\\n\\n\\nBrian Logan, Steven Reece, Alison Cawsey, Julia Galliers, and Karen\\n  Sparck Jones.\\n1994.\\nBelief revision and dialogue management in information retrieval.\\nTechnical Report 339, University of Cambridge, Computer Laboratory.\\n\\n\\nJoseph A. Luchok and James C. McCroskey.\\n1978.\\nThe effect of quality of evidence on attitude change and source\\n  credibility.\\nThe Southern Speech Communication Journal, 43:371-383.\\n\\n\\nMark T. Maybury.\\n1993.\\nCommunicative acts for generating natural language arguments.\\nIn Proceedings of the National Conference on Artificial\\n  Intelligence, pages 357-364.\\n\\n\\nKathleen R. McKeown.\\n1985.\\nText Generation : Using Discourse Strategies and Focus\\n  Constraints to Generate Natural Language Text.\\nCambridge University Press.\\n\\n\\nDonald D. Morley.\\n1987.\\nSubjective message constructs: A theory of persuasion.\\nCommunication Monographs, 54:183-203.\\n\\n\\nRichard E. Petty and John T. Cacioppo.\\n1984.\\nThe effects of involvement on responses to argument quantity and\\n  quality: Central and peripheral routes to persuasion.\\nJournal of Personality and Social Psychology, 46(1):69-81.\\n\\n\\nMartha E. Pollack.\\n1986.\\nA model of plan inference that distinguishes between the beliefs of\\n  actors and observers.\\nIn Proceedings of the 24th Annual Meeting of the Association for\\n  Computational Linguistics, pages 207-214.\\n\\n\\nAlex Quilici.\\n1992.\\nArguing about planning alternatives.\\nIn Proceedings of the 14th International Conference on\\n  Computational Linguistics, pages 906-910.\\n\\n\\nRachel Reichman.\\n1981.\\nModeling informal debates.\\nIn Proceedings of the 7th International Joint Conference on\\n  Artificial Intelligence, pages 19-24.\\n\\n\\nJohn C. Reinard.\\n1988.\\nThe empirical study of the persuasive effects of evidence, the status\\n  after fifty years of research.\\nHuman Communication Research, 15(1):3-59.\\n\\n\\nRodney A. Reynolds and Michael Burgoon.\\n1983.\\nBelief processing, reasoning, and evidence.\\nIn Bostrom, editor, Communication Yearbook 7, chapter 4, pages\\n  83-104. Sage Publications.\\n\\n\\nCandace L. Sidner.\\n1992.\\nUsing discourse to negotiate in collaborative activity: An artificial\\n  language.\\nIn AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent\\n  Systems, pages 121-128.\\n\\n\\nCandace L. Sidner.\\n1994.\\nAn artificial discourse language for collaborative negotiation.\\nIn Proceedings of the Twelfth National Conference on Artificial\\n  Intelligence, pages 814-819.\\n\\n\\nSRI Transcripts.\\n1992.\\nTranscripts derived from audiotape conversations made at SRI\\n  International, Menlo Park, CA.\\nPrepared by Jacqueline Kowtko under the direction of Patti Price.\\n\\n\\nKatia Sycara.\\n1989.\\nArgumentation: Planning other agents' plans.\\nIn Proceedings of the 11th International Joint Conference on\\n  Artificial Intelligence, pages 517-523.\\n\\n\\nMarilyn Walker and Steve Whittaker.\\n1990.\\nMixed initiative in dialogue: An investigation into discourse\\n  segmentation.\\nIn Proceedings of the 28th Annual Meeting of the Association for\\n  Computational Linguistics, pages 70-78.\\n\\n\\nMarilyn A. Walker.\\n1992.\\nRedundancy in collaborative dialogue.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics, pages 345-351.\\n\\n\\nMarilyn A. Walker.\\n1994.\\nDiscourse and deliberation: Testing a collaborative strategy.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics.\\n\\n\\nBonnie Webber and Aravind Joshi.\\n1982.\\nTaking the initiative in natural language data base interactions:\\n  Justifying why.\\nIn Proceedings of COLING-82, pages 413-418.\\n\\n\\nSteve Whittaker and Phil Stenton.\\n1988.\\nCues and control in expert-client dialogues.\\nIn Proceedings of the 26th Annual Meeting of the Association for\\n  Computational Linguistics, pages 123-130.\\n\\n\\nRobert S. Wyer, Jr.\\n1970.\\nInformation redundancy, inconsistency, and novelty and their role in\\n  impression formation.\\nJournal of Experimental Social Psychology, 6:111-127.\\n\\n\\nR. Michael Young, Johanna D. Moore, and Martha E. Pollack.\\n1994.\\nTowards a principled representation of discourse plans.\\nIn Proceedings of the Sixteenth Annual Meeting of the Cognitive\\n  Science Society, pages 946-951.\\n\\nFootnotes\\n\\n  This\\nmaterial is based upon work supported by the National Science\\nFoundation under Grant No. IRI-9122026.\\n  The notion of shared plan has\\n been used in ,. \\n  For details on\\nhow our model determines the acceptance of a belief using the ranking\\n of endorsements proposed by Galliers, see . \\n  The strength of a belief is classified as: warranted, strong, or weak, based on the endorsement of\\nthe belief.\\n   A recipe  is a template for performing actions. It contains the applicability conditions for\\nperforming an action, the subactions comprising the body of an action,\\netc.\\n  Applicability conditions are\\nconditions that must already be satisfied in order for an action to be\\nreasonable to pursue, whereas an agent can try to achieve unsatisfied\\npreconditions.\\n  This subdialogue is\\nconsidered an interrupt by Whittaker, Stenton, and Walker\\n ,, initiated to negotiate the truth of a piece of information.  However, the utterances they classify as\\ninterrupts include not only our negotiation subdialogues,\\ngenerated for the purpose of modifying a proposal, but also\\nclarification subdialogues, and information-sharing subdialogues\\n , which we contend should be part of the evaluation process. \\n  In collaborative dialogues, an agent should\\nreject a proposal only if she has strong evidence against it. When an\\nagent does not have sufficient information to determine the acceptance\\nof a proposal, she should initiate an information-sharing\\nsubdialogue to share information with the other agent and re-evaluate\\n the proposal . Thus, further research is needed to determine whether or not the focus of modification for a rejected\\nbelief will ever be nil in collaborative dialogues.\\n  Walker wal_coling94 has shown the\\nimportance of IRU's (Informationally Redundant Utterances) in\\nefficient discourse. We leave including appropriate IRU's for future\\nwork.\\n  Only MB(S,U,Postponed-Sabbatical(Smith, 1997)) will be proposed as\\njustification because the system believes that the evidential\\nrelationship needed to complete the inference is held by a\\nstereotypical user.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nIn collaborative planning activities, since the agents are autonomous\\nand heterogeneous, it is inevitable that conflicts arise in their\\nbeliefs during the planning process. In cases where such conflicts are\\nrelevant to the task at hand, the agents should engage in collaborative negotiation as an attempt to square away the\\ndiscrepancies in their beliefs. This paper presents a computational\\nstrategy for detecting conflicts regarding proposed beliefs and for\\nengaging in collaborative negotiation to resolve the conflicts that\\nwarrant resolution. Our model is capable of selecting the most\\neffective aspect to address in its pursuit of conflict resolution in\\ncases where multiple conflicts arise, and of selecting appropriate\\nevidence to justify the need for such modification. Furthermore, by\\ncapturing the negotiation process in a recursive Propose-Evaluate-Modify cycle of actions, our model can successfully\\nhandle embedded negotiation subdialogues.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nTense interpretation has received much attention in linguistics\\n ,,, inter alia] and natural language processing\\n ,,, inter alia]. Several researchers\\n ,,, have sought to explain the temporal relations induced by tense by treating\\nit as anaphoric, drawing on Reichenbach's separation between event,\\n speech, and reference times .  Specifically, to account for the forward progression of time induced by\\nsuccessive simple past tenses in a narrative, they treat the simple\\npast as referring to a time evoked by a previous past tense.  For\\ninstance, in Hinrichs's  proposal,\\n accomplishments and achievements introduce a new reference point that is temporally ordered after the time of the event itself, ``ensuring\\nthat two consecutive accomplishments or achievements in a discourse\\nare always ordered in a temporal sequence.''\\nOn the other hand, Lascarides and Asher  take the\\nview that temporal relations are resolved purely as a by-product of\\nreasoning about coherence relations holding between utterances, and in\\ndoing so, argue that treating simple and complex tenses as\\nanaphoric is unnecessary.  This approach parallels the treatment of\\npronoun resolution espoused by Hobbs , in which\\npronouns are modeled as free variables that are bound as a by-product\\nof coherence resolution.  The Temporal Centering framework\\n  integrates aspects of both approaches, but patterns with the first in treating tense as\\nanaphoric.\\n\\n\\nWe argue that aspects of both analyses are necessary to account for\\nthe recovery of temporal relations.  To demonstrate our approach we\\n will address the following examples; passages (-b) are taken from Lascarides and Asher :\\n\\n\\n  Max slipped.  He spilt a bucket of water. \\n\\n\\nMax slipped.  He had spilt a bucket of water.\\n\\n\\nMax slipped because he spilt a bucket of water.\\n\\n\\nMax slipped because he had spilt a bucket of water.\\n Passage () is understood as a narrative, indicating that the spilling was subsequent to the slipping.  Passages\\n (-d) are instead understood as the second clause explaining the first, indicating that the reverse temporal ordering holds.  We address two related questions; the first arises from\\ntreating the simple past as anaphoric.  Specifically, if a treatment\\nsuch as Hinrichs's is used to explain the forward progression of time\\n in example (), then it must be explained why sentence  () is as felicitous as sentence ().  That is, one would predict a clash of temporal relations for sentence\\n (), since the simple pasts induce the forward progression of time but the conjunction indicates the reverse temporal ordering.\\nThe second question arises from assuming that all temporal relations\\nare recovered solely from reasoning with coherence relations.\\nSpecifically, because the use of the simple past in passage\\n () is as felicitous as the past  perfect in passage () under the explanation interpretation (in these cases indicated explicitly by because),\\n then it must be explained why passage () is not  understood as an explanation as is passage (), where in each case the relationship needs to be inferred.  We present our\\nanalysis in the next section, and account for these facts in\\nSection 3.\\n\\n\\n  The Account \\n\\nWe postulate rules characterizing the referential nature of tense and\\nthe role of discourse relations in further constraining the temporal\\nrelations between clauses.  The rules governing tense are:\\n\\n\\n1.\\nMain verb tenses are indefinitely referential, creating a new temporal\\nentity under constraints imposed by its type (i.e., past,\\npresent, or future) in relation to a discourse reference\\n time tR.  For instance, a main verb past tense introduces a new temporal entity t under the constraint\\nprior-to(t,tR).  For simple tenses tR is the speech\\ntime, and therefore simple tenses are not anaphoric.\\n\\n\\n2.\\nTensed auxiliaries in complex tenses are anaphoric, identifying\\ntR as a\\npreviously existing temporal entity.  The indefinite main verb tense\\nis then ordered with respect to this tR.\\nThe tenses used may not completely specify the implicit temporal\\nrelations between the described events.  We claim that these relations\\nmay be further refined by constraints imposed by the coherence\\nrelation operative between clauses.  We describe three coherence\\nrelations relevant to the examples in this paper and give\\n temporal constraints for them. \\n\\n\\nNarration:\\n\\n\\nThe Narration relation is characterized by a series of events\\ndisplaying forward movement of time, such as in passage\\n (). As did Lascarides and Asher , we capture this ordering as a constraint imposed by the Narration\\n coherence relation itself: \\n\\n\\nIf \\n\\nNarration(A,B) then \\n\\ntA [ tB\\n\\n\\nParallel:\\n\\n\\nThe Parallel relation relates utterances that share a common topic.  This\\nrelation does not impose constraints on the temporal relations between the\\nevents beyond those provided by the tenses themselves.  For instance,\\n if passage () was uttered in response to the question What bad things happened to Max today? (inducing a Parallel\\nrelation instead of Narration), a temporal ordering among the\\nsentences is no longer implied.\\n\\n\\nExplanation:\\n\\n\\nThe Explanation relation denotes a cause-effect relationship with\\n reversed clause ordering, as in sentences (-d). Therefore, the second event is constrained to preceding the\\nfirst:\\n\\n\\nIf Explanation(A,B) then \\n\\ntB [ tA\\n\\n\\nTo summarize the analysis, we claim that tense operates as indefinite\\nreference with respect to a possibly anaphorically-resolved discourse\\nreference time.  The temporal relations specified may be further\\nrefined as a by-product of establishing the coherence relationship\\nextant between clauses, Narration being but one such relation.\\n\\n\\n  Examples \\n\\nWe now analyze the examples presented in Section 1, repeated below,\\nusing this approach:\\n\\n\\n  Max slipped.  He spilt a bucket of water. \\n\\n\\nMax slipped.  He had spilt a bucket of water.\\n\\n\\nMax slipped because he spilt a bucket of water.\\n\\n\\nMax slipped because he had spilt a bucket of water.\\n\\n\\nThe implicit ordering on the times indefinitely evoked by the simple\\n pasts in passage () results solely from understanding it  as a Narration.  In passage (), the auxiliary had refers to the event time of the slipping, and thus the past tense on\\nspill creates a temporal entity constrained to precede that\\ntime.  This necessitates a coherence relation that is consistent with this\\ntemporal order, in this case, Explanation.  In passage\\n (), the times evoked by the simple pasts are further ordered by the Explanation relation indicated by because,\\nresulting in the backward progression of time.  In passage\\n (), both the tense and the coherence relation order the times in backward progression.\\n\\n\\nRestating the first problem noted in Section 1, if treating the simple\\npast as anaphoric is used to account for the forward progression of\\n time in passage (), then one would expect the existence  of the Explanation relation in passage () to cause a  temporal clash, where in fact passage () is perfectly felicitous.  No clash of temporal relations is predicted by our\\naccount, because the use of the simple pasts do not in\\nthemselves imply a specific ordering between them.  The Narration\\nrelation orders the times in forward progression in passage\\n () and the Explanation relation orders them in backward  progression in passage (). The Parallel relation would specify no ordering (see the potential context for passage\\n () given in Section 2). \\n\\n\\nRestating the second problem noted in Section 1, if temporal relations\\ncan be recovered solely from reasoning with coherence relations, and\\n the use of the simple past in passage () is as felicitous as the past perfect in passage\\n () under the Explanation interpretation, then one asks  why passage () is not understood as an Explanation as is  passage (), where in each case the relationship needs to be inferred.  We hypothesize that hearers assume that speakers are\\nengaging in Narration in absence of a specific cue to the contrary.\\n The use of the past perfect (as in passage ()) is one such cue since it implies reversed temporal ordering; the use of an\\nexplicit conjunction indicating a coherence relation other than\\n Narration (as in passages (-d)) is another such cue.  While passage () could be understood as an Explanation on semantic grounds, the hearer assumes Narration since no other\\nrelation is cued.\\n\\n\\nWe see several advantages of this approach over that of Lascarides and\\nAsher .  First, LA note the\\n incoherence of example () ? Max poured a cup of coffee.  He had entered the room.\\n   in arguing that the past perfect should not be treated as anaphoric:\\nTheories that analyse the distinction between the\\nsimple past and pluperfect purely in terms of different relations\\nbetween reference times and event times, rather than in terms of\\n event-connections, fail to explain why [()] is acceptable  but [()] is awkward.  , pg. 470]  Example () indeed shows that coherence relations need to be utilized to account for temporal relations, but it does not bear on\\nthe issue of whether the past perfect is anaphoric.  The incoherence\\n of example () is predicted by both their and our accounts by virtue of the fact that there is no coherence relation that\\n corresponds to Narration with reverse temporal ordering. In addressing this example, LA specify a special rule (the Connections When\\nChanging Tense (CCT) Law) that stipulates that a sentence containing\\nthe simple past followed by a sentence containing the past perfect can\\nbe related only by a subset of the otherwise possible coherence relations.\\nHowever, this subset contains just those relations that are predicted\\nto be possible by accounts treating the past perfect as anaphoric;\\nthey are the ones that do not constrain the temporal order of the\\nevents against displaying backward progression of time.  Therefore, we\\nsee no advantages to adopting their rule; furthermore,\\nthey do not comment on what other laws have to be stipulated to\\naccount for the facts concerning other possible tense combinations.\\n\\n\\nSecond, to explain why the Explanation relation can be inferred for\\n passage () but not for passage (), LA stipulate that their causal Slipping Law\\n(stating that spilling can cause slipping) requires that the CCT Law\\nbe satisfied.  This constraint is imposed only to require that the\\nsecond clause contain the past perfect instead of the simple past.\\nHowever, this does not explain why the use of the simple past is\\nperfectly coherent when the Explanation relationship is indicated\\n overtly as it is in sentence (), nor do they adequately explain why CCT must be satisfied for this causal law and not for\\nthose supporting similar examples for which they successfully\\ninfer an unsignaled Explanation relation (see discussion of example\\n(2), pg. 463).\\n\\n\\nThird, the LA account does not explain why the\\npast perfect cannot stand alone nor discourses generally be opened\\n with it; consider stating sentence () in isolation: \\n\\n\\n Max had spilt a bucket of water.  Intuitively, such usage is infelicitous because of a dependency on a\\ncontextually salient time which has not been previously introduced.\\nThis is not captured by the LA account because\\nsentences containing the past perfect are treated as sententially\\nequivalent to those containing the simple past.  On the other hand,\\nsentences in the simple past are perfectly felicitous in standing\\nalone or opening a discourse, introducing an asymmetry in accounts\\ntreating the simple past as anaphoric to a previously evoked time.\\nAll of these facts are explained by the account given here.\\n\\n\\n  Conclusion \\n\\nWe have given an account of temporal relations whereby (1) tense is\\nresolved indefinitely with respect to a possibly\\nanaphorically-resolved discourse reference time, and (2) the resultant\\ntemporal relations may be further refined by constraints that\\ncoherence relations impose.  This work is being expanded to\\naddress issues pertaining to discourse structure and inter-segment\\ncoherence.\\n\\n\\n  Acknowledgments \\n\\nThis work was supported in part by National Science Foundation Grant\\nIRI-9009018, National Science Foundation Grant IRI-9350192, and a\\ngrant from the Xerox Corporation.  I would like to thank Stuart\\nShieber and Barbara Grosz for valuable discussions and comments on\\nearlier drafts.\\n\\nBibliography \\n\\nErhard Hinrichs.\\n1986.\\nTemporal anaphora in discourses of english.\\nLinguistics and Philosophy, 9:63-82.\\n\\n\\nJerry Hobbs.\\n1979.\\nCoherence and coreference.\\nCognitive Science, 3:67-90.\\n\\n\\nMegumi Kameyama, Rebecca Passoneau, and Massimo Poesio.\\n1993.\\nTemporal centering.\\nIn Proceedings of the 31st Conference of the Association for\\n  Computational Linguistics (ACL-93), pages 70-77, Columbus, Ohio, June.\\n\\n\\nAlex Lascarides and Nicolas Asher.\\n1993.\\nTemporal interpretation, discourse relations, and common sense\\n  entailment.\\nLinguistics and Philosophy, 16(5):437-493.\\n\\n\\nJohn Nerbonne.\\n1986.\\nReference time and time in narration.\\nLinguistics and Philosophy, 9:83-95.\\n\\n\\nBarbara Partee.\\n1984.\\nNominal and temporal anaphora.\\nLinguistics and Philosophy, 7:243-286.\\n\\n\\nHans Reichenbach.\\n1947.\\nElements of Symbolic Logic.\\nMacmillan, New York.\\n\\n\\nBonnie Lynn Webber.\\n1988.\\nTense as discourse anaphor.\\nComputational Linguistics, 14(2):61-73.\\n\\nFootnotes\\n\\n  We will limit the scope of\\nthis paper by restricting the discussion to accomplishments and\\nachievements.\\n  This term is borrowed from Kameyama et al.\\n.\\n  We assume here that the two\\nclauses in question are related directly by a coherence relation.\\nThis may not be the case; for instance the use of a past perfect may\\nsignal the start of an embedded discourse segment, as in Webber's\\n flower shop example ,.  How this account is to be extended to address coherence at the discourse segment level\\nis the subject of future work.\\n  The Cause-Effect relation\\nalso has this ordering constraint.\\n  For\\ninstance, in the same way that Explanation corresponds to Cause-Effect\\nwith reverse temporal ordering.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThe temporal relations that hold between events described by\\nsuccessive utterances are often left implicit or underspecified.  We\\naddress the role of two phenomena with respect to the recovery of\\nthese relations: (1) the referential properties of tense, and (2) the\\nrole of temporal constraints imposed by coherence relations.  We\\naccount for several facets of the identification of temporal\\nrelations through an integration of these.\\n\\n'],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nData sparseness is an inherent problem in statistical methods for\\nnatural language processing. Such methods use\\nstatistics on the relative frequencies of configurations of elements\\nin a training corpus to evaluate alternative\\nanalyses or interpretations of new samples of text or speech. The most\\nlikely analysis will be taken to be the one that contains the most\\nfrequent configurations.  The problem of data sparseness arises when\\nanalyses contain configurations that never\\noccurred in the training corpus. Then it is not possible to\\nestimate probabilities from observed frequencies, and some other\\nestimation scheme has to be used.\\n\\n\\nWe focus here on a particular kind of configuration, word\\ncooccurrence. Examples of such cooccurrences include relationships\\nbetween head words in syntactic constructions (verb-object or\\nadjective-noun, for example) and word sequences (n-grams).  In\\ncommonly used models, the probability estimate for a previously unseen\\ncooccurrence is a function of the probability estimates for the words\\nin the cooccurrence. For example, in the bigram models that we study\\nhere, the probability \\n\\nP(w2 | w1) of a conditioned word w2that has never occurred in training following the conditioning\\nword w1 is calculated from the probability of w2, as estimated\\nby w2's frequency in the corpus\\n ,.  This method depends on an independence assumption on the cooccurrence of w1 and w2: the\\nmore frequent w2 is, the higher will be the estimate of \\n\\nP(w2 |\\nw1), regardless of w1.\\n\\n\\nClass-based and similarity-based models provide an alternative to the\\nindependence assumption. In those models, the relationship between\\ngiven words is modeled by analogy with other words that are in some\\nsense similar to the given ones.\\n\\n\\n\\nsuggest a class-based n-gram model in which words with similar cooccurrence\\ndistributions are clustered in word classes.  The cooccurrence\\nprobability of a given pair of words then is estimated according to an\\naveraged cooccurrence probability of the two corresponding classes.\\n propose a ``soft'' clustering scheme\\nfor certain grammatical cooccurrences in which\\nmembership of a word in a class is probabilistic.  Cooccurrence\\nprobabilities of words are then modeled by averaged cooccurrence\\nprobabilities of word clusters.\\n\\n\\n argue that reduction to a relatively small\\nnumber of\\npredetermined word classes or clusters may cause a substantial loss of\\ninformation.  Their similarity-based model avoids clustering\\naltogether.  Instead, each word is modeled by its own specific class,\\na set of words which are most similar to it (as in k-nearest\\nneighbor approaches in pattern recognition).  Using this scheme, they\\npredict which unobserved cooccurrences are more likely than\\nothers. Their model, however, is not probabilistic, that is, it does\\nnot provide a probability estimate for unobserved cooccurrences. It\\ncannot therefore be used in a complete probabilistic framework, such\\nas n-gram language models or probabilistic lexicalized grammars\\n ,. \\n\\n\\nWe now give a similarity-based method for estimating the probabilities\\nof cooccurrences unseen in training.  Similarity-based estimation was\\nfirst used for language modeling in the cooccurrence smoothing\\nmethod of , derived from work on acoustic model\\nsmoothing by\\n.\\nWe present a different method that takes as starting point the\\nback-off scheme of .  We first allocate an\\nappropriate probability mass for unseen cooccurrences following the\\nback-off method.  Then we redistribute that mass to unseen\\ncooccurrences according to an averaged cooccurrence distribution of a\\nset of most similar conditioning words, using relative entropy as our\\nsimilarity measure. This second step replaces the use of the\\nindependence assumption in the original back-off model.\\n\\n\\nWe applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard back-off\\nmodel.  Testing on a held-out sample, the similarity model achieved a\\n20% reduction in perplexity\\nfor unseen bigrams. These constituted\\njust 10.6% of the test sample, leading to an overall reduction in\\ntest-set perplexity of 2.4%. We also experimented with an application\\nto language modeling for speech recognition, which yielded a\\nstatistically significant reduction in recognition error.\\n\\n\\nThe remainder of the discussion is presented in terms of bigrams, but\\nit is valid for other types of word cooccurrence as well.\\n\\n\\n  Discounting and Redistribution \\n\\nMany low-probability bigrams will be missing from any finite\\nsample. Yet, the aggregate probability of all these unseen bigrams is\\nfairly high; any new sample is very likely to contain some.\\n\\n\\nBecause of data sparseness, we cannot reliably use a maximum\\nlikelihood estimator (MLE) for bigram probabilities.  The MLE for the\\nprobability of a bigram (w1,w2) is simply:\\n\\n\\n\\n\\n\\nwhere \\n\\nc(w1,w2) is the frequency of (w1,w2) in the training\\ncorpus and N is the total number of bigrams. However,\\nthis estimates the probability of any unseen bigram to be zero, which is\\nclearly undesirable.\\n\\n\\nPrevious proposals to circumvent the above problem\\n ,,, take the MLE as an initial estimate and adjust it so that the total probability of\\nseen bigrams is less than one, leaving some probability mass for\\nunseen bigrams. Typically, the adjustment involves either interpolation, in which the new estimator is a weighted combination\\nof the MLE and an estimator that is guaranteed to be nonzero for\\nunseen bigrams, or discounting, in which the MLE is decreased\\naccording to a model of the unreliability of small frequency counts,\\nleaving some probability mass for unseen bigrams.\\n\\n\\nThe back-off model of  provides a clear\\nseparation between frequent events, for which observed frequencies are\\nreliable probability estimators, and low-frequency events, whose\\nprediction must involve additional information sources. In addition,\\nthe back-off model does not require complex estimations for\\ninterpolation parameters.\\n\\n\\nA back-off model requires methods for (a) discounting the\\nestimates of previously observed events to leave out some positive\\nprobability mass for unseen events, and (b) redistributing among the unseen events the probability mass freed by\\ndiscounting. For bigrams the resulting estimator has the\\ngeneral form\\n\\n\\n\\n\\n\\nwhere Pd represents the discounted estimate for seen\\nbigrams, Pr the model for probability redistribution among the\\nunseen bigrams, and \\nis a normalization factor. Since the\\noverall mass left for unseen bigrams starting with w1 is given by\\n\\n\\n\\n\\n\\nthe normalization factor required to ensure\\n\\n\\nis\\n\\n\\n\\n\\n\\nThe second formulation of the normalization is computationally\\npreferable because the total number of possible bigram types far exceeds\\n the number of observed types. Equation () modifies slightly Katz's presentation to include the placeholder Pr for\\nalternative models of the distribution of unseen bigrams.\\n\\n\\nKatz uses the Good-Turing formula to replace the actual\\nfrequency \\n\\nc(w1,w2) of a bigram (or\\nan event, in general) with a discounted frequency, \\n\\nc[*](w1,w2),\\ndefined by\\n\\n\\n\\n\\n\\nwhere nc is the number of different bigrams in the corpus that have\\nfrequency c.\\nHe then uses the discounted frequency in the conditional probability\\ncalculation for a bigram:\\n\\n\\n\\n\\n\\n In the original Good-Turing method  the free probability mass is redistributed uniformly among all unseen events.\\nInstead, Katz's back-off scheme redistributes the free probability\\nmass non-uniformly in proportion to the frequency of w2, by setting\\n\\n\\n\\n\\n\\nKatz thus assumes that for a given conditioning word w1 the\\nprobability of an unseen following word w2 is proportional to its\\nunconditional probability. However, the overall form of the model\\n () does not depend on this assumption, and we will next investigate an estimate for \\n\\nPr(w2| w1) derived by averaging\\nestimates for the conditional probabilities that w2 follows words\\nthat are distributionally similar to w1.\\n\\n\\n  The Similarity Model \\n\\nOur scheme is based on the assumption that words that are ``similar'' to\\nw1 can provide good predictions for the\\ndistribution of w1 in unseen bigrams.\\nLet \\n\\n\\ndenote a set of words which are most similar to\\nw1, as determined by some similarity metric.\\nWe define \\n\\n,\\nthe similarity-based model for the\\nconditional distribution of w1,\\nas a weighted average of the conditional distributions of the words in\\n\\n:\\n\\n\\n\\n\\n\\nwhere \\n\\nW(w'1,w1) is the (unnormalized) weight given to w'1,\\ndetermined by its degree of similarity to w1.\\nAccording to this scheme, w2 is more likely to follow w1 if it\\ntends to follow words that are most similar to w1.\\nTo complete the scheme, it is necessary to define the similarity\\nmetric and, accordingly, \\n\\n\\nand \\n\\nW(w'1,w1).\\n\\n\\nFollowing , we\\nmeasure word similarity by the\\nrelative entropy, or Kullback-Leibler (KL) distance,\\nbetween the corresponding conditional distributions\\n\\n\\n\\n\\n\\nThe KL distance is 0 when \\n\\nw1 = w'1, and it increases as the\\ntwo distribution are less similar.\\n\\n\\n To compute () and () we must have nonzero estimates of \\n\\n P(w2|w1) whenever necessary for () to be defined. We use the estimates given by the standard back-off model, which satisfy\\nthat requirement. Thus our application of the similarity model\\naverages together standard back-off estimates for a set of similar\\nconditioning words.\\n\\n\\nWe define\\n\\n\\nas the set of\\nat most k nearest words to w1 (excluding w1 itself), that\\nalso satisfy \\n\\n.\\nk and t are parameters that control the contents of \\n\\n\\nand are tuned experimentally, as we will see below.\\n\\n\\n\\nW(w'1,w1) is defined as\\n\\n\\n\\n\\n\\nThe weight is larger for words that are\\nmore similar (closer) to w1. The parameter\\n\\ncontrols the relative contribution of\\nwords in different distances from w1: as the value  of increases, the nearest words to w1 get relatively more weight.\\nAs \\ndecreases, remote words get a larger effect. Like k and\\nt, \\nis tuned experimentally.\\n\\n\\nHaving a definition for \\n\\n,\\nwe could use it\\ndirectly as \\n\\n Pr(w2|w1) in the back-off scheme (). We found that it is better to smooth \\n\\n\\nby\\ninterpolating it with the unigram probability P(w2) (recall that\\nKatz used P(w2) as \\n\\nPr(w2|w1)). Using linear interpolation we\\nget\\n\\n\\n\\n\\n\\nwhere \\nis an experimentally-determined interpolation parameter.\\nThis smoothing appears to compensate for\\ninaccuracies in \\n\\n,\\nmainly for infrequent\\nconditioning words.\\nHowever, as the evaluation below shows, good values for \\nare\\nsmall, that is, the similarity-based model plays a stronger role than\\nthe independence assumption.\\n\\n\\nTo summarize, we construct a similarity-based model for \\n\\nP(w2|w1)and then interpolate it with P(w2).\\n The interpolated model () is used in the back-off scheme as \\n\\nPr(w2|w1), to obtain better estimates for unseen bigrams.\\nFour parameters, to be tuned experimentally, are relevant for this\\nprocess: k and t, which determine the set of similar words to be\\nconsidered, ,\\nwhich determines the relative effect of these\\nwords, and ,\\nwhich\\ndetermines the overall importance of the similarity-based model.\\n\\n\\n    Evaluation\\n\\n\\n We evaluated our method by comparing its perplexity  and effect on speech-recognition accuracy with the baseline bigram back-off model developed by MIT\\nLincoln Laboratories for the Wall Street Journal (WSJ) text and\\ndictation corpora provided by ARPA's HLT program\\n .    The baseline back-off model follows closely the Katz design, except that for\\ncompactness all frequency one bigrams are ignored. The counts used in\\nthis model and in ours were obtained from\\n40.5 million words of WSJ text from the years 1987-89.\\n\\n\\nFor perplexity evaluation, we tuned the similarity model parameters\\nby minimizing perplexity on an\\nadditional sample of 57.5 thousand\\nwords of WSJ text, drawn from the ARPA HLT development test set.\\nThe best parameter values found were\\nk=60, t=2.5, \\nand\\n\\n.\\nFor these values, the improvement in perplexity for\\nunseen bigrams in a held-out 18 thousand word sample, in which 10.6%\\nof the bigrams are unseen,\\nis just over 20%. This improvement on unseen bigrams\\ncorresponds to an overall test set perplexity improvement of 2.4%\\n(from 237.4 to 231.7).\\n Table  shows reductions in training and test perplexity, sorted by training reduction, for different choices in the\\nnumber k of closest neighbors used. The values of ,\\nand t are the best ones found for each k.\\n\\n\\n From equation (), it is clear that the computational cost of applying the similarity model to an unseen bigram is\\nO(k). Therefore, lower values for k (and\\nalso for t) are computationally preferable.\\nFrom the table, we can see that reducing k to 30 incurs\\na penalty of less than 1% in the perplexity improvement, so\\nrelatively low values of k appear to be sufficient to achieve most\\nof the benefit of the similarity model. As the table also shows, the\\nbest value of \\nincreases as k decreases, that is, for lower\\nk a greater weight is given to the conditioned word's frequency.\\nThis suggests that the predictive power of neighbors beyond the\\nclosest 30 or so can be modeled fairly well by the overall frequency\\nof the conditioned word.\\n\\n\\nThe bigram similarity model was also tested as a language model in\\nspeech recognition. The test data for this experiment were pruned word\\nlattices for 403 WSJ closed-vocabulary test sentences.  Arc scores in those\\nlattices are sums of an acoustic score (negative log likelihood) and a\\nlanguage-model score, in this case the negative log probability\\nprovided by the baseline bigram model.\\n\\n\\nFrom the given lattices, we constructed new lattices in which the arc\\nscores were modified to use the similarity model instead of the\\nbaseline model.  We compared the best sentence hypothesis in each\\noriginal lattice and in the modified one, and counted the word\\ndisagreements in which one of the hypotheses is correct.  There were a\\ntotal of 96 such disagreements. The similarity model was correct in 64\\ncases, and the back-off model in 32. This advantage for the similarity\\nmodel is statistically significant at the 0.01 level. The overall\\nreduction in error rate is small, from 21.4% to 20.9%, because the\\nnumber of disagreements is small compared with the overall number of\\nerrors in our current recognition setup.\\n\\n\\n Table  shows some examples of speech recognition disagreements between the two models. The hypotheses are labeled `B'\\nfor back-off and `S' for similarity, and the bold-face words are\\nerrors. The similarity model seems to be able to model better\\nregularities such as semantic parallelism in lists and avoiding a past\\ntense form after ``to.'' On the other hand, the similarity model\\nmakes several mistakes in which a function word is inserted in a place\\nwhere punctuation would be found in written text.\\n\\n\\n  Related Work \\n\\nThe cooccurrence smoothing technique\\n , based on earlier stochastic speech modeling work by  is the main previous attempt to\\nuse similarity to estimate the probability of unseen events in\\nlanguage modeling. In addition to its original use in language\\nmodeling for speech recognition,\\n applied the cooccurrence\\nsmoothing technique to estimate the likelihood of selectional\\npatterns.  We will outline here the main parallels and differences\\nbetween our method and cooccurrence smoothing. A more\\ndetailed analysis would require an empirical comparison of the two\\nmethods on the same corpus and task.\\n\\n\\nIn cooccurrence smoothing, as in our method, a baseline model is\\ncombined with a similarity-based model that refines some of its\\nprobability estimates. The similarity model in cooccurrence smoothing\\nis based on the intuition that the similarity between two words wand w' can be measured by the confusion probability\\nPC(w'|w) that w' can be substituted for w in an arbitrary\\ncontext in the training corpus. Given a baseline probability model\\nP, which is taken to be the MLE,\\nthe confusion\\nprobability \\n\\nPC(w'1|w1) between conditioning words w'1 and\\nw1 is defined as\\n\\n\\n\\n\\n\\nthe probability that w1 is followed by the same context words as\\nw'1. Then the bigram estimate derived by cooccurrence\\nsmoothing is given by\\n\\n\\n\\n\\n\\nNotice that this formula has the same form as our similarity model\\n (), except that it uses confusion probabilities where we use normalized weights.\\n  In addition, we restrict the summation to sufficiently similar words, whereas the cooccurrence smoothing\\nmethod sums over all words in the lexicon.\\n\\n\\n The similarity measure () is symmetric in the sense that PC(w'|w) and PC(w|w') are identical up to frequency\\nnormalization, that is \\n\\n.\\nIn contrast, \\n\\n\\n () is asymmetric in that it weighs each context in proportion to its\\nprobability of occurrence with w, but not with w'.  In this way,\\nif w and w' have comparable frequencies but w' has a sharper\\ncontext distribution than w, then \\n\\n\\nis greater than\\n\\n.\\nTherefore, in our similarity model w' will play a\\nstronger role in estimating w than vice versa. These properties\\nmotivated our choice of relative entropy for similarity measure, because\\nof the intuition that words with sharper distributions are more\\ninformative about other words than words with flat distributions.\\n\\n\\nFinally, while we have used our similarity model only for missing\\nbigrams in a back-off scheme,  used linear\\ninterpolation for all bigrams to combine the cooccurrence smoothing\\nmodel with MLE models of bigrams and unigrams. Notice, however, that\\nthe choice of back-off or interpolation is independent from the\\nsimilarity model used.\\n\\n\\n  Further Research \\n\\nOur model provides a basic scheme for probabilistic similarity-based\\nestimation that can be developed in several directions.  First,\\n variations of () may be tried, such as different similarity metrics and different weighting schemes. Also, some simplification of\\nthe current model parameters may be possible, especially with respect\\nto the parameters t and k used to select the nearest neighbors of\\na word.  A more substantial variation would be to base the model on\\nsimilarity between conditioned words rather than on similarity between\\nconditioning words.\\n\\n\\nOther evidence may be combined with the similarity-based estimate.\\nFor instance, it may be advantageous to weigh those estimates by some\\nmeasure of the reliability of the similarity metric and of the\\nneighbor distributions.  A second possibility is to take into account\\nnegative evidence: if w1 is frequent, but w2 never followed it,\\nthere may be enough statistical evidence to put an upper bound on the\\nestimate of \\n\\nP(w2|w1). This may require an adjustment of the\\nsimilarity based estimate, possibly along the lines of\\n .  Third, the similarity-based estimate can be used to smooth the maximum likelihood estimate for small nonzero\\nfrequencies.  If the similarity-based estimate is relatively high, a\\nbigram would receive a higher estimate than predicted by the uniform\\ndiscounting method.\\n\\n\\nFinally, the similarity-based model may be applied to configurations\\nother than bigrams.  For trigrams, it is necessary to measure\\nsimilarity between different conditioning bigrams. This can be done\\ndirectly, by measuring the distance between distributions of the form\\n\\nP(w3|w1,w2), corresponding to different bigrams\\n(w1,w2). Alternatively, and more practically, it would be possible\\nto define a similarity measure between bigrams as a function of\\nsimilarities between corresponding words in them.  Other types of\\nconditional cooccurrence probabilities have been used in probabilistic\\n parsing .  If the configuration in question includes only two words, such as \\n\\nP(object|verb), then it is possible\\nto use the model we have used for bigrams. If the configuration\\nincludes more elements, it is necessary to adjust the method, along\\nthe lines discussed above for trigrams.\\n\\n\\n    Conclusions\\n\\n\\nSimilarity-based models suggest an appealing approach for dealing with\\ndata sparseness. Based on corpus statistics, they provide analogies\\nbetween words that often agree with our linguistic and domain\\nintuitions.  In this paper we presented a new model that implements\\nthe similarity-based approach to provide estimates for the conditional\\nprobabilities of unseen word cooccurrences.\\n\\n\\nOur method combines similarity-based estimates with Katz's back-off\\nscheme, which is widely used for language modeling in speech\\nrecognition. Although the scheme was originally proposed as a\\npreferred way of implementing the independence assumption, we suggest\\nthat it is also appropriate for implementing similarity-based models,\\nas well as class-based models.  It enables us to rely on direct\\nmaximum likelihood estimates when reliable statistics are available,\\nand only otherwise resort to the estimates of an ``indirect'' model.\\n\\n\\nThe improvement we achieved for a bigram model is statistically\\nsignificant, though modest in its overall effect because of the small\\nproportion of unseen events.  While we have used bigrams as an\\neasily-accessible platform to develop and test the model, more\\nsubstantial improvements might be obtainable for more informative\\nconfigurations.  An obvious case is that of trigrams, for which the\\nsparse data problem is much more severe.\\n  Our longer-term goal, however, is to apply similarity techniques to linguistically motivated word cooccurrence\\nconfigurations, as suggested by lexicalized approaches to parsing\\n ,. In configurations like verb-object and adjective-noun, there is some evidence\\n  that sharper word cooccurrence distributions are obtainable, leading to improved\\npredictions by similarity techniques.\\n\\n\\n  Acknowledgments \\n\\nWe thank Slava Katz for discussions on the topic of this paper, Doug\\nMcIlroy for detailed comments, Doug Paul for help with his baseline\\nback-off model, and Andre Ljolje and Michael Riley for providing the\\nword lattices for our experiments.\\n\\nBibliography \\n\\nBlack, Ezra, Fred Jelinek, John Lafferty, David M. Magerman, David Mercer, and\\n  Salim Roukos.\\n1993.\\nTowards history-based grammars: Using richer models for probabilistic\\n  parsing.\\nIn 30th Annual Meeting of the Association for Computational\\n  Linguistics, pages 31-37, Columbus, Ohio. Ohio State University,\\n  Association for Computational Linguistics, Morristown, New Jersey.\\n\\n\\nBrown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and\\n  Robert L. Mercer.\\n1992.\\nClass-based n-gram models of natural language.\\nComputational Linguistics, 18(4):467-479.\\n\\n\\nChurch, Kenneth W. and William A. Gale.\\n1991.\\nA comparison of the enhanced Good-Turing and deleted estimation\\n  methods for estimating probabilities of English bigrams.\\nComputer Speech and Language, 5:19-54.\\n\\n\\nDagan, Ido, Shaul Markus, and Shaul Markovitch.\\n1993.\\nContextual word similarity and estimation from sparse data.\\nIn 30th Annual Meeting of the Association for Computational\\n  Linguistics, pages 164-171, Columbus, Ohio. Ohio State University,\\n  Association for Computational Linguistics, Morristown, New Jersey.\\n\\n\\nEssen, Ute and Volker Steinbiss.\\n1992.\\nCoocurrence smoothing for stochastic language modeling.\\nIn Proceedings of ICASSP, volume I, pages 161-164. IEEE.\\n\\n\\nGood, I. J.\\n1953.\\nThe population frequencies of species and the estimation of\\n  population parameters.\\nBiometrika, 40(3):237-264.\\n\\n\\nGrishman, Ralph and John Sterling.\\n1993.\\nSmoothing of automatically generated selectional constraints.\\nIn Human Language Technology, pages 254-259, San Francisco,\\n  California. Advanced Research Projects Agency, Software and Intelligent\\n  Systems Technology Office, Morgan Kaufmann.\\n\\n\\nJelinek, Frederick, Robert L. Mercer, and Salim Roukos.\\n1992.\\nPrinciples of lexical language modeling for speech recognition.\\nIn Sadaoki Furui and M. Mohan Sondhi, editors, Advances in\\n  Speech Signal Processing. Mercer Dekker, Inc., pages 651-699.\\n\\n\\nKatz, Slava M.\\n1987.\\nEstimation of probabilities from sparse data for the language model\\n  component of a speech recognizer.\\nIEEE Transactions on Acoustics, Speeech and Signal\\n  Processing, 35(3):400-401.\\n\\n\\nLafferty, John, Daniel Sleator, and Davey Temperley.\\n1992.\\nGrammatical trigrams: aa probabilistic model of link grammar.\\nIn Robert Goldman, editor, AAAI Fall Symposium on\\n  Probabilistic Approaches to Natural Language Processing, Cambridge,\\n  Massachusetts. American Association for Artificial Intelligence.\\n\\n\\nPaul, Douglas B.\\n1991.\\nExperience with a stack decoder-based HMM CSR and back-off n-gram\\n  language models.\\nIn Proceedings of the Speech and Natural Language Workshop,\\n  pages 284-288, Palo Alto, California, February. Defense Advanced Research\\n  Projects Agency, Information Science and Technology Office, Morgan Kaufmann.\\n\\n\\nPereira, Fernando C. N., Naftali Z. Tishby, and Lillian Lee.\\n1993.\\nDistributional clustering of English words.\\nIn 30th Annual Meeting of the Association for Computational\\n  Linguistics, pages 183-190, Columbus, Ohio. Ohio State University,\\n  Association for Computational Linguistics, Morristown, New Jersey.\\n\\n\\nRosenfeld, Ronald and Xuedong Huang.\\n1992.\\nImprovements in stochastic language modeling.\\nIn DARPA Speech and Natural Language Workshop, pages\\n  107-111, Harriman, New York, February. Morgan Kaufmann, San Mateo,\\n  California.\\n\\n\\nSchabes, Yves.\\n1992.\\nStochastic lexicalized tree-adjoining grammars.\\nIn Proceeedings of the 14th International Conference on\\n  Computational Linguistics, Nantes, France.\\n\\n\\nSugawara, K., M. Nishimura, K. Toshioka, M. Okochi, and T. Kaneko.\\n1985.\\nIsolated word recognition using hidden Markov models.\\nIn Proceedings of ICASSP, pages 1-4, Tampa, Florida. IEEE.\\n\\nFootnotes\\n\\n  To appear in the proceedings of the 32nd Annual Meeting of the\\nAssociation for Computational Linguistics, New Mexico State University,\\nJune 1994.\\n  The\\nperplexity of a conditional bigram probability model \\nwith\\nrespect to the true bigram distribution is an\\n information-theoretic measure of model quality  that can be empirically estimated by \\n\\n\\nfor a test set of length N.\\nIntuitively, the lower the perplexity of a model the more\\nlikely the model is to assign high probability to bigrams that\\nactually occur. In our task, lower perplexity will indicate better\\nprediction of unseen bigrams.\\n  The ARPA WSJ development corpora come in two\\nversions, one with verbalized punctuation and the other without. We\\nused the latter in all our experiments.\\n  Values of\\n\\nand t refer to base 10 logarithms and exponentials in all\\ncalculations.\\n  This presentation corresponds to model 2-B in\\n. Their presentation follows the equivalent model\\n1-A, which averages over similar\\nconditioned words, with the similarity defined with the preceding word\\nas context. In fact, these equivalent models\\nare symmetric in their treatment of conditioning and conditioned word,\\nas they can both be rewritten as\\n\\n\\n\\n\\n\\nThey also consider other definitions of confusion\\nprobability and smoothed probability estimate, but the one above yielded\\nthe best experimental results.\\n  For WSJ trigrams, only 58.6%\\nof test set trigrams occur in 40M of words of training (Doug Paul,\\npersonal communication).\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nIn many applications of natural language processing it is\\nnecessary to determine the likelihood of a given word combination. For\\nexample, a speech recognizer may need to determine which of the two\\nword combinations ``eat a peach'' and ``eat a beach'' is more likely.\\nStatistical NLP methods determine the likelihood of a word combination\\naccording to its frequency in a training corpus. However, the\\nnature of language is such that many word combinations are infrequent\\nand do not occur in a given corpus. In\\nthis work we propose a method for estimating the probability of such\\npreviously unseen word combinations using available information on\\n``most similar'' words.\\nWe describe a probabilistic word association model based on\\ndistributional word similarity, and apply it to improving probability\\nestimates for unseen word bigrams in a variant of Katz's back-off\\nmodel. The similarity-based method yields a 20% perplexity improvement\\nin the prediction of unseen bigrams and statistically\\nsignificant reductions in speech-recognition error.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nIn applications such as speech recognition, handwriting recognition,\\nand spelling correction,\\nperformance is limited by the quality of the language model utilized\\n ,,,.  However, static language modeling performance has remained basically unchanged\\nsince the advent of n-gram language models forty years ago\\n .  Yet, n-gram language models can only capture dependencies within\\nan n-word window, where currently the largest practical n for\\nnatural language is three, and many dependencies in natural\\nlanguage occur beyond a three-word window.  In addition, n-gram\\nmodels are extremely large, thus making them difficult\\nto implement efficiently in memory-constrained applications.\\n\\n\\nAn appealing alternative is grammar-based language models.  Language\\nmodels expressed as a probabilistic grammar tend to be more compact than\\nn-gram language models, and have the ability to model long-distance\\n dependencies ,,.  However, to date there has been little success in constructing grammar-based language\\nmodels competitive with n-gram models in problems of any\\nmagnitude.\\n\\n\\nIn this paper, we describe a corpus-based induction algorithm for\\nprobabilistic context-free grammars that outperforms n-gram\\n models and the Inside-Outside algorithm  in medium-sized domains.  This result marks the first\\ntime a grammar-based language model has surpassed n-gram modeling\\nin a task of at least moderate size.  The algorithm employs a greedy\\nheuristic search within a Bayesian framework, and a post-pass\\nusing the Inside-Outside algorithm.\\n\\n\\n  Grammar Induction as Search \\n\\nGrammar induction can be framed as a search problem, and has been framed as\\n such almost without exception in past research . The search space is taken to be some class of grammars; for example,\\nin our work we search within the space of probabilistic context-free\\ngrammars.  The objective function is taken to be\\nsome measure dependent on the training data; one generally wants\\nto find a grammar that in some sense accurately models the training data.\\n\\n\\nMost work in language modeling, including n-gram models and the\\nInside-Outside algorithm, falls under the maximum-likelihood paradigm,\\nwhere one takes the objective function to be the likelihood\\nof the training data given the grammar.  However, the optimal grammar under\\nthis objective function is one which generates\\nonly strings in the training data and no other strings.  Such grammars\\nare poor language models, as they overfit the training data and\\ndo not model the language at large.  In n-gram models and the\\nInside-Outside algorithm, this issue is evaded by bounding the\\nsize and form of the grammars considered, so that the\\n``optimal'' grammar cannot be expressed.  However, in our\\nwork we do not wish to limit the size of the grammars considered.\\n\\n\\nThe basic shortcoming of the maximum-likelihood objective function is that\\nit does not encompass the compelling intuition\\nbehind Occam's Razor, that simpler (or smaller) grammars are preferable over\\ncomplex (or larger) grammars.  A factor in the objective function\\nthat favors smaller grammars over large can prevent the objective function from\\npreferring grammars that overfit the training data.\\n presents a Bayesian grammar induction framework\\nthat includes such a factor in a motivated manner.\\n\\n\\nThe goal of grammar induction is taken to be finding the grammar with\\nthe largest a posteriori probability given the training data,\\nthat is, finding the grammar G' where\\n\\n\\n\\n\\n\\nand where we denote the training data as O, for observations.\\nAs it is unclear how to estimate p(G|O) directly, we apply\\nBayes' Rule and get\\n\\n\\n\\n\\n\\nHence, we can frame the search for G' as a search with the objective function\\n\\np(O|G) p(G), the likelihood of the training data multiplied by the\\nprior probability of the grammar.\\n\\n\\nWe satisfy the goal of favoring smaller grammars by choosing a prior\\nthat assigns higher probabilities to such grammars.  In particular,\\nSolomonoff proposes the use of the universal a priori probability\\n , which is closely related to the  minimum description length principle later proposed by . In the case of grammatical language modeling, this\\ncorresponds to taking\\n\\n\\np(G) = 2[-l(G)]\\n\\n\\nwhere l(G) is the length of the description of the grammar\\nin bits.  The universal a priori probability has many elegant\\nproperties, the most salient of which is that it dominates all other\\n enumerable probability distributions multiplicatively. \\n\\n\\n  Search Algorithm \\n\\nAs described above, we take grammar induction to be the search\\nfor the grammar G' that optimizes the objective\\nfunction \\n\\np(O|G)p(G).  While this framework does not restrict us\\nto a particular grammar formalism, in our work we consider only\\nprobabilistic context-free grammars.\\n\\n\\nWe assume a simple greedy search strategy.  We maintain a single\\nhypothesis grammar which is initialized to a small, trivial grammar.\\nWe then try to find a modification to the hypothesis grammar, such\\nas the addition of a grammar rule, that results in a grammar\\nwith a higher score on the objective function.  When we find a superior\\ngrammar, we make this the new hypothesis grammar.  We repeat\\nthis process until we can no longer find a modification that improves\\nthe current hypothesis grammar.\\n\\n\\nFor our initial grammar, we choose a grammar that can generate\\nany string, to assure that the grammar can cover the training data.\\n The initial grammar is listed in Table . The sentential symbol S expands to a sequence of X's, where Xexpands to every other nonterminal symbol in the grammar.\\nInitially, the set of nonterminal symbols consists of\\na different nonterminal symbol expanding to each terminal symbol.\\n\\n\\nNotice that this grammar models a sentence as a sequence of independently\\ngenerated nonterminal symbols.  We maintain this property throughout\\nthe search process, that is, for every symbol A' that we add to the grammar,\\nwe also add a rule .\\nThis assures that\\nthe sentential symbol can expand to every symbol; otherwise, adding\\na symbol will not affect the probabilities that the grammar assigns\\nto strings.\\n\\n\\nWe use the term move set to describe the set of modifications\\nwe consider to the current hypothesis grammar to hopefully produce\\na superior grammar.  Our move set includes the following moves:\\nMove 1:\\nCreate a rule of the form Move 2:\\nCreate a rule of the form For any context-free grammar, it is possible to express a weakly equivalent\\ngrammar using only rules of these forms.\\nAs mentioned before, with each new symbol A we also create a rule .\\n\\n  Evaluating the Objective Function \\n\\nConsider the task of calculating the objective function\\n\\np(O|G)p(G) for some grammar G.  Calculating \\n\\np(G) = 2[-l(G)] is\\n inexpensive; however, calculating p(O|G) requires a parsing of the\\nentire training data.  We cannot afford to parse the training\\ndata for each grammar considered; indeed, to ever be practical\\nfor data sets of millions of words,\\nit seems likely that we can only afford to parse the data once.\\n\\n\\nTo achieve this goal, we employ several approximations.\\nFirst, notice that\\nwe do not ever need to calculate the actual value of the objective function;\\nwe need only to be able to distinguish when a move applied to the current\\nhypothesis grammar produces a grammar that has a higher score on the\\nobjective function, that is, we need only to be able to calculate\\nthe difference in the objective function resulting from a move.\\nThis can be done efficiently if we can quickly approximate how the\\nprobability of the training data changes when a move is applied.\\n\\n\\nTo make this possible, we approximate the probability of the\\ntraining data p(O|G) by the probability of the single most probable\\nparse, or Viterbi parse, of the training data.  Furthermore,\\ninstead of recalculating the Viterbi parse of the training data from\\nscratch when a move is applied, we use heuristics to predict how a move\\nwill change the Viterbi parse.\\nFor example, consider the case where the training\\ndata consists of the two sentences\\n\\n\\n\\n\\n\\n In Figure , we display the Viterbi parse of this data under the initial hypothesis grammar used in our algorithm.\\n\\n\\nNow, let us consider the move of adding the rule\\n\\n\\n\\n\\n\\nto the initial grammar\\n(as well as the concomitant rule ).  A reasonable heuristic\\nfor predicting how the Viterbi parse will change is to replace\\nadjacent X's that expand to \\n\\n\\nand\\n\\nrespectively with a single X that expands to B, as displayed\\n in Figure .  This is the actual heuristic we use for moves of the form ,\\nand we have analogous heuristics for each move\\nin our move set.  By predicting the differences in the Viterbi parse resulting\\nfrom a move, we can quickly estimate the change in the probability of\\nthe training data.\\n\\n\\nNotice that our predicted Viterbi parse can stray a great deal from\\nthe actual Viterbi parse, as errors can accumulate as move after move\\nis applied.  To minimize these effects,\\nwe process the training data incrementally.\\nUsing our initial hypothesis grammar, we parse the first sentence\\nof the training data and search for the optimal\\ngrammar over just that one sentence using the described\\nsearch framework.  We use the resulting grammar to parse the\\nsecond sentence, and then search for the optimal grammar over the\\nfirst two sentences using the last grammar as the starting point.\\nWe repeat this process, parsing the next\\nsentence using the best grammar found on the previous sentences\\nand then searching for the best grammar taking into account this\\nnew sentence, until the entire training corpus is covered.\\n\\n\\nDelaying the parsing of a sentence\\nuntil all of the previous sentences are processed should yield\\nmore accurate Viterbi parses during the search process than if\\nwe simply parse the whole corpus with the initial hypothesis grammar.\\nIn addition, we still achieve the goal of parsing each sentence\\nbut once.\\n\\n\\n  Parameter Training \\n\\nIn this section, we describe how the parameters of our grammar,\\nthe probabilities associated with each grammar rule, are set.\\nIdeally, in evaluating the objective function for a particular grammar\\nwe should use its optimal parameter settings given the training data,\\nas this is the full score that the given grammar can achieve.\\nHowever, searching for optimal parameter values is extremely expensive\\ncomputationally.  Instead, we grossly approximate the optimal values by\\ndeterministically setting parameters based on\\nthe Viterbi parse of the training data parsed so far.\\nWe rely on the post-pass, described later, to refine parameter values.\\n\\n\\n Referring to the rules in Table , the parameter  is set to\\nan arbitrary small constant.  The values of the parameters p(A) are\\nset to the (smoothed) frequency of the \\nreduction in the\\nViterbi parse of the data seen so far.  The remaining symbols\\nare set to expand uniformly among their possible expansions.\\n\\n\\n  Constraining Moves \\n\\nConsider the move of creating a rule of the form .\\nThis corresponds to k[3] different specific rules that might be created,\\nwhere k is the current number of symbols in the grammar.  As it is too\\ncomputationally expensive to consider each of these rules at every point in\\nthe search, we use heuristics to constrain which moves are appraised.\\n\\n\\nFor the left-hand side of a rule, we always create a new symbol.\\nThis heuristic selects the optimal choice the vast majority of\\nthe time; however, under this constraint the\\nmoves described earlier in this section cannot yield arbitrary context-free\\nlanguages.  To partially address this, we add the move\\nMove 3:\\nCreate a rule of the form \\n\\nWith this iteration move, we can construct grammars that generate\\narbitrary regular languages.  As yet, we have not implemented\\nmoves that enable the construction of arbitrary context-free grammars;\\nthis belongs to future work.\\n\\n\\nTo constrain the symbols we consider on the right-hand side of a new rule,\\n we use what we call triggers. A trigger is a phenomenon in the Viterbi parse of a sentence that is\\nindicative that a particular move might lead to a better grammar.\\n For example, in Figure  the fact that the symbols \\n\\n\\nand\\n\\n\\noccur\\nadjacently is indicative that it could be profitable to create\\na rule \\n\\n.\\nWe have developed a set of triggers for each move in our move set,\\nand only consider a specific move if it is triggered in the\\nsentence currently being parsed in the incremental processing.\\n\\n\\n  Post-Pass \\n\\nA conspicuous shortcoming in our search framework is\\nthat the grammars in our search space are fairly unexpressive.\\nFirstly, recall that our grammars model a sentence as a sequence of\\nindependently generated symbols; however, in language there is a large\\ndependence between adjacent constituents.  Furthermore, the only free\\nparameters\\nin our search are the parameters p(A); all other symbols (except S) are\\nfixed to expand uniformly.  These choices were necessary to make the\\nsearch tractable.\\n\\n\\nTo address this issue, we use an Inside-Outside algorithm post-pass.\\nOur methodology is derived from that\\ndescribed by .  We create n new nonterminal symbols\\n\\n,\\nand create all rules of the form:\\n\\n\\n\\n\\n\\n\\n\\ndenotes the set of nonterminal symbols\\nacquired in the initial grammar induction phase, and X1 is taken to be the\\nnew sentential symbol.  These new rules replace the first three rules\\n listed in Table . The parameters of these rules are initialized\\nrandomly.  Using this grammar as the starting point, we\\nrun the Inside-Outside algorithm on the training data until convergence.\\n\\n\\nIn other words, instead of using the naive \\n\\n\\nrule to attach\\nsymbols together in parsing data, we now use\\nthe Xi rules and depend on the Inside-Outside algorithm to\\ntrain these randomly initialized rules intelligently.\\nThis post-pass allows us to express dependencies between adjacent\\nsymbols.  In addition, it allows us to train parameters that were\\nfixed during the initial grammar induction phase.\\n\\n\\n\\n  Previous Work \\n\\nAs mentioned, this work employs the Bayesian grammar induction framework\\ndescribed by Solomonoff .\\nHowever, Solomonoff does not specify a concrete search\\nalgorithm and only makes suggestions as to its nature.\\n\\n\\nSimilar research includes work by Cook et al. (1976) and\\nStolcke and Omohundro (1994).\\nThis work also employs a heuristic\\nsearch within a Bayesian framework.  However, a different prior\\nprobability on grammars is used, and the algorithms are only\\nefficient enough to be applied to small data sets.\\n\\n\\nThe grammar induction algorithms most successful in language modeling\\n include the Inside-Outside algorithm ,,,  a special case of the Expectation-Maximization algorithm , and work by .  In the latter work, McCandless uses a\\nheuristic search procedure similar to ours, but a very different\\nsearch criteria.  To our knowledge, neither algorithm has surpassed\\nthe performance of n-gram models in a language modeling task\\nof substantial scale.\\n\\n\\n  Results \\n\\nTo evaluate our algorithm, we compare the performance of our algorithm\\nto that of n-gram models and the Inside-Outside algorithm.\\n\\n\\nFor n-gram models, we tried \\n\\n\\nfor each domain.\\nFor smoothing a particular n-gram model, we took a linear combination\\nof all lower order n-gram models.  In particular, we follow standard\\n practice ,, and take the smoothed i-gram probability to be a linear combination\\nof the i-gram frequency in the training data and the smoothed (i-1)-gram\\nprobability, that is,\\n\\n\\n\\n\\n\\nwhere c(W) denotes the count of the word sequence W in the training data.\\nThe smoothing parameters \\nare trained through\\n the Forward-Backward algorithm  on held-out data. Parameters \\nare tied together for similar c to prevent\\ndata sparsity.\\n\\n\\nFor the Inside-Outside algorithm, we follow the methodology described\\nby Lari and Young.  For a given n, we create a probabilistic\\ncontext-free grammar consisting\\nof all Chomsky normal form rules over the n nonterminal\\nsymbols \\n\\n\\nand the given terminal symbols, that is, all\\nrules\\n\\n\\n\\n\\n\\nwhere T denotes the set of terminal symbols in the domain.  All parameters\\nare initialized randomly.  From this starting point, the Inside-Outside\\nalgorithm is run until convergence.\\n\\n\\nFor smoothing, we combine the expansion distribution of each\\nsymbol with a uniform distribution, that is, we take the smoothed parameter\\n\\n\\nto be\\n\\n\\n\\n\\n\\nwhere \\n\\n\\ndenotes the unsmoothed parameter.  The value\\n\\nn[3] + n|T| is the number of different ways a symbol expands under\\nthe Lari and Young methodology.  The parameter \\nis trained through the\\nInside-Outside algorithm on held-out data.  This smoothing is\\nalso performed on the Inside-Outside post-pass of our algorithm.\\nFor each domain, we tried \\n\\n.\\n\\n\\nBecause of the computational demands of our algorithm, it is\\ncurrently impractical to apply it to large vocabulary or\\nlarge training set problems.  However, we present the results of\\nour algorithm in three medium-sized domains.  In each case,\\nwe use 4500 sentences for training, with 500 of these sentences held out\\nfor smoothing.  We test on 500 sentences, and measure performance\\nby the entropy of the test data.\\n\\n\\nIn the first two domains, we created the training and test data\\nartificially so as to have an ideal grammar in hand to benchmark results.\\nIn particular, we used a probabilistic grammar to generate\\nthe data.  In the first domain, we created this grammar by hand;\\nthe grammar was a small English-like probabilistic\\ncontext-free grammar consisting\\nof roughly 10 nonterminal symbols, 20 terminal symbols, and 30 rules.\\nIn the second domain, we derived the grammar from manually\\nparsed text.  From a million words of parsed Wall Street Journal\\ndata from the Penn treebank, we extracted the 20 most\\nfrequently occurring symbols, and the 10 most frequently occurring rules\\nexpanding each of these symbols.  For each symbol that occurs\\non the right-hand side of a rule but which was not one of\\nthe most frequent 20 symbols, we create a rule that expands\\nthat symbol to a unique terminal symbol.  After removing unreachable rules,\\nthis yields a grammar of roughly 30 nonterminals, 120 terminals, and 160 rules.\\nParameters are set to reflect the frequency of the\\ncorresponding rule in the parsed corpus.\\n\\n\\nFor the third domain, we took English text and reduced the size of\\nthe vocabulary by mapping each word to its part-of-speech tag.  We used\\ntagged Wall Street Journal text from the Penn treebank, which has\\na tag set size of about fifty.\\n\\n\\n In Tables , we summarize our results. The ideal grammar denotes the grammar used to generate\\nthe training and test data.  For each algorithm, we list\\nthe best performance achieved over all n tried,\\nand the best n column states which value realized\\nthis performance.\\n\\n\\nWe achieve a moderate but significant improvement in performance over\\nn-gram models and the Inside-Outside algorithm in the first two domains,\\nwhile in the part-of-speech domain we are outperformed by n-gram models\\nbut we vastly outperform the Inside-Outside algorithm.\\n\\n\\n In Table , we display a sample of the number of parameters and execution time (on a Decstation 5000/33) associated with each algorithm.\\nWe choose n to yield approximately equivalent performance for each\\nalgorithm.  The first pass row refers to the main grammar induction\\nphase of our algorithm, and the post-pass row refers to\\nthe Inside-Outside post-pass.\\n\\n\\nNotice that our algorithm produces a significantly more compact model\\nthan the n-gram model, while running significantly faster than the\\nInside-Outside algorithm even though we use an Inside-Outside post-pass.\\nPart of this discrepancy is due to the fact that we require a smaller\\nnumber of new nonterminal symbols to achieve equivalent performance, but\\nwe have also found that our post-pass converges more quickly even given\\nthe same number of nonterminal symbols.\\n\\n\\n  Discussion \\n\\nOur algorithm consistently outperformed the Inside-Outside algorithm\\nin these experiments.  While we partially attribute this difference to\\nusing a Bayesian instead of maximum-likelihood objective function, we\\nbelieve that part of this difference results from a more effective\\nsearch strategy.  In particular, though both algorithms employ\\na greedy hill-climbing strategy, our algorithm gains an advantage\\nby being able to add new rules to the grammar.\\n\\n\\nIn the Inside-Outside algorithm, the gradient descent search\\ndiscovers the ``nearest'' local minimum in the search landscape to the\\ninitial grammar.  If there are k rules in the grammar and thus kparameters, then the search takes place in a fixed k-dimensional\\nspace \\n\\n.\\nIn our algorithm, it is possible to expand\\nthe hypothesis grammar, thus increasing the\\ndimensionality of the parameter space that is being searched.\\nAn apparent local minimum in the space \\n\\n\\nmay no longer be a local\\nminimum in the space \\n\\n;\\nthe extra dimension may provide\\na pathway for further improvement of the hypothesis grammar.\\nHence, our algorithm should be less prone to\\nsuboptimal local minima than the Inside-Outside algorithm.\\n\\n\\nOutperforming n-gram models in the first two domains demonstrates\\nthat our algorithm is able to take advantage of the grammatical\\nstructure present in data.\\nHowever, the superiority of n-gram models in the part-of-speech\\ndomain indicates that to be competitive in modeling naturally-occurring\\ndata, it is necessary to model collocational information accurately.  We need\\nto modify our algorithm to more aggressively model n-gram information.\\n\\n\\n  Conclusion \\n\\nThis research represents a step forward in the quest for\\ndeveloping grammar-based language models for natural language.\\nWe induce models that, while being substantially more compact,\\noutperform n-gram language models in medium-sized domains.\\nThe algorithm runs essentially in time and space linear in\\nthe size of the training data, so larger domains are within our\\nreach.\\n\\n\\nHowever, we feel the largest contribution of this work does not lie in\\nthe actual algorithm specified, but rather in its indication of the\\npotential of the induction framework described by Solomonoff\\nin 1964.  We have implemented only a subset of the moves that\\nwe have developed, and inspection\\nof our results gives reason to believe that these additional moves may\\nsignificantly improve the performance of our algorithm.\\n\\n\\nSolomonoff's induction framework is not restricted to\\nprobabilistic context-free grammars.  After completing\\nthe implementation of our move set, we plan to explore\\nthe modeling of context-sensitive phenomena.  This work demonstrates\\nthat Solomonoff's elegant framework deserves much further consideration.\\n\\n\\n  Acknowledgements \\n\\nWe are indebted to Stuart Shieber for his suggestions and guidance,\\nas well as his invaluable comments on earlier drafts\\nof this paper.  This material is based on work supported by the\\nNational Science Foundation under Grant Number IRI-9350192 to\\nStuart M. Shieber.\\n\\nBibliography \\n\\nD. Angluin and C.H. Smith.\\n1983.\\nInductive inference: theory and methods.\\nACM Computing Surveys, 15:237-269.\\n\\n\\nL.R. Bahl, J.K. Baker, P.S. Cohen, F. Jelinek, B.L. Lewis, and R.L. Mercer.\\n1978.\\nRecognition of a continuously read natural corpus.\\nIn Proceedings of the IEEE International Conference on\\n  Acoustics, Speech and Signal Processing, pages 422-424, Tulsa, Oklahoma,\\n  April.\\n\\n\\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.\\n1983.\\nA maximum likelihood approach to continuous speech recognition.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\\n  PAMI-5(2):179-190, March.\\n\\n\\nJ.K. Baker.\\n1975.\\nThe DRAGON system - an overview.\\nIEEE Transactions on Acoustics, Speech and Signal Processing,\\n  23:24-29, February.\\n\\n\\nJ.K. Baker.\\n1979.\\nTrainable grammars for speech recognition.\\nIn Proceedings of the Spring Conference of the Acoustical\\n  Society of America, pages 547-550, Boston, MA, June.\\n\\n\\nL.E. Baum and J.A. Eagon.\\n1967.\\nAn inequality with application to statistical estimation for\\n  probabilistic functions of Markov processes and to a model for ecology.\\nBulletin of the American Mathematicians Society, 73:360-363.\\n\\n\\nPeter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and\\n  Robert L. Mercer.\\n1992.\\nClass-based n-gram models of natural language.\\nComputational Linguistics, 18(4):467-479, December.\\n\\n\\nA.P. Dempster, N.M. Laird, and D.B. Rubin.\\n1977.\\nMaximum likelihood from incomplete data via the EM algorithm.\\nJournal of the Royal Statistical Society, 39(B):1-38.\\n\\n\\nFrederick Jelinek and Robert L. Mercer.\\n1980.\\nInterpolated estimation of Markov source parameters from sparse\\n  data.\\nIn Proceedings of the Workshop on Pattern Recognition in\\n  Practice, Amsterdam, The Netherlands: North-Holland, May.\\n\\n\\nM.D. Kernighan, K.W. Church, and W.A. Gale.\\n1990.\\nA spelling correction program based on a noisy channel model.\\nIn Proceedings of the Thirteenth International Conference on\\n  Computational Linguistics, pages 205-210.\\n\\n\\nK. Lari and S.J. Young.\\n1990.\\nThe estimation of stochastic context-free grammars using the\\n  inside-outside algorithm.\\nComputer Speech and Language, 4:35-56.\\n\\n\\nK. Lari and S.J. Young.\\n1991.\\nApplications of stochastic context-free grammars using the\\n  inside-outside algorithm.\\nComputer Speech and Language, 5:237-257.\\n\\n\\nMing Li and Paul Vitnyi.\\n1993.\\nAn Introduction to Kolmogorov Complexity and its\\n  Applications.\\nSpringer-Verlag.\\n\\n\\nMichael K. McCandless and James R. Glass.\\n1993.\\nEmpirical acquisition of word and phrase classes in the ATIS\\n  domain.\\nIn Third European Conference on Speech Communication and\\n  Technology, Berlin, Germany, September.\\n\\n\\nFernando Pereira and Yves Schabes.\\n1992.\\nInside-outside reestimation from partially bracket corpora.\\nIn Proceedings of the 30th Annual Meeting of the ACL, pages\\n  128-135, Newark, Delaware.\\n\\n\\nP. Resnik.\\n1992.\\nProbabilistic tree-adjoining grammar as a framework for statistical\\n  natural language processing.\\nIn Proceedings of the 14th International Conference on\\n  Computational Linguistics.\\n\\n\\nJ. Rissanen.\\n1978.\\nModeling by the shortest data description.\\nAutomatica, 14:465-471.\\n\\n\\nY. Schabes.\\n1992.\\nStochastic lexicalized tree-adjoining grammars.\\nIn Proceedings of the 14th International Conference on\\n  Computational Linguistics.\\n\\n\\nC.E. Shannon.\\n1951.\\nPrediction and entropy of printed English.\\nBell Systems Technical Journal, 30:50-64, January.\\n\\n\\nR.J. Solomonoff.\\n1960.\\nA preliminary report on a general theory of inductive inference.\\nTechnical Report ZTB-138, Zator Company, Cambridge, MA, November.\\n\\n\\nR.J. Solomonoff.\\n1964.\\nA formal theory of inductive inference.\\nInformation and Control, 7:1-22, 224-254, March, June.\\n\\n\\nRohini Srihari and Charlotte Baltus.\\n1992.\\nCombining statistical and syntactic methods in recognizing\\n  handwritten sentences.\\nIn AAAI Symposium: Probabilistic Approaches to Natural\\n  Language, pages 121-127.\\n\\nFootnotes\\n\\nA very thorough discussion of the universal\\na priori probability is given by .\\n  Due to space limitations, we do not specify\\nour method for encoding grammars, , how we calculate l(G) for\\na given G.  However, this will be described in the author's\\nforthcoming Ph.D. dissertation.\\n  This is not to be confused with\\nthe use of the term triggers in dynamic language modeling.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe describe a corpus-based induction algorithm for\\nprobabilistic context-free grammars.  The algorithm employs a greedy\\nheuristic search within a Bayesian framework, and a post-pass\\nusing the Inside-Outside algorithm.  We compare the performance\\nof our algorithm to n-gram models and the Inside-Outside algorithm\\nin three language modeling tasks.  In two of the tasks, the training\\ndata is generated by a probabilistic context-free grammar and in both tasks\\nour algorithm outperforms the other techniques.  The third task involves\\nnaturally-occurring data, and in this task our algorithm does not perform\\nas well as n-gram models but vastly outperforms the Inside-Outside algorithm.\\n\\n'],\n",
              " [\"\\n\\n  The Combinatorial Explosion Puzzle \\n\\nThe alternative syntactic readings of a sentence such as synamb probably\\nnumber in the hundreds, whereas sentences such as hobbs would have\\nhundreds of thousands scopally distinct readings if all permutations of\\nscope-taking sentence constituents were considered admissible readings. Yet,\\nhuman beings appear able to deal with these sentences effortlessly.\\n\\n\\n\\nThis Combinatorial Explosion Puzzle is one of the most fundamental\\nquestions to be addressed by a theory of language processing, and a substantial\\nproblem for developers of Natural Language Processing () systems.  \\nsystems which have to perform non-linguistic actions like booking a flight in\\nresponse to an user's utterance must arrive at the preferred interpretation of\\ntheir input in the context of the conversation, if one exists; otherwise, they\\nmust realize that their input is ambiguous and request a clarification.\\nExamples such as synamb and hobbs indicate that such systems\\ncannot adopt the sentence processing strategy of generating all the readings of\\nan ambiguous sentence and choosing one of them, because there are too many such\\n readings. In order to develop such systems, a theory of ambiguity processing is needed that is consistent both\\nwith linguistic facts and with what is known about the way humans disambiguate.\\n\\n\\nWork on underspecified representations such as\\n ,,,, because it is explicitly motivated by the Combinatorial Explosion Puzzle, and aims at a unified account of all interpretation\\nprocesses, including those that occur before the scope of all operators has\\nbeen determined. The work on underspecified representations holds the promise\\nof yielding a better account of the way interpretive processes such as scope\\ndisambiguation and reference resolution affect each other.\\n\\n\\nThe existing theories of underspecification, however, have been motivated\\nalmost exclusively by computational considerations. For example, the semantics\\nassigned to underspecified representations is designed so as to support those\\ninferences that are deemed useful for an economical approach to disambiguation,\\nrather than being motivated by an analysis of the phenomenon of ambiguity.  In\\nthis paper I explore some of the issues that arise when trying to establish a\\nconnection between work on underspecification and, on the one side, work on\\nambiguity in semantics; on the other side, work on ambiguity in the\\npsychological literature. A theory of underspecification is developed `from the\\nfirst principles', i.e., starting from a definition of what it means for a\\nsentence to be semantically ambiguous and from what we know about the way\\nhumans deal with ambiguity. The goal is to arrive at a linguistically and\\ncognitively plausible theory of ambiguity and underspecification that, in\\naddition to computational gains, may provide a better understanding of how\\n humans process language. \\n\\n\\nMany of the issues discussed in this paper arose from work on the \\nproject at the University of Rochester, in which the issues of language\\ncomprehension, planning, and reasoning encountered in task-oriented natural\\n language conversations are studied . The theory of ambiguity proposed in this paper is the basis for the implemented surface\\ndiscourse interpretation system -93, used in the -93 demo\\n system. -93 is described in . \\n\\n\\n  Ambiguity in Natural Language \\n  Ambiguity and Grammar \\n  Characterizing Ambiguity \\n\\nThe dictionary definitions of the terms ambiguity and\\nambiguous try to capture the intuition that an expression is\\nambiguous if `it has multiple meanings'. An example are the following entries,\\nfrom Webster's:\\n\\nA more precise\\ncharacterization of the notion of ambiguity is required to differentiate\\nambiguity from vagueness or indeterminacy, for example, or\\nto clarify notions such as homonimy and polysemy (see\\n below). \\n\\n\\nAn early attempt at making the notion of ambiguity more precise was presented\\n in . Lakoff proposed linguistic tests that could be used to tell whether a sentence was ambiguous or\\n not. (Lakoff's tests were meant to provide a way for distinguishing ambiguous\\nsentences from indeterminate ones.)  Zwicky and Sadock\\n showed, however, that such tests do not result in\\nan unambiguous classification of sentences, and that a formal characterization\\nof the concepts of ambiguity and indeterminacy was required even to understand\\n what these `ambiguity tests' really test. \\n\\n\\n  Meaning, Sense, and Ambiguity \\n\\nOne problem to be tackled in attempting to make precise the definition of\\nambiguity is to say what `meanings' and `senses' are.  In modern semantic\\ntheory, the meaning assigned to an expression by a grammar is a function from\\ncontexts (or discourse situations) to senses.  Roughly\\nspeaking, the discourse situation provides a value for all context-dependent\\naspects of the sentence; the sense of a sentence (what we get once we resolve\\nits context-dependent aspects) tells us under which circumstances in the world\\n the sentence is true or false. \\n\\n\\nNot all notions of `sense' employed in the literature can serve as the basis\\nfor a definition of ambiguity. For example, of the various notions of\\nproposition (the sense of sentences), the simplest is the one\\naccording to which propositions are truth values. But if we were to use this\\nnotion of sense, the sentence Kermit croaked, ambiguous between a\\nreading in which Kermit utters a frog-like sound and a reading in which he\\ndies, would be classified as unambiguous with respect to all models in which\\nKermit has both the property of dying and the property of producing a frog-like\\nsound, or he (it) has neither property. In other words, in providing a\\ndefinition of ambiguity we find the same need for a fine-grained notion of\\nsense that has been observed in connection with the semantics of attitude\\n reports. A model-theoretic definition of ambiguity requires a finer-grained notion of\\nproposition than simply truth values. In most recent semantic theories, senses\\nare intensional objects; the simplest way of achieving intensionality is to use\\nfunctions from possible worlds or situations to referents as one finds in\\nMontague Grammar, where, for example, propositions are functions from possible\\nworlds to truth values. This simple form of intensionality will be sufficient\\n for the purposes of the present paper. \\n\\n\\n  A Semantic Theory of Ambiguity \\n\\nThe notions of `meaning' and `sense' just discussed are the starting point for\\nthe semantic account of the notion of ambiguity and its relation with vagueness\\n developed by Pinkal  introduces the notion of indefiniteness to subsume both\\nambiguity and vagueness. He defines indefiniteness as follows:\\n\\n\\n\\nPinkal formalizes the notion of indefiniteness in terms of\\n precisification.  According to Pinkal, a linguistic expression is semantically indefinite if it has the\\npotential for being made precise in distinct ways. For example, the\\nsentence The Santa Maria is a fast ship containing the degree\\nadjective fast can be `made precise' (and assigned a definite truth\\nvalue) either with respect to a context in which `fast' is interpreted as `fast\\nfor a modern ship', in which case the sentence is false; or with respect to a\\ncontext in which `fast' is interpreted as `fast for a ship of her age', in\\nwhich case the sentence can be true or false, depending on the class of\\ncomparison.  Let p and q be two propositions. Proposition p is\\nmore precise than q iff (i) p is true (false) under all states of\\nthe world under which q is true (false), and (ii) p is true or false under\\ncertain circumstances under which q is indefinite. The idea of\\nprecisification is defined as follows:\\n\\n\\n\\nThe connection between indefiniteness and precisification is provided by the\\nfollowing Precisification Principle:\\n\\n\\nPrecisification Principle\\n:\\nA sentence is of indefinite truth value in a context if and only if\\n  it can be precisified alternatively to ``true'' or to ``false''.\\nwhich Pinkal also reformulates as follows:\\nExtended Precisification Principle\\nAn expression is semantically\\nindefinite in a context iff it can assume different senses in that context.\\nPinkal does not equate ambiguity with vagueness.  His theory includes,\\nin addition to the notion of precisification, additional criteria to\\ndifferentiate different forms of ambiguity, as well as differentiating `pure'\\nambiguity from `pure' vagueness. The intuition he is trying to capture is that\\n``...whether an expression is ambiguous or only vague is a question that\\ncannot be cleared once and for all. Indefiniteness is perceived as ambiguity\\nwhen alternative precisifications are predominant, as vagueness when an\\nunstructured continuum presents itself:''\\n\\n\\nAmbiguity (Pinkal)\\n:\\nIf the precisification spectrum of an expression is perceived as discrete,\\n  we may call it ambiguous; if it is perceived as continuous, we may call\\n  it vague.\\nPinkal identifies two fundamental\\ntypes of ambiguity, according to whether an expression has, or does not have, a\\n`wider' sense that could be taken as most `basic'. For example, ball\\ndoes not have a wide sense of `round object and dancing party', whereas\\nAmerican may either mean `person from the US' or `person from the\\nAmerican continent'. He classifies expressions like American\\nwhich have a wider sense as having a multiplicity of use, whereas\\nexpressions such as ball or green which do require\\nprecisification are called narrowly ambiguous. The cases of ambiguity\\nin the narrow sense are further distinguished in two classes, depending on\\nwhether they are subject to the Precisification Imperative. Although\\nthe two interpretations of green are distinct, it is possible of an\\nobject to be both green in the `ripe' sense and green in the `color' sense: for\\nexample, a green apricot. An object cannot, however, be a `band' both in the\\nmusical group sense and in the piece of tape sense. Pinkal proposes that\\npolysemous expressions behave like green, and calls all of these\\nexpressions P-type ambiguous; expressions like band,\\nhowever, are true homonyms, and therefore he calls them H-type\\nambiguous. These latter are defined as follows:\\n\\n\\nPrecisification Imperative\\n: An expression is H-type ambiguous\\niff its base level is inadmissible, i.e., if it requires precisification.\\nFor my purposes,  it's\\nnot particularly important whether the difference between homonimy and polysemy\\nis completely captured by the Precisification Imperative; what is important is\\nthe claim that H-type ambiguous expressions need precisification, and\\nfurthermore, that the Precisification Imperative ``is a second order phenomenon\\n...that lies beyond the scope of a strictly truth-conditional approach.''\\n (, p. 86-87). I will provide below independent reasons for including a formalization of reasoning in context in a treatment of ambiguity,\\nand I will argue that such formalization provides the necessary tools to\\nexpress the Precisification Imperative.\\n\\n\\nTo summarize, a sentence is H-type ambiguous iff the grammar assigns to it\\ndistinct precisifications (senses) in a given discourse situation, and if the\\n`base level' of the expression requires precisification.  Thus, the sentence\\nKermit croaked is considered ambiguous since in the `empty context'\\nthat provides all the senses of the expression according to the grammar G for\\nEnglish, that sentence has two senses: the proposition that attributes to\\nKermit the property of producing the sound that frogs produce, and the\\nproposition that attributes to Kermit the property of dying.  (I am assuming\\nhere that terms like Kermit refer unambiguously.) On the other hand,\\nthe sentence Kermit kissed Miss Piggy would be considered\\nunambiguous with respect to the same context.\\n\\n\\nAlthough Pinkal is only concerned with lexical ambiguity, the precisification\\napproach can also be used to classify as ambiguous sentences which have more\\nthan one structural analysis (like the sentence They saw her duck)\\nor are scopally ambiguous (cfr. the sentence Everybody didn't leave)\\nwhenever the grammar assigns to them more than one sense. I will discuss below\\nhow Pinkal's system can be extended to scopal and referential\\n ambiguity. \\n\\n\\n  The Disjunction Fallacy \\n\\nIt is important to realize that saying that a sentence is ambiguous in a\\ncontext if it has distinct precisifications is not the same as saying that an\\nambiguous sentence is equivalent to the disjunction of its distinct\\nprecisifications. Intuitively, in uttering S, whose two precisifications are\\nthe propositions P and Q, a speaker may have meant P or she may have meant Q,\\nbut the following does not hold:\\n\\n\\n[[A means that P] [A means that Q]] \\n[A means that [P Q]]\\nTo treat an ambiguous sentence in such a way would be tantamount to propose\\nthat an ambiguous sentence has a single sense in any given discourse situation,\\nnamely, the proposition that is true at a situation if either of the distinct\\ninterpretations of the sentence is true at that situation; but according to the\\ndefinition above, an ambiguous sentence is one which has more than one sense at\\na discourse situation.  For example, according to the definition of ambiguity\\ndiscussed above, the listener of an utterance of They saw her duck\\ncould either interpret the speaker as saying that the contextually determined\\nset of individuals denoted by the pronoun they saw a contextually\\nspecified female person lowering herself, or as saying that that set of\\nindividuals saw the pet waterfowl of that female person. According to the\\ndisjunction theory, instead, the listener would attribute to the speaker of\\nthat sentence a single meaning, albeit a disjunctive one; namely, that it was\\neither the case that they saw a contextually specified female person\\nlowering herself, or it was the case that they saw the pet waterfowl\\n of that female person. \\n\\n\\nI will refer to the idea that a semantically ambiguous sentence denotes the\\ndisjunction of its alternative interpretations as the disjunction\\nfallacy. The disjunction fallacy can be found in the literature in two forms.\\nIts `purest' form is the hypothesis that the interpretation process literally\\ninvolves generating all of the senses of an expression and putting them\\ntogether in a disjunction. In this form, the disjunction 'theory' is not simply\\ncounterintuitive; it doesn't explain the combinatorial explosion puzzle at\\nall. As far as I know, this `explicit' form of the theory has only been\\ndiscussed jokingly.CHECK HOBBS REFERENCE FROM STALLARD. One can find\\nin the literature, however, an `implicit' form of the disjunction theory, in\\ntheories of underspecification that assign to underspecified expressions a\\nsemantics that makes them equivalent to the disjunction of their readings. One\\n such proposal is ; the semantics of UDRSs is also disjunctive .\\n\\n\\nThe Role of Syntactic and Semantic\\nConstraints\\n\\n\\nAlthough the number of logical form permutations that one can obtain for a\\nparticular sentence by, e.g., considering all the permutations of its operators\\nmay be rather large, constraints of a syntactic and/or semantic nature\\ndrastically reduce this number.\\n\\n\\nIn the case of scopal ambiguity, for example, permutations may not correspond\\nto actual readings for at least three reasons. First of all, some of these\\npermutations result in logical expressions that are either ill-formed or\\ncontradictory,as noted by, e.g., Hobbs an Shieber .\\nFor example, samb-ex:impossiblea, in the interpretation in which the\\npronoun he is anaphoric on the  every man, does not\\nhave a reading in which the  the woman hei married outscopes\\nthe  every mani. There is no well-formed logical expression\\nthat may represent this reading.  Hobbs and Shieber point out that this\\nconstraint also prevents a quantifier to scope between a noun and its\\ncomplement: for example, a meeting may not scope inside\\nmost but outside each in .\\n \\n\\n\\n\\nAnother reason why the number of actual readings of a sentence is much\\nsmaller than the number of permutations of its operators is that two distinct\\npermutations may correspond to semantically equivalent readings. For example,\\n has only one reading, even though (at least) two equivalent\\nlogical expressions can be obtained as the translation of the\\n sentence. \\n\\n\\n\\nFinally, the readings corresponding to certain permutations may be\\nunavailable because of syntactic constraints.  Much work on uncovering readings\\nthat are absent due to constraints on syntactic trasformations and/or\\nconditions on syntactic levels of representation has been done in the\\n generative tradition . \\n\\n\\nSome of the constraints proposed in this literature have been proved to yield\\nquite robust predictions.  Perhaps the best known example of syntactic\\nconstraint is the observation that a quantifier cannot take scope outside the\\nclause in which it appears.  The observation that clauses serve as `scope\\nislands' goes back at least to Rodman , but was discussed\\nmost extensively by May ; the constraint was called\\nScope Constraint by Heim . The Scope Constraint\\nis exemplified by the contrast in sc-1: whereas sc-1a has a\\nreading in which every department is allowed to take wide scope over\\na student, this reading is not available for sc-1b, even\\nthough arguably from every department and who was from every\\ndepartment have the same denotation.\\n\\nAlthough syntactic and semantic constraints do not rule out all possible\\nreadings--for example, a sentence like They saw her duck still has\\nmore than one interpretation under all of these theories--a theory of\\ndisambiguation must be such that these constraints can play a role.\\n\\n\\n\\n  Perceived  Ambiguity \\n\\nAs noted by Hirst , the discussions of\\nambiguity processing in the  literature tend to ignore the fact that\\nhumans are aware that sentences can be ambiguous, and that they can exploit the\\nambiguity of sentences for rhetorical effect.  Raskin, for example, claims\\n that humor crucially relies on ambiguity. He\\ndiscusses examples such as the following (p.  25-26):\\n\\n\\n\\nThe joke relies on two assumptions about human processing: first, that\\nthe clause the first thing that strikes a stranger in New York gets\\ninterpreted before the end of the sentence, with strikes receiving\\nthe `surprise' interpretation; and second, that the reader, upon reading\\nis a big car, will go back, produce a second interpretation, and\\nentertain both interpretations simultaneously. The joke could not be understood\\nunless the hearer were able to entertain the two interpretations of the\\nsentence simultaneously. These jokes can exploit other forms of ambiguity,\\ne.g., scopal ambiguity, as in Statistics show that every 11 seconds a\\nman is mugged here in New York City. We are here today to interview him.\\n\\n\\nThe reader's ability to entertain more than one interpretation simultaneously\\nis exploited in poetry, as well .  The linguistic articles\\ndiscussing ambiguity are another literary form that exploits this\\npossibility. Examples such as They saw her duck are a clear case of\\ndeliberate ambiguity; the whole point of these examples is to show that a\\nsentence can have more than one interpretation. The writer relies on the reader\\nbeing able to entertain more than one interpretation at once.\\n\\n\\nThe opposite is true, as well: when clarity is a goal, writers and speakers\\ntend to construct their sentences in such a way as to avoid ambiguity. Thus,\\nmost sentences one runs across in scientific texts or in transcripts of\\ntask-oriented conversations have a clearly preferred interpretation.  This\\ninterpretation is sometimes suggested by the context, sometimes by means of\\ndisambiguation markers--expressions such as each,\\na different, or the same that suggest which interpretation\\nis preferred. Thus, a writer will use sentences such as Every kid\\nclimbed the same tree, rather than Every kid climbed a tree, when\\nwe/she wants to make sure that the reader arrives at the interpretation in\\n which there is a single tree. \\n\\n\\nI will call the situation in which a listener arrives at more than one\\ninterpretation for an utterance perceived ambiguity.  A situation in\\nwhich B perceives an utterance as ambiguous may result in B's appreciating the\\njoke, the poetic phrase, or the point of the linguistic example; if the\\nambiguity is not perceived as intended, B may say saying something like\\nThis is not very clear, or perhaps This sentence is\\nambiguous. This situation can be informally characterized as follows:\\n\\n\\n\\nThe phenomenon of deliberate ambiguity  suggests that the solution to the\\nCombinatorial Explosion Puzzle cannot be that humans either generate only one\\ninterpretation at a time by using some clever heuristics, or do not generate\\nany interpretation at all. Humans entertain more than one interpretation at a\\ntime, and they may not be able to choose one among them. This conclusion is\\nalso supported by psychological results. There is evidence, for example, that\\nduring both lexical processing and syntactic processing several hypotheses are\\ngenerated in parallel, and only later filtered on the basis of contextual\\ninformation\\n. Kurtzman and MacDonald  suggest a similar model for scope\\ndisambiguation. As far as reference interpretation is concerned, there is some\\nevidence that all pragmatically available referents become active before a\\n referent is identified (see, e.g., ). \\n\\n\\nThese facts are consistent with the view of discourse interpretation taken in\\nArtificial Intelligence, in which processes such as reference resolution or\\nlexical disambiguation are modeled in terms of defeasible inference, which may\\nresult in alternative hypotheses. Examples include the theories of the effects\\nof semantic priming on lexical disambiguation, as formalized, e.g., in Hirst's\\nABSITY system  or, more recently, in\\nstatistically based terms; the theories about the effects of local focusing on\\n the choice of pronoun antecedents such as ; the work on temporal interpretation by Asher, Lascarides, and Oberlander (see, e.g.,\\n ); and work on scopal disambiguation such as  [,. \\n\\n\\n  Semantic Ambiguity versus Perceived Ambiguity \\n\\nA preliminary and, I hope, uncontroversial conclusion I intend to draw from the\\ndiscussion on deliberate ambiguity and ambiguity processing is that a theory of\\nambiguity that aims at explaining the Combinatorial Explosion Puzzle needs to\\nbe concerned both with the interpretation that the grammar assigns to a\\nsentence--i.e., what it means for a sentence to be semantically\\nambiguous--and with the process by which interpretations are generated, i.e.,\\nwith what it means for an utterance to be perceived as ambiguous.  On the one\\nhand, the theory must explain why the disambiguation process will not generate\\nall semantically available interpretations; on the other hand, it must predict\\nthat more than one interpretation will be generated. This conclusion is the\\ncentral idea of this paper, indeed, what gives the paper its title. The\\ninclusion of a theory of disambiguation will also remedy one of the omissions\\nin Pinkal's theory, namely, how to formalize the Precisification Imperative.\\n\\n\\nThe discussion of perceived ambiguity supports a stronger claim, namely, that\\nsemantic ambiguity and perceived ambiguity are distinct notions, in the sense\\nthat whereas a model of semantic ambiguity has to express the truth-conditional\\nproperties of an expression, the reasoning processes involved in\\ndisambiguation, and that may lead to a perceived ambiguity, consist of\\ndefeasible inferences that are not supported by the semantics of ambiguous\\nexpressions.\\n\\n\\nThe distinction I intend to draw, then, is as follows. Semantic ambiguity is\\npart of the specification of the grammar of a language; most, if not all,\\nsentences are semantically ambiguous, but their ambiguity need not be noticed\\nby listeners, and in fact it is typically discovered only by linguistic\\nresearch. Perceived ambiguity, on the other hand, is a result of the\\ninterpretation process, that is defeasible in nature, and may therefore result\\nin more than one interpretation in cases of miscommunication or when the\\nspeaker constructs the context appropriately to serve a rhetorical purpose, as\\nin the puns presented above.\\n\\n\\nSome readers may wonder why the developer of a  system should be\\nconcerned with perceived ambiguity, i.e., with generating all of the\\ncontextually available interpretations of a sentence. The answer is that\\ncertain applications need this information. Consider the following example,\\nagain from the  domain. Say that the user utters move the\\nengine to Avon, and say that two different engines have been discussed during\\nthe elaboration of the current part of the plan. Clearly, we do not want the\\nsystem to just come out with a plausible guess about which engine was meant:\\ninstead, we want it to recognize the ambiguity and ask for clarification. In\\ngeneral, all systems that engage in conversations with their users need to be\\nable to recognize an ambiguity, to ask for clarifications when necessary rather\\nthan guess one possible interpretation, and to make their own output\\nunambiguous. (Of course, the theory of contextual disambiguation must be such\\nthat no spurious ambiguities are obtained.)\\n\\n\\n\\n  The Underspecification Hypothesis \\n\\nAll theories of semantic interpretation based on Montague's general program as\\nexposed in Universal Grammar  assume that the grammar\\nof a language \\nspecifies two homomorphisms: one between syntactic trees\\nand a disambiguated language ,\\nand a second one between\\nthe disambiguated language and objects of the model M (the senses). These two\\nhomomorphisms can be composed, thus making the intermediate level of the\\ndisambiguated language dispensable. The grammar assigns to an ambiguous\\nexpression of \\ndistinct expressions of ,\\neach of which has\\na unique interpretation.\\n\\n\\nA direct implementation of this strategy in an  system would require\\ngenerating all senses of an ambiguous sentence-string, which would be clearly\\nproblematic.  Many  systems, instead, make use of heuristic methods that\\ngenerate only one interpretation and ignore the alternatives. These heuristics\\nwork fairly well fairly often; such systems, however, won't be able to perceive\\nan ambiguity even when it would be helpful to do so. Other systems therefore\\nsplit the semantic problem of computing all the interpretations of a sentence\\nfrom the processing problem of generating these interpretations in context, by\\nmaking use of an intermediate, underspecified level of\\nrepresentation.  One of the earliest examples of underspecified representations\\nis the `Logical Form' of Schubert and Pelletier\\n. The representation for \\nproposed by Schubert and Pelletier, shown in , is a typical\\nexample of these underspecified representations: quantifiers are left in place\\nand the referent for the definite description the tree is not\\nspecified.\\n\\nIn more recent years,\\nunderspecified representations similar to Schubert and Pelletier's have been\\nused by [], Fenstad  ,\\nin Allen's textbook  and, most recently, in the Core\\nLanguage Engine ; the `uninterpreted conditions'\\nproduced during the intermediate steps of the  construction algorithm in\\n[] can be considered underspecified representations as well.\\n\\n\\nUnderspecified representations were originally conceived as a way to solve a\\nproblem in system implementation, namely, separating `context-independent' from\\n`context dependent' aspects of the interpretation, thus making either part\\n reusable for different applications. Since the motivation was strictly computational, the underspecified representations used in most  systems\\nare little more than data structures, in the sense that they do not have a\\ninterpretation other than the one provided by the procedures that interpret\\nthem. These representations `encode' the ambiguity of a sentence in the sense\\nthat that sentence has the reading r iff that reading can be generated by\\nrepeatedly applying `construction rules' to the underspecified representation.\\n\\n\\nIn recent years, there has been growing interest for the hypothesis that the\\nability to encode multiple interpretations in an underspecified language may be\\n(part of) the explanation of the Combinatorial Explosion Puzzle. The idea is\\nthat humans, as well, make use of an underspecified language that can encode\\ndistinct meanings implicitly, and therefore do not need to generate all of\\nthese meanings.  A semantically ambiguous sentence, therefore, need not cause\\nproblems for a human to process, because it is not necessarily perceived\\nas ambiguous in the sense discussed in the previous section. I will call this\\nassumption the Underspecification Hypothesis:\\n\\n\\nUnderspecification Hypothesis\\n:\\nHuman beings represent semantic ambiguity implicitly by means of\\nunderspecified representations that leave some aspects of\\ninterpretation unresolved.\\nMy goal in the rest of the paper is to  spell out  the\\nUnderspecification Hypothesis both as a theory of grammar and as a theory of\\ndiscourse interpretation. I assume, that is, that the hypothesis is correct,\\nand try to answer questions such as: what kind of language are underspecified\\nrepresentations?  what is their semantics? and, what kind of inferences are\\ndone with them?\\n\\n\\nThe novel aspect of this work is that the answers I give are based on the\\ndiscussion of semantic ambiguity and perceived ambiguity in the previous\\nsection.  I hypothesize that underspecified representations are used by humans\\nas the translation of expressions that are indefinite in the sense of Pinkal,\\nand assign them a semantics that reflects this hypothesis. I assume that the\\ndisambiguation process is consists of defeasible inferences, and examine the\\ncharacteristics of defeasible reasoning with underspecified\\nrepresentations. Although the same position towards disambiguation and\\ndefeasible has been adopted in the Core Language Engine, most of the issues I\\ndiscuss have not been mentioned so far in the discussion on underspecified\\nrepresentations.\\n\\n\\nIn the literature on underspecification, one often finds the argument that\\nproviding a semantics to underspecified representations is necessary because\\ndisambiguation requires inference, and therefore a `logic of\\nunderspecification' is needed\\n.  However, it is not at all\\nclear whether the process of disambiguation involves much semantically\\njustified reasoning; disambiguation seems to consist mostly of defeasible\\ninferences. It is fair to say that the debate on this issue is very open at the\\nmoment, as certified by a number of recent panels on the subject.  But whatever\\nthe final conclusion on this topic will be, it is clear that under the\\nperspective that the grammar of a language \\nis a mapping from elements\\nof \\nto underspecified representations, the semantics of these\\nunderspecified representations becomes a central aspect of the specification of\\nthe grammar. Furthermore, it also becomes clear that the semantics of\\nunderspecified representations must be based on an analysis of semantic\\nambiguity, otherwise we wouldn't even know whether the form of underspecified\\nrepresentation we develop does the job it is supposed to do.\\n\\n\\n  An Underspecified Theory of Ambiguity, Part I:  Lexical           Ambiguity \\n\\nThe simplest way to illustrate my implementation of the Underspecification\\nHypothesis is to start with lexical ambiguity. I present in this section a\\ntheory of grammar which makes use of an underspecified language to encode the\\n`ambiguity potential' of lexically ambiguous expressions, as well as a simple\\nformalization of lexical disambiguation as defeasible inference over\\nunderspecified representations. In the next section I will show how to extend\\nthe approach presented here to deal with expressions that exhibit other forms\\nof semantic ambiguity.\\n\\n\\nI want to emphasize that I start with lexical disambiguation for expository\\npurposes only. Lexical ambiguity is the one case of ambiguity for which a\\n`generate and test' strategy may well be compatible with the psychological\\nresults, therefore the one for which the need for underspecified\\nrepresentations is less clear. Furthermore, I will only discuss cases of\\nlexical ambiguity in the narrow sense, which is perhaps the least interesting\\ncase of lexical indefiniteness. Discussing lexical disambiguation, however, is\\nthe simplest way to explain how underspecified representations can be given a\\nsemantics related to Pinkal's proposals about ambiguity, and how to defeasible\\nreasoning with underspecified representations. In the next sections I will\\ngeneralize the approach introduced here to cases of ambiguity for which the\\nunderspecified approach is much more plausible.  Furthermore, at least one\\ntheory of lexical disambiguation, Hirst's proposal\\n, makes use of `Polaroid words' which are\\nessentially underspecified interpretations of lexical items.\\n\\n  A Lexically Underspecified Grammar \\n\\nThe presentation of a lexically underspecified grammar below is centered on the\\nexample of (H-type) lexical ambiguity discussed above, the verb\\ncroak, which can take two precisifications. Let \\nbe the\\nlanguage which consists of the single sentence Kermit croaked. This\\nsentence is H-type ambiguous because it admits of two precisifications and it\\nis subject to the precisification imperative. A `Montagovian' grammar MG would\\nmap (syntactic analyses of) the sentence into distinct expressions of a\\n`disambiguated language' ,\\neach of which denotes a function from\\ndiscourse situations into intensional objects of the appropriate type (in this\\ncase, propositions). A grammar UHG that subscribes to the Underspecification\\nHypothesis, on the other hand, maps syntactic analyses of expressions of \\ninto a single expression of a `lexically underspecified language' \\n\\n.\\nThe semantics of \\n\\n\\nis based on the Precisification\\nPrinciple: expressions of \\n\\n\\ndenote at each discourse situation a\\nset of senses of the type they would be assigned by a Montagovian\\n grammar. \\n\\n\\nThe lexically underspecified language \\n\\n\\nhas the following\\ningredients:\\n\\n\\n\\n\\n\\nNote that in addition to two predicates croak1 and croak2,\\ncorresponding to the disambiguated senses of croak, the language\\nincludes an `underspecified' predicate croakU.  The interpretation\\nfunction for \\n\\n,\\n, is defined as follows.  Let M\\n= UF be a model just like the one that would be used for a\\ndisambiguated language .\\nThe interpretation function\\n assigns to an expression  of \\n\\n\\na value\\nwith respect to M and a discourse situation d.\\n\\nThe language \\n\\n\\nhas been deliberately kept  simple\\nto make it clear that the underspecified languages I propose have two basic\\nproperties: (i) the value of an expression at a discourse situation is a set of\\nsenses of the type that a sense of that expression would have in a\\ndisambiguated language ;\\nand (ii) expressions can be divided into\\nexpressions whose denotation at a discourse situation is a singleton set, such\\nas k or croak1, and expressions such as croakU that\\ndenote a non-singleton set.  The latter expressions provide the interpretation\\n for ambiguous expressions of . \\n\\n\\nThe clauses for application and the connectives show how ambiguity `percolates\\nup' from lexical items. The value of an expression like ()\\nis obtained by taking the cross-product of the values of  and ,\\nand it includes one function f per distinct pair of functions\\n11 in the denotations of  and . The\\nvalue assigned by the function f to the situation s is defined by\\napplying a certain operation (in this case, application) to the values assigned\\nto s by the functions \\nand .\\nThus, if both the\\ndenotation of  and the denotation of  are singleton sets, the\\ndenotation of () is also a singleton set; otherwise, ambiguity\\n `multiplies,' as it where. The same `multiplication' technique is also used to define the denotation of\\n connectives. \\n\\n\\nThe following grammar generates an underspecified representation of\\nKermit croaked by mapping the semantically ambiguous predicate\\ncroaked into an `ambiguous' predicate of \\n\\n\\nas follows:\\n\\n\\n\\nThe underspecified translation of Kermit croaked in \\n\\n,\\ncroakU(k), denotes a set of two propositions at a situation d:\\nthe function that assigns 1 to a situation iff Kermit produced a frog-like\\nsound in that situation, and the function that assigns 1 to a situation iff\\nKermit died in that situation.  This makes the sentence indefinite in Pinkal's\\n sense. By contrast, an indeterminate sentence such as Kermit is the\\nRuritanian secretary of state would have a single sense at a given discourse\\nsituation.\\n\\n\\nPinkal's Precisification Imperative is an attempt at making more precise the\\nobservation that human beings don't seem to have good intuitions concerning\\nwhat follows from a H-type ambiguous sentence.  Even when subjects are able to\\npass judgments about what follows from an ambiguous sentence, it's arguable\\nthat they do not give judgments concerning what follows from the underspecified\\nrepresentation: rather, they first generate one interpretation, then decide\\nwhat follows from that.  The conclusion that I would be inclined to draw is\\nthat a relation of semantic entailment capturing human intuitions can only be\\ndefined, if at all, between expressions whose interpretation is not subject to\\nthe Precisification Imperative. So, although it would be possible to define,\\nfor example, a `strong' notion of entailment as what follows from all senses,\\nthis definition would be rather artificial. For this reason I will not attempt\\nto define a notion of entailment between expressions of \\n\\n;\\nthe\\nreaders interested in the issue are referred to Pinkal's book and to the\\ndiscussion in van Deemter's dissertation .\\n\\n\\nDiscourse Interpretation and Perceived Ambiguity\\n\\n  Discourse Interpretation and Defeasible Reasoning \\n\\nA theory of ambiguity processing solves the Combinatorial Explosion Puzzle if\\nit does not require that all distinct interpretations of a semantically\\nambiguous sentence are actually generated. A grammar consistent with the\\nUnderspecification Hypothesis such as the one just discussed moves us one step\\ntowards that goal, since it only imposes the constraint that a single\\nunderspecified interpretation be generated.\\n\\n\\nOn the other hand, we can conclude from the discussion of deliberate ambiguity\\nand of the psychological work on ambiguity that a psychologically plausible\\ntheory of ambiguity must also predict that more than one interpretation may\\nbecome available in a given context, although the number of such\\ninterpretations will in general be much smaller than the number of possible\\nsemantic interpretations.\\n\\n\\nAs discussed above, the view of discourse interpretation that I am going to\\ntake is the one typically found in the AI literature, according to which\\ndisambiguation involves the generation of (possibly distinct) hypotheses in\\nparallel by means of defeasible inference. This perspective is found, for\\nexample, in the work on abductive discourse interpretation by Hobbs and\\ncolleagues , in the work on Bayesian\\ndisambiguation by, e.g., Charniak and his students \\nand in the work on DICE and discourse interpretation by Asher, Lascarides, and\\nOberlander . Some of the formal models of\\ndefeasible reasoning that can be used to formalize the situation in which\\nconflicting hypotheses are generated include Reiter's default logic\\n, the abductive model\\n, Bayesian Nets, and DICE\\n ,]. \\n\\n\\n  Lexical Disambiguation Using Defaults \\n\\nAs a model of defeasible reasoning, I adopt Reiter's Default Logic.  In\\ndefault logic, the process that generates defeasible hypotheses is seen as the\\ncomputation of the extensions of a default theory (D,W)\\nwhere D is a set of default inference rules and W is a set of formulas. I will\\nformalize discourse interpretation as the process of generating the extensions\\nof the theory (DI,UF), where DI--the Discourse Interpretation\\nPrinciples--are default inference rules, and UF is a set of expressions of an\\nunderspecified language like \\n\\n.\\nLet us ignore for the moment the\\nfact that the formulas in UF are underspecified representations. The Discourse\\nInterpretation Principles formalize the defeasible inferences that take place\\nin discourse interpretation, such as disambiguating inferences. These rules are\\noperations that map a set of wffs that allow of a certain number of\\ninterpretations into a new set of wffs with a more restricted number of\\ninterpretations. An example of Discourse Interpretation Principle is the\\nfollowing:\\n\\n\\n\\n This inference rule  reads: if the set of wffs UF includes the fact that the object x has the property croakU\\nand the property frog, and if it is consistent to assume that the\\ninterpretation croak1 of croakU was intended, then the\\ninference rule CROAK1-IF-FROG produces a new set of wffs that includes the fact\\ncroak1(x). The application of CROAK1-IF-HUMAN-LIKE would be\\nblocked by the presence in UF of the wff croak1k. Using\\nReiter's definition of extension in an `intuitive' fashion, we can see that the\\ndefault theory\\n\\n\\n\\nhas the  following (unique) extension:\\n\\n\\n\\nThe denotation of a set of wffs {1 ...n} will be defined as\\nthe denotation of the conjunction 1 ...n of these\\nwffs. I also assume that the empty set of wffs denotes the function TRUE that\\nis true at every situation. With this definition, and under the assumption that\\neach `unambiguous' interpretation of the word croak is incompatible\\nwith the others (i.e., under the assumption that croak1x croak2x), the extension of (DF,UI) admits of only one\\ndenotation, the one under which the denotation of k produced a sound\\n like the one frogs produce. \\n\\n\\nA default theory always has an extension as long as all defaults are\\n normal, but it may have more than one extension if the set of Discourse Interpretation Principles contains\\ntwo inference rules that both apply but generate a conflict. Consider, for\\nexample, the default theory consisting of a set of discourse interpretation\\nprinciples DI that includes, in addition to CROAK1-IF-FROG, a second\\ndiscourse interpretation principle (let's call it CROAK2-IF-HUMAN-LIKE) stating\\nthat the croak2 interpretation is plausible for human-like beings;\\nand of a set of wffs UF including the fact that Kermit is a human-like\\nbeing.\\n\\n\\n\\nthis theory would have two extensions:\\n\\n\\n\\n\\n\\nPerceived ambiguity can now be redefined more precisely as the state that\\nobtains when the default theory `encoding' the listener's discourse\\ninterpretation processes has more than one extension; and the cases of\\ndeliberate ambiguity discussed in section ambiguity_section can be\\nformalized as cases in which the speaker has `reasoned about the other agent's\\nreasoning,' as it were.\\n\\n\\n  Constraints on Discourse Interpretation and the               Anti-Random Hypothesis \\n\\nOnce we start allowing discourse interpretation processes like those just\\ndiscussed, the Underspecification Hypothesis is not sufficient to explain the\\nCombinatorial Explosion Puzzle anymore. The UH does not rule out a theory of\\ndiscourse interpretation in which after an underspecified interpretation has\\nbeen obtained, all possible senses of a sentence are generated. In fact, a lot\\nof  systems work this way, as well as interpretation procedures such as\\nHobbs and Shieber's scoping algorithm . In the\\nframework for discourse interpretation just presented, theories of this kind\\ncould be formalized by including discourse interpretation principles that\\ngenerate all the semantically justified interpretations at random. For the case\\nof lexical disambiguation, for example, we could have a theory that includes\\nthe two following inference rules:\\n\\n\\n\\n\\n\\n\\nA theory of lexical disambiguation of this kind would simply produce all\\nsemantically justified interpretations of a sentence, and the Combinatorial\\nExplosion Puzzle would remain a puzzle. To solve the puzzle, a theory of\\ndisambiguation must therefore supplement the Underspecification Hypothesis with\\nconstraints on discourse interpretation that ensure that only a few extensions\\nare generated.\\n\\n\\nThe constraints need not be the same for all classes of ambiguity. For certain\\n classes of ambiguity, including perhaps lexical ambiguity,  the explanation may simply be that the disambiguation process is incremental, i.e., it takes place as the text\\nis processed word by word or constituent by constituent, and each ambiguity is\\nresolved locally; in this way, only a small number of alternative\\nhypotheses have to be considered every time. For other classes of ambiguity,\\nhowever, such as scopal ambiguity and referential ambiguity, incremental\\n processing does not seem to be the solution, and  different constraints must apply. In , the following constraint was proposed:\\n\\n\\nAnti-Random Hypothesis (Informal)\\nHumans do not randomly generate alternative interpretations of an ambiguous\\nsentence; only those few interpretations are obtained that (i) are consistent\\nwith syntactic and semantic constraints and (ii) are suggested by the context.\\nThe Anti-Random Hypothesis should be thought of as a `meta-constraint' on\\ntheories of interpretation: if we intend to account for the Combinatorial\\nExplosion Puzzle, we have to develop theories of interpretation (e.g., theories\\nof parsing, or theories of definite description interpretation) that satisfy\\nthis constraint, i.e., in which discourse interpretation principles like\\nCROAK1-AT-RANDOM and CROAK2-AT-RANDOM are not allowed.\\n\\n\\n In order to illustrate more concretely the difference between theories\\nof discourse interpretation that satisfy the Anti-Random Hypothesis, and\\ntheories that do not, let us consider how one could formalize a theory of\\npronominal interpretation. A `random' theory of pronoun interpretation would go\\nas follows: first, compute all possible antecedents of the pronoun in the\\ndiscourse. Then, generate an hypothesis for each of them, stating that the\\npronoun refers to that antecedent. Finally, rank these hypotheses according to\\ntheir plausibility. A random hypothesis generation process usually leaves the\\ntask of choosing one hypothesis to plan recognition; the problem is that most\\noften, the alternatives are equally plausible.\\n\\n\\nIn contrast, centering theory  is an example of\\nnon-random pronoun interpretation theory.  According to centering theory, each\\nutterance establishes a `backward looking center' (Cb), and a pronoun is by\\ndefault interpreted to refer to the Cb. (I am glossing over a number of\\ncomplexities here.)  Such a theory would generate a single (or a few)\\nhypothesis concerning the antecedent of a pronoun; the other possibilities,\\nalthough semantically possible, would simply never come up. Examples of\\ntheories of definite description interpretation, tense interpretation, the\\ninterpretation of modals in discourse, and scope disambiguation that satisfy\\n the Anti-Random Hypothesis are discussed in . \\n\\n\\nThe Anti-Random Hypothesis can be made more formal in the framework for\\ndiscourse interpretation adopted here by introducing a slightly different\\nsyntax for default inference rules, one in which the underspecified condition\\nis syntactically separated from additional contextual requirements such as the\\nrequirement in CROAK1-IF-FROG that the object in question be a frog:\\n\\n\\n\\nExcept for the fact that one of the prerequisite wffs is `singled out', an\\ninference rule thus rewritten has the same interpretation as one of Reiter's\\ndefault rules. We can then require the contextual requirements to be\\nnon-trivial (i.e., not satisfied in every situation) as follows:\\n\\n\\nAnti-Random Hypothesis\\nA discourse interpretation theory (DI,UF) is Anti-Random iff\\nfor all  discourse interpretation principles ::/  in\\nDI,  is not satisfied in every situation.\\n\\n\\n  The Condition on Discourse Interpretation \\n\\nThe framework just introduced can also be used to formalize the `second order'\\naspects of Pinkal's theory, such as the Precisification Imperative. The\\nPrecisification Imperative can be seen as imposing a constraint on the\\nextensions of a discourse interpretation theory, namely, as the requirement\\nthat extensions include a `disambiguating wff' like croak1k for\\neach H-type ambiguous constituent of the set UF such as croakUk. I\\nwill call this constraint Condition on Discourse\\nInterpretation. In first instance, the Condition on Discourse\\nInterpretation might be formulated as follows, for the case of lexical\\nambiguity:\\n\\n\\nCondition on Discourse Interpretation (Preliminary):\\nEach\\nextension E of a discourse interpretation theory (DI,UF) must include, for each\\nliteral L in UF whose predicate is H-type ambiguous, a distinct\\ndisambiguating literal, i.e., a literal whose denotation is a single\\nfunction among those in the denotation of L.\\nThe definition of the Condition on Discourse Interpretation just given is not\\nvery general: it depends on the assumption that all cases of H-type ambiguity\\nare originated by predicates. A simpler, and more general, formulation of the\\nCondition on Discourse Interpretation can be obtained by generalizing the\\nformat for the discourse interpretation principles once more.\\n\\n\\nDefault inference rules are typically used to augment a set of wffs with\\nadditional facts inferred by default: the fact that a particular bird flies,\\nfor example. But the purpose of discourse interpretation rules used for\\ndisambiguation, like CROAK1-IF-FROG, is to restrict the interpretation by\\neliminating certain readings. In this perspective, leaving the underspecified\\nwffs around doesn't make much sense. I propose therefore to allow discourse\\ninterpretation principles to rewrite their `triggering wff' whenever this\\nwff encodes an H-type ambiguity, in addition to adding new wffs to a set. The\\nmore general format for discourse interpretation principles is as follows:\\n\\n\\n\\nA  rule of this form is an operation from sets of wffs into sets of wffs that\\ngiven a set W of wffs containing  and  and not containing\\n , produces a set W of wffs containing , and in which  has been replaced by . I will call  the\\ntriggering condition. For example, a version of CROAK1-IF-FROG in\\nwhich the triggering condition croakUx is rewritten by the\\nconsequent croak1x is as follows:\\n\\n\\n\\nIf all disambiguation rules are rewritten in this format,\\na completely disambiguated extension can simply be characterized as one which\\ndoesn't contain any H-type ambiguous wffs. The notion of H-type\\nambiguous wff can be characterized either syntactically (by identifying certain\\nsyntactic constituents as specifying H-type ambiguity, and by classifying as\\n H-type ambiguous a wff that contains one of these constituents) or model-theoretically, e.g., by means of a function \\nsuch that if X is a set of senses, (X) is 1 if the set\\nof senses is admissible, 0 if it is inadmissible in Pinkal's sense.  Whatever\\nway we choose to define a H-type ambiguous wff, the Condition on Discourse\\nInterpretation can now be formulated as follows:\\n\\n\\nCondition on Discourse Interpretation\\n: An\\nextension E of a discourse interpretation theory (DI,UF) cannot contain an\\nH-type ambiguous wff.\\nNotice that the statement of the Condition on Discourse Interpretation as a\\ncondition on pragmatic reasoning gives it the status of a felicity condition\\nrather than of a hard constraint on interpretation.\\n\\n\\n  Extensions, closure and consistency checking in an  underspecified default theory \\n\\nSo far, I've been using the terminology from default logic as if the shift to\\nan underspecified representation had no side effects, but this is not the\\ncase. Consider the way in which Reiter defines the notion of extension of a\\n (closed) default theory, for example:\\n\\n\\n\\nThis definition crucially relies on the notion of deductive closure Th(S),\\ndefined as the set of wffs {w | S \\nw}; but what we said with\\nsemantic entailment holds for provability, as well: no clear notion exists of\\nwhat it means for an expression of an underspecified language to follow from a\\nset of wffs of the same language.  Two routes are open to us. One is to define\\na notion of `underspecified provability' ,\\nand to use \\nto\\ndefine an `underspecified' notion of closure ThU(S). For example, we could\\nsay that w \\nw iff for each expression w that\\ndenotes a single one of the interpretations of w, w w.  This route is not very appealing, however, if for no other reason\\nthat it's not clear that any way of defining an underspecified notion of\\nprovability will do.\\n\\n\\nThe alternative is to adopt a new notion of extension that does not rely on\\ndeductive closure, i.e., one in which an extension is a fixed point of the\\noperator ,\\nwhich does not include condition D2 of the definition of\\n:\\n\\n\\n\\nReplacing \\nwith \\nin the definition of extension has several\\nconsequences. First and foremost, dropping the requirement of deductive closure\\nmakes the test of whether it is consistent to assume 1, ...,\\nm essentially syntactic: i.e., it is possible for j not to be\\nincluded in (S) even though it is derivable from (S). (In\\ngeneral, this definition of extension is a much closer description of the\\nbehavior of actual implementations of non-monotonic reasoning than the original\\ndefinition.) And therefore, a logic defined in this way does not have the\\nproperty of Reiter's logic that a (closed) default theory (D,W) has an\\ninconsistent extension iff W is inconsistent.\\n\\n\\nIn fact, each extension of a discourse interpretation theory under this new\\ndefinition will, in general, be H-type ambiguous, some of the interpretations\\nbeing inconsistent. However, if we adopt the `rewriting' version of\\ndisambiguation discussed above, and impose the Condition on Discourse\\nInterpretation, each extension will have a single interpretation, and therefore\\nits consistency can be checked. I propose therefore to define the notion of\\nextension of a discourse interpretation theory as follows:\\n\\n\\nExtension:\\nA set of closed wffs E L is an\\nextension for the discourse interpretation theory iff E is a fixed point of the operator \\nand satisfies the Condition on\\nDiscourse Interpretation.\\n\\n\\n\\n\\n  Other Forms of Ambiguity \\n\\nThe theory of ambiguity introduced in the previous sections can be\\nstraightforwardly extended to obtain a treatment of two other classes of\\nsemantic ambiguity: scopal ambiguity and referential ambiguity. These\\nextensions preserve the basic ideas of the theory, semantic ambiguity as\\nmultiplicity of meanings, and perceived ambiguity as multiple extensions of a\\ndefault theory; what changes is that on the one hand, a more complex\\nunderspecified language is introduced, capable of encoding other forms of\\nambiguity; on the other hand, more complex inference rules are used.\\n\\n  Scopal Ambiguity \\n\\nI will call the sentence constituents that modify the parameters of evaluation,\\nand therefore affect the interpretation of other sentence constituents `in\\ntheir scope', operators. Examples of operators are quantifiers\\n(that affect the choice of the variable assignment used to evaluate expressions\\nin their scope) and modals (that affect the choice of the world / situation at\\nwhich expressions in their scope are evaluated). As it is well-known, one\\ncause of semantic ambiguity is that sentences may contain more than one\\noperator, and their relative scope is not completely determined by the\\nsentence's syntactic structure.  Sentences that have more than one meaning due\\nto the interaction between operators are called scopally\\n ambiguous. \\n\\n\\nHistorically, most underspecified representations have been introduced to deal\\nwith scopal ambiguity.  Typically, an intermediate step of processing is\\nassumed in which operators are left `in place,' as well as a subsequent step of\\nprocessing in which their relative scope is determined by contextual\\nprocessing. Schubert and Pelletier's underspecified representation of\\nEvery kid climbed a tree in  is an example of\\nunderspecified representation in which the operators are left `in situ'.\\n\\n\\nThese representations are typically justified in terms of ease of processing,\\nand their ability to represent `intermediate' readings. It is clear however\\nthat for the purposes of developing a `principled' theory of ambiguity\\nprocessing, it would be much better to stick to as few new `levels of\\nrepresentation' as possible.\\n\\n\\nIn fact, there is no need to introduce a new level of representation. The two\\nrequirements on a scopally underspecified representation--that it allow\\nrepresenting the structural information provided by a sentence, and\\nrepresenting the intermediate steps of disambiguation--can be satisfied by\\nusing as an underspecified representation the syntactic structure of the\\nsentence, augmented with information about the semantic interpretation of\\nword-forms. In this way we can also maintain semantic translation of lexical\\nitems used in Montague grammar, that determine how they combine with other\\nsentence constituents to determine a sentence's meaning.\\n\\n\\nThe `lexically and scopally underspecified language' \\n\\n\\nI introduce\\nto encode scopal ambiguity generalizes the language \\n\\n\\nintroduced in\\nthe previous section by allowing for arbitrary functional types. In this way,\\nthe lexical item every can be given its usual\\netett translation:\\n\\nThe second augmentation to \\n\\n\\nis the inclusion of tree-like\\nexpressions used to translate syntactic phrases. For example, the \\nNPDetevery Ndog translates into the\\nexpression:\\n\\n\\n\\nThe\\nexpression in  is the underspecified translation of\\nthe sentence Every dog saw a frog:\\n\\n\\n\\nBesides reducing the\\nnumber of representations floating around, this was of talking about scopal\\nunderspecification has two additional advantages over underspecified\\nrepresentations in which all syntactic information except for the position of\\noperators is lost, such as Schubert and Pelletier's underspecified logical\\nforms, Reyle's underspecified s or the Core Language Engine's\\ns. First of all, the semantics of expressions such as \\n--that, for historical reasons, I call logical forms-- can be\\ncomputed in a completely classical fashion using the storage mechanism\\n, with the result that syntactic constraints on the available\\nreadings, such as the Scope Constraint , can play a role\\nin determining the semantics of these objects, without the need for additional\\nconstraints such as the label ordering constraints used in UDRS\\n. Secondly, all structural information is preserved, not just\\ninformation about the relative position of operators. Some of this syntactic\\ninformation is used as a clue during disambiguation, for example, for\\ninterpreting pronouns, but also in certain theories of scopal\\ndisambiguation. (See, e.g., [] and\\n  for an account of scope disambiguation which esploits the syntactic information encoded by underspecified expressions such as\\n.)\\n\\n  A Lexically and Scopally Underspecified Language \\n\\nThe semantics of the underspecified language  \\n\\n\\nis classically based on a set  of semantic types, the smallest\\nset such that (i) e and t are types; and (ii) If\\n and  are types,  is a\\ntype.  The set of meaningful expressions of type  is indicated by\\nME.\\nThe set of non-logical constant expressions of type  is\\nindicated as CE\\nME.\\n\\n\\nThe semantics of \\n\\n\\nis based on the same idea as the semantics of\\n\\n.\\nNatural language expressions are assigned objects of the same\\n type that they would receive in  (as revised by Partee and Rooth parteero), with the difference that, when I talk about\\n`meaningful expressions of type ' below, therefore, I am really talking\\nabout expressions that denote sets of functions from the set of situations\\n to elements of  (the domain of type ). Thus\\nfor example, sentences are of type t both in Dowty, Wall and Peters'\\nsystem, and in the current proposal; but a meaningful expression of type\\nt in \\n\\n\\ndenotes a (function from a discourse situation to a)\\nset of functions from situations to truth values. Or to make another\\nexample, relations have the same type eet here that they\\nhave in Dowty, Wall and Peters, but meaningful expressions of type\\neet now denote sets of functions from  to\\neet.\\n\\n\\nThe sets of meaningful expressions of \\n\\n\\ninclude all the\\nexpressions in \\n\\n:\\n\\nThe set ME,\\nfor any type , includes a denumerably infinite\\nset of variables of type . \\n\\n\\nalso includes\\nlambda-abstracts and quantified expressions, defined below. The language also\\nincludes the new syntactic category of logical forms. The sets of\\nlogical forms of syntactic category XP, LFXP, are defined as follows:\\n\\nMeaningful expressions are assigned a value with respect to a universe\\n.  I use below the notation s to indicate that a\\n`stands for' an object in , i.e., it is part of the metalanguage, as\\nopposed to being a meaningful expression of the object language. The models\\nwith respect to which a  expression is evaluated include a set\\n of situations. The only fact about situations I use here is that\\nthey have constituents.\\n\\n\\nThe interpretation of types with respect to  is defined as usual:\\nDe,U = ; Dt,U = {0,1}; D\\n\\n\\n=\\nDb[Da]. In the rest of this paper, I generally drop the indication of the\\nuniverse (e.g., I write De instead of De,U).  The model of\\ninterpretation for  expressions is the triple ,\\n, I. The interpretation function `I' assigns an\\ninterpretation to constants of type .\\n\\n\\nThe value of meaningful expressions is specified by a function .\\nthat includes an assignment function among its parameters, since the terms of\\n\\n\\ninclude variables. The interpretation of variables is specified\\nby the following clause:\\n\\n\\n\\nThe interpretation of constants, connectives and application is as in \\n\\n.\\nThe denotation of the other expressions is discussed below.\\n\\n\\n  An Example of a Scopally and Lexically Underspecified Grammar \\n\\nThe following grammar extends the grammar discussed in section\\nlexamb_section by adding determiners and relations as new lexical\\nitems:\\n\\nand by adding  phrase structure rules for s and transitive verbs:\\n\\n\\n\\nThis grammar generates, in addition to lexically ambiguous sentences such as\\nKermit croaked, scopally ambiguous sentences such as Every\\ndog saw a frog.\\n\\n\\n  The Denotation of Logical Forms \\n\\nThe denotation of logical forms is specified using the storage method,\\ndeveloped by Robin Cooper Cooper, R.  as a way around a problem with\\nMontague's quantifying in technique, namely, the fact that in order\\nto get all the readings of a scopally ambiguous sentence, one has to stipulate\\n that the sentence is syntactically ambiguous (see ). \\n\\n\\nCooper proposed that the value of a syntactic tree is a set of\\nsequences, each sequence representing a distinct `order of\\napplication' of the operators that may result in a admissible interpretation of\\na sentence. For example, the quantifier a frog can `enter' the\\nderivation of the  saw a frog in two different ways. The narrow\\nscope reading is obtained by immediately applying the interpretation of the\\nquantifier to the translation of saw; but it is also possible to\\napply the predicate to the variable quantified over, and `wait' before applying\\nthe quantifier, in which case the wide scope reading is obtained. The value of\\nthe  a frog, then, is the set of two sequences shown in\\n. One sequence consists of a single element, the\\n`traditional' Montague-style translation of every frog. The second\\nsequence consists of two elements: the variable y, and the semantic\\ntranslation of the quantified , put `in storage'.\\n\\n\\n\\nAmbiguity `propagates up' as follows. The value of the  saw a\\nfrog in csex-a also consists of two sequences, one obtained by applying\\nthe first element of the first sequence in the denotation of every\\nfrog to the predicate saw, the other obtained by applying the predicate\\nsaw to the first element of the second sequence (the variable\\ny). The result is as in .\\n\\n\\n\\nFinally, the value  of a sentence is obtained by combining the value\\nof the  with the value of the  in the usual fashion: the value of\\nSEvery dog saw a frog is a set of two sequences, each\\nrepresenting a distinct reading of the sentence.\\n\\n\\nIt's easy to see that Cooper's technique can be used to assign to\\nunderspecified representations like  a `multiple sense'\\ndenotation like those assigned to lexically ambiguous expressions in the\\nprevious section. All that is needed is a function CV that assigns to each\\nexpression of the form XP its `Cooper Value'; the\\ndenotation of sentence translations like S can then be\\ndefined in terms of CV as follows:\\n\\n\\n\\n(I have taken into account the fact that an expression of our\\nunderspecified language denotes a set of objects, therefore each scopally\\ndisambiguated translation of a sentence will still denote a set of\\npropositions.)\\n\\n\\n Cooper discusses in detail in  how semantic and syntactic constraints on scope can be implemented as requirements that the storage be\\n`discharged' at certain positions--, that no element in storage be `carried\\nacross' syntactic constructions that produce scope islands, such as S. In\\nthis way, no operator in a clause may take scope over operators in an higher\\nclause, or in a sister clause, thus enforcing the Scope Constraint\\n discussed in syn_sem_constr_section. \\n\\n\\nThe CV function used to define the interpretation of logical forms is based on\\nan implementation of the storage idea less general than Cooper's, but simpler.\\nIn order to arrive at a uniform specification of the Cooper Value of all\\nconstructs, it is useful to define construct-specific versions of application\\nin which to `bury' the differences in storage manipulation. These operations\\nare defined as follows:\\n\\n\\n\\nNext, we need an operation that\\ncombines two sets of sequences into one. The result of applying this operation\\nto two sets of sequences X and Y is the set of sequences obtained by (typed)\\napplying the first element of a sequence in X to the first element of a\\nsequence in Y and then merging the rest of the sequences, as follows:\\n\\n\\n\\nWe also need an operation to put operators into store, and one to\\n`discharge' them. The  operation takes a set consisting a single\\nsingle-element sequence and a result, and returns a set that consists\\nof two sequences: the original sequence, and a new sequence consisting of the\\nresult and the operator in store.\\n\\n\\n\\nThe  operation takes a sequence and applies all operators back to\\nobtain a set of sequences with a single element and an empty store. For\\nsimplicity, we will assume that all operators are generalised quantifiers,\\ni.e., of type ett. (No other operators are specified in the\\ngrammar above.)  is defined as follows:\\n\\n\\n\\n\\n\\n[*](X), where X is a set of sequences, is the union \\n\\n\\n(x). We can now specify the Cooper value of logical forms with\\nrespect to model M, variable assignment g, and discourse situation d as\\n follows: \\n\\n\\n\\nThere are three tricky aspects to  the definition of CV: the discharge\\noperation in the definition of the Cooper Value of a sentence translation, the\\ndefinition of CV(NPDet N)\\nin which an operator is put in store, and the definition of\\nCV(VPV NP in which two\\nstores are combined, and that has different results depending on whether the\\n is of type e or is a quantifier. I'll illustrate these cases by\\nlooking at the main steps of the computation of the CV of :\\n\\n\\n\\nThe Scope Constraint is enforced by requiring a complete discharge\\nat the sentential level, which means no operators can `move up' outside the\\nsentence in which it occurs, although of course this couldn't occur in this\\ngrammar since it doesn't cover relative clauses, sentential complements or\\ncoordination.  I have assumed that discharge only takes place at sentential\\nlevel, i.e., there are no operators taking scope over s; doing this would\\ncomplicate matters a bit in that a `partial' discharge operation should be\\n defined. \\n\\n\\n  Lambda Abstracts \\n\\nSome care is required in the system developed here to get a semantics for\\nlambda-abstraction that preserves properties such as - and\\n-reduction.  The clause specifying the denotation of lambda-abstraction\\nin Dowty, Wall and Peters's book is the following:\\n\\n\\n\\nIf we generalize this clause in the `obvious'  way we get:\\n\\n\\n\\nLambda-abstraction defined in this way does not have the required\\n properties. To show that it does not preserve -reduction,  it is sufficient to consider the following example: let  = {s1,\\ns2},  = {a,b}, and let the\\nexpression  of type  have the following\\ndenotation:\\n\\n\\n\\nThen ()M,g{/a},d is as\\nfollows:\\n\\nand ()M,g{/b},dis as\\nfollows:\\n\\n\\n\\nThen, under the definition above, ()\\nwill contain the following function, that is not part of the denotation of\\n (hence, -reduction is not a sound inference rule):\\n\\n\\n\\nIntuitively, the problem with the definition above is that it does not\\n`preserve' the functions in the denotation of . A definition of\\nlambda-abstraction that does preserve these functions, and therefore preserves\\nthe soundness of - and -reduction, can be obtained as\\n follows. \\n\\n\\nThe denotation function  used so far assigns a value to\\nexpression  ME\\nin model M with respect to the parameters\\nof evaluation g and d,   \\n. Another way of specifying the value of expressions is to define\\na function  that assigns as value to  at discourse\\nsituation d a set of functions of type (Ass (\\n)), from assignments to functions in (\\n). For example, Dowty, Wall and Peters' clause for\\nlambda abstraction could be rewritten as follows:\\n\\n\\n\\nThis definition can then be generalized as follows:\\n\\n\\n\\nLambda-abstraction defined this way does support\\n -reduction.  Since this more general way of assigning a value is not needed to provide a semantics for the\\nother constructs of \\n\\n,\\nI will continue using a function\\n., but the reader should keep in mind that a denotation function of\\nthis form is needed to deal with lambda abstraction, hence, with\\nquantification. (And for referential ambiguity, as we will see below.)\\n\\n\\n  Quantification \\n\\nThe treatment of quantifiers in \\n\\n\\nis based on Generalized\\nQuantifiers Theory , i.e., the idea that determiners\\ndenote relations between two sets. The `restricted quantification' notation\\nused in the examples above is defined in terms of the two determiners\\nevery and a, as follows:\\n\\n\\n\\nA `single-valued'  semantics for\\nevery(,) could be\\ndefined, in first approximation, as in the following clause:\\n\\n\\n\\nThis definition can be generalized as follows into one that\\nworks in the case in which . is a set:\\n\\n\\n\\nThe interpretation of expressions of the form\\na(,) is defined in a\\nsimilar fashion, with the obvious semantics.\\n\\n\\n  Scope Disambiguation by Defeasible Inference \\n\\nHaving extended the language into one that can be used to describe scopal\\nunderspecification, the framework for discourse interpretation developed in\\nsection disc_int_section can also be used to formalize the inferences\\ninvolved in scope disambiguation.  Partially disambiguated interpretations can\\nbe represented by expressions which mix logical forms with `traditional'\\nexpressions, as done in . For example, one could formalize Ioup's\\n Grammatical Function Principle, stating that an  in\\nsubject position by default takes scope over s in other position, as\\nfollows:\\n\\n\\n\\nLogical forms in LFS are sentential expressions, and can therefore\\nserve as triggering condition of discourse interpretation principles. They can\\nalso occur embedded in other expressions of \\n\\n.\\nDuring the scope\\ndisambiguation process, `less ambiguous' expressions are inferred by deriving\\nexpressions such as\\nypySNPy\\nVP in which some quantifiers have been extracted, by a\\nprocess very similar to the one used in the top-down version of the \\nconstruction algorithm []. `Partial' scopal disambiguation is\\nthus represented by \\n\\n\\nexpressions which still contain logical\\nforms.\\n\\n\\nAs some readers will have already observed, the rule\\nGRAMMATICAL-FUNCTION-PRINCIPLE does not satisfy the Anti-Random restriction\\nproposed in disc_int_section: the rule does not contain a non-trivial\\nrestriction on the contexts in which it can operate. The already mentioned\\n proposal in  overcomes this problem by making the activation of scope disambiguation rules depend on whether the appropriate\\ndomain for the quantifier (its resource situation) has been\\nidentified; a presentation of that proposal would however require introducing\\ntoo much additional material.\\n\\n\\n\\n  Referential Ambiguity \\n  Referential Expressions as Cases of Semantic Ambiguity \\n\\nYet another way in which the semantics of sentences is `underspecified' by\\ntheir syntax is in the interpretation of anaphoric expressions and other\\nexpressions whose interpretation has to be fixed in context. In semantics,\\nreferential expressions are traditionally translated as free variables whose\\ninterpretation depends on the choice of an assignment function (for the cases\\nof deictic anaphora) or by assigning them the same variable bound by the\\nquantifier that serves as their antecedent (for the cases of bound\\nanaphora). This translation does capture the intuition that the truth\\nconditions of a sentence containing a referential expression can only be\\nevaluated after fixing the value of the referential expressions. It is also\\nclear, however, that distinct propositions are obtained depending on the value\\nassigned to these expressions, much as distinct propositions are obtained\\ndepending on the choice of an interpretation for lexical items, or of a scope\\nfor operators: in other words, a sentence which includes a referential\\nexpression is semantically ambiguous much in the way a sentence containing a\\n lexically ambiguous item is. \\n\\n\\nA complete discussion of reference interpretation would require introducing a\\nformalization of context, so I will only consider here the issue of providing\\nan underspecified treatment of intra-clausal and deictic anaphora. I propose\\nthat referential expressions are cases of semantic ambiguity, and translate\\ninto a special kind of underspecified object that I will call\\nparameters.  Semantically, a parameter is a type e expression\\nthat, in a discourse situation d, denotes a set of functions from\\nsituations to elements of e in d. For example, the pronoun\\nhe would translate into a parameter x which, in a discourse\\nsituation d with constituents a1 ...an,\\nand given the set  of situations, will denote a set of functions\\n{f1, ..., fm, ...} from situations in  to\\na1 ...an, including at least the set of all\\nconstant functions that map each situation s into aj if\\naj is a constituent of that situation (see below), and the set of\\nall variable denotations.  The reader will immediately realize that parameters\\nare the equivalent for type e expressions of `underspecified predicates'\\n like croakU introduced above. \\n\\n\\nMore formally, I propose to extend the set of terms of \\n\\n\\nwith a new\\nclass of parameters, whose interpretation is defined as follows. First of all,\\nlet us reformulate the semantics of variables given before, and make variables\\nfunctions from assignments to values (rather than the other way around). This\\ninvolves again using as interpretation function one that maps expressions into\\nfunctions from assignments to meanings, as done for lambda-abstracts.\\n\\n\\n\\nThis definition of the meaning of a variable allows us to abstract\\naway from assignments. We can now define the semantics of parameters as\\nfollows:\\n\\nFor example, if the subset of e in d consists of\\nthe two atoms j and b, then xe =\\n{f1,f2,...fi,...}, where f1, f2 etc. are the functions\\nthat may serve as the denotation of constants and variables--f1 is the\\nfunction that maps each situation of which j is a constituent into\\nj, f2 is the function that maps each situation of which\\nb is a constituent into b-- and the other functions\\nrepresent all the possible denotations of objects that the parameter may be\\nresolved to. Note that the discourse situations plays here the role played by\\nthe variable assignment in `free variable' theories of context dependence.\\n\\n\\nThe grammar presented in the previous section can be straightforwardly extended\\nas follows to generate sentences such as It croaked:\\n\\n\\n\\nThe definition of the interpretation of logical forms  given above\\nalready gives the correct results for these cases.\\n\\n\\n  Parameters and Discourse Interpretation \\n\\nReferential ambiguity gets `resolved' by anchoring a parameter. A\\nparameter is anchored if only one among the functions in its\\ndenotation results in a consistent interpretation of the set of sentences in\\nwhich the parameter occurs; a parameter can be anchored by means of equality\\nstatements of the form =xa, where a is not\\nparametric, or is already anchored: such equality statements make all but one\\nof the interpretations of the parameter inadmissible. Once a parameter is\\nanchored, it can be `replaced' by a term that denotes the one function among\\nthose in the interpretation of the parameter that does not result in an\\ninconsistent interpretation, much as in the previous discussion of lexical\\ndisambiguation, an H-type ambiguous predicate could be replaced by a\\ndisambiguated version. So, the discourse interpretation principles formalizing\\npronoun disambiguation involve a rewriting operation, just as the discourse\\ninterpretation principles formalizing lexical disambiguation.\\n\\n\\nAn apparent disadvantage of the present theory with respect to the `free\\nvariable' theory of context dependence is that we can derive from the latter\\nthat the value of referential expressions has to be fixed in order to get the\\nmeaning of the sentence in which they occur. A conversation is infelicitous\\nunless the referents of all pronouns and definite descriptions have been\\nidentified, the domain of quantification of all quantifiers has been\\nappropriately restricted, and so forth: so much so that listeners appear to be\\nready to accomodate new information (e.g., to introduce into the\\ndiscourse some otherwise unspecified antecedent for a pronoun) rather than\\nleave the interpretation unspecified .  But this\\nfact about referential expressions also follows if we treat context dependence\\nas a case of (H-type) semantic ambiguity; it is just a corollary of Pinkal's\\nprecisification imperative, from which I derived the Condition on Discourse\\nInterpretation in section lexamb_section. Accomodation procedures can\\nthen be seen as a way of `precisifying' in lack of sufficient information.\\n\\n\\n\\n  Syntactic Ambiguity \\n\\nThe one case of ambiguity that requires extending the framework introduced here\\nconsiderably is syntactic ambiguity, as in They saw her\\nduck. Furthermore, I haven't considered the problem of structural\\ndisambiguation in any detail.  I refer the interested readers to\\n , for a sketchy discussion of how to encode  encoding syntactic ambiguity in an underspecified representation. \\n\\n\\n\\n  Discussion \\n\\nI have suggested that to develop a theory of discourse interpretation that is\\nconsistent with what we know about the problem of ambiguity, we need to look\\nboth at the grammar and at discourse interpretation. I proposed a theory of\\ngrammar consistent with what I have called the Underspecification\\nHypothesis and which is not based on the assumption that all natural language\\nexpressions can be disambiguated; and a theory of discourse interpretation\\naccording to which a perceived ambiguity occurs when defeasible interpretation\\nprinciples result in conflicting hypothesis. The interpretation process is\\nsubject to two constraints: the Anti-Random Hypothesis (interpretations\\nare not generated at random) and the Condition on Discourse\\nInterpretation, derived from the Precisification Imperative (H-type\\nambiguity has to be resolved). Although treatments of disambiguation based on\\ndefeasible reasoning have been proposed elsewhere in the literature (e.g., in\\n ), I am not aware of any discussion of the characteristics of this inferential process, the consequences of reasoning with\\nan underspecified representation, or the need for constraints on the inference\\nrules.\\n\\n\\nIn the theory, semantic ambiguity is characterized model-theoretically in terms\\nof multiplicity of sense, whereas perceived ambiguity is characterized in terms\\nof inference. One may wonder if the distinction is really necessary; i.e., if\\nit is really the case that the meaning of natural language expressions can be\\nspecified a priori. Two arguments in favor of a distinction are that it\\nprovides for a clean distinction between the role of grammar and the role of\\ndiscourse interpretation; and that perceived ambiguity may also reflect\\nnon-semantic distinctions, e.g., distinctions in speech act interpretation;\\nthis question is not however totally resolved in the paper.\\n\\n\\nThere are two obvious directions in which the present model needs to extended:\\nto provide a model of syntactic ambiguity, and to account for the effect of\\nincrementality in sentence processing. Preliminary work in this direction is\\n discussed in . \\n\\n\\nAn issue that deserves further inspection is whether the formal similarity\\nbetween the system used here to assign a denotation to indefinite sentences,\\nand the systems developed by Hamblin for dealing with questions\\n  and by Rooth for its alternative semantics   has some significance. In particular, it would be interesting to explore the consequences of using parameters as the translation\\nof focused elements.\\n\\nBibliography \\n\\nAllen, J. F. 1987.\\nNatural Language Understanding.\\nMenlo Park, CA: Benjamin Cummings.\\n\\n\\nAllen, J. F., L. K. Schubert, G. Ferguson, P. Heeman, C. H. Hwang, T. Kato,\\n  M. Light, N. Martin, B. Miller, M. Poesio, and D. R. Traum.\\n1995.\\nThe TRAINS project: a case study in building a conversational\\n  planning agent.\\nJournal of Experimental and Theoretical AI 7:7-48.\\n\\n\\nAlshawi, H. (ed.). 1992.\\nThe Core Language Engine.\\nThe MIT Press.\\n\\n\\nAltmann, G. T. M. (ed.). 1989.\\nParsing and Interpretation.\\nHove, East Sussex, UK: Lawrence Erlbaum.\\n\\n\\nAltmann, G. T. M., and M. Steedman.\\n1988.\\nInteraction with Context during Human Sentence Processing.\\nCognition 30:191-238.\\n\\n\\nAsher, N., and M. Morreau. 1991.\\nCommon Sense Entailment: A Modal Theory of Commonsense Reasoning.\\nIn Proc. 12th IJCAI.\\n\\n\\nBarwise, J., and R. Cooper.\\n1981.\\nGeneralized Quantifiers and Natural Language.\\nLinguistics and Philosophy 4(2):159-219.\\n\\n\\nBarwise, J., and R. Cooper. 1993.\\nExtended Kamp Notation.\\nIn Situation Theory and its Applications, v.3, ed. P. Aczel,\\n  D. Israel, Y. Katagiri, and S. Peters.\\nChap. 2, 29-54.\\nCSLI.\\n\\n\\nBarwise, J., and J. Perry. 1983.\\nSituations and Attitudes.\\nCambridge, MA: MIT Press, Cambridge Mass.\\n\\n\\nCharniak, E., and R. P. Goldman. 1988.\\nA Logic for Semantic Interpretation.\\nIn Proc. ACL-88, 87-94.\\nBuffalo, NY.\\n\\n\\nChierchia, G., and S. McConnell-Ginet. 1990.\\nMeaning and Grammar: An Introduction to Semantics.\\nCambridge, MA: The MIT Press.\\n\\n\\nChomsky, N. 1981.\\nLectures on Government and Binding.\\nDordrecht: Foris.\\n\\n\\nCooper, R. 1983.\\nQuantification and Syntactic Theory.\\nDordrecht, Holland: D. Reidel Publishing Company.\\n\\n\\nCrain, S., and M. Steedman. 1985.\\nOn not being led up the garden path: the use of context by the\\n  psychological syntax processor.\\nIn Natural Language Parsing: Psychological, Computational and\\n  Theoretical perspectives, ed. D. R. Dowty, L. Karttunen, and A. M. Zwicky.\\n320-358.\\nNew York: Cambridge University Press.\\n\\n\\nCrouch, R. 1995.\\nEllipsis and Quantification: A Substitutional Approach.\\nIn Proceedings 7th Conference of the European Chapter of the\\n  Association for Computational Linguistics, 229-236.\\nDublin. Dublin City University.\\n\\n\\nDalrymple, M., S. M. Shieber, and F. C. N. Pereira.\\n1991.\\nEllipsis and Higher-Order Unification.\\nLinguistics and Philosophy 14(4):399-452.\\n\\n\\nDowty, D. R., R. E. Wall, and S. Peters. 1981.\\nIntroduction to Montague Semantics.\\nDordrecht, Holland: D. Reidel.\\n\\n\\nFenstad, J.E., P.K. Halvorsen, T. Langholm, and J. van Benthem. 1987.\\nSituations, Language and Logic.\\nDordrecht: D.Reidel.\\n\\n\\nFine, K.\\n1975.\\nVagueness, Truth, and Logic.\\nSynthese 30:265-300.\\n\\n\\nFrazier, L., and J. D. Fodor.\\n1978.\\nThe sausage machine: A new two-stage parsing model.\\nCognition 6:291-295.\\n\\n\\nGawron, J. M., and S. Peters. 1990.\\nAnaphora and Quantification in Situation Semantics.\\nLecture Notes, Vol. 19.\\nCSLI.\\n\\n\\nGibson, E. 1991.\\nA Computational Theory of human linguistic processing: memory\\n  limitations and processing breakdown.\\nDoctoral dissertation, Carnegie Mellon University, Pittsburgh.\\n\\n\\nGillon, B.\\n1990.\\nAmbiguity, generality, and indeterminacy: Tests and definitions.\\nSynthese 85:391-416.\\n\\n\\nGrosz, B.J., A.K. Joshi, and S. Weinstein. 1983.\\nProviding a Unified Account of Definite Noun Phrases in Discourse.\\nIn Proc. ACL-83, 44-50.\\n\\n\\nHaegeman, L. 1991.\\nAn Introduction to Government and Binding Theory.\\nBasil Blackwell.\\nFirst edition.\\n\\n\\nHamblin, C.\\n1973.\\nQuestions in Montague English.\\nFoundations of Language 10:41-53.\\n\\n\\nHeim, I. 1982.\\nThe Semantics of Definite and Indefinite Noun Phrases.\\nDoctoral dissertation, University of Massachusetts at Amherst.\\n\\n\\nHirst, G. 1987.\\nSemantic Interpretation and the Resolution of Ambiguity.\\nStudies in Natural Language Processing.\\nCambridge, UK: Cambridge University Press.\\n\\n\\nHobbs, J. R. 1983.\\nAn Improper Treatment of Quantification in Ordinary English.\\nIn Proc. ACL-83, 57-63.\\nCambridge, MA, June.\\n\\n\\nHobbs, J. R., and S. M. Shieber.\\n1987.\\nAn Algorithm for Generating Quantifier Scopings.\\nComputational Linguistics 13(1-2):47-63.\\n\\n\\nHobbs, J. R., M. Stickel, P. Martin, and D. Edwards. 1990.\\nInterpretation as Abduction.\\nTechnical Note 499.\\nMenlo Park, CA: SRI International, December.\\n\\n\\nHwang, C. H., and L. K. Schubert. 1993.\\nEpisodic Logic: A Situational Logic for Natural Language Processing.\\nIn Situation Theory and its Applications, v.3, ed. P. Aczel,\\n  D. Israel, Y. Katagiri, and S. Peters.\\n303-338.\\nCSLI.\\n\\n\\nIoup, G. 1975.\\nSome Universals for Quantifier Scope.\\nIn Syntax and Semantics 4, ed. J. Kimball.\\n37-58.\\nNew York: Academic Press.\\n\\n\\nKamp, H., and U. Reyle. 1993.\\nFrom Discourse to Logic.\\nDordrecht: D. Reidel.\\n\\n\\nKaplan, D. 1977.\\nDemonstratives. An Essay on the Semantics, Logic, Metaphysics and\\n  Epistemology of Demonstratives and Other indexicals.\\nUnpublished manuscript, University of California, Los Angeles.\\n\\n\\nKeller, W. R. 1988.\\nNested Cooper Storage: The Proper Treatment of Quantification in\\n  Ordinary Noun Phrases.\\nIn Natural Language Parsing and Linguistic Theories, ed.   U. Reyle and C. Rohrer.\\n432-447.\\nDordrecht: D. Reidel.\\n\\n\\nKempson, R., and A. Cormack.\\n1981.\\nAmbiguity and Quantification.\\nLinguistics and Philosophy 4(2):259-310.\\n\\n\\nKurtzman, H. 1985.\\nStudies in Syntactic Ambiguity Resolution.\\nDoctoral dissertation, MIT, Cambridge, MA.\\n\\n\\nKurtzman, H. S., and M. C. MacDonald.\\n1993.\\nResolution of Quantifier Scope Ambiguities.\\nCognition 48:243-279.\\n\\n\\nLakoff, G. P.\\n1970.\\nA note on vagueness and ambiguity.\\nLinguistic Inquiry 1(3):357-359.\\n\\n\\nLascarides, A., N. Asher, and J. Oberlander. 1992.\\nInferring Discourse Relations in Context.\\nIn Proc. ACL-92, 1-8.\\nUniversity of Delaware.\\n\\n\\nLewis, D. K.\\n1979.\\nScorekeeping in a language game.\\nJournal of Philosophical Logic 8:339-359.\\n\\n\\nMay, R. 1985.\\nLogical Form in Natural Language.\\nThe MIT Press.\\n\\n\\nMontague, R.\\n1970.\\nUniversal Grammar.\\nTheoria 36:373-398.\\n Reprinted in . \\n\\n\\nPartee, B. H., and M. Rooth. 1983.\\nGeneralized Conjunction and Type Ambiguity.\\nIn Meaning, Use and Interpretation of Language, ed.   R. Bauerle, C. Schwarze, and A. von Stechow.\\nBerlin, West Germany: Walter de Gruyter.\\n\\n\\nPereira, F. C. N.\\n1990.\\nCategorial Semantics and Scoping.\\nComputational Linguistics 16(1):1-10.\\n\\n\\nPereira, F. C. N., and M. E. Pollack.\\n1991.\\nIncremental Interpretation.\\nArtificial Intelligence 50:37-82.\\n\\n\\nPinkal, M. 1985.\\nLogik und Lexikon: Die Semantik des Unbestimmten.\\nBerlin: de Gruyter.\\n\\n\\nPinkal, M. 1995.\\nLogic and Lexicon.\\nLondon: Oxford.\\n\\n\\nPoesio, M. 1991.\\nRelational Semantics and Scope Ambiguity.\\nIn Situation Semantics and its Applications, vol.2, ed.   J. Barwise, J. M. Gawron, G. Plotkin, and S. Tutiya.\\nChap. 20, 469-497.\\nStanford, CA: CSLI.\\n\\n\\nPoesio, M. 1994.\\nDiscourse Interpretation and the Scope of Operators.\\nDoctoral dissertation, University of Rochester, Department of\\n  Computer Science, Rochester, NY.\\n\\n\\nPoesio, M. 1995.\\nA Model of Conversation Processing Based on Micro Conversational\\n  Events.\\nIn Proceedings of the Annual Meeting of the Cognitive Science\\n  Society.\\nPittsburgh.\\n\\n\\nRaskin, V. 1985.\\nSemantic Mechanisms of Humor.\\nDordrecht and Boston: D. Reidel.\\n\\n\\nReiter, R.\\n1980.\\nA Logic for Default Reasoning.\\nArtificial Intelligence 13(1-2):81-132.\\n\\n\\nReyle, U.\\n1993.\\nDealing with ambiguities by underspecification: Construction,\\n  Representation and Deduction.\\nJournal of Semantics 3.\\n\\n\\nRodman, R. 1976.\\nScope Phenomena, ``Movement Transformations,'' and Relative Clauses.\\nIn Montague Grammar, ed. Barbara Partee.\\n165-176.\\nAcademic Press.\\n\\n\\nRooth, M. 1985.\\nAssociation with Focus.\\nDoctoral dissertation, University of Massachusetts, Amherst.\\n\\n\\nSchubert, L. K. 1986.\\nAre There Preference Trade-Offs in Attachment Decisions.\\nIn Proceedings of the Fifth National Conference on Artificial\\n  Intelligence, 601-605.\\nPhiladelphia, Pennsylvania, August. American Association for\\n  Artificial Intelligence.\\n\\n\\nSchubert, L. K., and F. J. Pelletier.\\n1982.\\nFrom English to Logic: Context-Free Computation of 'Conventional'\\n  Logical Translations.\\nAmerican Journal of Computational Linguistics 10:165-176.\\n\\n\\nSpivey-Knowlton, M., J. Sedivy, K. Eberhard, and M. Tanenhaus. 1994.\\nPsycholinguistic Study of the Interaction between Language and\\n  Vision.\\nIn Proceedings of 12th National Conference on Artificial\\n  Intelligence (AAAI-94).\\nSeattle.\\n\\n\\nStallard, D. 1987.\\nThe Logical Analysis of Lexical Ambiguity.\\nIn Proceedings of the 25th Meeting of the ACL, 179-185.\\n\\n\\nSu, S. P. 1994.\\nLexical Ambiguity in Poetry.\\nLondon: Longman.\\n\\n\\nSwinney, D. A.\\n1979.\\nLexical Access During Sentence Comprehension: (Re)consideration of\\n  Context Effects.\\nJournal of Verbal Learning and Verbal Behavior 18:545-567.\\n\\n\\nThomason, R. H. (ed.). 1974.\\nFormal Philosophy: Selected Papers of Richard Montague.\\nNew York: Yale University Press.\\n\\n\\nTurner, R. 1992.\\nProperties, propositions and semantic theory.\\nIn Computational Linguistics and Formal Semantics, ed.   M. Rosner and R. Johnson.\\nCUP, Cambridge.\\n\\n\\nvan Deemter, K. 1991.\\nOn the Composition of Meaning.\\nDoctoral dissertation, University of Amsterdam.\\n\\n\\nVerkuyl, H. J. 1992.\\nSome Issues in the Analysis of Multiple Quantification with Plural\\n  NPs.\\nOTS Working Papers OTS-WP-TL-92-005.\\nThe Netherlands: University of Utrecht, Research Institute for\\n  Language and Speech.\\nTo appear in F. Hamm and E. Hinrichs, editors, Plural\\n  Quantification, Kluwer.\\n\\n\\nZwicky, A., and J. Sadock. 1975.\\nAmbiguity Tests and How to Fail Them.\\nIn Syntax and Semantics 4, ed. J. Kimball.\\n1-36.\\nNew York: Academic Press.\\n\\nFootnotes\\n\\n  This paper will appear\\nin K. van Deemter and S. Peters (eds), Semantic Ambiguity and\\nUnderspecification, CSLI.\\n  With current technology, the real problem is not so\\nmuch the size of the search space, but how to choose among these\\ninterpretations, most of which are plausible.\\n  An early attempt\\nat a model of discourse interpretation of this kind was made by Hobbs, e.g.,\\n . \\n  A recent example of work also\\nattempting to exploit the properties of underspecified representations to\\n address linguistic questions is . \\n  An example of these tests are the identity tests,\\none of which is the conjunction test. The (presumed) ambiguity of\\nsentences such as They say her duck derives from the fact that the\\nphrase her duck can either be a NP or a bare infinitival\\ncomplement. The sentence They saw her swallow should have a similar\\nambiguity. If these two sentences were really ambiguous as claimed, a sentence\\nsuch as They saw her swallow and her duck should only have two\\nreadings instead of four since conjunction requires its two arguments to be of\\nthe same type, and therefore the 'crossed' readings should not be\\navailable. This is indeed the case. On the other hand, an indeterminate\\nsentence such as My sister is the Ruritanian secretary of state\\nmaintains all of its indeterminateness once conjoined, as in My\\nsister is the Ruritanian secretary of state and a prominent composer.\\n  For example, Zwicky\\nand Sadock observed that such tests can only identify polar\\nambiguities (such as the ambiguity of game between two entirely\\nindependent readings), but not privative ambiguities, like those\\ndisplayed by a term like dog which can be used both to indicate\\ngeneric individual of the Canis species and a male element of it.\\n  A sentence is indeterminate, or\\nunspecified, if it is definitely true or false, but it could be made\\nmore specific.  Zwicky and Sadock  bring the\\nexample of the sentence My sister is the Ruritanian secretary of\\nstate, which is indeterminate as to whether ``...my sister is older or\\nyounger than I am, whether she acceeded to her post recently or some time ago,\\nwhether the post is hers by birth or by merit,'' and so forth. The point is\\nthat these additional facts do not affect the truth value of the sentence. It\\nhardly needs to be pointed out that just about every sentence is indeterminate\\n/ unspecific in some respects. I will use the term indeterminate for these\\nsentences, and reserve the term underspecified for sentences which\\nmay have different truth values depending on the way the facts are `filled in'\\n(see below).\\n  For a discussion of these assumptions,\\n see , , or chapter 2 of . \\n  In the case of attitude reports, the problem is to make sure\\nthat if John is tall and John is stupid are both true in\\na model, Bill believes that John is tall does not entail\\nBill believes that John is stupid in that model, assuming that\\npropositions are the semantic correlate of sentential complements.\\n  More complex notions of\\npropositions have been introduced in the literature on propositional attitudes,\\nsuch as those used in Situation Semantics  or Property\\nTheory .\\n  Another way of providing a precise definition of\\nambiguity has been explored in the literature, that we might call structural or syntactic. An example of structural definition is the\\n following, from Gillon (, p. 400): \\n\\n\\nAmbiguity (Gillon)\\n: An expression is ambiguous iff the\\nexpression can accomodate more than one structural analysis.\\nThis definition relies on the assumption made in transformational theories of\\ngrammar such as Government and Binding theory\\n, that each interpretation of a sentence is a\\nquadruple PF,DS,SS,LF,\\neach of whose elements is a structured\\nobject: PF characterizes the phonetic interpretation, DS the predicate/argument\\ncomposition of the sentence, SS its surface syntactic analysis, and LF its\\nlogical analysis.  A sentence (string) is ambiguous iff it can be characterized\\nby distinct quadruples, and this may happen not only for phonetical or\\nsyntactic reasons, but also for semantical reasons, since one of the components\\nof a structural characterization of a sentence, the `LF,' encodes the semantic\\ninterpretation of the sentence.\\n\\n\\nThe problem with this definition is that a purely syntactic analysis of meaning\\nintroduces spurious distinctions: for example, unless something is said about\\ninvariance under renaming of variables, one would predict from a structural\\ndefinition that even a sentence with a single quantifier such as\\nEvery man left is infinitely ambiguous, because all structures of\\nthe form SNPevery x manxleft, for any\\nchoice of the variable, are appropriate (and distinct) LF constituents of a\\nsentence's interpretation. Another case of spurious ambiguity is discussed\\nbelow.  A semantic characterization of ambiguity avoids this problem.\\n  A treatment of ambiguity and vagueness in\\n terms of precisifications was proposed early on in . \\n  Pereira\\n argues that this constraint is best formulated as a\\ncondition on semantic derivations rather than as a condition on the syntax of\\nlogical expressions.\\n  This is one of the reasons for preferring a semantic account\\nof ambiguity to a syntactic account which makes ambiguity depend on the\\nexistence of two distinct logical forms.\\n  A cute example of the problems with the theory\\n is presented by  . If ambiguous sentences were to denote the disjunction of their readings, then the answer to the question\\nDoes the butcher have kidneys? should always be 'yes'.\\n  Such sentences are used, for example, to\\nget a 'baseline' interpretation in psychological work on ambiguity.\\n  The results about lexical disambiguation are fairly\\nwell established, but there is some controversy about syntactic processing. A\\nconstrasting view on syntactic disambiguation is discussed in\\n[].\\n  In addition to the\\nreferences above see [].\\n  The technique of assigning sets of senses as the denotation\\nof sentences dates back at least to Hamblin \\nwho used it to extend Montague's fragment to questions.\\n  A lexical item can also be\\nambiguous in that it may be associated with lexical entries of different\\nsyntactic categories: for example, the word duck can either be\\ninterpreted as a noun or as an verb, as shown by the example They saw\\nher duck. I assume a syntactic ambiguity in these cases, i.e., I assume that\\nthe grammar would assign two syntactic analyses to the word duck,\\neach of which would then get an interpretation in \\n\\n.\\n  It is perhaps worth emphasizing a\\ndifference between the semantics just sketched and virtually all other\\napproaches to underspecification I am aware of. In this proposal, the\\nunderspecified language \\n\\n\\ndoes not serve as a `meta-language' to be\\ngiven a semantics in terms of the values assigned to the expressions of a\\n`disambiguated language'; instead, it has a semantics of its own, defined\\nbottom-up much in the way the semantics of \\nwould be defined. In\\nother words, the approach just sketched does not rely on the assumption that a\\n`disambiguated language' can be defined, which, at the light of Pinkal's\\ntreatment of indefiniteness, appears to be questionable. For example, for\\nPinkal an expression is `purely vague' is no natural precisification exists.\\n  I will ignore in what follows the issue of partiality,\\ne.g., what happens when a conjunct has a value other than 0 or 1.\\n  The denotation assigned to an indefinite sentence by the\\ngrammar above is a simplification of the denotation that Pinkal would assign to\\n such a sentence in , which, in addition the set of senses associated with a natural language expression, would also include a partial\\norder relation of precisification between them. Such an order relation plays an\\nimportant role in the meaning of vague sentences such as Kermit is\\ntall is considered, which has distinct senses depending on the degree of\\nprecision with which the discourse situation is specified, but a less important\\none in the cases of `narrow sense' ambiguity with which I am concerned here.\\n  CROAK1-IF-FROG is an open inference\\nrule. Such rules act like inference rule schemas.\\n  See the discussion below.\\n  I.e., of the form :/ .\\n  See however\\n . \\n  In a sentence such as\\nJohn gave a present to each child, for example, the indefinite\\na present takes narrow scope with respect to the quantifier\\neach child. The interpretation of the sentence must therefore either\\nremain partially underspecified until the quantifier is processed, or be\\nrevised when the quantifier is encountered. Similarly, when processing a\\nsentence such as John always invites MARY to the movies, whose\\npreferred interpretation is that whenever John goes to the movies, he invites\\nMary, the restriction of the adverb of quantification always is not\\nencountered until the PP to the movies is encountered.\\n\\n\\n There is little doubt that part of the solution to the Combinatorial\\nExplosion Puzzle is that some forms of ambiguity, at least, are solved locally\\nand incrementally. Garden-path phenomena, for example, are commonly interpreted\\nas providing evidence for this hypothesis\\n  [,,, and,\\nas discussed above, similar effects can exploit forms of ambiguity other than\\nsyntactic ambiguity: e.g., scopal ambiguity. The examples just discussed\\nsuggest however that an incremental account of discourse interpretation, as\\nwell, must be supplemented with a theory of underspecification.\\n  Strictly speaking, one should check that  does\\nnot occur in the extension itself, not in the intermediate sets of wffs; this\\nform makes sense however once we adopt a `syntactic' definition of extension\\n(see below).\\n  Both\\nthe treatment of scopal ambiguity and the treatment of referential ambiguity\\nproposed below are such that the constituents that introduce H-type ambiguity\\ncan be identified syntactically.\\n  A closed default theory is one in which no default contains\\nopen variables. All really interesting cases of default inference rules do\\ninclude such variables; but Reiter derives the definition of the extension of\\nan `open' default theory from the definition of extension for closed theories.\\n  `Ambiguity elimination' solutions to the combinatorial\\nexplosion puzzle, such as Kempson and Cormack's \\nor Verkuyl's  have had some success in showing that\\ncertain cases of `ambiguity'--especially `ambiguities' associated with plural\\nnoun phrases or certain classes of scopal ambiguities--are in fact cases of\\nindeterminacy.  Zwicky and Sadock noted that the identity tests do not classify\\na sentence as ambiguous if the propositions expressed by the sentence are such\\nthat one entails the other. This is the case, for instance, with sentences such\\nas every-kid\\n ,,]. \\n\\n\\n\\n[] claim that\\nsentences like  are not ambiguous, but indeterminate: according to\\nthem, such sentences semantically denote the weaker reading (the one in which\\nthe universal quantifier takes scope over the existential).  The stronger\\nreading is the result of pragmatic reasoning.\\n\\nKempson and Cormack propose in fact that all quantified sentences denote a\\nsingle proposition; in this way, the combinatorial explosion puzzle\\ndisappears, at least as far as scopal ambiguity is concerned. However, it is\\nnot true in general that a sentence with two quantifiers has two\\ninterpretations, one of which entails the other.   does not\\nhave an interpretation weak enough to be entailed by all others, yet able to\\ncapture the truth conditions correctly.\\n\\n\\n\\nA second  problem with the proposal  of Kempson and Cormack is that if one\\nwants to claim that the meaning of a sentences such as  is\\nsomething like , as Kempson and Cormack do, then one ends up\\npredicting that the meaning of  should be something like\\n, the strongest interpretation of the sentence. In other\\nwords, one either has to give up compositionality for sentences like\\n, or to abandon the strategy of letting sentences\\ndenote their weakest interpretation .\\n\\n\\n\\nIt should also be clear that whatever the case for scopal ambiguity, other\\nkinds of ambiguity, such as structural and H-type lexical ambiguity, cannot be\\nreduced to indeterminacy.\\n  The definition of\\nstorage generates spurious readings in the case of embedded NPs such as\\na representative of every company, that have to be eliminated via a\\nseparated filter . Keller introduced a\\n`nesting' technique that obviates the problem\\n.  More recently, Pereira\\n  argued that the right scoping properties can be obtained without additional stipulations from the natural deduction approach to parsing.\\nI only consider here `basic' NPs that do not create problems for the simplest\\nversion of Cooper's technique.\\n  Some arguments for VP scope are discussed in\\n ,]. \\n  Strictly speaking, the form CV()(M,g,d) should be\\nused. I omit the indices below.\\n  I.e.,\\nthat () .\\n  I wish to thank an anonymous reviewer for suggesting this\\nsolution to the problem just discussed, much simpler than the solution proposed\\n in . \\n  The proof is as\\nfollows. () = {f (Ass \\n(( ))) such\\nthat for all g Ass, s , a in\\n, and for some m ()M,\\nf(g)(s)(a) = m(g{/a})(s). }\\nBecause of the definition of (), this is the set of\\nfunctions f such that f(g)(s)(a) =\\np(g{/a})(s)[q(g{/a})(s)],\\nfor some p M and some q in M, i.e., of the\\nfunctions which occur in M since\\nq(g{/a})(s) = a.\\n  Pinkal takes pretty much the same\\n position in . He also introduces a distinction there between a `speaker-oriented' perspective on meaning versus a `hearer-oriented'\\nperspective. A speaker may well have a single interpretation in mind for a\\nparticular anaphoric expressions, but the hearer may have to recover this\\ninterpretation among the many that are possible in that particular\\ncontext. This is of course true of all kinds of ambiguities, also those which\\nhave a pragmatic rather than a semantic nature, but in the case of referential\\nambiguity, the alternative interpretations correspond to distinct propositions\\nin the semantic sense as well.\\n  The term `parameter' comes\\nfrom Situation Semantics (e.g., []), where the lexical\\nitems whose interpretation depends on context are called parametric,\\nin the sense that their interpretation depends on the value assigned in context\\nto one or more parameters. Parameters are also used in situation theory to\\ntranslate pronouns and other anaphoric expression; but although the name and\\nthe `dotted' notation is preserved here, the parameters I have just introduced\\nare an entirely different type of objects than the parameters of situation\\ntheory, which are a special sort of objects in the universe, entirely distinct\\nfrom individuals.\\n  Most\\nsystems making use of underspecified representations perform structural\\ndisambiguation independently from the other forms of disambiguation\\n [,,,. There is evidence, however, that structural disambiguation interacts at least with\\nreference interpretation [,] and a lot\\nof the recent work on statistical parsing relies on the hypothesis that lexical\\ninterpretation affects parsing as well. Nothing in the proposal relies on\\nstructural disambiguation occurring prior to the other stages of\\ndisambiguation.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nI explore some of the issues that arise when trying to establish a connection\\nbetween the underspecification hypothesis pursued in the NLP literature and\\nwork on ambiguity in semantics and in the psychological literature.  A theory\\nof underspecification is developed `from the first principles', i.e., starting\\nfrom a definition of what it means for a sentence to be semantically ambiguous\\nand from what we know about the way humans deal with ambiguity. An\\nunderspecified language is specified as the translation language of a grammar\\ncovering sentences that display three classes of semantic ambiguity: lexical\\nambiguity, scopal ambiguity, and referential ambiguity. The expressions of this\\nlanguage denote sets of senses. A formalization of defeasible reasoning with\\nunderspecified representations is presented, based on Default Logic.  Some\\nissues to be confronted by such a formalization are discussed.\\n\\n\"],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nWe define a new grammar formalism, called D-Tree Grammars (DTG), which\\n arises from work on Tree-Adjoining Grammars (TAG) .  A salient feature of TAG is the extended domain of locality it\\nprovides.  Each elementary structure can be associated with a lexical\\nitem (as in Lexicalized TAG (LTAG) []).\\nProperties related to the lexical item (such as subcategorization,\\nagreement, certain types of word order variation) can be expressed\\n within the elementary structure ,.  In addition, TAG remain tractable, yet their generative capacity is sufficient to account\\nfor certain syntactic phenomena that, it has been argued, lie beyond\\n Context-Free Grammars (CFG) .  TAG, however, has two limitations which provide the motivation for this work. The first\\n problem (discussed in Section ) is that the TAG operations of substitution and adjunction do not map cleanly onto the\\nrelations of complementation and modification. A second problem\\n (discussed in Section ) has to do with the inability of TAG to provide analyses for certain syntactic phenomena.  In\\ndeveloping DTG we have tried to overcome these problems while remaining\\nfaithful to what we see as the key advantages of TAG (in particular, its\\n enlarged domain of locality). In Section  we introduce some of the key features of DTG and explain how they are intended to address\\nthe problems that we have identified with TAG.\\n\\n    Derivations and Dependencies\\n\\n\\nIn LTAG, the operations of substitution and adjunction relate two\\nlexical items.  It is therefore natural to interpret these operations as\\nestablishing a direct linguistic relation between the two lexical items,\\nnamely a relation of complementation (predicate-argument relation) or of\\nmodification.  In purely CFG-based approaches, these relations are\\nonly implicit.  However, they represent important linguistic intuition,\\nthey provide a uniform interface to semantics, and they are, as Schabes\\n Shieber  argue, important in order to support\\nstatistical parameters in stochastic frameworks and appropriate\\nadjunction constraints in TAG.  In many frameworks, complementation\\nand modification are in fact made explicit:\\nLFG [] provides a separate functional (f-)\\nstructure, and dependency grammars (see e.g. Mel'cuk (1988)) use\\nthese notions as the principal basis for syntactic representation.  We\\nwill follow the dependency literature in referring to complementation\\nand modification as syntactic dependency.  As observed by Rambow and\\nJoshi , for TAG, the importance of the\\ndependency structure means that not only the derived phrase-structure\\ntree is of interest, but also the operations by\\nwhich we obtained it from elementary structures.\\nThis information is encoded in the\\n derivation tree . \\n\\n\\nHowever, as Vijay-Shanker  observes, the TAG\\ncomposition operations are not used uniformly: while substitution is\\nused only to add a (nominal) complement, adjunction is used both for\\nmodification and (clausal) complementation. Clausal complementation\\ncould not be handled uniformly by substitution because of the existence\\nof syntactic phenomena such as long-distance wh-movement in\\nEnglish.  Furthermore, there is an inconsistency in the directionality\\nof the operations used for complementation in TAG@: nominal\\ncomplements are substituted into their governing verb's tree, while the\\ngoverning verb's tree is adjoined into its own clausal complement.  The\\nfact that adjunction and substitution are used in a linguistically\\nheterogeneous manner means that (standard) TAG derivation trees do\\nnot provide a good representation of the dependencies between the words\\nof the sentence, i.e., of the predicate-argument and modification\\nstructure.\\n\\n\\n For instance, English sentence () gets the derivation structure shown on the left in\\n Figure .   \\nWhen comparing this derivation structure to the dependency structure in\\n Figure , the following problems become apparent. First, both adjectives depend on hotdog, while in the derivation\\nstructure small is a daughter of spicy. In addition,\\nseem depends on claim (as does its nominal argument, he), and adore depends on seem. In the derivation\\nstructure, seem is a daughter of adore (the direction does\\nnot express the actual dependency), and claim is also a daughter\\nof adore (though neither is an argument of the other).\\n\\n\\nSchabes  Shieber  solve the first problem by\\ndistinguishing between the adjunction of modifiers and of clausal\\ncomplements.  This gives us the derivation structure shown on the right\\n in Figure .  While this might provide a satisfactory treatment of modification at the derivation level, there are now three\\ntypes of operations (two adjunctions and substitution) for two types of\\ndependencies (arguments and modifiers), and the directionality problem\\nfor embedded clauses remains unsolved.\\n\\n\\nIn defining DTG we have attempted to resolve these problems with the use\\nof a single operation (that we call subsertion) for handling all\\ncomplementation and a second operation (called sister-adjunction)\\nfor modification.  Before discussion these operations further we\\nconsider a second problem with TAG that has implications for the design\\nof these new composition operations (in particular, subsertion).\\n\\n\\n    Problematic Constructions for TAG\\n\\n\\nTAG cannot be used to provide suitable analyses for certain syntactic\\n phenomena, including long-distance scrambling in German ,  Romance Clitics , wh-extraction out of  complex picture-NPs , and Kashmiri wh-extraction (presented here).  The problem in describing these phenomena with\\nTAG arises from the fact (observed by Vijay-Shanker )\\nthat adjoining is an overly restricted way of combining structures.  We\\nillustrate the problem by considering Kashmiri wh-extraction,\\ndrawing on Bhatt .  Wh-extraction in\\nKashmiri proceeds as in English, except that the wh-word ends up\\nin sentence-second position, with a topic from the matrix clause in\\n sentence-initial position.  This is illustrated in ()  for a simple clause and in () for a complex clause. \\n\\n\\n\\n\\n\\nSince the moved element does not appear in sentence-initial position,\\nthe TAG analysis of English wh-extraction of\\nKroch  (in which the matrix clause is adjoined\\ninto the embedded clause) cannot be transferred, and in fact no\\nlinguistically plausible TAG analysis appears to be available.\\n\\n\\nIn the past, variants of TAG have been developed to extend the range of\\n possible analyses.  In Multi-Component TAG (MCTAG) , trees are grouped into sets which must be adjoined together (multicomponent\\nadjunction). However, MCTAG lack expressive power since, while syntactic\\nrelations are invariably subject to c-command or dominance constraints,\\nthere is no way to state that two trees from a set must be in a\\ndominance relation in the derived tree.  MCTAG with Domination\\n Links (MCTAGDL)  are multicomponent systems that allow for the expression of dominance constraints.  However, MCTAGDL share a\\nfurther problem with MCTAG: the derivation structures cannot be given a\\nlinguistically meaningful interpretation. Thus, they fail to address the\\n first problem we discussed (in Section ). \\n\\n\\n    The DTG Approach\\n\\n\\nVijay-Shanker  points out that use of adjunction for\\nclausal complementation in TAG corresponds, at the level of dependency\\n structure, to substitution at the foot node of the adjoined tree. However, adjunction (rather than substitution) is used since, in general,\\nthe structure that is substituted may only form part of the clausal\\ncomplement: the remaining substructure of the clausal complement appears\\nabove the root of the adjoined tree.  Unfortunately, as seen in the\\n examples given in Section , there are cases where satisfactory analyses cannot be obtained with adjunction. In particular,\\nusing adjunction in this way cannot handle cases in which parts of the\\nclausal complement are required to be placed within the structure of the\\nadjoined tree.\\n\\n\\nThe DTG operation of subsertion is designed to overcome this limitation.\\nSubsertion can be viewed as a generalization of adjunction in which\\ncomponents of the clausal complement (the subserted structure) which are\\nnot substituted can be interspersed within the structure that is the\\n site of the subsertion. Following earlier work ,, DTG provide a mechanism involving the use of domination links (d-edges) that ensure\\nthat parts of the subserted structure that are not\\nsubstituted dominate those parts that are. Furthermore, there is a need\\nto constrain the way in which the non-substituted components can be\\n interspersed. This is done by either using appropriate feature constraints at nodes or by means of\\n subsertion-insertion constraints (see Section ). \\n\\n\\nWe end this section by briefly commenting on the other DTG operation\\nof sister-adjunction. In TAG, modification is performed with adjunction\\nof modifier trees that have a highly constrained form. In particular,\\nthe foot nodes of these trees are always daughters of the root and\\neither the leftmost or rightmost frontier nodes. The effect of adjoining\\na tree of this form corresponds (almost) exactly to the addition of a\\nnew (leftmost or rightmost) subtree below the node that was the site of\\nthe adjunction.  For this reason, we have equipped DTG with an\\noperation (sister-adjunction) that does exactly this and nothing\\n more. From the definition of DTG in Section  it can be seen that the essential aspects of Schabes  Shieber \\ntreatment for modification, including multiple modifications of a\\nphrase, can be captured by using this\\n operation. \\n\\n\\n After defining DTG in Section , we discuss, in  Section , DTG analyses for the English and Kashmiri  data presented in this section. Section  briefly discusses DTG recognition algorithms.\\n\\n\\n\\n    Definition of D-Tree Grammars\\n\\n\\nA d-tree is a tree with two types of edges: domination edges (d-edges) and immediate domination edges (i-edges).  D-edges and\\ni-edges express domination and immediate domination relations between\\nnodes. These relations are never rescinded when d-trees are\\ncomposed. Thus, nodes separated by an i-edge will remain in a\\nmother-daughter relationship throughout the derivation, whereas nodes\\nseparated by an d-edge can be equated or have a path of any length\\ninserted between them during a derivation.  D-edges and i-edges are not\\ndistributed arbitrarily in d-trees.  For each internal node, either all\\nof its daughters are linked by i-edges or it has a single daughter that\\nis linked to it by a d-edge.  Each node is labelled with a terminal\\nsymbol, a nonterminal symbol or the empty string.  A d-tree containing\\nn d-edges can be decomposed into n+1 components containing\\nonly i-edges.\\n\\n\\nD-trees can be composed using two operations: subsertion and sister-adjunction.  When a d-tree \\nis subserted into another\\nd-tree ,\\na component of \\nis substituted at a frontier\\nnonterminal node (a substitution node) of \\nand all\\ncomponents of \\nthat are above the substituted component are\\ninserted into d-edges above the substituted node or placed above the\\nroot node. For example, consider the d-trees \\nand \\nshown\\n in Figure . Note that components are shown as triangles.  In the composed d-tree \\nthe component \\nis\\nsubstituted at a substitution node in .\\nThe components,\\n,\\n,\\nand \\nof above \\ndrift up the path in \\nwhich runs from the\\nsubstitution node.  These components are then inserted into\\nd-edges in \\nor above the root of .\\nIn general, when a\\ncomponent \\nof some d-tree \\nis inserted into a d-edge\\nbetween nodes \\nand \\ntwo new d-edges are created, the\\nfirst of which relates \\nand the root node of ,\\nand\\nthe second of which relates the frontier node of \\nthat\\ndominates the substituted component to .\\nIt is possible for\\ncomponents above the substituted node to drift arbitrarily far up the\\nd-tree and distribute themselves within domination edges, or above the\\nroot, in any way that is compatible with the domination relationships\\npresent in the substituted d-tree. DTG provide a mechanism\\ncalled subsertion-insertion constraints to control what can appear\\nwithin d-edges (see below).\\n\\n\\nThe second composition operation involving d-trees is called\\nsister-adjunction. When a d-tree \\nis sister-adjoined at a node\\n\\nin a d-tree \\nthe composed d-tree \\nresults from the\\naddition to \\nof \\nas a new leftmost or rightmost\\nsub-d-tree below .\\nNote that sister-adjunction involves the\\naddition of exactly one new immediate domination edge and that several\\nsister-adjunctions can occur at the same node. Sister-adjoining\\nconstraints specify where d-trees can be sister-adjoined and whether\\nthey will be right- or left-sister-adjoined (see below).\\n\\n\\nA DTG is a four tuple \\n\\nG=(VN,VT,S,D) where VN and VT are\\nthe usual nonterminal and terminal alphabets, \\nis a\\ndistinguished nonterminal and D is a finite set of elementary\\nd-trees. A DTG is said to be lexicalized if each d-tree in the\\ngrammar has at least one terminal node.  The elementary d-trees of a\\ngrammar G have two additional annotations: subsertion-insertion\\nconstraints and sister-adjoining constraints. These will be described\\nbelow, but first we define simultaneously DTG derivations and\\nsubsertion-adjoining trees (SAtrees), which are partial derivation\\nstructures that can be interpreted as representing dependency\\ninformation, the importance of which was stressed in the\\n introduction. \\n\\n\\nConsider a DTG \\n\\nG=(VN,VT,S,D). In defining SAtrees, we assume some\\nnaming convention for the elementary d-trees in D and some consistent\\nordering on the components and nodes of elementary d-trees in D.  For\\neach i, we define the set of d-trees Ti(G) whose derivations are\\ncaptured by SAtrees of height i or less.  Let T0(G) be the set Dof elementary d-trees of G.  Mark all of the components of each d-tree\\n in T0(G) as being substitutable.  Only components marked as substitutable can be substituted in a subsertion operation.  The SAtree for \\n\\n\\nconsists of a single node labelled by the elementary d-tree name\\nfor .\\n\\n\\nFor i]0 let Ti(G) be the union of the set \\n\\nTi-1(G) with the set\\nof all d-trees \\nthat can be produced as follows. Let \\n\\n\\nand let \\nbe the result of subserting or sister-adjoining the\\nd-trees \\n\\n\\ninto \\nwhere\\n\\n\\nare all in \\n\\nTi-1(G), with the subsertions\\ntaking place at different substitution nodes in \\nas the\\nfootnote.  Only substitutable components of \\n\\ncan be substituted in these subsertions. Only the new components of\\n\\nthat came from \\nare marked as substitutable in\\n.\\nLet \\n\\n\\nbe the SAtrees for\\n\\n,\\nrespectively.  The SAtree \\nfor\\n\\nhas root labelled by the name for \\nand k subtrees\\n\\n.\\nThe edge from the root of \\nto the root of\\nthe subtree \\nis labelled by li (\\n\\n)\\ndefined as\\nfollows.  Suppose that \\nwas subserted into \\nand the\\nroot of \\nis labelled by the name of some \\n\\n.\\nOnly\\ncomponents of \\nwill have been marked as substitutable in\\n.\\nThus, in this subsertion some component \\n\\n\\nwill\\nhave been substituted at a node in \\nwith address n. In this\\ncase, the label li is the pair (j,n).  Alternatively, will have been d-sister-adjoined at some node with address n in\\n,\\nin which case li will be the pair (d,n) where\\n\\n.\\n\\n\\nThe tree set T(G) generated by G is defined as the set of\\ntrees \\nsuch that: \\n\\n\\nfor some ;\\n\\nis rooted with the nonterminal S; the frontier of is a string in VT[*]; and \\nresults from the removal of all\\nd-edges from .\\nA d-edge is removed by merging the nodes at\\neither end of the edge as long as they are labelled by the same symbol.\\nThe string language L(G) associated with G is the set of\\nterminal strings appearing on the frontier of trees in T(G).\\n\\n\\nWe have given a reasonably precise definition of SAtrees since\\nthey play such an important role in the motivation for this work.  We\\nnow describe informally a structure that can be used to encode a DTG\\nderivation. A derivation graph for \\n\\n\\nresults from the\\naddition of insertion edges to a SAtree \\nfor .\\nThe location in \\nof an inserted elementary component can be unambiguously determined by identifying the source of the node\\n(say the node with address n in the elementary d-tree )\\nwith\\nwhich the root of this occurrence of \\nis merged with when\\nd-edges are removed. The insertion edge will relate the two (not\\nnecessarily distinct) nodes corresponding to appropriate occurrences of\\n\\nand \\nand will be labelled by the pair (i,n).\\n\\n\\nEach d-edge in elementary d-trees has an associated subsertion-insertion\\nconstraint (SIC). A SIC is a finite set of elementary node\\naddresses (ENAs). An ENA \\nspecifies some elementary d-tree\\n\\n,\\na component of \\nand the address of a node within\\nthat component of .\\nIf a ENA \\nis in the SIC associated\\nwith a d-edge between \\nand \\nin an elementary d-tree\\n\\nthen \\ncannot appear properly\\nwithin the path that appears from\\n\\nto \\nin the derived tree \\n\\n.\\n\\n\\nEach node of elementary d-trees has an associated sister-adjunction\\nconstraint (SAC). A SAC is a finite set of pairs, each pair\\nidentifying a direction (left or right) and an elementary d-tree. A\\nSAC gives a complete specification of what can be sister-adjoined at\\na node.  If a node \\nis associated with a SAC containing a pair\\n\\n\\nthen the d-tree \\ncan be d-sister-adjoined at\\n.\\nBy definition of sister-adjunction, all substitution nodes and\\nall nodes at the top of d-edges can be assumed to have SACs that are\\nthe empty-set. This prevents sister-adjunction at these nodes.\\n\\n\\nIn this section we have defined ``raw'' DTG. In a more refined\\nversion of the formalism we would associate (a single) finite-valued\\n feature structure with each node.  It is a matter of further research to determine to what extent SICs and\\nSACs can be stated globally for a grammar, rather than being attached\\n to d-edges/nodes.  See the next section for a brief discussion of linguistic principles from which a grammar's SICs could be derived.\\n\\n\\n  Linguistic Examples \\n\\nIn this section, we show how an account for the data introduced in\\n Section  can be given with DTG. \\n\\n  Getting Dependencies Right: English \\n\\n In Figure , we give a DTG that generates sentence  ().  Every d-tree is a projection from a lexical anchor.  The label of the maximal projection is, we assume, determined\\nby the morphology of the anchor.  For example, if the anchor is a finite\\nverb, it will project to S, indicating that an overt syntactic\\n(``surface'') subject is required for agreement with it (and perhaps\\ncase-assignment).  Furthermore, a finite verb may optionally also\\nproject to S' (as in the d-tree shown for claims), indicating\\nthat a wh-moved or topicalized element is required.  The finite\\nverb seems also projects to S, even though it does not itself\\nprovide a functional subject.  In the case of the to adore tree,\\nthe situation is the inverse: the functional subject requires a finite\\nverb to agree with, which is signaled by the fact that its component's\\nroot and frontier nodes are labelled S and VP, respectively, but the\\nverb itself is not finite and therefore only projects to VP[-fin].  Therefore, the subject will have to raise out of its clause for\\nagreement and case assignment.  The direct object of to adore has\\nwh-moved out of the projection of the verb (we include a trace for\\nthe sake of clarity).\\n\\n\\nWe add SICs to ensure that the projections are respected by components\\nof other d-trees that may be inserted during a derivation.  A SIC is\\nassociated with the d-edge between VP and S node in the seems\\nd-tree to ensure that no node labelled S' can be inserted within it -\\ni.e., it can not be filled by with a wh-moved element.  In\\ncontrast, since both the subject and the object of to adore have\\nbeen moved out of the projection of the verb, the path to these\\n arguments do not carry any SIC at all. \\n\\n\\nWe now discuss a possible derivation.  We start out with the most deeply\\nembedded clause, the adores clause.  Before subserting its nominal\\narguments, we sister-adjoin the two adjectival trees to the tree for\\nhotdogs.  This is handled by a SAC associated with the N'node that allows all trees rooted in AdjP to be left sister-adjoined.\\nWe then subsert this structure and the subject into the to adore\\nd-tree.  We subsert the resulting structure into the seems clause\\nby substituting its maximal projection node, labelled VP[fin:\\n-], at the VP[fin: -] frontier node of seems, and by\\ninserting the subject into the d-edge of the seems tree.  Now,\\nonly the S node of the seems tree (which is its maximal\\nprojection) is substitutable.  Finally, we subsert this derived\\nstructure into the claims d-tree by substituting the S node of\\nseems at the S complement node of claims, and by inserting\\nthe object of adores (which has not yet been used in the\\nderivation) in the d-edge of the claims d-tree above its S node.\\n The derived tree is shown in Figure .  The SA-tree for this derivation corresponds to the dependency tree given\\n previously in Figure . \\n\\n\\nNote that this is the only possible derivation involving these three\\nd-trees, modulo order of operations.  To see this, consider the\\nfollowing putative alternate derivation.  We first subsert the to\\nadore d-tree into the seems tree as above, by substituting the\\nanchor component at the substitution node of seems.  We insert the\\nsubject component of to adore above the anchor component of seems.  We then subsert this derived structure into the claims\\ntree by substituting the root of the subject component of to adore\\nat the S node of claims and by inserting the S node of the seems d-tree as well as the object component of the to adore\\nd-tree in the S'/S d-edge of the claims d-tree.  This last\\n operation is shown in Figure .  The resulting phrase structure tree would be the same as in the previously discussed\\nderivation, but the derivation structure is linguistically meaningless,\\nsince to adore would have been subserted into both seems and\\nclaims.  However, this derivation is ruled out by the restriction\\nthat only substitutable components can be substituted: the subject\\ncomponent of the adore d-tree is not substitutable after\\nsubsertion into the seems d-tree, and therefore it cannot be\\nsubstituted into the claims d-tree.\\n\\n\\nIn the above discussion, substitutability played a central role in\\nruling out the derivation. We observe in passing that the SIC associated\\nto the d-edge in the seems d-tree also rules out this\\nderivation. The derivation requires that the S node of seems be\\ninserted into the S'/S d-edge of claims.  However, we would have\\nto stretch the edge over two components which are both ruled out by the\\nSIC, since they violate the projection from seems to its S node.\\nThus, the derivation is excluded by the independently motivated SICs,\\nwhich enforce the notion of projection.  This raises the possibility\\nthat, in grammars that express certain linguistic principles,\\nsubstitutability is not needed for ruling out derivations of this\\nnature.  We intend to examine this issue in future work.\\n\\n\\n  Getting Word Order Right: Kashmiri \\n\\n Figure  shows the matrix and embedded clauses for  sentence ().  We use the node label VP throughout and use features such as top (for topic) to differentiate different\\nlevels of projection.  Observe that in both trees an argument has been\\nfronted.  Again, we will use the SICs to enforce the projection from a\\nlexical anchor to its maximal projection.  Since the direct object of\\nkor has wh-moved out of its clause, the d-edge connecting\\nit to the maximal projection of its verb has no SIC.  The d-edge\\nconnecting the maximal projection of baasaan to the Aux component,\\nhowever, has a SIC that allows only  VP[wh: +, top:\\n-] nodes to be inserted.\\n\\n\\nThe derivation proceeds as follows.  We first subsert the embedded\\nclause tree into the matrix clause tree.  After that, we subsert the\\nnominal arguments and function words.  The derived structure is shown in\\n Figure .  The associated SA-tree is the desired, semantically motivated, dependency structure: the embedded clause\\ndepends on the matrix clause.\\n\\n\\nIn this section, we have discussed examples where the elementary objects\\nhave been obtained by projecting from lexical items. In these cases, we\\novercome both the problems with TAG considered in\\n Section .  The SICs considered here enforce the same notion of projection that was used in obtaining the elementary\\nstructures. This method of arriving at SICs not only generalizes for\\nthe English and Kashmiri examples but also appears to apply to the case\\nof long-distance scrambling and topicalization in German.\\n\\n\\n\\n    Recognition\\n\\n\\nIt is straightforward to adapt the polynomial-time CKY-style recognition\\nalgorithm for a lexicalized UVGDL of Rambow  for\\nDTG.  The entries in this array recording derivations of substrings of\\ninput contain a set of elementary nodes along with a multi-set of\\ncomponents that must be inserted above during bottom-up\\nrecognition. These components are added or removed at substitution and\\ninsertion. The algorithm simulates traversal of a derived tree; checking\\nfor SICs and SACs can be done easily. Because of lexicalization, the\\nsize of these multi-sets is polynomially bounded, from which the\\npolynomial time and space complexity of the algorithm follows.\\n\\n\\nFor practical purposes, especially for lexicalized grammars, it is\\npreferable to incorporate some element of prediction. We are developing\\na polynomial-time Earley style parsing algorithm. The parser returns a\\nparse forest encoding all parses for an input string. The performance\\nof this parser is sensitive to the grammar and input. Indeed it appears\\nthat for grammars that lexicalize CFG and for English grammar (where\\nthe structures are similar to the LTAG developed at University of\\n Pennsylvania ) we obtain cubic-time complexity. \\n\\n\\n  Conclusion \\n\\nDTG, like other formalisms in the TAG family, is lexicalizable, but\\nin addition, its derivations are themselves linguistically meaningful.\\nIn future work we intend to examine additional linguistic data, refining\\naspects of our definition as needed.  We will also study the formal\\nproperties of DTG, and complete the design of the Earley style parser.\\n\\n\\n  Acknowledgements \\n\\nWe would like to thank Rakesh Bhatt for help with the Kashmiri data.  We\\nare also grateful to Tilman Becker, Gerald Gazdar, Aravind Joshi, Bob\\nKasper, Bill Keller, Tony Kroch, Klaus Netter and the ACL-95 referees.\\nRambow was supported by the North Atlantic Treaty Organization under a\\nGrant awarded in 1993, while at TALANA,\\nUniversit Paris 7.\\n\\nBibliography \\n\\nT. Becker, A. Joshi,  O. Rambow.\\n1991.\\nLong distance scrambling and tree adjoining grammars.\\nIn EACL-91, 21-26.\\n\\n\\nR. Bhatt.\\n1994.\\nWord order and case in Kashmiri.\\nPh.D. thesis, Univ. Illinois.\\n\\n\\nT. Bleam.\\n1994.\\nClitic climbing in spanish: a GB perspective.\\nIn TAG+ Workshop, Tech. Rep. TALANA-RT-94-01,\\nUniversit Paris 7, 16-19.\\n\\n\\nJ. Bresnan  R. Kaplan.\\n1982.\\nLexical-functional grammar: A formal system for grammatical\\n  representation.\\nIn J. Bresnan, ed., The Mental Representation of Grammatical\\n  Relations. MIT Press.\\n\\n\\nR. Frank.\\n1992.\\nSyntactic Locality and Tree Adjoining Grammar: Grammatical,\\n  Acquisition and Processing Perspectives.\\nPh.D. thesis, Dept. Comp.  Inf. Sc.,\\n  Univ. Pennsylvania.\\n\\n\\nA. Joshi.\\n1987.\\nAn introduction to tree adjoining grammars.\\nIn A. Manaster-Ramer, ed., Mathematics of Language,\\n87-114.\\n\\n\\nA. Joshi, L. Levy,  M. Takahashi.\\n1975.\\nTree adjunct grammars.\\nJ. Comput. Syst. Sci., 10(1):136-163.\\n\\n\\nA. Joshi  Y. Schabes.\\n1991.\\nTree-adjoining grammars and lexicalized grammars.\\nIn M. Nivat  A. Podelski, eds., Definability and\\n  Recognizability of Sets of Trees.\\n\\n\\nR. Kasper, B. Kiefer, K. Netter,  K.\\nVijay-Shanker\\n1995.\\nCompilation of HPSG to TAG.\\nIn ACL-95.\\n\\n\\nA. Kroch.\\n1987.\\nSubjacency in a tree adjoining grammar.\\nIn A. Manaster-Ramer, ed., Mathematics of Language,\\n  143-172.\\n\\n\\nA. Kroch.\\n1989.\\nAsymmetries in long distance extraction in a Tree Adjoining\\n  Grammar.\\nIn Mark Baltin  Anthony Kroch, editors, Alternative\\n  Conceptions of Phrase Structure, 66-98.\\n\\n\\nA. Kroch  A. Joshi.\\n1986.\\nAnalyzing extraposition in a tree adjoining grammar.\\nIn G. Huck  A. Ojeda, eds., Syntax  Semantics:\\n  Discontinuous Constituents,  107-149.\\n\\n\\nI. Mel'cuk.\\n1988.\\nDependency Syntax: Theory and Practice.\\n\\n\\nO. Rambow.\\n1994.\\nFormal and Computational Aspects of Natural Language Syntax.\\nPh.D. thesis, Dept. Comput.  Inf. Sc.,\\n  Univ. Pennsylvania.\\n\\n\\nO. Rambow.\\n1994.\\nMultiset-Valued Linear Index Grammars.\\nIn ACL-94, 263-270.\\n\\n\\nO. Rambow  A. Joshi.\\n1992.\\nA formal look at dependency grammars and phrase-structure\\ngrammars, with special consideration of word-order phenomena.\\nIn Intern. Workshop on The Meaning-Text Theory,\\n  Darmstadt.\\nArbeitspapiere der GMD 671, 47-66.\\n\\n\\nB. Santorini  S. Mahootian.\\n1995.\\nCodeswitching and the syntactic status of adnominal adjectives.\\nLingua, 95.\\n\\n\\nY. Schabes  S. Shieber.\\n1994.\\nAn alternative conception of tree-adjoining derivation.\\nComput. Ling., 20(1):91-124.\\n\\n\\nS. Shieber.\\n1985.\\nEvidence against the context-freeness of natural language.\\nLing.  Phil., 8:333-343.\\n\\n\\nK. Vijay-Shanker.\\n1987.\\nA Study of Tree Adjoining Grammars.\\nPh.D. thesis, Dept. Comput.  Inf. Sc.,\\n  Univ. Pennsylvania.\\n\\n\\nK. Vijay-Shanker.\\n1992.\\nUsing descriptions of trees in a tree adjoining grammar.\\nComput. Ling., 18(4):481-517.\\n\\n\\nThe XTAG Research Group.\\n1995.\\nA lexicalized tree adjoining grammar for English.\\nTech. Rep. IRCS Report 95-03, Univ. Pennsylvania.\\n\\nFootnotes\\n\\n  For clarity, we depart from\\nstandard TAG notational practice and annotate nodes with lexemes and\\narcs with grammatical function.\\n  In these cases the\\nfoot node is an argument node of the lexical anchor.\\n   This was also observed by\\nRambow , where an integrity constraint (first\\n defined for an IDLP version of TAG ) is defined for a MCTAGDL version called VTAG.  However, this was found to be\\ninsufficient for treating both long-distance scrambling and\\nlong-distance topicalization in German.  VTAG retains adjoining (to\\nhandle topicalization) for this reason.\\n  Santorini and\\nMahootian  provide additional\\nevidence against the standard TAG approach to modification from code\\nswitching data, which can be accounted for by using sister-adjunction.\\n  Due to space limitations, in the following\\ndefinitions we are forced to be somewhat imprecise when we identify a\\nnode in a derived d-tree with the node in the elementary d-trees\\n(elementary nodes) from which it was derived. This is often done in\\nTAG literature, and hopefully it will be clear what is intended.\\n  We will\\ndiscuss the notion of substitutability further in the next section.  It\\nis used to ensure the SAtree is a tree. That is, an elementary\\nstructure cannot be subserted into more than one structure since this\\nwould be counter to our motivations for using subsertion for\\ncomplementation.\\n  Trees used in\\n Section  make use of such feature structures. \\n  In this context, it might be beneficial\\nto consider\\nthe expression of a feature-based lexicalist theory such as\\nHPSG in DTG, similar to the compilation of HPSG to\\n TAG . \\n  We enforce island\\neffects for wh-movement by using a [extract] feature\\non substitution nodes.  This corresponds roughly to the analysis in\\nTAG, where islandhood is (to a large extent) enforced by designating a\\n particular node as the foot node . \\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nDTG are designed to share some of the advantages of TAG\\nwhile overcoming some of its limitations.  DTG involve two\\ncomposition operations called subsertion and sister-adjunction. The most\\ndistinctive feature of DTG is that, unlike TAG, there is complete\\nuniformity in the way that the two DTG operations relate lexical\\nitems: subsertion always corresponds to complementation and\\nsister-adjunction to modification.  Furthermore, DTG, unlike TAG,\\ncan provide a uniform analysis for wh-movement in English and\\nKashmiri, despite the fact that the wh element in Kashmiri appears\\nin sentence-second position, and not sentence-initial position as in\\nEnglish.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nIn this paper we describe a method for analysing the temporal\\nstructure of a discourse. This component was implemented as part of a\\ndiscourse grammar for English. The goals of the temporal component\\nwere to yield a detailed representation of the temporal structure of\\nthe discourse, taking into account the effect of tense, aspect and\\ntemporal expressions while at the same time minimising unnecessary\\nambiguity in the temporal structure.  The method combines a\\nconstraint-based approach with an approach based on preferences: we\\nexploit the  HPSG type hierarchy and unification to arrive at a\\ntemporal structure using constraints placed on that structure by\\ntense, aspect, rhetorical structure and temporal expressions, and we\\nuse the temporal centering preferences described\\n by , to rate the  possibilities for temporal\\nstructure and choose the best among them.\\n\\n\\nThe starting point for this work was Scha and\\nPolanyi's discourse grammar (Scha  Polanyi 1988;\\nPrst et al 1994).  For the\\nimplementation we extended the  HPSG\\ngrammar [] which Gerald Penn and Bob Carpenter\\n first encoded in  ALE .  This paper will focus on our temporal processing algorithm,\\nand in particular on our analysis of narrative\\nprogression, rhetorical structure, perfects and temporal expressions.\\n\\n\\n  Constraints on narrative continuations \\n\\nProbably the best known algorithm for tracking narrative progression\\nis that developed by Kamp ,\\nHinrichs , and \\nPartee , which formalises the\\nobservation that an event will occur just after a preceding\\nevent, while a state will overlap with a preceding event.  This\\nalgorithm gives the correct results in examples such as the\\nfollowing:\\njjk John entered the room.  Mary stood up.jjk2 \\nIn jjk the event of Mary's standing is understood to occur just\\nafter John enters the room, while in jjk2 the state in which\\nMary is seated is understood to overlap with the event of John's\\nentering the room.\\n\\n\\nHowever, if there is a rhetorical\\nrelationship between two eventualities such as causation, elaboration\\nor enablement, the temporal defaults can be overridden, as in the\\nfollowing examples:\\nmary a. John fell. Mary pushed him.\\t\\tb. \\n In () there is a causal relationship between Mary's pushing John and his falling, and the second event is understood to precede\\n the first.  In (), the second sentence is an elaboration of the first, and they therefore refer to aspects of the same event \\nrather than to two sequential events.\\n\\n\\nIt has been suggested that only world knowledge allows one to detect\\nthat the default is being overridden here. For example, Lascarides \\nAsher (1991) suggest that general knowledge postulates (in the case of\\n (): that a pushing can cause a falling) can be invoked to generate the backward movement reading.\\n\\n\\nThe problem for practical systems is twofold:  we could assume that\\nin the case of narrative the Kamp/Hinrichs/Partee algorithm is the \\ndefault, but\\neach time the default is applied we would need to check all our \\navailable\\nworld knowledge to see whether there isn't a world knowledge\\npostulate which might be overriding this assumption. \\nClearly this would make the processing of text a very expensive\\noperation.\\n\\n\\nAn alternative is to assume that the temporal ordering between events\\nin two consecutive sentences can be any of the four\\npossibilities (just_after, precede, same-event\\nand overlap).  But then the resulting temporal structures will\\nbe highly ambiguous even in small discourses. And sometimes this\\nambiguity is unwarranted. Consider:\\nmarc \\nHere, it would appear, only one reading is possible, i.e. the one\\nwhere John gave Mary her slice of pizza just after \\nshe stared\\nor started to stare at him. \\nIt would be undesirable \\nfor the temporal processing mechanism to postulate an ambiguity\\nin this case.\\n\\n\\nOf course, sometimes it is possible to take advantage of certain cue\\nwords which either indicate or constrain the rhetorical relation.  For\\nexample, in 4 the order of the events is understood to be the\\nreverse of that in jjk due to the cue word because\\nwhich signals a causal relationship between the events:\\n4 \\nAs Kehler  points out,\\nif forward movement of time is considered a default with\\nconsecutive event sentences, then the use of ``because'' in 4\\nshould cause a temporal clash--whereas it is perfectly\\nfelicitous. \\nTemporal expressions such as at noon and the previous\\nThursday can have a similar effect: they too can override the default\\ntemporal relations and place constraints on tense.  In cons1,\\nfor example, the default interpretation would be that John's being in\\nDetroit overlaps with his being in Boston, but the phrase the\\nprevious Thursday overrides this, giving the interpretation that\\nJohn's being in Detroit precedes his being in Boston:\\ncons1 \\nThis suggests that \\nthe temporal information given by\\ntense acts as a weaker constraint on temporal structure than the \\ninformation\\ngiven by temporal adverbials.  \\n\\n\\nThe possibilities for rhetorical relations (e.g., whether something is\\nnarration, or elaboration, or a causal relation) can be further \\nconstrained by\\naspect. For example, a state can elaborate another state or an event:\\n5 a. Mary was tired.  She was exhausted.\\t\\tb. \\nBut an event can only elaborate another event, as in alab:\\n\\n\\n\\nalab a. \\n\\t\\tb. \\n For the eventive second sentence of () to be an elaboration of the first sentence, it must occur in a stative form--for example\\nas a progressive (i.e., She was building a dog house).\\n\\n\\nBecause of considerations like these, our aim in the implementation\\nwork was to treat tense, aspect, cue words and rhetorical relations as\\nmutually constraining, with more specific information such as explicit \\ncue\\nwords having higher\\npriority than less specific information such as tense.\\nThe main advantage of\\nthis approach is\\nthat it \\nreduces \\ntemporal structure ambiguity \\nwithout having\\nto rely on detailed world knowledge postulates.  \\n\\n\\nTable 1 lists the possible temporal relations between the\\neventualities described by two consecutive sentences without temporal\\nexpressions or cue words, where the first\\nsentence (S1) may have any tense and aspect and the second\\nsentence (S2) expresses a simple past event.  We constrain\\nS2 in this way because of lack of space; additional constraints\\n are given in . For example, if a simple past\\neventive sentence follows a simple past eventive sentence the second\\nevent can be \\nunderstood to occur just after the first, to precede the\\nfirst or to refer to the same event as the first (an elaboration\\nrelation), but the two events cannot overlap; these constraints are \\nweaker,\\nhowever, than explicit clues such as cue words to rhetorical relations \\nand\\ntemporal expressions.\\nWhen S1 expresses a state, it is possible for the temporal relation to\\nhold between the event described by S2 and the event or activity \\nmost\\nclosely preceding S1, i.e., the temporal focus of S1, here \\nreferred to as\\n  TF1. \\n\\n\\nHowever, we haven't solved the problem completely at this point:\\nalthough tense can provide \\na further constraint on the temporal structure of such discourses,\\nit can also add a further ambiguity.  Consider pp:\\npp Sam rang the bell.  He had lost the key.\\nClearly, the event described by the past perfect sentence must precede\\nthe event described by the first, simple past sentence.\\nHowever, if a third sentence is added, an ambiguity results.\\nConsider the following possible continuations of pp:\\npp2 a. ...Hannah opened the door.\\t\\tb. \\nThe temporal relation between these continuations and the portion of\\nearlier text they attach to is constrained along the lines sketched\\nbefore. The problem here is determining which thread in pp they \\n continue; () continues the thread in which Sam loses the key.\\n\\n\\nA further ambiguity is that when the third sentence is past perfect,\\nit may be a continuation of a preceding thread or the start of\\na new thread itself. Consider:\\n9 a. \\n\\t\\tb. \\nIn (a) the third sentence continues the thread about losing the key;\\n in (b) the third starts a new thread. \\n\\n\\nFor the problem with multi-sentence discourses, and the ``threads''\\nthat sentences continue, we use an implementation of temporal\\n centering ,.  This is a  technique similar to\\nthe type of centering used for nominal\\n anaphora ,.  Centering assumes that discourse understanding requires some notion of ``aboutness.''  While nominal\\ncentering assumes there is one object that the current discourse is\\n``about,'' temporal centering assumes that there is one thread that\\nthe discourse is currently following, and that, in addition to tense\\nand aspect constraints, there is a preference for a new utterance to\\ncontinue a thread which has a parallel tense or \\nwhich is semantically related to it and a preference\\nto continue the current thread rather than switching to another\\nthread.  Kameyama et al. (1993) confirmed these preferences when\\ntesting their ideas on the Brown corpus.\\n\\n\\n\\n\\nAs an example of how the temporal centering preference techniques can \\nreduce\\nambiguity, recall example pp and the possible continuations shown in\\npp2.\\nThe difficulty in these examples is determining whether the third\\nsentence continues the thread begun by the first or second sentence.\\n For example, in ()  the preference technique which\\nallows us to choose the first thread over the second is one which \\nassigns a\\nhigher rating to a thread whose tense is parallel to that of\\nthe new\\nsentence; in this case both Sam rang the bell and Hannah \\nopened the\\ndoor are in the simple past tense.\\n In example () with the second thread. To handle an example like third, we employ a preference for \\nrelating a\\nsentence to a thread that has content words that are rated as \\nsemantically\\n``close'' to that of the sentence:\\nthird \\n\\n\\nWe store semantic patterns between words as a\\ncheap and quick form of world knowledge;\\nthese patterns are easier \\nto provide than are the detailed\\nworld knowledge postulates required in some other approaches, and result\\nin similar and sometimes more precise temporal structures with less\\nprocessing overhead.\\nUsing the semantic patterns we know that key and keyring are\\nsemantically close, and through that semantic link between the second and third\\nsentences we prefer to connect the third sentence to the thread begun\\n by the second.  The approach to  representing semantic relationships we\\ntake is one used by Morris  Hirst \\nwherein the words in the lexicon are associated with each other\\nin a thesaurus-like fashion\\nand given a rating according to how semantically ``close'' they are.\\nWe thus avoid relying on high-level inferences and very specific world\\nknowledge postulates, our goal being to determine the temporal\\nstructure as much as possible prior to the application of higher-level\\ninferences.\\n\\n\\n  An  HPSG implementation of a discourse grammar \\n\\nFollowing Scha  Polanyi  and Prst \\net al (1994), our model of discourse\\nconsists of units called Discourse Constituent Units () which are\\nrelated by various temporal and rhetorical relations.  A basic  \\nrepresents a\\nsentence (or clause), and complex  are built up from basic and complex\\n.  \\nIn our  implementation, a  is simply a sign with certain\\ninformation that is unnecessary for discourse processing removed.\\n\\n\\nIn our  ALE implementation, a  contains the following slots\\nfor temporal information:\\n\\n\\n CUE/SMALL>_WORD:\\nCues to rhetorical structure, e.g., ``because.''\\n V/SMALL>_AND/SMALL>_NP/SMALL>_LIST:\\nContains content words found in this \\n DCU,\\nand is used to compare the content words  of  the  current    with \\nthose\\nin previous threads, in order to rate the semantic ``closeness'' of the \\n to each thread.\\n SEM/SMALL>_ASPECT:\\nContains the semantic aspect (event, state,\\nactivity).  We have extended the Penn  Carpenter implementation of the \\n grammar so that semantic aspect is calculated\\ncompositionally (and stored here).\\n RHET/SMALL>_RELN:\\nThe relation between this  DCU and a previous \\none. Lexical items and phrases such as cue words (stored in  CUE/SMALL>_WORD)\\naffect the value of this slot.\\n TEMP/SMALL>_CENTER:\\nUsed for temporal centering; Keeps track of \\nthe thread currently being followed (since\\nthere is a preference for continuing the current thread) and all the threads\\nthat have been constructed so far in the discourse.\\n FWD/SMALL>_CENTER:\\nExisting threads\\n BKWD/SMALL>_CENTER:\\nThe thread currently being followed\\n CLOSED/SMALL>_THREADS:\\n Threads no longer available for continuation\\n TEMP/SMALL>_EXPR/SMALL>_RELNS:\\nStores the semantic interpretation of \\ntemporal expressions associated with this\\n.\\n TEMP/SMALL>_RELNS:\\nStores the temporal relations between the\\neventualities in the discourse. \\n TEMPFOC:\\nThe most recent event in the current thread \\nwhich a\\nsubsequent eventuality may elaborate upon (same-event),\\noverlap, come just_after or\\nprecede. \\n TENASP:\\nKeeps track of the tense and syntactic aspect \\nof the  (if the  is simple).\\n TENSE:\\npast, pres, fut\\n ASPECT:\\nsimple, perf, prog, perf_prog\\n\\n\\nTo allow the above-mentioned types of information to mutually constrain\\neach other, we employ a hierarchy of rhetorical and temporal relations\\n(illustrated in Figure 1), using the  ALE system in such a way\\nthat clues such as tense and cue words work together to reduce the\\nnumber of possible temporal structures.  This approach improves upon\\nearlier work on discourse structure such\\n as [] and  in reducing the number of possible ambiguities; it is also more precise than\\nthe Kamp/Hinrichs/Partee approach in that it takes into account ways in \\nwhich the\\napparent defaults can be overridden and differentiates between events and\\nactivities, which behave differently in narrative progression.\\n\\n\\nTense, aspect, rhetorical relations and temporal expressions affect the \\nvalue of the \\nRHET/SMALL>_RELN type that expresses \\nthe relationship between two :\\ncue words are lexically marked according to what rhetorical relation\\nthey specify, and this relation is passed on to the .  \\nExplicit relation markers such as cue words and temporal relations must be\\nconsistent and take priority over indicators such as tense and aspect.  For\\nexample, sentence ruled will be ruled out because the cue phrase \\nas\\na result conflicts with the temporal expression ten minutes earlier:\\nruled \\nOn the other hand, if temporal expressions indicate an overlap relation \\nand cue\\nwords indicate a background relation as in cl, these contributions \\nare consistent and\\nthe  RHET/SMALL>_RELN type will contain a background value (the more\\nspecific value of the two):\\ncl \\n\\n\\n  The algorithm \\n\\nFor reasons of space it is difficult to give examples of the\\nsign-based output of the grammar, or of the  rules, so we will\\nrestrict ourselves here to a summary of the algorithm and to a very\\nlimited rendition of the system output. The algorithm used for\\ncalculating the temporal structure of a discourse can be summarised as\\nfollows. It consists of two parts, the constraint-based portion and\\nthe preference-based portion:\\n\\n\\n1.\\nThe possible temporal/rhetorical relations are constrained.\\n(a)\\nIf there is a temporal expression, it determines the temporal\\nrelationship of the new  to the previous ones, and defaults are ignored.\\n(b)\\nLexical items such as cue words influence the value of the \\nRHET/SMALL>_RELN type (See Figure 1).\\n(c)\\nIf steps (a) and (b) attempt to place conflicting values in the \\nRHET/SMALL>_RELN slot, the parse will fail.\\n  (d)\\nIf there is no temporal expression or cue phrase, tense and semantic \\naspect also influence the value of the \\nRHET/SMALL>_RELN type (See Table 1), so that rhetorical relations, tense and \\naspect constrain each\\nother.  \\n  2.\\nIf more than one possibility exists, semantic preferences are used to\\nchoose between the possibilities.\\n(a)\\nA ``semantic distance'' rating between the new  and each previous\\nthread is determined.  (If there are no existing threads a new thread is\\nstarted.)\\n(b)\\nOther preferences, such as a preference for relating the new \\n to a\\nthread with parallel tense, are employed\\n (See , for details), and the  resulting ratings are\\nfactored into the rating for each thread.\\n  (c)\\nIf the thread currently being followed is among the highest rated\\nthreads, this thread is continued.  (This corresponds to temporal centering's\\npreference to continue the current thread.)\\n  (d)\\nIf not, the  may continue any of the highest rated threads, and\\neach of these solutions is generated.\\n\\n\\nCharts such as Table 1 provide the observations we use to fill in the \\nvalue of\\n RHET/SMALL>_RELN.  Those observations are summarised below. In\\nwhat follows, the event\\nvariable associated with i is ei and the  TEMPFOC of \\ne1is the most recent event/activity processed, possibly e1 itself:\\n\\n\\n\\ne2 can overlap with e1 if \\n\\n2 describes a state, or\\n\\n1 describes a state and 2 describes an activity.\\n\\n\\n\\ne2 can occur just-after the  TEMPFOC of e1 if \\n\\n2 describes a simple tense event, or\\n\\n1 describes a complex tense clause and 2 \\ndescribes a\\ncomplex tense event, or\\n\\n1 describes an event and 2 describes an atelic \\nor a\\nsimple tense state, or\\n\\n1 describes a state and 2 describes a simple tense \\nactivity.\\n\\n\\n\\ne2 can precede e1 if \\n\\n2 describes an event, or\\n\\n1 doesn't describe an activity and 2 describes \\na past perfect stative.\\n\\n\\n\\ne2 can elaborate on e1 if\\n\\n1 describes an event, or\\n\\n1 describes an activity and 2 describes an \\natelic, or\\n\\n1 and 2 describe states and either 2describes a simple tense state or 1 describes a complex tense state.\\n\\n\\n\\n\\n\\n\\nUsing this algorithm, we can precisely identify the rhetorical and temporal\\nrelations when cue words to rhetorical structure are present, as in j1:\\nj1 \\t\\t TEMP/SMALL>_RELNS: e2 precedes e1\\nWe can also narrow the possibilities when no cue word is present by using\\nconstraints based on observations of tense and aspect interactions such as\\nthose shown in Table 1.  For example, if 1 represents a simple \\npast eventive\\nsentence and 2 a past perfect eventive sentence, then in spite \\nof the lack of\\nrhetorical cues we know that e2 precedes e1, as in j2:\\nj2 \\t\\t TEMP/SMALL>_RELNS: e2 precedes e1\\nAlso, when several structures are possible we can narrow the possibilities by\\nusing preferences, as in the examples below:\\nvvg \\t\\ta. ...He rang the bell (e3).\\t\\t\\t\\t TEMP/SMALL>_RELNS: e2 precedes e1, \\t\\t\\t\\t\\t\\te3 just-after e1\\t\\tb. \\t\\t\\t\\t TEMP/SMALL>_RELNS: e2 precedes e1, \\t\\t\\t\\t\\t\\te3' just-after e2\\nIf we allow any of the four possible temporal relations between events, both\\ncontinuations of sentence vvg would have 17 readings (4 x 4 + 1 reading\\nin which the third sentence begins a new thread).\\nUsing constraints, we reduce the number of readings to 4.  Using preferences,\\nwe reduce that to 2 readings for each continuation.  The correct temporal\\n relations are shown in vvg. \\n\\n\\n  An underspecified representation \\n\\nBy using constraints and preferences, we can considerably reduce the \\namount of\\nambiguity in the temporal/rhetorical structure of a discourse.  However,\\nexplicit cues to rhetorical and temporal relations are not always available,\\nand these cases result in more ambiguity than is desirable when processing\\nlarge discourses.\\n\\n\\nConsider, however, that instead of generating all the possible\\ntemporal/rhetorical structures, we could use the information available to fill\\nin the most restrictive type possible in the type hierarchy of\\ntemporal/rhetorical relations shown in Figure 1.  \\nWe can then avoid generating the structures until higher-level information can\\nbe applied to complete the disambiguation process.\\n\\n\\n  Conclusion \\n\\nWe presented a brief description of an algorithm for determining the\\ntemporal structure of discourse. The algorithm is part of an \\nHPSG-style discourse grammar implemented in Carpenter's  ALE\\nformalism. Its novel features are that it treats tense, aspect,\\ntemporal adverbials and rhetorical relations as mutually constraining; it\\npostulates less ambiguity than current temporal structuring algorithms\\ndo; and it uses semantic closeness and other preference techniques \\nrather than full-fledged world\\nknowledge postulates to determine preferences over remaining\\nambiguities.  We also recommended using an underspecified representation of\\ntemporal/rhetorical structure to avoid generating all solutions until\\nhigher-level knowledge can aid in reducing ambiguity.\\n\\nBibliography \\n\\nBob Carpenter, 1993.\\nALE: The Attribute Logic Engine User's Guide.\\nLaboratory for Computational Linguistics, Philosophy Department,\\n  Carnegie Mellon University, version ,\\nMay.\\n\\n\\nBarbara J. Grosz, Aravind Joshi, and Scott Weinstein.\\n1983.\\nProviding a unified account of definite noun phrases in discourse.\\nIn the proceedings of the 21st Annual Meeting of the \\nAssociation for Computational\\n  Linguistics, pages 44-50.\\n\\n\\nErhard W. Hinrichs.\\n1981.\\nTemporale anaphora in englischen.\\nStaatsExamen thesis, Universitt Tubingen.\\n\\n\\nJanet Hitzeman, Claire Grover, and Marc Moens.\\n1994.\\nThe implementation of the temporal portion of the discourse \\ngrammar.\\nDeliverable D.2.Temporal, LRE 61-062, University of\\n  Edinburgh, December.\\n\\n\\nMegumi Kameyama, Rebecca Passonneau, and Massimo Poesio.\\n1993.\\nTemporal centering.\\nIn the proceedings of the 31st Annual Meeting of the \\nAssociation for Computational\\n  Linguistics, pages 70-77, Columbus, OH.\\n\\n\\nHans Kamp.\\n1979.\\nEvents, instant and temporal reference.\\nIn R. Bauerle, U. Egli, and A. von Stechow, editors, \\nSemantics\\n  from Different Points of View, pages 376-417, Springer-Verlag.\\n\\n\\nAndrew Kehler.\\n1994.\\nTemporal relations: Reference or discourse coherence?\\nIn the proceedings of the 32nd Annual Meeting of the \\nAssociation for Computational\\n  Linguistics, pages 319-321, June.\\n\\n\\nAlex Lascarides and Nicholas Asher.\\n1991.\\nDiscourse relations and defeasible knowledge.\\nIn the proceedings of the 29th Annual Meeting of the \\nAssociation for Computational\\n  Linguistics, pages 55-63, University of California at\\n  Berkeley.\\n\\n\\nMarc Moens.\\n1987.\\nTense, Aspect and Temporal Reference.\\nPh.D. thesis, University of Edinburgh.\\n\\n\\nJ. Morris and Graeme Hirst.\\n1991.\\nLexical cohesion computed by thesaural relations as an indicator of\\n  the structure of text.\\nComputational Linguistics, 17(1):21-48.\\n\\n\\nTerence Parsons.\\n1990.\\nEvents in the semantics of English: A Study in Subatomic\\n  Semantics.\\nMassachusetts Institute of Technology.\\n\\n\\nBarbara Hall Partee.\\n1984.\\nNominal and temporal anaphora.\\nLinguistics and Philosophy, 7:243-286.\\n\\n\\nMassimo Poesio.\\n1994.\\nDiscourse Interpretation and the Scope of Operators.\\nPh.D. thesis, University of Rochester, Department of Computer\\n  Science, Rochester, NY.\\n\\n\\nCarl Pollard and Ivan A. Sag.\\n1994.\\nHead-Driven Phrase Structure Grammar.\\nUniversity of Chicago Press and CSLI Publications.\\n\\n\\nHub Prst, Remko Scha and Martin van den Berg.\\n1994.\\nDiscourse grammar and verb phrase anaphora.\\nLinguistics and Philosophy, 17:261-327.\\n\\n\\nRemko Scha and Livia Polanyi.\\n1988.\\nAn augmented context free grammar for discourse.\\nIn Proceedings of the 12th Conference on Computational\\n  Linguistics, pages 573-577, Prague, August.\\n\\n\\nCandace L. Sidner.\\n1983.\\nFocusing in the comprehension of definite anaphora.\\nIn M. Brady and R. Berwick, editors, Computational \\nModels of\\n  Discourse. MIT Press, Cambridge, MA.\\n\\nFootnotes\\n\\n   We\\nwould like to thank Alex Lascarides and Massimo Poesio for comments on an\\nearlier draft.\\n   This work was supported in \\npart by the European Commission's programme on\\nLinguistic Research and Engineering through\\nproject LRE-61-062, ``Towards a declarative theory of discourse.''\\n  In this chart it appears\\nthat whether the tense is simple past or past perfect makes no \\ndifference, and\\nthat only aspect affects the possible temporal relations between \\nS1 and\\nS2.  However, it is important not to ignore tense because other \\ncombinations of tense and aspect do show that tense\\naffects which relations are possible, e.g., a simple past stative \\nS2cannot have a precede relation with any S1, while a past \\nperfect\\nstative S2 can.\\n  We will not discuss the\\n additional problem that if the final sentence in () is the end of the text, the text is probably ill-formed.  This is because a\\nwell-formed text should not leave threads ``dangling'' or unfinished.\\nThis is probably also the reason for the awkwardness of the\\nwell-known example Max poured a cup of coffee. He had entered the\\nroom.\\n  Semantic closeness ratings won't help in examples\\npp - pp2 because\\nthere is as strong a relationship between door and bell as\\nthere is between door and key.\\n  The other reading, in which \\nthe third\\nsentence is an elaboration of one of the preceding events, must not be ruled\\nout\\nbecause there are cases such as Sam arrived at the house at eight.  He\\nrang the bell.  He let it ring for two minutes, in which such \\nelaboration is possible.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nWe describe a method for analysing the temporal structure of\\na discourse which takes into account the effects of tense, aspect,\\ntemporal adverbials and rhetorical structure and which minimises\\nunnecessary ambiguity in the temporal structure.  It is part of a\\ndiscourse grammar\\nimplemented in Carpenter's  ALE formalism.\\nThe method for building up the temporal structure of the discourse\\ncombines constraints and preferences: we use constraints to reduce the\\nnumber of possible structures, exploiting the  HPSG type hierarchy and\\nunification for this purpose; and we apply preferences to choose\\nbetween the remaining options using a temporal centering mechanism.\\nWe end by recommending that an underspecified representation of the structure\\nusing these techniques be used to avoid generating the temporal/rhetorical\\nstructure until higher-level information can be used to disambiguate.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nEllipsis is pervasive in natural language, and hence has received much\\nattention within both computational and theoretical linguistics.\\nHowever, the conditions under which a representation of an utterance\\nmay serve as a suitable basis for interpreting subsequent elliptical\\nforms remain poorly understood; specifically, past attempts to\\ncharacterize these processes within a single traditional module of\\nlanguage processing (e.g., considering either syntax, semantics, or\\ndiscourse in isolation) have failed to account for all of\\nthe data.  In this paper, we claim that a variety of facts concerning\\nellipsis resolution, event reference, and interclausal coherence can be\\nexplained by the interaction between the syntactic and semantic properties\\nof the form in question and the type of discourse inference operative\\nin establishing the coherence of the antecedent and elided clauses.\\n\\n\\nIn the next section, we introduce the facts concerning gapping,\\nVP-ellipsis, and non-elliptical event reference that we seek to explain.\\nIn Section 3, we categorize elliptical\\nand event referential forms according to two features: (1) whether the\\nexpression leaves behind an empty constituent in the syntax, and (2)\\nwhether the expression is anaphoric in the semantics.  In\\nSection 4 we describe two types of\\ndiscourse inference, namely Common Topic inference and Coherent Situation inference, and make a specific proposal concerning\\nthe interface between these and the syntactic and semantic\\nrepresentations they utilize.  In Section 5, we\\nshow how this proposal accounts for the data presented in\\nSection 2.  We contrast the account with\\nrelevant past work in Section 6, and conclude in\\nSection 7.\\n\\n\\n    Ellipsis and Interclausal Coherence\\n\\n\\nIt has been noted in previous work that the felicity of certain forms of\\nellipsis is dependent on the type of coherence relationship extant\\n between the antecedent and elided clauses ,. In this section we review the relevant facts for two such forms of\\nellipsis, namely gapping and VP-ellipsis, and also compare\\nthese with facts concerning non-elliptical event reference.\\n\\n\\nGapping is characterized by an antecedent sentence (henceforth called the\\nsource sentence) and the elision of all but two constituents\\n(and in limited circumstances, more than two constituents) in one\\nor more subsequent target sentences, as exemplified in sentence\\n (): \\n\\n\\nBill became upset, and Hillary\\n angry.  We are concerned here with a\\nparticular fact about gapping noticed by Levin and Prince\\n, namely that gapping is\\nacceptable only with the purely conjunctive symmetric meaning of\\nand conjoining the clauses, and not with its causal asymmetric meaning (paraphraseable by ``and as a result'').  That is,\\n while either of sentences () or () can have the  purely conjunctive reading, only sentence () can be understood to mean that Hillary's becoming angry was caused by or came\\nas a result of Bill's becoming upset.\\n\\n\\nBill became upset, and Hillary became\\n angry.  This can be seen by embedding each of these examples in a context that\\nreinforces one of the meanings.  For instance, gapping is felicitous\\n in passage (), where context supports the symmetric  reading, but is infelicitous in passage () under the  intended causal meaning of and. \\n\\n\\nThe Clintons want to get the national debate\\nfocussed on health care, and are getting annoyed because the media is\\npreoccupied with Whitewater.  When a reporter recently asked a\\nWhitewater question at a health care rally, Bill became\\nupset, and Hillary became/\\nangry.\\n\\n\\nHillary has been getting annoyed at Bill for his inability\\nto deflect controversy and do damage control.  She has repeatedly told\\nhim that the way to deal with Whitewater is to play it down and not to\\noverreact.  When a reporter recently asked a Whitewater question at a\\nhealth care rally, Bill became upset, and (as a result) Hillary\\nbecame/# \\nangry.\\n   The common stipulation within the literature stating that gapping applies to\\ncoordinate structures and not to subordinate ones does not account\\nfor why any coordinated cases are unacceptable.\\n\\n\\nVP-ellipsis is characterized by an initial\\nsource sentence, and a subsequent target sentence with a\\nbare auxiliary indicating the elision of a verb phrase:\\n\\n\\n Bill became upset, and Hillary did too.  \\n\\n\\nThe distribution of VP-ellipsis has also been shown to be sensitive to\\nthe coherence relationship extant between the source and target\\nclauses, but in a different respect.  In a previous paper\\n , five contexts for VP-ellipsis were examined to determine whether the representations\\nretrieved are syntactic or semantic in nature.  Evidence was given that\\nVP-ellipsis copies syntactic representations in what was termed parallel constructions (predicting the unacceptability\\n of the voice mismatch in example () and nominalized source in  example ()), but copies semantic representations in non-parallel constructions (predicting the acceptability of the voice  mismatch in example () and the nominalized source in example  ()): \\n\\n\\n# The decision was reversed by the FBI, and the ICC did\\n too.  [ reverse the decision ]   \\n\\n\\nIn March, four fireworks manufacturers asked that the decision be\\nreversed, and on Monday the ICC did. [ reverse the decision ]\\n\\n\\n# This letter provoked a response from Bush, and\\n Clinton did too.  [ respond ]   \\n\\n\\nThis letter was meant to provoke a response from\\n Clinton, and so he did. [ respond ]  These examples are analogous with the gapping cases in that\\nconstraints against mismatches of syntactic form hold for the\\nsymmetric (i.e., parallel) use of and  in examples\\n () and (), but not the asymmetric (i.e.,  non-parallel) meaning in examples () and  ().  In fact, it appears that gapping is felicitous in those constructions where VP-ellipsis requires a syntactic antecedent,\\nwhereas gapping is infelicitous in cases where VP-ellipsis requires\\nonly a suitable semantic antecedent.  Past approaches to VP-ellipsis\\nthat operate within a single module of language processing fail to\\nmake the distinctions necessary to account for these differences.\\n\\n\\nSag and Hankamer  note that while elliptical\\n sentences such as () are unacceptable because of a voice mismatch, similar examples with non-elided event referential forms\\nsuch as do it are much more acceptable:\\n\\n\\n   The decision was reversed by the FBI, and the ICC did it too.  [ reverse the decision ] \\nAn adequate theory of ellipsis and event reference must\\naccount for this distinction.\\n\\n\\nIn sum, the felicity of both gapping and VP-ellipsis appears to be\\ndependent on the type of coherence relation extant between the source\\nand target clauses.  Pronominal event reference, on the other hand,\\nappears not to display this dependence.  We seek to account for these\\nfacts in the sections that follow.\\n\\n\\n    Syntax and Semantics of Ellipsis and Event Reference\\n\\n\\nIn this section we characterize the forms being addressed in terms of\\ntwo features: (1) whether the form leaves behind an empty constituent\\nin the syntax, and (2) whether the form is anaphoric in the semantics.\\nIn subsequent sections, we show how the distinct mechanisms for\\nrecovering these types of missing information interact with two types\\nof discourse inference to predict the phenomena noted in the previous\\nsection.\\n\\n\\nWe illustrate the relevant syntactic and semantic properties of these\\nforms using the version of Categorial Semantics described in Pereira\\n.  In the Montagovian tradition, semantic\\nrepresentations are compositionally generated in correspondence with\\nthe constituent modification relationships manifest in the syntax;\\npredicates are curried.  Traces are associated with assumptions which\\nare subsequently discharged by a suitable construction.\\n Figure  shows the representations for the sentence Bill became upset; this will serve as the initial source clause\\n representation for the examples that follow. \\n\\n\\nFor our analysis of gapping, we follow Sag\\n in hypothesizing that a post-surface-structure\\nlevel of syntactic representation is used as the basis for\\ninterpretation.  In source clauses of gapping constructions,\\nconstituents in the source that are parallel to the overt constituents\\nin the target are abstracted out of the clause\\n representation. For simplicity, we will assume that this abstraction is achieved by fronting the constituents in the\\npost-surface-structure, although nothing much hinges on this; our\\nanalysis is compatible with several possible mechanisms.  The\\nsyntactic and semantic representations for the source clause of\\n example () after fronting are shown in Figure  ; the fronting leaves trace assumptions behind that are discharged when combined with their antecedents.\\n\\n\\nTarget clauses in gapping constructions are therefore represented with\\nthe overt constituents fronted out of an elided sentence node; for\\ninstance the representation of the target clause in example\\n () is shown in Figure  (the empty node is indicated by ).\\nThe empty constituent is reconstructed\\nby copying the embedded sentence from the source to the\\ntarget clause, along with parallel trace assumptions which are to be\\nbound within the target.  The semantics for this embedded sentence is\\nthe open proposition that the two clauses share.  This semantics, we\\nclaim, can only be recovered by copying the syntax, as gapping\\ndoes not result in an independently anaphoric expression in the\\n semantics.  In fact, as can be  seen from Figure    ,  before copying takes place there is no sentence-level semantics for gapped\\nclauses at all.\\n\\n\\nLike gapping, VP-ellipsis results in an empty constituent in the\\nsyntax, in this case, a verb phrase.  However, unlike gapping,\\nVP-ellipsis also results in an independently anaphoric form in the\\n semantics.  Figure     shows the representations for the clause Hillary did (the anaphoric expression is indicated by P).\\n\\n\\n Given the representation in Figure  as the source, the semantics for the missing VP may be\\nrecovered in one of two ways.  The syntactic VP could be copied down\\nwith its corresponding semantics, from which the semantics for the\\ncomplete sentence can be derived.  In this case, the anaphoric\\nexpression is constrained to have the same semantics as the copied\\nconstituent.  Alternatively, the anaphoric expression could be\\nresolved purely semantically, resulting in the discharge of the\\nanaphoric assumption P.  The higher-order unification method\\ndeveloped by Dalrymple et al.  could be used\\nfor this purpose; in this case the sentence-level semantics is\\nrecovered without copying any syntactic representations.\\n\\n\\nEvent referential forms such as do it, do that, and do so constitute full verb phrases in the syntax.  It has been often\\n noted , inter alia] that it is the main verb do that is operative in these forms of anaphora, in contrast to\\n the auxiliary do operative in VP-ellipsis. It is the pronoun in event referential forms that is anaphoric; the fact that\\nthe pronouns refer to events results from the type constraints imposed by the\\nmain verb do.  Therefore, such forms are anaphoric in the\\nsemantics, but do not leave behind an empty constituent in the syntax.\\n\\n\\nTo summarize this section, we have characterized the forms being\\naddressed according to two features, a summary of which appears in\\n Table . \\nWhereas anaphoric forms in the semantics for these forms are\\nindependently resolved, empty syntactic constituents in and of\\nthemselves are not anaphoric, and thus may only be restored when some\\nindependently-motivated process necessitates it.  In the section that\\nfollows we outline two types of discourse inference, one of which\\nrequires such restoration of empty constituents.\\n\\n\\n    Discourse Inference\\n\\n\\nTo be coherent, utterances within a discourse segment require more\\nthan is embodied in their individual syntactic and semantic\\nrepresentations alone; additional inter-utterance constraints must be\\nmet.  Here we describe two types of inference used to enforce the\\nconstraints that are imposed by coherence relations.\\nIn each case, arguments to coherence relations take\\nthe form of semantic representations retrieved by way of their\\ncorresponding node(s) in the syntax; the operations performed on these\\nrepresentations are dictated by the nature of the constraints imposed.  The two\\ntypes of inference are distinguished by the level in the syntax from\\n which these arguments are retrieved. \\n\\n  Common Topic Inference \\n\\nUnderstanding segments of utterances standing in a Common Topic\\nrelation requires the determination of points of commonality\\n(parallelism) and departure (contrast) between sets of corresponding\\nentities and properties within the utterances.  This process is reliant\\non performing comparison and generalization operations on the\\n corresponding representations ,,,.  Table  sketches definitions for some Common Topic relations, some taken from and others adapted from Hobbs\\n.  In each case, the hearer is to understand the\\nrelation by inferring \\n\\np0(a1,...,an) from sentence S0 and\\ninferring \\n\\np1(b1,...,bn) from sentence S1 under the listed\\n constraints. In order to meet these constraints, the identification of p0 and p1 may require arbitrary\\nlevels of generalization from the relations explicitly stated in the\\nutterances.\\n\\n\\n  Examples of these relations are given in sentences (). \\n\\n\\n   John organized rallies for Clinton, and Fred distributed pamphlets for him.  (Parallel)\\nJohn supported Clinton, but Mary supported Bush.  (Contrast)\\nYoung aspiring politicians usually support their party's\\npresidential candidate.  For instance,\\nJohn campaigned hard for Clinton in 1992. (Exemplification)\\nA young aspiring politician was arrested in Texas today.  John\\nSmith, 34, was nabbed in a Houston law firm while attempting to\\nembezzle funds for his campaign. (Elaboration)\\n Passage (), for instance, is coherent under the  understanding that John and Fred have a common property, namely having done something to support Clinton.  Passage () is likewise coherent by virtue of the inferences resulting from\\nidentifying parallel elements and properties, including that John is a\\nyoung aspiring politician and that he's a Democrat (since Clinton is\\nidentified with his party's candidate).  The characteristic that\\nCommon Topic relations share is that they require the identification\\nof parallel entities (i.e., the ai and bi) and relations\\n(p0 and p1) as arguments to the constraints.  We posit\\nthat the syntactic representation is used both to guide the\\nidentification of parallel elements and to retrieve their\\nsemantic representations.\\n\\n\\n  Coherent Situation Inference \\n\\nUnderstanding utterances standing in a Coherent\\nSituation relation requires that hearers convince themselves that the\\nutterances describe a coherent situation given their knowledge of the\\nworld.  This process requires that a path of inference be established between\\nthe situations (i.e., events or states) described in the participating\\nutterances as a whole, without regard to any constraints on\\nparallelism between sub-sentential constituents.\\nFour such relations are summarized in\\n Table .  In all four cases, the hearer is to infer A from sentence S1 and B from sentence S2under the constraint that the presuppositions\\n listed be abduced : \\n\\n\\n  Examples of these relations are given in sentences (). \\n\\n\\n  Bill is a politician, and therefore he's dishonest. (Result) Bill is dishonest because he's a politician.  (Explanation)\\nBill is a politician, but he's honest.  (Violated Expectation)\\nBill is honest, even though he's a politician. (Denial of Preventer)\\nBeyond what is asserted by the two clauses individually, understanding\\neach of these sentences requires the presupposition that being a\\npolitician implies being dishonest.  Inferring this is only reliant\\non the sentential-level semantics for the clauses as a whole;\\nthere are no p,\\nai, or bi to be independently identified. The same is true for what Hume\\ncalled Contiguity relations (perhaps including Hobbs' Occasion and Figure-ground relations); for the purpose of this\\npaper we will consider these as weaker cases of Cause or Effect.\\n\\n\\nTo reiterate the crucial observation, Common Topic inference utilizes\\nthe syntactic structure in identifying the semantics for the\\nsub-sentential constituents to serve as arguments to the coherence\\nconstraints.  In contrast, Coherent Situation inference utilizes only\\nthe sentential-level semantic forms as is required for abducing a\\ncoherent situation.  The question then arises as to what happens when\\nconstituents in the syntax for an utterance are empty.  Given that the\\ndiscourse inference mechanisms retrieve semantic forms through nodes\\nin the syntax, this syntax will have\\nto be recovered when a node being accessed is missing.  Therefore, we\\nposit that missing constituents are recovered as a by-product of Common\\nTopic inference, to allow the parallel properties and entities serving\\nas arguments to the coherence relation to be accessed from within the\\nreconstructed structure.  On the other hand, such copying is not\\ntriggered in Coherent Situation inference, since the arguments are\\nretrieved only from the top-level sentence node, which is always\\npresent.  In the next section, we show how this difference accounts\\nfor the data given in Section 2.\\n\\n\\n\\n    Applying the Analysis\\n\\n\\nIn previous sections, we have classified several elliptical and event\\nreferential forms as to whether they leave behind an empty constituent in\\nthe syntax and whether they are anaphoric  in the\\nsemantics.  Empty constituents in the syntax are not in themselves\\nreferential, but are recovered during Common Topic inference.\\nAnaphoric expressions in the semantics are independently referential\\nand are resolved through purely semantic means regardless of the type\\nof discourse inference.  In this section we show how the\\nphenomena presented in Section 2 follow from these properties.\\n\\n    Local Ellipsis\\n\\n\\nRecall from Section 2 that\\n gapping constructions such as () are only felicitous with the symmetric (i.e., Common Topic) meaning of and:\\n\\n\\n Bill became upset, and Hillary angry.  This fact is predicted by our account in the following way.  In the\\ncase of Common Topic constructions, the missing sentence in the target\\nwill be copied from the source, the sentential semantics may be\\nderived, and the arguments to the coherence relations can be\\nidentified and reasoning carried out, predicting felicity.  In the\\ncase of Coherent Situation relations, no such recovery of the syntax\\ntakes place.  Since a gapped clause in and of itself has no sentence-level\\nsemantics, the gapping fails to be felicitous in these cases.\\n\\n\\nThis account also explains similar differences in felicity for other\\ncoordinating conjunctions as discussed in Kehler\\n, as well as why gapping is\\ninfelicitous in constructions with subordinating conjunctions\\nindicating Coherent Situation relations, as exemplified in\\n (). \\n\\n\\n# Bill became upset,  \\n\\n\\n Hillary angry.  \\n\\n\\nThe stripping construction is similar to gapping except that there\\nis only one bare constituent in the target (also generally receiving\\ncontrastive accent); unlike VP-ellipsis there is no stranded\\nauxiliary.  We therefore might predict that stripping is also\\nacceptable in Common Topic constructions but not in Coherent Situation\\n constructions, which appears to be the case: \\n\\n\\nBill became upset,   \\n\\n\\n Hillary.   \\n\\n\\nIn summary, gapping and related constructions are infelicitous in those\\ncases where Coherent Situation inference is employed, as there is no\\nmechanism for recovering the sentential semantics of the elided clause.\\n\\n\\n    VP-Ellipsis\\n\\n\\nRecall from Section 2 that only in Coherent Situation\\nconstructions can VP-ellipsis obtain purely semantic antecedents\\nwithout regard to constraints on structural parallelism, as\\n exemplified by the voice mismatches in sentences () and  (). \\n\\n\\n# The decision was reversed by the FBI, and the ICC did\\n too.  [ reverse the decision ]   \\n\\n\\nIn March, four fireworks manufacturers asked that the decision be\\nreversed, and on Monday the ICC did. [ reverse the decision ]\\n  These facts are also predicted by our account.  In the case of Common\\nTopic constructions, a suitable syntactic antecedent must be\\nreconstructed at the site of the empty VP node, with the result that\\nthe anaphoric expression takes on its accompanying semantics.\\nTherefore, VP-ellipsis is predicted to require a suitable syntactic\\nantecedent in these scenarios.  In Coherent Situation constructions,\\nthe empty VP node is not reconstructed.  In these cases the anaphoric\\nexpression is resolved on purely semantic grounds; therefore VP-ellipsis is\\nonly constrained to having a suitable semantic antecedent.\\n\\n\\nThe analysis accounts for the range of data given in Kehler\\n,\\nalthough one point of departure exists between that\\naccount and the current one with respect to clauses conjoined with\\nbut.  In the previous account these cases are all classified as\\nnon-parallel, resulting in the prediction that they only require\\nsemantic source representations.  In our analysis, we expect cases of\\npure contrast to pattern with the parallel class since\\nthese are Common Topic constructions; this is opposed to the violated expectation use of but which indicates a Coherent\\nSituation relation.  The current account makes the\\n correct predictions; examples () and (), where but has the contrast meaning, appear to be markedly less  acceptable than examples () and (), where but  has the violated expectation meaning: \\n\\n\\n?? Clinton was introduced by John, but Mary didn't.\\n [ introduce Clinton ]  \\n\\n\\n?? This letter provoked a response from Bush, but\\n Clinton didn't. [ respond ]  Clinton was to have been introduced by someone, but\\nobviously nobody\\n did. [ introduce Clinton ]  \\n\\n\\nThis letter deserves a response, but before you do, ...\\n [ respond ]    To summarize thus far, the data presented in the earlier account\\nas well as examples that\\nconflict with that analysis are all predicted by the account given here.\\n\\n\\nAs a final note, we consider the interaction between VP-ellipsis and\\ngapping.  The following pair of examples are adapted from those of Sag\\n:\\n\\n\\nJohn supports Clinton, and Mary \\nBush, although\\n she doesn't know why she does.  \\n\\n\\n?? John supports Clinton, and Mary \\nBush, and\\n Fred does too.   Sag defines an alphabetic variance condition that correctly predicts\\n that sentence () is infelicitous, but incorrectly predicts  that sentence () is also.  Sag then suggests a weakening of his condition, with the result that both of the above\\nexamples are incorrectly predicted to be acceptable;  he doesn't\\nconsider a solution predicting  the judgements  as stated.\\n\\n\\n The felicity of sentence () and the infelicity of sentence  () are exactly what our account predicts.  In example  (), the third clause is in a Common Topic relationship with the second (as well as the first) and therefore requires that the VP\\nbe reconstructed at the target site.  However, the VP is not in a\\nsuitable form, as the object has been abstracted out of it (yielding a\\ntrace assumption).  Therefore, the subsequent VP-ellipsis fails to be\\nfelicitous.  In contrast, the conjunction although used before\\n the third clause in example () indicates a Coherent Situation relation.  Therefore, the VP in the third clause need not be\\nreconstructed, and the subsequent semantically-based resolution of the\\nanaphoric form succeeds.  Thus, the apparent paradox between examples\\n () and () is just what we would expect. \\n\\n\\n    Event Reference\\n\\n\\nRecall that Sag and Hankamer\\n note that whereas\\n elliptical sentences such as () are unacceptable due to a voice mismatch, similar examples with event referential forms are much\\n more acceptable as exemplified by sentence (): \\n\\n\\n   # The decision was reversed by the FBI, and the ICC did too.  [ reverse the decision ]\\nThe decision was reversed by the FBI, and the ICC did\\nit too.  [ reverse the decision ] \\nAs stated earlier, forms such as do it are anaphoric, but leave no empty\\nconstituents in the syntax.  Therefore, it follows under the present\\naccount that such reference is successful without regard to the type\\nof discourse inference employed.\\n\\n\\n\\n    Relationship to Past Work\\n\\n\\nThe literature on ellipsis and event reference is voluminous, and so\\nwe will not attempt a comprehensive comparison here. Instead, we\\nbriefly compare the current work to three previous studies that\\nexplicitly tie ellipsis resolution to an account of discourse\\nstructure and coherence, namely our previous account\\n  and the accounts of Prst .\\n\\n\\nIn Kehler , we presented an analysis of\\nVP-ellipsis that distinguished between two types of relationship\\nbetween clauses, parallel and non-parallel.  An\\narchitecture was presented whereby utterances were parsed into\\npropositional representations which were subsequently integrated into\\na discourse model.  It was posited that VP-ellipsis could access\\neither propositional or discourse model representations: in the\\ncase of parallel constructions, the source resided in the\\npropositional representation; in the case of non-parallel\\nconstructions, the source had been integrated into the discourse\\nmodel.  In Kehler\\n, we showed how this architecture also accounted\\nfor the facts that Levin and Prince noted about gapping.\\n\\n\\nThe current work improves upon that analysis in several respects.\\nFirst, it no longer needs to be posited that syntactic representations\\n disappear when integrated into the discourse model; instead, syntactic and semantic representations co-exist. Second, various issues with regard to the interpretation of\\npropositional representations are now rendered moot.  Third, there is\\nno longer a dichotomy with respect to the level of representation from\\nwhich VP-ellipsis locates and copies antecedents.  Instead, two\\ndistinct factors have been separated out: the resolution of missing\\nconstituents under Common Topic inference is purely syntactic whereas\\nthe resolution of anaphoric expressions in all cases is purely\\nsemantic; the apparent dichotomy in VP-ellipsis data arises out of the\\ninteraction between these different phenomena.  Finally,\\nthe current approach more readily scales up to more complex cases.  For\\ninstance, it was not clear in the previous account how non-parallel\\nconstructions embedded within parallel constructions would be handled,\\n as in sentences (): \\n\\n\\n  Clinton was introduced by John because Mary had refused to, and Gore was too.  [ introduced by John because Mary had refused to ]\\n# Clinton was introduced by John because Mary had refused to,\\nand Fred did too.  [ introduced Clinton because Mary had refused to ]\\nThe current approach accounts for these cases.\\n\\n\\nThe works of Prst  and Asher\\n provide analyses of\\n VP-ellipsis in the context of an account of discourse structure and coherence.  With Prst utilizing a mixed\\nrepresentation (called syntactic/semantic structures) and Asher\\nutilizing Discourse Representation Theory constructs, each defines\\nmechanisms for determining relations such as parallelism and contrast,\\nand gives constraints on resolving VP-ellipsis and related forms\\nwithin their more general frameworks.  However, each essentially\\nfollows Sag in requiring that elided VP representations be alphabetic\\nvariants of their referents.  This constraint rules out cases where\\nVP-ellipsis obtains syntactically mismatched antecedents, such as\\n example () and other non-parallel cases given in Kehler .  It also appears that neither approach\\ncan account for the infelicity of mixed gapping/VP-ellipsis\\n cases such as sentence (). \\n\\n\\n    Conclusion\\n\\n\\nIn this paper, we have categorized several forms of ellipsis and event\\nreference according to two features: (1) whether the form leaves\\nbehind an empty constituent in the syntax, and (2) whether the form is\\nanaphoric in the semantics.  We have also described two forms of\\ndiscourse inference, namely Common Topic inference and Coherent Situation inference.  The interaction between the two\\nfeatures and the two types of discourse inference predicts facts\\nconcerning gapping, VP-ellipsis, event reference, and interclausal\\ncoherence for which it is otherwise difficult to account.  In future\\nwork we will address other forms of ellipsis and event reference, as\\nwell as integrate a previous account of strict and sloppy ambiguity into\\n this framework . \\n\\n\\n  Acknowledgments \\n\\nThis work was supported in part by National Science Foundation Grant\\nIRI-9009018, National Science Foundation Grant IRI-9350192, and a\\ngrant from the Xerox Corporation.  I would like to thank Stuart\\nShieber, Barbara Grosz, Fernando Pereira, Mary Dalrymple, Candy\\nSidner, Gregory Ward, Arild Hestvik, Shalom Lappin, Christine Nakatani,\\nStanley Chen, Karen Lochbaum, and two anonymous reviewers for valuable\\ndiscussions and comments on earlier drafts.\\n\\nBibliography \\n\\nNicholas Asher.\\n1993.\\nReference to Abstract Objects in Discourse.\\nSLAP 50, Dordrecht, Kluwer.\\n\\n\\nMary Dalrymple, Stuart M. Shieber, and Fernando Pereira.\\n1991.\\nEllipsis and higher-order unification.\\nLinguistics and Philosophy, 14:399-452.\\n\\n\\nM.A.K. Halliday and Ruqaiya Hasan.\\n1976.\\nCohesion in English.\\nLongman's, London.\\nEnglish Language Series, Title No. 9.\\n\\n\\nJerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul Martin.\\n1993.\\nInterpretation as abduction.\\nArtificial Intelligence, 63:69-142.\\n\\n\\nJerry Hobbs.\\n1990.\\nLiterature and Cognition.\\nCSLI Lecture Notes 21.\\n\\n\\nDavid Hume.\\n1748.\\nAn Inquiry Concerning Human Understanding.\\nThe Liberal Arts Press, New York, 1955 edition.\\n\\n\\nAndrew Kehler.\\n1993a.\\nA discourse copying algorithm for ellipsis and anaphora resolution.\\nIn Proceedings of the Sixth Conference of the European Chapter\\n  of the Association for Computational Linguistics (EACL-93), pages 203-212,\\n  Utrecht, the Netherlands, April.\\n\\n\\nAndrew Kehler.\\n1993b.\\nThe effect of establishing coherence in ellipsis and anaphora\\n  resolution.\\nIn Proceedings of the 31st Conference of the Association for\\n  Computational Linguistics (ACL-93), pages 62-69, Columbus, Ohio, June.\\n\\n\\nAndrew Kehler.\\n1994.\\nA discourse processing account of gapping and causal implicature.\\nManuscript presented at the Annual Meeting of the Linguistic Society\\n  of America, January.\\n\\n\\nNancy Levin and Ellen Prince.\\n1982.\\nGapping and causal implicature.\\nPresented at the Annual Meeting of the Linguistic Society of America.\\n\\n\\nFernando Pereira.\\n1990.\\nCategorial semantics and scoping.\\nComputational Linguistics, 16(1):1-10.\\n\\n\\nEllen Prince.\\n1986.\\nOn the syntactic marking of presupposed open propositions.\\nIn Papers from the Parasession on pragmatics and grammatical\\n  theory at the 22nd regional meeting of the Chicago Linguistics society,\\n  pages 208-222, Chicago, IL.\\n\\n\\nHub Prst.\\n1992.\\nOn Discourse Structuring, VP Anaphora, and Gapping.\\nPh.D. thesis, University of Amsterdam.\\n\\n\\nIvan Sag and Jorge Hankamer.\\n1984.\\nToward a theory of anaphoric processing.\\nLinguistics and Philosophy, 7:325-345.\\n\\n\\nIvan Sag.\\n1976.\\nDeletion and Logical Form.\\nPh.D. thesis, MIT.\\n\\n\\nRemko Scha and Livia Polanyi.\\n1988.\\nAn augmented context free grammar for discourse.\\nIn Proceedings of the International Conference on Computational\\n  Linguistics (COLING-88), pages 573-577, Budapest, August.\\n\\n\\nMark Steedman.\\n1990.\\nGapping as constituent coordination.\\nLinguistics and Philosophy, 13(2):207-263.\\n\\nFootnotes\\n\\n  This behavior is not\\nlimited to the conjunction and; a similar distinction holds\\nbetween symmetric and asymmetric uses of or and but.  See\\nKehler  for further discussion.\\n  These examples have been taken or adapted from\\nKehler .  The phrases shown in brackets indicate the\\nelided material under the intended interpretation.\\n  We will ignore\\nthe tense of the predicates for ease of exposition.\\n  It has been noted that in gapping\\nconstructions, contrastive accent is generally placed on parallel\\nelements in both the target and the source clauses, and that\\nabstracting these elements results in an ``open proposition'' that\\nboth clauses share\\n ,,.  This proposition needs to be presupposed (or accommodated) for the gapping to be felicitous, for\\ninstance, it would be infelicitous to open a conversation with\\n sentence such as (), whereas it is perfectly felicitous in response to the question How did the Clintons react?.  Gapping\\nresolution can be characterized as the restoration of this open proposition in\\nthe gapped clause.\\n  This claim is supported by well-established facts\\nsuggesting that gapping does not pattern with standard forms of\\nanaphora.  For instance, unlike VP-ellipsis and overt pronouns,\\ngapping cannot be cataphoric, and can only obtain its\\nantecedent from the immediately preceding clause.\\n  Unlike gapping, VP-ellipsis patterns with\\nother types of anaphora, for instance it can be cataphoric and can\\nlocate antecedents from clauses other than the most immediate one.\\n  For\\ninstance, other auxiliaries can appear in elided forms but cannot be\\nfollowed by it, that, or so as in example\\n (), and a pronominal object to the main verb do cannot  refer to a state as VP-ellipsis can as in example (). \\n\\n\\nGeorge was going to the golf course and Bill was /(#\\n it)/(# that)/(# so) too.  \\n\\n\\nBill dislikes George and Hillary does /(#\\n it)/(# that)/(# so) too.  \\n  Hobbs\\n, following Hume ##1internalciteHume, suggests a\\nclassification of coherence relations into three broad categories,\\nnamely Resemblance, Cause or Effect, and Contiguity\\n(Hume's terminology).  Here, Resemblance relations appear to\\npattern well with those employing our Common Topic inference, and likewise\\nCause or effect and Contiguity with our Coherent Situation\\ninference.\\n  Following Hobbs, by  ai and bi being similar we mean\\nthat for some salient property qi, qi(ai) and qi(bi) holds.\\nLikewise by dissimilar we mean that for some qi,\\nqi(ai) and \\n\\n\\nholds.\\n  These relations are what Hume\\nmight have termed Cause or Effect.\\n  We are using\\nimplication in a very loose sense here,\\nas if to mean ``could plausibly follow from''.\\n  Stripping is also\\npossible in comparative deletion constructions.  A comprehensive\\nanalysis of stripping, pseudo-gapping, and VP-ellipsis in such cases\\nrequires an articulation of a syntax and semantics for these\\nconstructions, which will be carried out in future work.\\n  These examples\\nhave been adapted from several in Kehler .\\n  Sag and\\nHankamer\\nclaim that all such cases of VP-ellipsis require syntactic\\nantecedents, whereas we suggest that in Coherent Situation relations\\nVP-ellipsis operates more like their Model-Interpretive\\nAnaphora, of which do it is an example.\\n  This\\nclaim could be dispensed with in the treatment of VP-ellipsis,\\nperhaps at the cost of some degree of theoretical inelegance.\\nHowever, this aspect was crucial for handling the gapping data, since\\nthe infelicity of gapping in non-parallel constructions hinged on\\nthere no longer being a propositional representation available as a\\nsource.\\n  In addition, Prst  addresses gapping, and Asher\\naddresses event reference.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nIt is claimed that a variety of facts concerning ellipsis, event\\nreference, and interclausal coherence can be explained by two features\\nof the linguistic form in question: (1) whether the form  leaves\\nbehind an empty constituent in the syntax, and (2) whether the\\nform is anaphoric in the semantics.  It is proposed that these\\nfeatures interact with one of two types of discourse inference, namely\\nCommon Topic inference and Coherent Situation inference.\\nThe differing ways in which these types of inference utilize syntactic\\nand semantic representations predicts phenomena for which it\\nis otherwise difficult to account.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nOur aim is to formalize constraints that are needed to develop a\\nparser based on unification grammar (called ``UG'' henceforth) so that\\nour parser can deal with variety of types of sentences in Japanese.\\nHowever just parsing syntactically is not enough for natural language\\nunderstanding.  One important and necessary task to be done, when a\\nparser processes a discourse in Japanese, is the so called zero\\nanaphora resolution.  All of syntactic, semantic, and pragmatic\\nconstraints are to be involved to resolve zero anaphora.  Of course, some\\nof omitted pronouns are syntactically resolved. For instance, VP with\\nsuffix te is not regarded as a clause but a conjunct VP.\\nTherefore the subject of the VP with te, which is possibly\\nomitted from surface, should corefer with the subject of the sentence.\\nOne example is\\n\\n\\n`Hanako felt  cold and closed the window.'\\n\\n\\nwhere both of zero subjects \\n\\n\\nand \\n\\n  refer to the sentential topic Hanako\\n  .  In this example, one of the possible accounts for this\\ninterpretation is the following. Zero subject of -te phrase is [ +\\n anaphoric, + pronominal ] or PRO in GB term . As the result, \\n\\n\\nis controlled by the subject \\n\\nof the main VP, which is also zero subject.  \\n\\n\\nis, in GB\\nterm, [ - anaphoric, + pronominal ] or pro.  The sentential\\ntopic Hanako is the only possible antecedent of this zero\\nsubject in this example.  However, in complex sentences, things are\\nquite different.  Consider the following sentence.\\n\\n\\n1. `Since Hanako behaved like feeling cold, I closed the window.'\\n2. `Since I behaved like feeling cold, Hanako closed the window.'\\n\\n\\nIf contextually we can take only Hanako and the speaker of this sentence\\nas candidates of antecedent of \\n\\n\\nor \\n\\n,\\nintuitively the following two interpretations are equally likely.\\n\\n\\na.\\n\\n\\n= Hanako, \\n\\n\\n= speaker\\nb.\\n\\n\\n= speaker, \\n\\n\\n= Hanako\\n\\n\\nTherefore \\n\\n\\nand \\n\\n\\nare both pro. In fact this\\nfact is well known among Japanese linguists, i.e.\\n ,.  As a result, zero anaphora resolution of complex sentence is not only to be done syntactically, but also to be done\\npragmatically and/or semantically.  One of the promising candidate for\\n this is the centering theory ,.  To apply the centering theory that is originally for a sequence of sentences, namely\\ndiscourse, we regard the subordinate clause and the main clause as\\na segment of discourse respectively. Moreover Hanako who is marked by\\n`wa' is regarded as the topic for these two clauses. Then, the topic\\nHanako is the strongest candidate for the backward center of the\\nsubordinate clause. Therefore the backward center of the subordinate\\nclause is Hanako, and consequently zero subject \\n\\n\\nrefers to\\nHanako. By the same way as the subordinate clause case is dealt with,\\nthe zero subject of the main clause \\n\\n\\nis known to refer to\\nHanako, too. This result is neither interpretation a nor b\\nshown above. Another candidate is the property sharing thoery\\n . In her theory, since the both of zero subjects share the subjecthood, both of them finally are known to refer to Hanako that\\nis the topic for both of these clauses. Therefore the property sharing\\ntheory also fails to account for the intuitive interpretations.\\n\\n\\nThen we shift our attention to more microscopic one, in\\nwhich ,roughly speaking, the important part of semantics of complex\\nsentence is formalized as relations among semantic roles that appear in\\nthe main clause or the subordinate clause. At the first glance, the\\nconstraints about these relations are not local in terms of main or\\nsubordinate clauses. In other words, semantic roles that appear in\\nsubordinate clause and semantic roles that appear in the main clause\\nseem to be directly constrained by the constraints of complex sentence.\\nHowever, looking more carefully, we find that the constraints of\\nsubordinate clause and the constraints of main clause are represented as\\nlocal constraints by introducing the new notion of motivated which\\nis characterized as a person who has enough reason to act as the main\\nclause describes.  More precisely, motivated is one of the\\npragmatic roles that appear in a subordinate clause, and the constraints\\nin subordinate clause are stated as identity relations between motivated and other semantic/pragmatic roles appearing in subordinate\\nclause.  Therefore these constraints are local in subordinate clause.\\nThe constraints in main clause are stated as identity relations between\\nmotivated which comes from subordinate clause, and other semantic\\nroles appearing in main clause.  Therefore in understanding the main\\nclause we don't have to be care about semantic/pragmatic roles in\\nsubordinate clause other than a motivated. In this sense, the\\nconstraints in the main clause can be treated as almost local\\nconstraints of the main clause.\\n\\n\\nThe next question is how to represent the semantics of complex sentence\\nin feature structure( called FS henceforth ).  For this, we should write\\ndown the constraints about these relations among semantic/pragmatic\\nroles in a feature structure formalism.\\nDue to the space limitation, in this\\npaper we mainly pursue the constraints about semantic feature\\nstructures.\\n\\n\\n  Hierarchical Structure of Complex Sentence \\n\\nWe pay our attention to the general structure of Japanese utterance\\nwhich is helpful to represent semantics of complex sentence.  Several\\nJapanese linguists have already proposed the general structure of\\n Japanese utterances ,,,. Mikami categorized clauses into three classes, namely `open', `semi-open' and\\n`closed.' This categorization indicates how freely the content of\\nclause interacts with the outside of clause. For instance, they are\\ncategorized by the degree of possibilities of coreference between zero\\npronouns inside the subordinate clause and nominal or topic that\\nappear in the main clause. Following Mikami's idea, Minami proposed\\nfour levels, namely level A, B, C and D which correspond roughly to\\nVP, proposition, sentence without communication mood and utterance\\nwhich takes into account a hearer, respectively.\\n  divided level A into two levels. One of them corresponds to VP, the other corresponds to VP + a certain\\nkind of subject which is called ``objective subject.''\\nGunji proposed the more detailed structure, in which starting from\\npredicate, say, verb and adjective, objects, voice,\\nsubject, aspect, tense, modality, topic and mood are or\\nmight be sequentially added to make an informationally more fulfilled\\nsentence component. Finally, it ends up with an utterance. In Gunji's\\nstructure, some node can have more than two daughter nodes to make\\nmore complex sentence. Following them, the structure of the so called\\n(cluase level) complex sentence is the following shown in\\nFig.1.\\n\\n\\nIn Fig.1 , Sub-Clause and Conjunct mean subordinate clause and\\nconjunctive particle respectively.  Note that Fig.1 represents not only\\nthe hierarchical structure but also the word order of a complex sentence\\nin Japanese. The structure is almost the same as Gunji's structure\\nexcept for explicitly showing complex proposition, subordinate-clause\\nand conjunctive-particle that are newly added to deal with complex\\nsentences. Note that `Comment' appearing in `Sub-Clause' has the same\\nstructure as `Comment' appearing just below `Judgement'. That is to say,\\n`Comment' is recursively defined. However, in practice, the more the\\nlevel of depth of recursively appearing `Comment' is, the less\\ncomprehensible the sentence is.\\n\\n\\n\\n\\n\\n  Subordinate Clause \\n\\nIn this section, at first we show the predicate categories used in the\\nsubordinate clauses that we deal with in this paper, in\\n Table.. \\nIn each category of 2,3,4,5 and 6,\\nexists there a person who is affected by the situation described by\\nthe subordinate clause.  On the contrary, in category 1, there is\\nnot necessarily an explicit affected person. In our theory, this\\naffected person plays a key role for semantics of complex sentence. As\\nthe result, in general we cannot derive a useful result for category 1\\n in our theory.  Therefore we don't deal with category 1\\nin this paper.\\n\\n\\nAt this moment, we should explain the nature of the so called subjective\\n predicate mentioned in Table..  In short a subjective predicate describes the experiencer's inner state which can\\nexclusively be known by the experiencer him/herself.\\n\\n\\nNext we focus on verbal suffix garu. Firstly we show garu's syntax. Garu is the present form and its root form is\\ngar. Therefore inflections are as follows: gar-re,gar-i, etc. In addition, garu has an allophonic\\nroot form gat and, gat-ta(past-form), gat-teiru(progressive-form) and so on are derived from gat.\\nSome of these forms will appear in our examples.  Next we talk about\\nthe semantics of garu.  Garu roughly means ``show a sign\\n of'' or ``behave like ..ing''.  Also in  its semantics is informally explained, however our proposal is to\\nformalize garu's semantics in UG or more generally in\\ncomputational linguistics.  For this, first of all, we introduce a new\\npragmatic role called observer.\\n\\n\\nDefinition  1 (Observer)    \\nObserver is a person who directly observes or is indirectly informed the\\nsituation described by the proposition part. Therefore an observer has a\\ncertain evidence to be convinced that that situation actually happens.\\n\\n\\nAlthough this notion of observer shares a large part with PIVOT of\\n , our notion of observer is introduced only by garu.  Therefore it is much narrower notion.  As you will see later, this newly introduced role is playing a key role which bridges semantic\\nroles of subordinate clause to semantic roles of main clause.\\n\\n\\nAs for an observer introduced by garu, one of the widely\\nknown consequence about the nature of subjective predicate is the\\nfollowing. In a sentence, if a subjective adjective is used without\\nbeing followed by a verbal suffix\\ngaru, the experiencer of the subjective adjective should be the\\nspeaker of the sentence.\\n\\n\\nThe next thing we should do about a newly introduced notion of observer is to make clear the way to deal with it in FS. First of all,\\nin our FS, a semantic content:SEM is basically a soa (state of affair)\\nform of situation semantics. However we use semantic role like ``agent'', ``patient'', ``experiencer'', and so on, as\\nargument roles of soa.  Since an observer observes the situation\\nwhich is characterized by a soa, if we know that there exists an observer, the observed soa is embedded in observing situation, which,\\nin turn, is embedded in the whole semantic content. In this sense, the\\nobserved soa's argument role is observed.  But as far as we have\\nno confusion, we omit role name `observed' henceforth.  A typical schema\\nof SEM of FS of this type is the following. Note that we use garu\\nas a value of the relation feature meant by `rel.' The English gross of\\nthis relation garu is `observe.'\\n\\n\\nSEM =\\nrel: garu\\n         observer:#11\\n         soa:rel:Ragent:#12experiencer:#13\\n                             patient:#14....\\n\\n\\nNow we explain the semantics of clause which consists of subjective\\nadjective with garu or ta-garu, that are in categories\\n4 and 5.\\nThese categories' forms are ``\\n\\n\\nP-garu'' or its past form\\n``\\n\\n\\nP-gat-ta'', where P is a subjective adjective (category\\n 4 in Table.) or is a verb followed by ta-gar (category  5 in Table.), and  \\n\\nis the experiencer of P which\\nis possibly zero. In these categories, there exist observers who are\\nnot the experiencer of P, and observe that experience. The\\nSEM feature of ``\\n\\n\\nP-garu/gat-ta'' is the following.\\n\\n\\nrel:garuobserver:#11\\nwhere \\n\\n\\nsoa:rel:Pexp:#13\\n\\n\\nwhere `` \\n'' means ``not token\\nidentical.''\\n\\n\\nIn our FS, constraints for tokens like \\nare written with\\n``where'' as shown in this FS. Since constraint satisfaction method in\\nUG has been and is developed by many researchers recently i.e.\\n , our theory will be able to be implemented in systems like theirs.\\n\\n\\nIf the sentence finishes just after ``garu/gat-ta'', the important\\npoints are 1) an introduced observer is the speaker, and\\nconsequently 2) the experiencer cannot be the speaker. If a clause\\nwith ``garu/gat-ta''is a subordinate clause, the experiencer\\ncannot be identified with a semantic role corresponding to the subject\\nof main clause or higher clause.\\n\\n\\nAs for category 2, subjective verbs like ``kurusimu''(feel sick) and\\n``kanasimu''(feel sadness) that describe subjective and/or emotional\\nexperience in verb form, are used.  Like the case of garu, an\\nobserver who observers the experience can be introduced.  However this\\nobserver is not obligatory. Therefore unlike the ``garu/gat-ta'' case,\\nthe experiencer also can be an obligatory semantic role of higher\\nclause as well as the speaker.\\n\\n\\n  Complex Sentence \\n  Feature Structure \\n\\nAccording to the hierachical structure of Japanese sentence shown in\\nFig.1 , the essential part of hierarchical structure of the\\n following sentence () is shown in Fig.2 .  In this figure, the structure just below each proposition is replaced with the\\ncorresponding parts of sentence.\\n\\n\\n`Since \\n\\n\\nbehaved like feeling cold,\\n\\n\\nclosed the window.'\\n\\n\\n\\n\\n\\nBasically the embedding structure of FS corresponds to the hierarchy\\nshown in the hierarchical structure Fig.1 . To grasp the\\nimage of the relation between a hierarchical structure and the\\ncorresponding FS, we show an example of FS of the above complex\\n sentence () analyzed based on this hierarchical structure in the following.  This FS is the result of the unification between\\nthe FSs of subordinate clause and main clause, where the contents of\\nsyntactic feature HEAD , namely\\n\\n\\nis omitted.\\n\\n\\n     MORPH:`samu-gat-ta node,mado o sime-ta' \\n              HEAD:\\n\\n\\n              SEM:\\n\\n#20\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n=\\n              rel: sime \\n               agent:#11\\n object:windowtense:past\\n\\n\\n\\n=\\nrel: node \\n            motivated: #11\\n            soa:rel:garuobserver:\\n                soa:rel: samu-i \\n                experiencer:#22\\n                tense:past   \\n\\n\\nwhere English grosses of relation name is the following: sime:`close',\\nnode:`because', samu-i:`feel cold'.\\n\\n\\nThe key point of the semantics of complex sentence is the role motivated that appears in \\n\\n\\nwhich corresponds to the\\ncontent of the subordinate clause. The role motivated is the link\\nbetween the content of subordinate clause and the main clause.\\nSemantically motivated is characterized as the following.\\n\\n\\nDefinition  2 (Motivated)    \\nMotivated is a person who is affected by the situation described\\nby the subordinate clause deeply enough to feel or act as the main\\nclause describes.\\n\\n\\nThe important and indispensable part of semantics of complex sentence\\nis, roughly speaking, the relation between a subordinate clause and\\nthe main clause. But if you look more closely, this relation is\\nactually the relations among semantic/pragmatic roles appearing in the\\nsubordinate clause and those appearing in the main clause. The newly\\nintroduced role of motivated gives the most important clue for\\nthis relation. Therefore, in the rest of this paper, our effort will\\nbe concentrated into whom a motivated refers to. More precisely,\\nin FS, our main concerns are which semantic role in the SEM\\nof subordinate clause the motivated can or cannot be unified\\nwith, and which semantic role in the SEM of main clause the\\nmotivated can or cannot be unified with.\\n\\n\\n  Constraints \\n\\nIn this subsection, we propose the constraints on complex sentence.  For\\nthis, at first we categorize the relations between subordinate clause\\nand main clause based on their semantics. They are divided up to many\\ntypes of complex sentence. We show the most important and typical types\\n in Table., where SC and MC mean `subordinate clause' and `main clause' respectively.  In this table, the first column is for a\\nname of sentence type, the second column indicates a rough meaning of\\nthe relation between subordinate clause SC and main clause MC of complex\\nsentence, and the third column shows Japanese conjunctive particles used\\nto represent a type of complex sentence in the same row.\\n\\n\\nThree VP adjuncts, te, tutu, and nagara, are\\nusually used to express events ocurring simultaneously. However, if they\\nare used with aspectual suffix i which means perfective, for\\ninstance i-nagara, they are regarded as clause conjuncts and are to\\n be interpreted as `although'.  We don't deal with type 4, because a temporal adverbial clause just describes an event that occurs\\nbefore, simultaneously or after another event which is described by the\\nmain clause.  Therefore generally we don't expect essential information\\nfor relations among semantic roles appearing in adverbial or main clause\\nfrom this type of sentence.\\n\\n\\nNow we focus on type 1,2 and 3, where a motivated plays the key role\\n in the constraints. In Table. we show the constraints that  say which semantic/pragmatic role of subordinate clause can be a motivated. Table. shows which semantic role of main clause can be unified with the motivated.  In these tables, the first\\ncolumn of the first row is for constraint names, the second column shows\\na set of sentence types for which the constraints shown in the second\\n row apply.  The third column of Table. shows predicate patterns of subordinate clause, and the third column of\\n Table. shows semantic categories of predicate of main clause. For them, constraints written in the second row apply.  Note\\n that all of these constraints in Table. are local in a subordinate clause, because both sides of = of constraints are roles of\\nsubordinate clause.  In case of subjective adjective without garu,\\nthe constraint `motivated = experiencer' holds also for type\\n1 except for the case where directionally auxiliary verb ``yaru(give)'',\\n``kureru(be given)''  are used. Analysis for these cases is one of\\nour future problem.\\n\\n\\n As for Table.,  \\n\\nis a state except for the\\ncase that there exists a third party who is a motivated puts the\\nexperiencer into that state. For instance, the experiencer\\nis permitted to do something by the motivated. Since in this kind\\nof case things are quite complicated, we omit it here because of the\\n limited space.  Constraints in Table. are also local in a main clause because every semantic role that appeares in the righthand\\nside of the constraints is defined within the main clause. Needless to\\nsay, the influence from a subordinate clause comes only via role motivated.\\n\\n\\nwhere `name' means a name of each constraint.\\n\\n\\nIn the rest of this section we show the examples that exemplify these\\nconstraints.\\n\\n\\n First, we take () of type 1. The constraints to be applied are S1 and M1 as you know from the contents of subordinate and main\\nclause. By combination of S1 and M1, zero agent of main\\nclause:\\n\\n\\nis the observer of the situation described by\\nthe subordinate clause, where \\n\\n\\nbehaved like feeling cold.\\nThis interpretation coincides with native's intuition.\\n\\n\\nLook at the following pair of example.\\n\\n\\n`Although \\n\\n\\nbehaved like feeling bad, \\n\\n\\ndidn't take a\\nmedicine at last.'\\n\\n\\n`Although \\n\\n\\nwanted to stay, \\n\\n\\nfinally forced him out.'\\n\\n\\n In both of () and (), the motivateds of subordinate clause are constrained by S2, namely motivateds can be\\neither \\n\\n\\nor the observer of subordinate clause.\\nConstraint M1 says that in both cases, \\n\\n\\nis unified with the\\n motivated. Intuitively in (),  \\n\\nis\\n\\n.\\n On the other hand in (),  \\n\\nis the\\nobserver. Both of these interpretations comply with constraints\\nS2, and M1.\\n`Since it is hot, I am in trouble.'\\n\\n\\nIntuitively \\n\\n\\ncorefer with \\n\\n.\\nThis\\ninterpretation is expected by constraint S3 and M2 that apply in this\\ncase. As you know from these examples, our constraints are not strong\\nenough to identify the antecedent of \\n\\n\\nuniquely, but makes\\nsafe interpretations.  Moreover disambiguation done by these\\nconstraints is useful for further inference that will be done with\\ncommonsense knowledge or with a special vocabulary like\\n `kekkyoku(finally)' used in (). \\n\\n\\nIn case of S5, namely intransitive passive or adversity passive, it is\\nwell known, i.e.\\n  that there exists a person who is affected by the situation described by the passive sentence. An example sentence is\\nthe following.\\n\\n\\n`Although his wife had gone, \\n\\n\\ndoesn't show a bit of sadness.\\n\\n\\n The semantic role of this affected person , in () zero  role:\\n\\n\\nwhose wife was dead, is an affected.\\nThe intuitive interpretation that \\n\\n,\\n is expected by our constraints: S5 of Table.  and M1 of Table.. On the contrary, in case of S6, namely transitive passive, generally we don't have an affected.\\nHowever in some context, a transitive passive form may require the\\nrole affected which is inherent to adversity passive.\\nFor instance,\\n`\\n\\n's wallet was stolen.'\\n\\n\\nIn this case, a person whose wallet was stolen is not explicit but\\nregarded as an affected.  Another case having an affected\\nis that a relational noun is the subject of transitive passive. Then a\\nperson who is in the relation expressed by the relational noun is\\nthought to be affected by that situation ,too.  Here we take `mother',\\n`father', `daughter', `son', `supervisor', and so forth as a\\nrelational noun. A couple of example sentences are the following.\\n\\n\\n`Since his henchman was attacked, the boss retaliated.'\\n\\n\\n`Although his henchman was attacked, the boss didn't retaliate.'\\n\\n\\n\\n\\n who retaliated () (or didn't retaliate  ()) has a certain relation between the henchman who had been attacked. For instance, \\n\\n\\nmay be the boss of that\\n henchman. In (), since constraint S6 of  Table. and M1 of Table. apply,  \\n\\nis\\nan affected of attacking event described in the subordinate\\nclause. This interpretation coincides with native's intuition.\\n\\n\\nIn sum, with these constraints, a constraint satisfaction\\nprocess in UG based parsing can be done locally and consequently very\\nefficiently.  In other words, primarily a constraint satisfaction\\nprocess of a subordinate clause can be done within the analysis of\\nsubordinate clause, and that of the main clause can be done within it\\nexcept for using motivated whose value has already been constrained\\nin the subordinate clause.\\n\\n\\n\\n  Related Works and Conclusions \\n\\nOne of the relevant researches to ours is JPSG that has been developed by\\n Gunji, and is further studied by the ICOT working group. Our focus is a more pragmatics oriented one than JPSG is.\\nMany Japanese linguists have already done the enormous amount of basic\\nobservations and proposed linguistic theories about the phenomena we\\ndeal with in this paper\\n ,,,,,,,,. Of course our research is based on their works and observations. In\\n , it is said that if garu is used in a subordinate clause, the subject of the main clause is not the experiencer of the\\nsubordinate clause. In\\n , she says that 1) a cognizer that corresponds to our observer is introduced if garu is used, and 2) if an observer is introduced in the subordinate clause, the mentally\\nresponsible person appearing in the main clause is identical with the\\nobserver. In linguistic phenomena, these observations are\\nsimilar to the constraint we propose here. So what is new? The answer\\nis that: 1) We explicitly state the semantics of complex sentence as\\nthe relations among semantic roles.  Namely, since we use semantic/pragmatic\\nroles instead of grammatical roles in constraints, our constraints can\\naccount for zero anaphora in a sentence where the main clause is\\npassive where an agent or an experiencer is not\\nnecessarily the subject, like the following example.\\n`Since Taro behaved like hating to go to school, he was scolded.' \\nwhere the intuitive reading is the following: \\n\\n,\\nthat is zero\\nsubject, refers to Taro, and \\n\\n,\\nthat is not the zero subject,\\nrefers to Taro's parents who are the observer and motivated\\nof the subordinate clause.  2) We formalize this theory in UG formalism,\\neven though the details are omitted due to the space limitation. 3) We\\nfind that the constraints of complex sentences are actually local ones.\\nThis localization of constraint was found by introducing new pragmatic\\nroles observer and motivated, and is extremely important for\\nefficiency of UG based parsing. This localization also makes the\\nproposed constraints be compositional ones, because in the case of\\ndeeply embedded complex sentence to identify the referent of each motivated that bridges between a subordinate clause and its main\\nclause, the constraints we proposed are resolved with computation\\nconfined within each clause.\\n\\n\\nAnalysis of case in which a directional auxiliary verb i.e.\\n`yaru',`kureru' is used is left as the future problem. Finally, we\\nimplemented a Japanese language understanding system based on the\\ntheory we state in this paper, but due to the space limitation we will\\nreport the detail of implementation in other place in the near future.\\n\\nBibliography \\n\\n  Brennan, S., M. Walker Friedman and\\nC.Pollard (1987). A Centering Approach to Pronouns. 25th Annual Meeting of ACL,\\npp.155-162\\n\\n\\n  Gunji, T.(1987). Japanese Phrase Structure Grammar.\\nReidel, Dordrecht\\n\\n\\n  Gunji,T. (1989). Relevance of the Formalization of\\nPhrase Structure Grammar to Mechanical Language Processing. Report of\\nTokutei-Kenkyu, Ministry of Education and Academy\\n\\n\\n  Iida,M. and P.Sells(1988). Discourse Factors in\\nthe Binding of zibun.\\nin Japanese Syntax (ed. W.Poser) CSLI, Stanford\\n\\n\\n  Kameyama, M. (1988). Japanese Zero Pronominal\\nBinding: Where Syntax and Discourse Meet. in Japanese Syntax (ed. W.Poser)\\nCSLI, Stanford\\n\\n\\n  Katagiri,Y. (1991). Perspectivity and\\nJapanese Reflexive `zibun'. in CSLI Lecture Notes No.26, Situation Theory\\nand its Applications Vol.2, J.Barwise et al eds. pp.425-447\\n\\n\\n  Kuno, S. (1973).  The structure of the Japanese\\nLanguage.\\nCambridge, MIT Press\\n\\n\\n  Kuno,S.(1978). Danwa no Bunpou. Taishukan, Tokyo\\n\\n\\n  Ohye,S.(1975). Nitieigo no Hikakukenkyu. Taishukan,\\nTokyo\\n\\n\\n  Saito,R.(1992). Shinjou Jutugo no Goyouronteki\\nBunseki (Pragmatic Analysis about Psychological Predicates). Nihongogaku,\\nVol.11, No.6, pp.110-116\\n\\n\\n  Mikami,A.(1953). Gendai Gohou Josetu.\\nKuroshio-Shuppan, Tokyo\\n\\n\\n  Minami,F.(1974). Gendai Nihongo no Kouzou.\\nTaishukan, Tokyo\\n\\n\\n  Palmer, F.R.,(1986). Mood and Modality.\\nCambridge University Press,Cambridge\\n\\n\\n  Sells, P. (1985). Lectures on Contemporary\\nSyntactic Theories. CSLI Stanford\\n\\n\\n  Takubo,Y. (1987). Tougokouzou to Bunmyakujouhou\\n(Syntactic Structure and Contextual Information. Nihongogaku\\n1987-5,Meiji-shoin,Tokyo\\n\\n\\n  Teramura,H.(1984). Nihongo no sintakusu to\\nimi II `Japanese Syntax and Semantics II'. Kuroshio-Shuppan,Tokyo\\n\\n\\n  Teramura,H.(1990). Nihongo no sintakusu to\\nimi III `Japanese Syntax and Semantics III'. Kuroshio-Shuppan,Tokyo\\n\\n\\n  Tsuda,H.,Hasida,K.,Sirai,H. (1989). JPSG Parser on\\nConstraint Logic Programming. 4th ACL European Chapter\\n\\n\\n  Walker,M.,M. Iida and S. Cote(1990). Centering\\nin Japanese Discourse.\\nCOLING'90\\n\\nFootnotes\\n\\nHenceforth, \\n\\n\\nmeans zero $$$.., where $$$.. is either\\ngrammatical, semantic or pragmatic role. For instance, \\n\\n\\nmeans\\nzero subject,\\n\\n\\nmeans zero agent, \\n\\n\\nmeans zero\\nexperiencer, and so forth.\\n   `Hanako' is a typical girl's name.\\nThe examples shown below are a tip of iceberg we actually analyzed, of\\ncourse. We gather the data about native's intuitive interpretation\\nfrom more than twenty natives around authors.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThe important part of semantics of complex sentence is captured as\\nrelations among semantic roles in subordinate and main clause\\nrespectively.  However if there can be relations between every pair of\\nsemantic roles, the amount of computation to identify the relations that\\nhold in the given sentence is extremely large. In this paper, for\\nsemantics of Japanese complex sentence, we introduce new pragmatic roles\\ncalled observer and motivated respectively to bridge\\nsemantic roles of subordinate and those of main clauses. By these new\\nroles constraints on the relations among semantic/pragmatic roles are\\nknown to be almost local within subordinate or main clause. In other\\nwords, as for the semantics of the whole complex sentence, the only role\\nwe should deal with is a motivated.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nDetermining the referential property of noun phrases is essential not\\nonly to understanding a text, but also to decide how to generate it in\\nEnglish.  This paper proposes a heuristic algorithm to determine the\\nreferential properties of noun phrases in a Japanese text.  The\\noriginal motivation of the research was to improve the quality of\\nEnglish output by NTT Communication Science Laboratories' Japanese to\\nEnglish machine translation system ALT-J/E\\n ,.  We expect, however, that the results will also be useful for text extraction and general text\\nunderstanding.\\n\\n\\nIn this paper we use the term noun phrase reference to describe\\nthe relation between a noun phrase and what it stands for when it is\\nused.  We distinguish between three uses of noun phrases, two\\nreferential and one non-referential.  A noun phrase can be used to\\nrefer in two different ways:  GENERIC where a noun phrase is used\\nto refer to a whole class, and  REFERENTIAL where a noun phrase\\nrefers to a particular entity or entities.  A third use is \\n  ASCRIPTIVE where a noun phrase is used not to refer to anything but\\nrather, normally with a copular verb, to ascribe a property to some\\nreferent.  Although  ASCRIPTIVE noun phrases are non-referring,\\nwe will refer to all three uses under the general term of noun phrase\\nreference.  This three-way distinction of noun phrase reference was\\nintroduced in  and used as a base to determine the\\ncountability and number of noun phrases in Japanese-to-English machine\\ntranslation.  In this paper we define exactly what is meant by the\\nthree kinds of reference and show how the distinction is essential in\\nthe generation of articles.\\n\\n\\nThis paper is structured as follows.  First, we define the three kinds\\nof referentiality which we distinguish and justify the definitions on\\ntheoretical and practical grounds, comparing them with those suggested\\nby other researchers.  We then describe in detail a heuristic method\\nfor determining noun phrase reference in Japanese sentences.  Next, we\\nshow how the distinction is used in a Japanese to English machine\\ntranslation system to generate articles and number.  Finally, we look\\nat experimental results gained by implementing the proposed methods\\nand compare them to those achieved by an earlier version of the same\\nsystem, and by other systems.\\n\\n\\n    Definition of noun phrase reference\\n\\n\\nNoun phrase reference is of fundamental importance in any discussion\\n of meaning .  In English, it is also important in determining how articles should be used.  In this section we give a\\nmore detailed definition of the three kinds of noun phrase reference\\nunder discussion and compare them with the definitions used in other\\nmachine translation systems.\\n\\n\\nGeneric:\\nNoun phrases with generic reference denote an entire\\nclass: e.g. mammoths in Mammoths are extinct.  In English \\n  generic noun phrases can normally be expressed in three ways, as\\n   discussed in Section . \\n\\n\\nReferential:\\nReferential noun phrases are those that refer to\\n  some entity or entities in the discourse world: e.g. a mammoth\\n  in There is a mammoth in my garden!  Referential noun phrases \\n  are plural if there is more than one discrete referent, and are\\n  marked for definiteness.  \\nAscriptive:\\nAscriptive noun phrases are\\n  used with a copular verb, or in an appositive expression, to ascribe\\n  a property to their subject: e.g. a mammoth in That animal\\n    is a mammoth.  Because ascriptive noun phrases are non-referring\\n  they cannot be the antecedent of other noun phrases.\\n\\n\\n distinguishes between  GENERIC and \\n  IDENTIFYING, which appear to be equivalent to our  GENERIC and\\n REFERENTIAL.  's examples do contain\\nascriptive noun phrases, for example a human being in `A\\n  spectator is a human being', instead they appear to be treated as\\nadjective phrases in the rules (for example in their rule 14 (p. 797\\nop cit) where the complement of the copulative predicate with a\\ngeneric subject is an evaluative adjective phrase).  If the definition\\nof adjective phrase has been expanded to include  ASCRIPTIVE noun\\n phrases then our analysis is compatible.  Unfortunately there is no discussion in  as to how effective\\ntheir rules are when actually used in a machine translation system so\\nwe cannot make a quantitative comparison.\\n\\n\\n distinguish between  GENERIC and \\n  NON-GENERIC, which is further divided into  DEFINITE and \\n  INDEFINITE, using heuristics similar to rewriting rules in expert\\nsystems.  They make no distinction between  REFERENTIAL and \\n  ASCRIPTIVE for non-generic noun phrases.  This leaves open the\\npossibility for conflict with their rule that a noun phrase will be\\ndefinite if it has been presented previously.  Consider the following\\n sentence: zo-wa   honyurui da-si, manmosu-mo honyurui da. `Elephant- TOP\\nmammal be-and mammoth- ALSO mammal be.' Elephants are\\n  mammals and mammoths are also mammals.  This will become   Elephants are mammals and mammoths are also the mammals\\nusing the rules given.  Distinguishing between  REFERENTIAL and\\n ASCRIPTIVE prevents this kind of problem from occurring.  We\\n compare their results to ours quantitatively in Section . \\n\\n\\n    Determination of noun phrase reference\\n\\n\\nAll proper nouns are, by definition,  REFERENTIAL.  The algorithm\\nused to determine the referential property of noun phrases headed by\\n common nouns is shown in Figure .  The algorithm presented is based on single sentences, it does not address the\\nconsiderable problems of using information from outside the sentence\\n being considered. \\n\\n\\nIt is possible for the algorithm to be applied to the Japanese parse\\n tree as part of the semantic analysis.  In   ALT-J/E, however, the algorithm is applied after the semantic analysis has finished, during the transfer stage, because much of the\\nsemantic information is stored in the transfer dictionaries where the\\ncombination of Japanese and English makes it easy to disambiguate word\\nsenses.  The overall process of translation in ALT-J/E is\\ndivided into seven parts.  First, the system splits the Japanese text\\ninto morphemes and assigns parts of speech.  Second, it parses the\\nsegmented text, often giving multiple possible interpretations.\\nThird, it rewrites complicated Japanese expressions into simpler ones.\\nFourth, ALT-J/E semantically evaluates the various\\ninterpretations.  Fifth, syntactic and semantic criteria are used to\\nselect the best interpretation.  Sixth, the selected interpretation is\\ntransferred into English.  Finally, the English sentence is adjusted\\nto give the correct inflectional forms. The algorithm described in\\nthis section has been implemented as part of the sixth stage.\\nHowever, it could be implemented as part of the fifth stage.\\n\\n\\n Rules are applied in the order shown in Figure , with later rules over-ruling earlier ones.\\n\\n\\nThe default assumption is that a noun phrase will be used to refer to\\nsome specific entity or entities in the discourse world, i.e. that it is \\n REFERENTIAL.  \\n\\n\\nThere are five rules that are applied at the sentence level, which use\\nthe meanings of verbs combined with the semantic categories of\\n nouns.  These can all be overridden by subsequent rules.  The subjects of verbs that\\npredicate over an entire class, and the objects of verbs which\\npredicate  EMOTIVE ACTION or  EMOTIVE STATE, are \\n  GENERIC.  Verbs that trigger these rules, e.g. evolve, die\\n   out are marked in the lexicon .  For copulas, the subject is  GENERIC if its semantic category is a descendent\\nof the semantic category of the object, while it's complement is taken\\n to be  ASCRIPTIVE by default.  Finally, appositive noun phrases will be judged to be  ASCRIPTIVE, as\\nthough they were the complement of a copula.\\n\\n\\nRecall that these rules are only applied if the noun phrase in\\n question is headed by a common noun.  In sentence , the semantic category of meeting place is  ACTUAL PLACE,\\nwhich is a child of the semantic category of Aoi hall \\n  PUBLIC PLACE. \\nAoi hall, however, is a proper noun so the rule is not applied.\\n\\n\\n The next level of rules (level ) applies to noun phrases modified by embedded sentences.  Japanese makes no\\nphonological, morphological, or syntactic distinctions between\\n restrictive and non-restrictive relative clauses , 235]. This algorithm uses a simple heuristic: a noun phrase modified by a\\ntensed embedded sentence is  REFERENTIAL.\\n\\n\\n The next level of rules (level ) is based on  post-modification in the Japanese sentence.  The use of some   setsubiji `suffixes' implies that their modificant is  GENERIC.  For example muke `aimed at' in   josei-muke-no-zasshi `woman aimed-at  GEN magazine' a\\n  magazine aimed at women.  Similarly the construction   A-to-iu-no-wa `things called A' implies that its modificant is\\n GENERIC.  It can in fact be thought of as a pseudo-particle, the\\nwhole construction acting as a single marker which has the effect of\\nmarking it's modificant as being a generic noun phrase used as the\\n topic. \\n\\n\\n The next level of rules (level ) makes a noun phrase whose head is modified by a demonstrative, numeral or the genitive\\nconstruction NP-no `NP's'  REFERENTIAL.  Note that only\\nnoun phrases modified by no judged to be genitive are \\n  REFERENTIAL.  Partitive constructions such as   okami-no-mure `pack of wolf' a pack of wolves are not\\nincluded in this judgment.  The genitive construction may be\\ntranslated into English in a variety of ways including a prepositional\\nphrase headed by `of', a possessive phrase with a clitic in the\\ndeterminer position, or a possessive pronoun. \\n\\n\\n Finally (level ), noun phrases headed by nouns that are marked in the lexicon as likely to have a unique referent, such as\\nchikyu `the earth' are assumed to be  REFERENTIAL.\\n\\n\\nThe algorithm presented in this section is only heuristic.  Further\\nwork remains to be done to refine it.  In particular: using the wa/ga\\ndistinction in conjunction with noun anaphora relations to distinguish \\nbetween  GENERIC and  REFERENTIAL, and improving the rules at\\n level  for relative clauses. \\n\\n\\n  Using noun phrase referentiality to select articles and    determine number \\n\\nKnowledge of a noun phrase's referential use is essential when\\ntranslating from Japanese to English, as it plays a large part in\\ndetermining how a noun phrase is expressed in English.  In this\\nsection we show how articles and number are generated differently for\\nthe three different referentialities in the machine translation system\\nALT-J/E.  Correct generation of articles and number is important\\nnot only to express meaning accurately, but because it is one of the\\nmajor factors in determining the readability of Japanese-to-English\\ntranslations.\\n\\n    Translation of generic noun phrases\\n\\n\\nA  GENERIC noun phrase (with a countable head noun) can generally\\n be expressed in three ways .  We call these    GEN `a', where the noun phrase is indefinite: A mammoth\\n  is a mammal;  GEN `the', where the noun phrase is definite:\\nThe mammoth is a mammal; and  GEN ,\\nwhere\\nthere is no article: Mammoths are mammals.  Uncountable\\nnouns and pluralia tantum can only be expressed by  GEN (eg: Furniture is expensive).  They cannot take  GEN\\n`a' and they do not take  GEN `the', because then the noun phrase\\nwould normally be interpreted as having definite reference.  Nouns\\nthat can be either countable or uncountable take only  GEN or `a': Cake is delicious/Cakes are delicious,\\nA cake is a kind of food.  These combinations are shown\\n in Table .  Noun phrases that cannot be used to show    GENERIC reference are marked with an asterisk (*).\\n\\n\\nThe use of all three kinds of  GENERIC noun phrases is not acceptable\\nin some contexts, for example *a mammoth evolved.  Sometimes a\\nnoun phrase can be ambiguous, for example I like the elephant,\\nwhere the speaker could like a particular elephant, or all elephants.\\n\\n\\nBecause the use of  GEN \\nis acceptable in all contexts,   ALT-J/E generates all  GENERIC noun phrases as such, that is as bare\\nnoun phrases.  The number of the noun phrase depends on the\\ncountability preference of the noun phrase heading it and there will\\nbe no article.\\n\\n\\n    Translation of referential noun phrases\\n\\n\\nThe countability and number of  REFERENTIAL noun phrases can be\\ndetermined with heuristics that use information from the Japanese\\nsentence along with knowledge of English countability stored in the\\nlexicon.  This is described in .\\n\\n\\nAccording to , for  REFERENTIAL noun phrases: \\nThe definite article the is used to mark the phrase it\\n  introduces as referring to something which can be identified\\n  uniquely in the contextual or general knowledge shared by speaker\\n  and hearer.\\n\\n\\nWhether or not a  REFERENTIAL noun phrase is definite or not is\\ndetermined using heuristic criteria based on whether there is enough\\ninformation to uniquely identify the noun phrase's referent, such as\\nthe following:\\n\\n\\n\\nif the head noun is marked in the lexicon as being unique:  \\n  the earth\\n\\nif the noun phrase is made logically unique by a modifier:  \\n  the best price\\n\\nif the noun phrase's referent is restrictively described: \\n  the man who came to dinner, the aim of this research\\n\\ndirect and indirect anaphoric reference: \\n  I saw a cat and a dog.  The dog chased the cat.\\n\\n\\n\\n\\nAs the above criteria are only meaningful for  REFERENTIAL noun\\nphrases, it is essential to determine whether the noun phrase is\\nreferential as a first step.\\n\\n\\nWhen it has been determined whether a noun phrase is definite or\\n indefinite, then articles can be generated.  In the final stage of processing, if there is no determiner, definite noun phrases\\ntake the definite article the.  Indefinite countable singular\\nnoun phrases will take the indefinite article a/an, while\\nindefinite countable plural and uncountable noun phrases will take the\\nzero article .\\n This is summarized in Table . \\n\\n\\n    Translation of ascriptive noun phrases\\n\\n\\nThe countability and number of  ASCRIPTIVE noun phrases matches\\nthat of their subject, and the countability and number of two\\nappositive noun phrases match each other as described in\\n, with the following proviso.  If one element is\\nplural and the other is a collective noun such as group, then\\nthey need not match.  For example, many insects, a whole swarm,\\n  ... as opposed to many insects, bees I think, ....\\n\\n\\nALT-J/E makes the simplifying assumption that all  ASCRIPTIVE\\nnoun phrases are indefinite.  Therefore, articles will be generated in\\nthe same way as for indefinite  REFERENTIAL noun phrases.  Countable\\nsingular noun phrases will therefore take the indefinite article   a/an, and countable plural and uncountable noun phrases will take\\nthe zero article .\\n\\n\\n\\n    Results\\n\\n\\nThe processing described above has been implemented in ALT-J/E.\\nThe rules were designed using data from a specially constructed set of\\ntest sentences collected by the authors.  The algorithm was evaluated\\non a collection of newspaper articles from the Nikkei-Sangyou\\nnewspaper by an English native speaker not connected with the\\ndevelopment of the algorithm.  The results are summarized in\\n Table . \\n\\n\\nNew shows the results using the proposed method.\\n\\n\\nOld shows the results using the unmodified system.\\n\\n\\nWe tested the system on newspaper articles, in the articles tested,\\nthere were an average of 7 noun phrases in each sentence.  The\\narticles were translated by ALT-J/E and the raw output examined\\nby an English native speaker.  Each noun phrase was given one of the\\nfollowing scores:\\n STRUCTURE:\\nproblem with structure or choice of\\n translation  BEST:\\nthe most appropriate article/number\\n ARTICLE:\\ninappropriate article\\n NUMBER:\\ninappropriate number\\n POSSESSIVE:\\ninappropriate use of possessive determiner\\n COUNTABILITY:\\nproblem with countability\\n REFERENCE:\\nproblem with referential property\\nFor the purpose of evaluating the generation of articles and number,\\nnoun phrases that were either the  BEST possible translation, or\\nthat had a problem only with  STRUCTURE/CHOICE OF TRANSLATION,\\nwere judged to be successful.  A third-party evaluator gave the\\nsuccess rates as 77% for the system with the proposed method and 65%\\nfor the original system.  The method of evaluation described above\\ndoes not give a reproducible, absolute level of success.  It does,\\nhowever, successfully show the overall level of\\nimprovement/degradation, and help to identify the remaining problems.\\n\\n\\nOur initial evaluation was done by the the authors, who found the\\nsuccess rates at the noun phrase level to be 92% for the proposed\\nmethod and 76% for the system as it used to be.  Nakazawa points out\\nthat this shows that the evaluation method is not reproducible\\n(personal communication May 1995).  Because the goal is to produce a\\ntranslation, which is new text, there is no objective target to\\ncompare the results with.  This is a perennial problem for machine\\ntranslation output.   in a small pilot study\\nshowed that humans could replace articles (a/an and   the) in an English text in which the articles had been replaced\\nby blanks with an accuracy of around 95%.  Raw machine translation\\noutput is less coherent than normal English text and so deciding which\\narticle is appropriate is an even harder task.\\n\\n\\n    Discussion\\n\\n\\nIn this section we discuss the remaining errors and compare\\nthe results to two other systems.\\n\\n\\n168 of the 717 noun phrases in the machine translation of the\\nnewspaper articles had some problem. An brief analysis of the errors\\n is given in Table . \\n\\n\\n  machine translation of the newspaper articles\\n\\n\\nTesting on the newspaper articles revealed one major heuristic that\\nhad been overlooked in the algorithm presented in\\n section : some nouns when heading a construction such as `N-of-NP' carry an implication that the complement NP has \\n  GENERIC reference: for example, the applications of\\n  databases.  This rule will be added to the algorithm at\\n level , reducing the number of errors by around 8%. Apart from this there were no major changes that needed to be made to\\nthe algorithm.\\n\\n\\nOverall, the largest sources of errors are problems with the source\\nlanguage analysis and dictionaries (22% each).  These are not\\nproblems with the proposed algorithm but with the machine translation\\nsystem as a whole.  Another major source of errors is the translation\\nof numerical expressions (12%).  The processing for handling\\nnumerical expressions is currently being overhauled.  The errors\\ncaused by lack of information in the dictionaries are solvable\\nimmediately, which will reduce the number of errors by around 20%.\\n\\n\\nIn the generation of articles and numbers for  REFERENTIAL noun\\nphrases some of the errors can simply be solved by the addition of new\\nrules: for example, adding rules which use the meaning of adverbs to\\ndetermine number or rules using pre-head modifiers to determine\\ndefiniteness.  The problems of common sense deduction and indirect\\nanaphora, however, require a large scale knowledge base and inference\\nrules.  While both are being researched at the moment, they are\\nunlikely to be implemented soon.  We estimate that the number of\\nerrors caused by insufficiencies in the generation of articles and\\nnumbers for  REFERENTIAL noun phrases can be reduced at least a\\nquarter, thus reducing the total number of errors by around 8%.\\n\\n\\nCombining the above figures, we predict it is possible to reduce the\\nerrors by around 30%, bringing the total success rate to 84% for a\\nwindow test.  To go beyond this needs new processing to improve the\\nsource language analysis, the translation of numerical expressions and\\nmore use of contextual inferences.\\n\\n\\nIn addition examining even this small sample of text we came up with\\none major addition to the algorithm for determining noun phrase\\nreference.  Therefore the algorithm needs to be tested on a wider\\nrange of texts before the rules can be considered comprehensive.  We\\nhave started testing the algorithm on a larger corpus of newspaper\\narticles and are investigating methods for automatically learning\\nrules.\\n\\n\\nIn  success rates of 68.9% for referential\\nproperty and 85.6% for number were given for unknown texts of the\\nsame genre as that used in development of the rules.  Their approach\\nseems effective, although we predict the lack of a  ASCRIPTIVE\\nclass will cause problems.  It is impossible to directly compare our\\nresults as 's testing was all carried out in\\nJapanese by the developers, so the problems of actually generating the\\nEnglish and getting an impartial evaluation were not addressed.\\nSetting these considerations aside, when we separate our results for\\nnoun phrase reference (counting as failures noun phrases with errors\\nin article use, noun phrase reference or the use of possessive\\ndeterminers), and countability and number (counting as failures noun\\nphrases with errors in number or countability), our proposed algorithm\\ngave success rates of 74% and 85% respectively.\\n\\n\\n Another approach is that of , who proposed using an automated post-editor to correct articles.  Their prototype\\nhas a success rate for learning to replace articles when they have\\nbeen removed from English texts of 78%.  At present however the\\nprototype cannot be used to post-edit output from a typical machine\\ntranslation system as it assumes the knowledge that an article should\\nbe used in a given position, which is not normally available, and that\\nthe generation rules can function using machine translation output,\\nwhich has not been shown.\\n\\n\\n    Conclusion\\n\\n\\nThis paper proposes a method that uses the information available in a\\nJapanese sentence to identify a noun phrase as being used either\\n GENERICALLY,  REFERENTIALLY or  ASCRIPTIVELY.  This\\ndistinction is shown to be both theoretically justified and\\npractically useful.  The three way distinction in noun phrase\\nreference is used as a base to determine a noun phrase's number and to\\ngenerate appropriate articles and possessive pronouns when translating\\nfrom Japanese to English.  Incorporating this method into the machine\\ntranslation system ALT-J/E helped to improve the\\npercentage of noun phrases with correctly generated articles and\\nnumber from 65% to 77%.  It is shown that the proposed method can be\\nextended straightforwardly to increase the success rate to 84%.\\n\\n\\nSeveral problems remain to be explored.  We consider the following to\\nof primary importance:\\n1.\\nExtension of the algorithm to translate texts as coherent passages,\\nnot just as single sentences.\\n2.\\nImprovement of the reproducibility of the evaluation method.\\n3.\\nInvestigation of the coverage of the algorithm on a wider\\n  collection of texts.\\n\\n\\n  Acknowledgments \\n\\nThe paper has benefited greatly from the comments of the anonymous\\nreviewers for TMI, Graham, Monique and Mitsuyo Bond, Satoru Ikehara,\\nRoly Sussex and especially Tsuneko Nakazawa.  We would like to thank\\nToshiaki Nebashi, Kazuya Fukamachi and Yoshitake Ichii for their\\ninvaluable help in implementing the processing described here.\\n\\nBibliography \\n\\n BOND, FRANCIS,  KENTARO OGURA,   SATORU IKEHARA.\\n1994.\\nCountability and number in Japanese-to-English machine\\n  translation.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics (COLING '94), 32-38.\\n(cmp-lg/9511001).\\n\\n\\n BOND, FRANCIS,  KENTARO OGURA,  \\n   SATORU IKEHARA.\\n1995.\\nPossessive pronouns as determiners in Japanese-to-English machine\\n  translation.\\nIn Proceedings of the 2nd Pacific Association for Computational\\n  Linguistics Conference (PACLING '95).\\n(cmp-lg/9601006).\\n\\n\\n BOND, FRANCIS,  KENTARO OGURA,\\n   SATORU IKEHARA,   SATOSHI SHIRAI.\\n1993.\\nUsing the meanings of verbs to select the countability of English\\n  noun phrases.\\nIn Proceedings of the 1993 IEICE Fall Conference, 6:61-62.\\n  IEICE.\\n\\n\\n HAWKINS, JOHN A.\\n1991.\\nOn (in)definite articles: implicatures and (un)grammaticality\\n  prediction.\\nJournal of Linguistics 27.405-442.\\n\\n\\n HUDDLESTON, RODNEY.\\n1984.\\nIntroduction to the Grammar of English.\\nCambridge textbooks in linguistics. Cambridge: Cambridge University\\n  Press.\\n\\n\\n IKEHARA, SATORU,  SATOSHI SHIRAI,  AKIO YOKOO,  \\n  HIROMI NAKAIWA.\\n1991.\\nToward an MT system without pre-editing - effects of new methods\\n  in ALT-J/E-.\\nIn Proceedings of MT Summit III, 101-106.\\n(cmp-lg/9510008).\\n\\n\\n KNIGHT, KEVIN,   ISHWAR CHANDER.\\nAutomated postediting of documents.\\nIn Proceedings of AAAI '94.\\n\\n\\n KUNO, SUSUMU.\\n1973.\\nThe Structure of the Japanese Language.\\nCambridge, Massachusetts, and London, England: MIT Press.\\n\\n\\n LYONS, JOHN.\\n1977.\\nSemantics, volume 2.\\nCambridge: Cambridge University Press.\\n\\n\\n MURATA, MASAKI, 1993.\\nResearch into the determination of referential property and number of\\n  nouns using Japanese structure as a guide.\\nBachelor's thesis, Kyoto University, Kyoto, Japan.\\n(in Japanese)\\n\\n\\n MURATA, MASAKI,   MAKOTO NAGAO.\\n1993.\\nDetermination of referential property and number of nouns in\\n  Japanese sentences for machine translation into English.\\nIn Proceedings of the Fifth International Conference on\\n  Theoretical and Methodological Issues in Machine Translation (TMI '93),\\n  218-25.\\n\\n\\n OGURA, KENTARO,  AKIO YOKOO,  SATOSHI SHIRAI,  \\n  SATORU IKEHARA.\\n1993.\\nJapanese to English machine translation and dictionaries.\\nIn Proceedings of the 44th Congress of the International\\n  Astronautical Federation, Graz, Austria.\\n\\n\\n QUIRK, RANDOLPH,  SIDNEY GREENBAUM,  GEOFFREY LEECH,  \\n   JAN SVARTVIK.\\n1985.\\nA Comprehensive Grammar of the English Language.\\nEssex: Longman.\\n\\n\\n SHIRAI, SATOSHI,  SATORU IKEHARA,   TSUKASA KAWAOKA.\\n1993.\\nEffects of automatic rewriting of source language within a Japanese\\n  to English MT system.\\nIn Proceedings of the Fifth International Conference on\\n  Theoretical and Methodological Issues in Machine Translation (TMI '93),\\n  226-239.\\n\\n\\n ZELINSKY-WIBBELT, CORNELIA.\\n1992.\\nExploiting linguistic iconism for article selection in machine\\n  translation.\\nIn Proceedings of the 14th International Conference on\\n  Computational Linguistics (COLING '92), 792-798.\\n\\nFootnotes\\n\\n  Now at Doshisha University, Kyoto, \\n      JAPAN: [kawaoka@wise.doshisha.ac.jp].\\n  We feel this expanded definition is plausible, since\\n  the copula and ascriptive noun phrase combination fulfills the same\\n  semantic role as the copula and adjective phrase, that is, to\\n  ascribe a property.\\n  Examples are given with the (romanized) Japanese\\n  original, a gloss and the human translation.  The examples have been\\n  simplified to exemplify points more clearly; a new translation has\\n  been made for each simplified sentence.  Japanese particles are\\n  glossed as follows:  TOP for wa which marks the topic,\\n   OBJ for o which marks the object and  GEN for\\n  no which shows a genitive relation.\\n  Algorithms to use contextual information\\n  from outside the sentence are currently being implemented.\\n  For information\\n  retrieval it is obviously essential to determine the referentiality\\n  of noun phrases as part of the source language analysis.\\n  The meanings of nouns are given in terms of a semantic\\n  hierarchy of 2,800 nodes.  Each node is called a semantic category.\\n  Edges in the hierarchy represent  IS-A relationships, so that\\n  the child of a semantic category  IS-A instance of it.  For\\n   example,  ORGAN IS-A BODY-PART . \\n  If the complement is later\\n  judged to be  REFERENTIAL by a subsequent rule it is equivalent\\n  to judging that the copula has been used equatively.\\n  setsubiji are a Japanese part\\n  of speech made up of suffixes that cannot stand alone, but change\\n  the meaning of the word they modify.\\n  In ALT-J/E the entire construction (and the\\n  similar construction A-to-iu-mono-wa `things called A') is\\n  rewritten during the Japanese rewriting stage into a pseudo-particle\\n   , which marks its modificant as being a generic   noun phrase in the ha-case ( TOPIC).  It is not however\\n  necessary to do this, as shown in , where this\\n  construction is found by matching against the Japanese dependency\\n  structure.\\n  As well as\\n  generating definite and indefinite articles, ALT-J/E also\\n   generates possessive pronouns  and some/any   for  REFERENTIAL noun phrases when appropriate.\\n  This includes any major problems not connected\\n    with articles or number, such as outputing Japanese characters or\\n    spelling errors.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper shows the necessity of distinguishing different\\n  referential uses of noun phrases in machine translation.  We argue\\n  that differentiating between the generic, referential and ascriptive\\n  uses of noun phrases is the minimum necessary to generate articles\\n  and number correctly when translating from Japanese to English.\\n  Heuristics for determining these differences are proposed for a\\n  Japanese-to-English machine translation system.  Finally the results\\n  of using the proposed heuristics are shown to have raised the\\n  percentage of noun phrases generated with correct use of articles\\n  and number in the Japanese-to-English machine translation system\\n  ALT-J/E from 65% to 77%.\\n\\n'],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nCorrectly determining number is a difficult problem when translating\\nfrom Japanese to English.  This is because in Japanese, noun phrases\\nare not normally marked with respect to number.  Japanese nouns have\\nno equivalent to the English singular and plural forms and verbs do\\n not inflect to agree with the number of the subject . In addition, there is no grammatical marking of\\n countability. \\n\\n\\nIn order to generate English correctly, it is necessary to know\\nwhether a given noun phrase is countable or uncountable and, if\\ncountable, whether it is singular or plural.  Deciding this is a\\nproblem even for humans translating from Japanese to English, but they\\nhave their own knowledge of both languages to draw on.  A machine\\ntranslation system needs to have this knowledge codified in some way.\\nAs generating articles and number is only important when the rest of\\nthe sentence has been correctly generated, there has not been a lot of\\nresearch devoted to it.  Recently,  have\\nproposed a method of determining the referentiality property and\\nnumber of nouns in Japanese sentences for machine translation into\\nEnglish, but the research has not yet been extended to include the\\nactual English generation.\\n\\n\\nThis paper describes a method that extracts information relevant to\\ncountability and number from the Japanese text and combines it with\\nknowledge about countability and number in English.  First countability\\nin English is discussed at the noun phrase and then the noun level.\\nAs a noun phrase's countability in English is affected by its\\nreferential property (generic, referential or ascriptive) we present\\na method of determining the referential use of Japanese noun phrases.\\nNext the process of actually determining noun phrase countability and\\nnumber is described.  This is followed by some examples of sentences\\ntranslated by the proposed method and a discussion of the results.\\n\\n\\nThe processing described in this paper has been implemented in NTT\\nCommunication Science Laboratories' experimental machine translation\\n system ALT-J/E .  Along with new processing for the generation of articles, which is not discussed in detail in\\nthis paper, it improved the percentage of noun phrases with correctly\\ngenerated determiners and number from 65% to 73%.\\n\\n\\n    Countability\\n\\n  Noun Phrase Countability \\n\\nWe adopt the definition of countability in English given in\\n.  A countable noun phrase is defined as\\nfollows:\\nI\\nIf the head constituent of an NP falls within the scope of a\\ndenumerator it is countable.\\nII\\nIf the head constituent of an NP is plural it is countable.\\n\\n\\nWhere ``the phrase `falls within the scope [or domain] of a\\ndenumerator' means `is denumerated' by it; i.e the NP reference is\\nquantified by the denumerator as a number of discrete entities.''\\n\\n\\nNot all nouns in English can become the head of a countable noun\\nphrase.  In particular, noun phrases whose heads fall within the scope\\nof a denumerator (`denumerated' noun phrases) must be headed by a noun\\nthat has both singular and plural forms.  Nouns that do not have both\\nforms, like equipment or scissors, require a classifier to be\\nused.  The classifier becomes the head of a countable noun phrase with\\nthe original noun attached as the complement of a prepositional\\nphrase headed by of: a pair of scissors, a piece of equipment.\\n\\n\\nWhether a noun can be used to head a countable noun phrase or not\\ndepends both on how it is interpreted, and on its inherent\\ncountability preference.  Noun countability preferences are discussed\\nin the next section.\\n\\n\\n  Noun Countability Preferences \\n\\nA noun's countability preference determines how it will behave in\\ndifferent environments.  We classify nouns into seven countability\\npreferences, five major and two minor, as described below.\\n\\n\\nThe two most basic types are `fully countable' and `uncountable'.\\nFully countable nouns, such as knife have both singular and\\n plural forms, and cannot be used with determiners such as   much. Uncountable nouns, such as furniture, have no plural form, and can be used with\\nmuch.\\n\\n\\nBetween these two extremes there are a vast number of nouns, such as\\ncake, that can be used in both countable and uncountable noun\\nphrases.  They have both singular and plural forms, and can also be\\nused with much. Whether such nouns will be used countably or\\nuncountably depends on whether their referent is being thought of as\\nmade up of discrete units or not.  As it is not always possible to\\nexplicitly determine this when translating from Japanese to English,\\nwe divide these nouns into two groups: `strongly countable', those\\nthat are more often used to refer to discrete entities, such as   cake, and `weakly countable', those that are more often used to\\nrefer to unbounded referents, such as beer.\\n\\n\\nThe last major type of countability preference is `pluralia tanta':\\nnouns that only have a plural form, such as scissors.  They can\\nneither be denumerated nor modified by much. We further\\nsubdivide pluralia tanta into two types, those that can use the\\nclassifier pair to be denumerated, such as a pair of\\n  scissors and those that can't, such as clothes.  `pair'\\npluralia tanta have a singular form when used as modifiers (a\\n  scissor movement).  Pluralia tanta such as clothes, use the\\nplural form even as modifiers (a clothes horse), and need a\\ncountable word of similar meaning to be substituted when they are\\ndenumerated: a garment, a suit, ....\\n\\n\\nThe two minor types are subsets of fully countable and uncountable\\nnouns respectively.  Unless explicitly indicated, they will be treated\\nthe same as their supersets.  `Collective' nouns share all the\\nproperties of fully countable nouns.  In addition they can have\\nsingular or plural verb agreement with the singular form of the noun:\\nThe government has/have decided.  `Semi-countable' nouns share\\nthe properties of uncountable nouns, except that they can be modified\\ndirectly by a/an; for example a knowledge [of Japanese].\\n\\n\\nExamples of the information about countability and number stored in\\nthe Japanese to English noun transfer dictionary are given in table\\n .  The information about noun countability preferences cannot be found in standard dictionaries and must be\\nentered by an English native speaker.  Some tests to help determine a\\ngiven noun's countability preferences are described in\\n, which discusses the use of noun countability\\npreferences in Japanese to English machine translation.\\n\\n\\n\\n    Determination of NP Referentiality\\n\\n\\nThe first stage in generating the countability and number of a\\ntranslated English noun phrase is to determine its referentiality.  We\\ndistinguish three kinds of referentiality: `generic', `referential'\\nand `ascriptive'.\\n\\n\\nWe call noun phrases used to make general statements about a class\\ngeneric; for example Mammoths are extinct.  The way generic\\nnoun phrases are expressed in English is described in Section\\n .  Referential noun phrases are ones that refer to some specific referent; for example Two dogs chase a\\n    cat. Their number and countability are ideally determined by the\\nproperties of the referent.  Ascriptive noun phrases are used to\\nascribe a property to something; for example Hathi is an\\n    elephant. They normally have the same number and countability as\\nthe noun phrase whose property they are describing.\\n\\n\\nThe process of determining the referentiality of a noun phrase is\\n shown in Figure .  The tests are processed in the order shown.  As far as possible, simple criteria that can be implemented\\n using the dictionary have been chosen.  For example, Test  `` if a NP is modified by aimed at, for ...then it is\\n`generic''' is applied as part of translating NP1-muke into\\n``for NP1''. The transfer dictionary includes the information that in\\nthis case, NP1 should be generic.\\n\\n\\n Tests  a show two more heuristic methods for determining whether a noun phrase has generic reference.  In Test\\n , if the predicate is marked in the dictionary as one that only applies to classes as a whole, such as evolve or be\\n  extinct, then the sentence is taken to be generic.  In Test\\n , ALT-J/E's semantic hierarchy is used to test whether a sentence is generic or not.  For example in Mammoths\\n  are animals, mammoth has the semantic category \\n  ANIMAL so the sentence is judged to be stating a fact true of all\\nmammoths and is thus generic.\\n\\n    Generic noun phrases\\n\\n\\nA generic noun phrase (with a countable head noun) can generally be\\n expressed in three ways .  We call these GEN `a', where the noun phrase is indefinite: A mammoth is a mammal;\\nGEN `the', where the noun phrase is definite: The mammoth is a\\n  mammal; and GEN , where there is no article: Mammoths\\n  are mammals.  Uncountable nouns and pluralia tanta can only be\\nexpressed by GEN  (eg: Furniture is expensive).  They\\ncannot take GEN `a' because they cannot be modified by a.  They\\ndo not take GEN `the', because then the noun phrase would normally be\\ninterpreted as having definite reference.  Nouns that can be either\\ncountable or uncountable also only take GEN : Cake is\\n  delicious/Cakes are delicious.  These combinations are shown\\n in Table , noun phrases that can not be used to show generic reference are marked *.\\n\\n\\nThe use all three kinds of generic noun phrases is not acceptable in\\nsome contexts, for example * a mammoth evolved.  Sometimes a\\nnoun phrase can be ambiguous, for example I like the elephant,\\nwhere the speaker could like a particular elephant, or all elephants.\\n\\n\\nBecause the use of GEN  is acceptable in all contexts,   ALT-J/E generates all generic noun phrases as such, that is as bare\\nnoun phrases.  The number of the noun phrase is then determined by the\\ncountability preference of the noun phrase heading it.  Fully\\ncountable nouns and pluralia tanta will be plural, all others are\\nsingular.\\n\\n\\n\\n    Determination of NP Countability and Number\\n\\n\\nThe following discussion deals only with referential and ascriptive noun\\n phrases as generic noun phrases were discussed in Section , \\n\\n\\nThe definitions of noun phrase countability given in Section\\n , while useful for analyzing English, are not sufficient for translating from Japanese to English.  This is because\\nin many cases it is impossible to tell from the Japanese form or\\nsyntactic shape whether a translated noun phrase will fall within the\\nscope of a denumerator or not.  Japanese has no equivalent to   a/an and does not distinguish between countable and uncountable\\nquantifiers such as many/much and little/few.  Therefore\\nto determine countability and generate number we need to use a\\ncombination of information from the Japanese original sentence, and\\ndefault information from the Japanese to English transfer dictionary.\\nAs much as possible, detailed information is entered in the transfer\\ndictionaries to allow the translation process itself to be made\\nsimple.\\n\\n\\nThe process of determining a noun phrase's countability and number is\\n shown in Figure .  The process is carried out during the transfer stage so information is available from both the Japanese\\noriginal and the selected English translation.\\n\\n\\nTo make the task of determining countability and number simpler, we\\ndefine combinations of different countabilities for nouns with\\ndifferent countability preferences that we can use in the\\ndictionaries.  The effects of the four most common types on the five\\n major noun countability preferences are shown in Table . \\n\\n\\nNoun phrases modified by Japanese/English pairs that are translated as\\ndenumerators we call denumerated.  For example a noun modified by   onoono-no ``each'' is denumerated - singular, while one modified\\nby ryouhou-no ``both'' is denumerated - plural.  Uncountable\\nand pluralia tantum nouns in denumerated environments are translated\\nas the prepositional complement of a classifier.  A default classifier\\nis stored stored in the dictionary for uncountable nouns and pluralia\\ntanta.  Ascriptive noun phrases whose subject is countable will also\\nbe denumerated.\\n\\n\\n The two `mass'  environments shown in Table     are used to show the countability of nouns that can be either countable or uncountable.\\nWeakly countable nouns will only be countable if used with a\\ndenumerator.  Strongly countable nouns will be countable and plural in\\nsuch mass - countable environments as the object of collect\\n(vt): I collect cakes, and uncountable and singular in mass\\n-uncountable environments such as I ate too much cake.  In\\nfact, both I collect cake and I ate too many cakes are\\npossible.  As Japanese does not distinguish between the two the system\\nmust make the best choice it can, in the same way a human translator\\nwould have to.  The rules have been implemented to generate the\\ntranslation that has the widest application, for example generating\\nI ate too much cake, which is true whether the speaker only ate\\npart or all of one cake or if they ate many cakes, rather than I\\n  ate too many cakes which is only true if the speaker ate many\\ncakes.\\n\\n\\nSometimes the choice of the English translation of a modifier will\\ndepend on the countability of the noun phrase.  For example,   kazukazu-no and takusan-no can all be translated as\\n``many''.  kazukazu-no implies that it's modificant is made up\\nof discrete entities, so the noun phrase it modifies should be\\ntranslated as denumerated - plural.  takusan-no does not carry\\nthis nuance so ALT-J/E will translate a noun phrase modified by\\nit as mass - uncountable, and takusan-no as many if\\nthe head is countable and much otherwise.\\n\\n\\nRules that translate the nouns with different noun countability\\npreferences into other combinations of countable and uncountable are\\nalso possible.  For example, sometimes even fully countable nouns can\\nbe used in uncountable noun phrases.  If an elephant is referred to\\nnot as an individual elephant but as a source of meat, then it will be\\nexpressed in an uncountable noun phrase: I ate a slice of\\n  elephant.  To generate this the following rule is used: ``nouns\\nquantified with the classifier kire ``slice'' will be\\ngenerated as the prepositional complement of slice, they will\\nbe singular with no article unless they are pluralia tanta, when they\\nwill be plural with no article''.\\n\\n\\nNote that countable indefinite singular noun phrases without a\\ndeterminer will have a/an generated.  Countable indefinite\\nplural noun phrases and uncountable noun phrases may have some\\ngenerated; a full discussion of this is outside the scope of this\\narticle.\\n\\n\\n    Experimental Results\\n\\n\\nThis processing described above has been implemented in ALT-J/E.\\nIt was tested, together with new processing to generate articles, on a\\nspecially constructed set of test sentences, and on a collection of\\nnewspaper articles.  The results are summarized in Table\\n . \\n\\n\\nIn the newspaper articles tested, there were an average of 7.0 noun\\nphrases in each sentence.  For a sentence to be judged as correct all\\nthe noun phrases must be correct.  The introduction of the proposed\\nmethod improved the percentage of correct sentences from 5% to 12%.\\n\\n\\nSome examples of translations before and after the introduction of the\\nnew processing are given below.  The translations before the proposed\\nprocessing was implemented are marked  OLD, the translations\\nproduced by ALT-J/E using the proposed processing are marked\\n NEW.\\n\\n\\n\\n\\n\\n In (), the noun phrase headed by otona ``adult'' is judged to be prescriptive, as it is the complement of the copular   naru ``become''.  Therefore the proposed method translates it\\nwith the same number as the subject.\\n\\n\\n\\n\\n\\nzetumetu ``die out'', is entered in the lexicon as a verb\\nwhose subject must be generic.  manmosu ``mammoth'' is fully\\ncountable so the generic noun phrase is translated as a bare plural.\\n\\n\\n\\n\\n\\nThe old version recognizes that a denumerated noun phrase headed by an\\nuncountable noun tofu requires a classifier but does not\\ngenerate the correct structure neither does it generate a classifier\\nfor the pluralia tanta scissors.  The version using the\\nproposed method does.\\n\\n\\n\\n\\n\\nAs the subject of the copula that is countable it's complement\\nis judged to be denumerated by the proposed method.  As the complement\\nis headed by an uncountable noun it must be embedded in the\\nprepositional complement of a classifier.\\n\\n\\nThere are three main problems still remaining.  The first is that\\ncurrently the rules for determining the noun phrase referentiality are\\ninsufficiently fine.  We estimate that if referentiality could be\\ndetermined 100% correctly then the percentage of noun phrases with\\ncorrectly generated articles and number could be improved to 96% in\\nthe test set we studied.  The remaining 4% require knowledge from\\noutside the sentence being translated.  The biggest problem is noun\\nphrases requiring world knowledge that cannot be expressed as a\\ndictionary default.  These noun phrases cannot be generated correctly\\nby the purely heuristic methods proposed here.  The last problem is\\nnoun phrases whose countability and number can be deduced from\\ninformation in other sentences.  We would like to extend our method to\\nuse this information in the future.\\n\\n\\n    Conclusion\\n\\n\\nThe quality of the English in a\\nJapanese to English Machine Translation system can be improved by the\\nmethod proposed in this paper.  This method uses the information\\navailable in the original Japanese sentence along with information\\nabout English countability at both the noun phrase and noun level that\\ncan be stored in Japanese to English transfer dictionaries.\\nIncorporating this method into the machine translation system\\n ALT-J/E helped to improve the percentage of noun phrases\\nwith correctly generated articles and number from 65% to 73%.\\n\\nBibliography \\n\\n ALLAN, KEITH.\\n1980.\\nNouns and countability.\\nLanguage 56.541-67.\\n\\n\\n BOND, FRANCIS,   KENTARO OGURA.\\n1993.\\nDetermination of whether an English noun phrase is countable or not\\n  using 6 levels of lexical countability.\\nIn Proceedings of the 46th Annual Convention IPSJ Japan,\\n  6:107-108.\\n(in Japanese).\\n\\n\\n HUDDLESTON, RODNEY.\\n1984.\\nIntroduction to the Grammar of English.\\nCambridge textbooks in linguistics. Cambridge: Cambridge University\\n  Press.\\n\\n\\n IKEHARA, SATORU,  SATOSHI SHIRAI,  AKIO YOKOO,  \\n  HIROMI NAKAIWA.\\n1991.\\nToward an MT system without pre-editing - effects of new methods\\n  in ALT-J/E-.\\nIn Proceedings of MT Summit III, 101-106.\\n(cmp-lg/9510008).\\n\\n\\n KUNO, SUSUMU.\\n1973.\\nThe Structure of the Japanese Language.\\nCambridge, Massachusetts, and London, England: MIT Press.\\n\\n\\n MURATA, MASAKI,   MAKOTO NAGAO.\\n1993.\\nDetermination of referential property and number of nouns in\\n  Japanese sentences for machine translation into English.\\nIn Proceedings of the Fifth International Conference on\\n  Theoretical and Methodological Issues in Machine Translation (TMI-93),\\n  218-25.\\n\\nFootnotes\\n\\n  Japanese does not have obligatory plural\\n  morphemes.  Plurality can be marked but only rarely is, for example\\nby adding a suffix such as tachi ``and others'' (this can\\nnormally only be used with people or animals).\\n  The determiners much, little, a little, less\\n    and overmuch. can all be used for this test\\n  We called these environments `mass' because\\n  they both can be used to show a mass or unbounded interpretation.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper presents a heuristic method that uses information in the\\n  Japanese text along with knowledge of English countability and\\n  number stored in transfer dictionaries to determine the countability\\n  and number of English noun phrases.  Incorporating this method into\\n  the machine translation system ALT-J/E, helped to raise the\\n  percentage of noun phrases generated with correct use of articles\\n  and number from 65% to 73%.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nPossessive pronouns are often used as determiners in English when no\\nequivalent would be used in a Japanese sentence with the same meaning.\\nFor example, when referring to specific family members in English, it\\nis normal to specify whose relations they are.  In Japanese these are\\nonly specified if they are not obvious from the context.  For a\\nmachine translation system to generate appropriate English when\\ntranslating from Japanese, it is necessary to determine which pronouns\\nshould be used and when.\\n\\n\\nThe similar problem of determining article usage and noun phrase\\n number has recently been approached in three ways: using expert-system-like rules to determine the\\n referential property and number of nouns ; using heuristic rules based on the meaning of the Japanese sentence and the\\nproperties of the generated English to determine the referentiality\\n and number of English noun phrases  and using a Context Monitor to maintain contextual information dynamically\\n .  The problem of generating possessive pronouns as determiners in translation where there is no equivalent in the\\nJapanese has not previously been addressed; it requires not\\nonly contextual information such as that used to determine noun phrase\\nreferentiality, but also information about the conventional usage of\\npossessive pronouns in English.\\n\\n\\nIn this paper, we propose a method of generating possessive pronouns\\nas determiners for noun phrases where there is no equivalent in the\\nJapanese, based on treating information about the conventional use of\\npossessive pronouns in English as a lexical property of nouns.  In\\naddition, the method uses contextual information about noun phrase\\nreferentiality, the meaning and modality of the main verb and the\\ndenotation of the subject of the sentence that the noun phrase appears\\nin.  The method has been implemented in NTT Communication Science\\n Laboratories' Japanese-to-English machine translation system   ALT-J/E ,. \\n\\n\\nThe rest of this document is organized as follows: We begin in\\n Section  by examining the distribution of possessive pronouns in 6,200 Japanese sentences with English translations.  657\\nnoun phrases containing possessive pronouns were found in the\\ntranslations.  Existing algorithms, described in\\n Section , are capable of translating 52% of these 657 noun phrases, mainly those in which there was a possessive\\nconstruction in the Japanese.  The proposed method for appropriately\\ngenerating possessive pronouns for the remaining 48% is presented in\\n Section .  The result of implementing the proposed  method is evaluated in Section .  Finally some  concluding remarks are given in Section . \\n\\n\\n    Differences in the use of possessive pronouns in Japanese and\\n  English\\n\\n\\nIn order to examine the use of possessive pronouns when translating\\nfrom Japanese to English, a study was made of 6,200 sentence pairs of\\nJapanese sentences with English translations, produced by a\\nprofessional translator.  These pairs make up a test set (taken mainly\\nfrom written Japanese such as in newspaper articles) designed to test\\nthe capabilities of Japanese-to-English machine translation systems.\\nA description of the test set and it's design is given in\\n.  The use of possessive pronouns is not one of\\nthe criteria specifically tested by the test set.\\n\\n\\nThe English translations of the test set contain 657 noun phrases with\\npossessive pronouns.  The sentences containing these noun phrases were\\nexamined in order to determine how the possessive pronouns could be\\ngenerated by a machine translation system.  The noun phrases were\\ndivided into three groups, according to whether the possessive pronoun\\nhad an equivalent in the original Japanese, or could be predicted as\\nan obligatory part of an English expression or if neither of the above\\nconditions held.\\n\\n\\nThere were 193 noun phrases (30%) in the first group (I) where the\\noriginal Japanese noun phrase contains a possessive expression, either\\na pronoun or the reflexive jibun `self' followed by the\\ngenitive postposition no `of' that indicates possession.  The\\ngenitive pronoun construction ( PRONOUN-no) can be\\ndirectly translated into English as a possessive pronoun.  An example\\n is shown in sentence (). \\n\\n\\nThe genitive reflexive construction jibun-no `one's own'\\nappears in 25 cases (4% of the total).  jibun-no is\\ntranslated as  POSSESSIVE PRONOUN own.  The relation\\nbetween the pronoun and its antecedent is not given explicitly, it\\ndepends on the context.  An example of this is shown in\\n sentence ().  When the subject is kanojo `she' jibun-no `one's own' is translated as her own.  If the\\nsubject were changed to kare `he' or John then   jibun-no `one's own' would be translated as his own.\\n\\n\\nThe second group (II), with 105 noun phrases (16%), consisted of those in\\nwhich the possessive pronoun appeared as part of an English expression\\nwhere both the use of a possessive pronoun and its antecedent could be\\ndeduced from the form of the expression, but there was no equivalent\\npossessive construction in the Japanese original sentence.  For\\nexample, in the expression 20-dai-no-josei `20 generation \\n  GEN woman' a women in her twenties the possessive pronoun\\nher is an obligatory part of the English expression, and its\\nantecedent is always the modificant of the prepositional phrase.  An\\n expression may be based on a verb, as in sentence () where the Japanese idiom chie-o shiboru `wring knowledge'\\nis translated into an English idiom rack  POSSESSIVE PRONOUN\\n  brains in which the antecedent of the possessive pronoun is the\\nnoun phrase that is the subject of the verbal idiom.\\n\\n\\nGroups I and II can be translated straightforwardly by a machine\\ntranslation system.  A discussion of how this is done in ALT-J/E\\n is given in Section .   \\n\\n\\nThe third and final group (III) consists of 359 noun phrases (54%)\\nwhere the original Japanese had neither a possessive construction, as\\nin group I, nor arose in an English expression in which it was\\nobligatory, as in group II.  These noun phrases were those where\\nEnglish conventionally uses a possessive pronoun to indicate a\\nrelationship such as ownership, as in my wallet, or a family\\nrelationship, my father, but Japanese does not.  The use of\\npossessive pronouns with the nouns which head the noun phrases in\\ngroup III seems to be tied to the particular words.  In particular,\\nwords which denote  BODY PARTS, WORK, PERSONAL POSSESSIONS,\\n  ATTRIBUTES and relational nouns such as  KIN and  PEOPLE\\n  DEFINED BY THEIR RELATION TO ANOTHER PERSON (e.g. assailant,\\n  partner, subordinate) were commonly translated with possessive\\npronouns.  The semantic hierarchy of 2,800 categories used in   ALT-J/E was not fine-grained enough to identify the words by their\\ndenotation alone.  We therefore identified the nouns manually and\\nmarked them with a special flag in the lexicon.  These nouns will be\\nreferred to as `trigger-nouns' as they trigger the use of possessive\\npronouns when they are used in English.  We are investigating\\nautomating the identification process using a parsed bilingual corpus\\naligned at the noun phrase level.\\n\\n\\nThere were 205 different trigger-nouns in the test set used.  They\\nheaded 825 noun phrases.  The human translations of 355 out of the 825\\nnoun phrases headed by trigger-nouns (43%) contained possessive\\npronouns, even though the original Japanese did not contain a\\npossessive construction, and the possessive pronoun was not part of an\\nEnglish expression in which it was obligatory.  A new heuristic method\\nfor translating these cases that uses the head noun's lexical\\ninformation as a trigger to generate possessive pronouns, in\\nconjunction with contextual information is proposed in\\n Section . \\n\\n\\nThe distribution of the above three groups is summarized in\\n Table . \\n\\n\\n    Existing translation algorithms\\n\\n\\nThis section describes the overall process of translation in   ALT-J/E, and in particular how the possessive pronouns in noun\\nphrases from groups (I) and (II) are translated.\\n\\n\\nThe overall process of translation can be divided into seven parts.\\nFirst, ALT-J/E splits the Japanese text into morphemes.  Second,\\nit analyses the sentence syntactically, often giving multiple possible\\ninterpretations.  Third, it rewrites complicated Japanese expressions\\ninto simpler ones.  Fourth, ALT-J/E semantically evaluates the\\nvarious interpretations.  Fifth, syntactic and semantic criteria are\\nused to select the best interpretation.  Sixth, the selected\\ninterpretation is transferred into English.  Finally, the English\\nsentence is adjusted to give the correct inflectional forms.\\n\\n\\nNoun phrases in group I, where the Japanese contains a possessive\\nexpression are directly translated at the beginning of the (sixth)\\ntransfer stage.  In order to determine the antecedent of the pronoun\\nin noun phrases with the genitive reflexive construction   jibun-no, ALT-J/E uses a simple algorithm that identifies the\\nsubject of the clause containing jibun-no `one's own' as the\\nantecedent after elided subjects have been\\n supplemented.  ALT-J/E uses a simple mapping of antecedent to pronoun based on the antecedent's\\nsyntactic features of  PERSON,  GENDER and the semantic\\n feature  HUMAN as shown in Figure .  If the possessive pronoun itself appears in the subject, then the pronoun is\\njudged to be used deictically and is determined according to the\\nmodality of the sentence, for example for declarative sentences the\\npronoun is taken to refer to the speaker (giving my),\\nwhereas for imperative or interrogative sentences it is taken to refer \\nto the hearer (giving your).\\n\\n\\nThe translation rules for English expressions with obligatory\\npossessive pronouns identify the antecedent within the rule.  For\\n example the rule used in translating sentence () can be glossed as follows:\\n\\n\\nN1-wa chie-o shiboru `N1- TOP knowledge-\\n  OBJ wring'  \\n\\n\\n N1 racks N1's brains \\n\\n\\nWhen the Japanese analysis stage has parsed the\\nsentence correctly and an appropriate pattern has been chosen in the\\ntransfer stage then the correct possessive pronoun will be generated.\\n\\n\\nFor the 105 sentences of group II where the translator uses an idiom\\ncontaining a possessive pronoun, the machine translation system does\\nnot always choose the same idiom as the human translator.  In the\\ncases where the machine generates an idiom that does use a possessive\\npronoun it is generated correctly.\\n\\n\\n     Generating possessive pronouns in noun phrases headed by\\n  trigger-nouns\\n\\n\\nThis section describes the proposed method for appropriately\\ngenerating possessive pronouns for noun phrases headed by trigger\\nnouns.  The discussion will be illustrated with examples of\\ntranslations from two versions of ALT-J/E.  The original version\\n(hereafter the '93 version) does not use the proposed method for\\ngenerating possessive pronouns.  The version that uses the proposed\\n method will be referred to as the '94 version. \\n\\n\\nThe generation of possessive pronouns in noun phrases headed by\\ntrigger-nouns occurs at the end of the transfer phrase.  We shall call\\nthe pronouns generated for these noun phrases `default possessive\\npronouns' because they are generated as a default, not as a result of\\nbeing explicitly indicated in the Japanese or in the translation\\npattern.  \\n\\n\\n The proposed algorithm is outlined in Figure . First, the noun phrase's referential property is determined as\\n described in Section .  If the noun phrase's determiner slot is already filled, then it cannot have a possessive pronoun.\\nSome of the ways that the determiner slot can be filled are described\\n in Section .  Finally, if the noun phrase is headed by a trigger-noun and is neither the subject of the sentence nor the\\ndirect object of a verb with meaning  POSSESSION or \\n  ACQUISITION, then it will be generated with a possessive pronoun\\nwhose antecedent is the subject of the sentence.  The significance of\\n the verb meaning is discussed in Section . \\n\\n\\nThe extra rules for noun phrases headed by trigger-nouns denoting \\n   KIN or  BODY PARTS are described in Section . \\n\\n    Effects of noun phrase referentiality\\n\\n\\nThe use of heuristic rules to determine the referentiality of noun\\nphrases in the machine translation system ALT-J/E is discussed\\nin detail in .  In the following discussion we\\nwill assume that the referentiality of a noun phrase can be correctly\\ndetermined.\\n\\n\\nConsider the translation of hana `nose' in sentences\\n () and (). \\n\\n\\n In sentence () the subject nose is determined by the human translator to have generic reference and no possessive\\n pronoun is used.  In sentence () the subject is determined to refer to a specific person's nose, and so a possessive\\npronoun is used.  In general, noun phrases with generic reference are\\nnot modified by possessive pronouns.  Similarly, noun phrases used\\nascriptively, to ascribe an attribute to another noun phrase, do not\\nuse possessive pronouns, e.g. That is a big nose! Therefore,\\nwe restrict the problem of determining when possessive phrases should\\nbe used to referential noun phrases.\\n\\n\\n In sentence (), the '93 version does not differentiate between generic and referential noun phrases.  By chance, the\\ntranslation given (A nose is a sensory organ) has a generic\\ninterpretation so the translation is judged as correct.  In the '94\\nversion, the system determines that the sentence is generic, because\\nit is stating a general truth, and thus the subject has generic\\nreference.  The judgment is done with the following rule: if the\\nsemantic category of the subject of a copula is a child of the\\nsemantic category of the object then the noun phrase in the subject\\nposition has generic reference.  In this case, the semantic attributes\\nstored in the lexicon for nose and sensory organ are\\n NOSE and  ORGAN respectively, and the category  NOSE\\nis a child of  ORGAN, that is, a  NOSE IS-A ORGAN.  Generic\\nnoun phrases headed by countable nouns are translated as bare plurals\\nand are not candidates for the generation of default possessive\\n pronouns.  In sentence (), however, the subject is determined to be referential.  Therefore, as nose is a\\ntrigger-noun a possessive pronoun with deictic reference is generated.\\n\\n\\n    Filling the determiner slot\\n\\n\\nDefault possessive pronouns will not be generated if the determiner\\nslot has been filled.  The determiner slot can be filled by elements\\ndirectly translated from the Japanese: e.g. the demonstrative   kono `this' fills the determiner slot as kono `this'.  It\\ncan also be filled by the rules that generate definite and indefinite\\n articles: e.g., in the noun phrase in example () even though it is headed by the trigger-noun saifu `wallet' the\\ndeterminer slot is filled by the definite article so the noun phrase\\nis not a candidate for a default possessive pronoun.  In contrast, a\\n possessive pronoun is generated in example () where there is no definite article generated.  The rules that can fill the\\ndeterminer slot are too numerous to be enumerated here.\\n\\n\\n    Restrictions determined from the meanings of verbs\\n\\n\\nExamining the test set showed that the meanings of verbs can be used\\nto determine whether a possessive pronoun should be generated or not\\nfor noun phrases headed by trigger-nouns.  Noun phrases which are the\\ndirect objects of verbs that express possession, such as own,\\n  have or possess and noun phrases that are the object of\\nverbs that express that the object has just been acquired, for\\nexample, the direct object of buy, acquire or steal\\nare translated with an indefinite article rather than a possessive\\npronoun even when headed by trigger-nouns.\\n\\n\\nBoth these cases can be explained by considering the verb's meaning.\\nIn the first case the verb itself shows that the subject is the\\npossessor of the object, so a possessive pronoun is not needed to show\\nthe meaning.  If a possessive pronoun is used, it especially\\nemphasises the fact that the subject's referent possesses the referent\\n of the object. In the second case, in which the subject `acquires' the object, the object is not `possessed' by the\\nsubject until after the action described by the verb is completed, so\\na possessive pronoun is not used.\\n\\n\\nALT-J/E classifies verb meanings using the system of 97 verbal\\nsemantic attributes introduced in .  Verbs with\\nsimilar meanings share the same verbal semantic attributes which\\nallows a rule to be written as follows:\\n\\n\\n\\nIf a noun phrase headed by a trigger-noun is the direct object\\n  of a verb of  POSSESSION or  ACQUISITION then do not\\n   generate a possessive pronoun. \\n\\n\\n\\n\\n This rule is exemplified in sentence ().  If this rule were not implemented then because kuruma `car' is a\\ntrigger-noun the sentence would have been incorrectly translated as\\n`Do you have your car?' which introduces an emphasis that the\\noriginal Japanese lacks.\\n\\n\\n     KIN and  BODY PARTS\\n\\n\\nIn the test set, noun phrases denoting  KIN or  BODY PARTS\\nare modified by possessive pronouns used deictically when they are the\\nsubject of the sentence.  Therefore, the pronoun is determined\\naccording to the modality of the sentence: e.g. for declarative\\nsentences the pronoun is first person singular (giving my),\\nwhereas for imperative or interrogative sentences it is the second\\nperson (giving your).\\n\\n\\nTwo special cases were identified.  Nouns which explicitly denote \\n   PARENTS or  CHILDREN are only translated with possessive pronouns if they appear together in the same sentence.\\nIn this case, they are translated as though they are related to each\\nother but not to the speaker.  Therefore the following special rule\\nhas been implemented: Only generate a possessive pronoun for trigger\\nnouns which explicitly denote  PARENTS or  CHILDREN if a\\nsentence contains one of each category, in which case the first to\\nappear is the antecedent of the second to appear.\\n\\n\\nThe second special case was for sentences with compound subjects that\\ninclude nouns that denote  KIN.  For example, if the subject is\\nme and my spouse and the person in the noun phrase in question\\nis a member of the family other than `our' children (or grandchildren)\\nthen they will normally be either related to me or to my\\n  spouse, but not both, therefore they will be modified by my\\nrather than our: e.g. My wife and I gave my sister a book\\nbut My wife and I gave our child a book.  Similarly siblings\\nwill not normally have children or grandchildren in common so   my will be used for children and grandchildren: My sister\\n  and I gave our mother a book but My sister and I gave my child\\n  a book.  These rules have not yet been implemented.\\n\\n\\n\\n    Results\\n\\n\\nA preliminary evaluation of ALT-J/E's generation of possessive\\npronouns was conducted on the test set of 6,200 sentences described in\\n Section .  All save two of the 168 noun phrases in group I,in which the original Japanese contained an explicit possessive\\nexpression, are translated correctly. Two of the 25 sentences\\ncontaining jibun-no in the test set were translated\\nincorrectly.  Both of the errors were caused by the subject being\\nincorrectly identified in embedded sentences.  For the 105 noun\\nphrases in group II, in which a possessive pronoun is required by an\\nEnglish expression in the human's translation but there is no\\npossessive expression in the Japanese, ALT-J/E did not always\\nselect the same expression as the human translator.  When   ALT-J/E selected an expression that requires a possessive pronoun,\\n such as to rack one's brains in sentence () or  to wash one's hands in sentence (), it was generated correctly.\\n\\n\\nThere were 825 noun phrases in the test set headed by trigger-nouns.\\nIn 9% of the noun phrases (73), there were errors in the analysis or\\ntransfer stages which made evaluation of the appropriateness of the\\npossessive pronoun impossible. The results of the generation of the\\n remaining 752 noun phrases are given in Table . \\n\\n\\nThe evaluation was conducted by comparing the machine generated\\ntranslation of the noun phrases headed by trigger-nouns with the human\\ntranslations.  A machine generated possessive pronoun is judged to be\\nappropriate if it also appears in one or more of the human\\ntranslations.  If a pronoun is generated that does not appear in the\\n human translations, it is judged to be not appropriate. 429 (57%) of the noun phrases headed by trigger-nouns do not require a possessive\\npronoun to be generated by the proposed method.  For example the noun\\nphrase phrase is non-referential, or the determiner slot is already\\nfilled, or the noun phrases was dominated by a verb of \\n  POSSESSION or  ACQUISITION.  These noun phrases are all\\ntranslated correctly by the `93 version as it has no special\\nprocessing for generating possessive pronouns.  It fails, however, to\\ngenerate possessive pronouns when they are judged as necessary in the\\nremaining 323 noun phrases (43%).  Thus the accuracy of the '93\\nversion (the number judged correct over the total number) is only 57%\\n (429/752). \\n\\n\\nIn the '94 version, using the proposed method, noun phrases are\\ngenerated when wanted 80% of the time (the number of noun phrases\\nwith appropriate possessive pronouns generated (263) over the number\\nof noun phrases where a possessive pronoun was judged appropriate\\n(323)).  The errors caused by not generating the appropriate pronoun\\nare mainly due to errors in the parse selected in the analysis stage\\nand conflicts with other rules.  We estimate that overall improvements \\nin the parsing and transfer stages can solve these problems for\\n30 of the noun phrases considered.  Thus the estimated potential\\nsuccess rate is 91% (293/323).   \\n\\n\\nThe proposed method, however, introduces a new source of errors,\\nover-generation of possessive pronouns.  Possessive pronouns are\\ninappropriately generated for 83 noun phrases headed by trigger-nouns\\n(11% of the total number).  Two solutions are proposed.  First, to\\nimprove the processing that determines the noun phrase referentiality\\nand definiteness, this would block possessive pronouns from being\\ngenerated by filling the determiner slot with a more appropriate\\ndeterminer.  Second, to introduce explicit semantic constraints (e.g.:\\nonly generate a possessive pronoun for trigger-nouns that denote \\n  CLOTHING in the object position if the subject denotes a \\n  HUMAN), these would stop pronouns from being generated\\nunnecessarily.  We estimate that a combination of these solutions can\\nreduce the over-generation to around 45 noun phrases (6%).  To reduce\\nthe errors beyond this, we would require a discourse analysis capable\\nof actually determining explicit possessive relationships within a\\nlocal world model. Until such an analysis becomes feasible, some\\nover-generation is inevitable with the proposed method.  We make it\\neasier to correct for this during post-editing by tagging possessive\\npronouns generated by the proposed method as being less reliable than\\npossessive pronouns generated from directly from possessive\\nexpressions in the source text or transfer patterns.  The tagged\\npronouns can then be marked when presented to a post editor (for\\nexample in a different colour or font) for special attention.\\n\\n\\nThe accuracy of the '94 version (the number judged correct over the\\ntotal number) is 81% (609/752), an improvement of 24%.  The\\nprecision of for the new method (the number judged correct out of the\\ntotal number generated by the proposed method) is 88% (609/692).  The\\naccuracies and precisions achieved by the '93 version (which does not\\nuse the proposed method) and the '94 method (which uses the proposed\\nmethod) for the generation of possessive pronouns in noun phrases\\n headed by trigger-nouns are summarized in Table . \\n\\n\\n    Conclusion\\n\\n\\nIn order to examine when possessive pronouns should be generated when\\ntranslating between Japanese and English 6,200 Japanese sentences with\\nEnglish translations were examined.  657 examples of noun phrases\\ncontaining possessive pronouns were found in the human translations.\\nThe existing algorithms used by the Japanese-to-English machine\\ntranslation system ALT-J/E were sufficient for 46% of the noun\\nphrases. A heuristic method for appropriately generating possessive\\npronouns for the remaining 54% was proposed.  The method uses cue\\nwords we call trigger-nouns, along with contextual information about\\nnoun phrase referentiality and the subject and main verb of the\\nsentence that the noun phrase appears in.  The proposed method was\\nimplemented in ALT-J/E.  It increased the number of noun phrases\\nwith appropriate possessive pronouns generated by 263 to 609, but at\\nthe cost of generating 83 noun phrases with inappropriate possessive\\npronouns.  We intend to increase the number of appropriate possessive\\npronouns generated by resolving rule conflicts and to reduce the\\nnumber of inappropriate possessive pronouns generated by adding more\\nsemantic constraints.\\n\\n\\n  Acknowledgments \\n\\nWe would like to thank Tsuneko Nakazawa for her comprehensive\\ncriticism and advice; the reviewer, Graham, Monique and Mitsuyo\\nBond for their comments and suggestions; and Toshiaki Nebashi, Kazuya\\nFukamachi and Yoshitake Ichii for their invaluable help in\\nimplementing the processing described here.\\n\\nBibliography \\n\\n BOND, FRANCIS,  KENTARO OGURA,   SATORU IKEHARA.\\n1994.\\nCountability and number in Japanese-to-English machine\\n  translation.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics (COLING '94), 32-38.\\n(cmp-lg/9511001).\\n\\n\\n BOND, FRANCIS,  KENTARO OGURA,   TSUKASA\\n  KAWAOKA.\\n1995.\\nNoun phrase reference in Japanese-to-English machine translation.\\nIn Proceedings of the Sixth International Conference on\\n  Theoretical and Methodological Issues in Machine Translation (TMI '95),\\n  1-14.\\n(cmp-lg/9601008).\\n\\n\\n CORNISH, TIM,  KIMIKAZU FUJITA,   RYOCHI SUGIMURA.\\n1994.\\nTowards machine translation using contextual information.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics (COLING '94), 51-56.\\n\\n\\n IKEHARA, SATORU,  SATOSHI SHIRAI,   KENTARO OGURA.\\n1994.\\nCriteria for evaluating the linguistic quality of Japanese to\\n  English machine translations.\\nJournal of Japanese Society for Artificial Intelligence 9.\\n(in Japanese).\\n\\n\\n IKEHARA, SATORU,  SATOSHI SHIRAI,  AKIO YOKOO,\\n   HIROMI NAKAIWA.\\n1991.\\nToward an MT system without pre-editing - effects of new methods\\n  in ALT-J/E-.\\nIn Proceedings of MT Summit III, 101-106.\\n(cmp-lg/9510008).\\n\\n\\n MURATA, MASAKI,   MAKOTO NAGAO.\\n1993.\\nDetermination of referential property and number of nouns in\\n  Japanese sentences for machine translation into English.\\nIn Proceedings of the Fifth International Conference on\\n  Theoretical and Methodological Issues in Machine Translation (TMI '93),\\n  218-25.\\n\\n\\n NAKAIWA, HIROMI,   SATORU IKEHARA.\\n1992.\\nZero pronoun resolution in a Japanese to English machine\\n  translation system using verbal semantic attributes.\\nIn Proceedings of the 3rd Conference on Applied Natural Language\\n  Processing (ANLP '92), 201-208.\\n\\n\\n NAKAIWA, HIROMI,  AKIO YOKOO,   SATORU\\n  IKEHARA.\\n1994.\\nA system of verbal semantic attributes focused on the syntactic\\n  correspondence between Japanese and English.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics (COLING '94), 672-678.\\n\\n\\n OGURA, KENTARO,  AKIO YOKOO,  SATOSHI SHIRAI,  \\n  SATORU IKEHARA.\\n1993.\\nJapanese to English machine translation and dictionaries.\\nIn Proceedings of the 44th Congress of the International\\n  Astronautical Federation, Graz, Austria.\\n\\nFootnotes\\n\\n  Japanese does not have articles, and noun phrases are\\n  normally not marked for number.\\n  Examples are given\\n  with the (romanized) Japanese original, a gloss and the human\\n  translation.  The examples have been simplified to exemplify points\\n  more clearly; a new translation has been made for each simplified\\n  sentence.  Japanese particles are glossed as follows:  TOP for\\n  wa which marks the topic,  OBJ for o which\\n  marks the object and  GEN for no which shows a genitive\\n  relation.\\n  Elided subjects are supplemented using\\n  information both from within the sentence being translated and from\\n   the surrounding paragraph . \\n  N1's\\n     represents a possessive pronoun with N1 as its antecedent.\\n  Translations\\n  made by ALT-J/E before the proposed processing was included\\n  are marked ``MT-93''.  Translations done by the current version of\\n  ALT-J/E which includes the proposed processing are marked\\n  ``MT-94''.\\n  A Japanese sentence that emphasises this\\n  possessive relationship would explicitly use jibun-no\\n  `self- GEN'.  In this case ALT-J/E will generate a\\n  possessive pronoun.  For example, jibun-no kutsushita-o\\n    motteimasu-ka `self- GEN sock- OBJ have- Q'     Do you have your own socks?.\\n  Furthermore if the noun\\n    phrase has no pre-determiner, determiner or post-determiner then\\n    maybe generate the determiner some (or any\\n    depending on the sentence aspect and noun phrase countability and\\n    number).\\n  That is nouns such as     child but not nouns such as son.\\n  In\\n  25% of the noun phrases in which the human translation had no\\n  possessive pronoun but ALT-J/E generated one the developers\\n  judge that generating a possessive pronoun gives an interpretation\\n  as appropriate as the human translation.  For the purpose of this\\n  evaluation however they are treated as incorrect.\\n  Note that the original method only actually handles\\n  52% of the total noun phrases correctly, however for the purpose of\\n  evaluating the algorithm we are ignoring the 73 noun phrases where\\n  the errors in the analysis and transfer stages are so great as to\\n  make the output unevaluable.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nPossessive pronouns are used as determiners in English when no\\n  equivalent would be used in a Japanese sentence with the same\\n  meaning.  This paper proposes a heuristic method of generating such\\n  possessive pronouns even when there is no equivalent in the\\n  Japanese.  The method uses information about the use of possessive\\n  pronouns in English treated as a lexical property of nouns, in\\n  addition to contextual information about noun phrase referentiality\\n  and the subject and main verb of the sentence that the noun phrase\\n  appears in.  The proposed method has been implemented in NTT\\n  Communication Science Laboratories' Japanese-to-English machine\\n  translation system ALT-J/E.  In a test set of 6,200 sentences,\\n  the proposed method increased the number of noun phrases with\\n  appropriate possessive pronouns generated, by 263 to 609, at the\\n  cost of generating 83 noun phrases with inappropriate possessive\\n  pronouns.\\n\\n\"],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nIn collaborative expert-consultation dialogues, two participants\\n(executing agent and consultant) work together to construct a plan for\\nachieving the executing agent's domain goal. The executing agent and\\nthe consultant bring to the plan construction task different knowledge\\nabout the domain and the desirable characteristics of the resulting\\ndomain plan.  For example, the consultant presumably has more\\nextensive and accurate domain knowledge than does the executing agent,\\nbut the executing agent has knowledge about his particular\\ncircumstances, intentions, and preferences that are either\\n restrictions on or potential influencers  of the domain plan being constructed. In agreeing to collaborate on constructing the\\ndomain plan, the consultant assumes a stake in the quality of the\\nresultant plan and in how the agents go about constructing it. For\\nexample, a consultant in a collaborative interaction must help the\\nexecuting agent find the best strategy for constructing the domain\\nplan, may initiate additions to the domain plan, and must negotiate\\nwith the executing agent when the latter's suggestions are not\\naccepted (rather than merely agreeing to what the executing agent\\nwants to do). Thus a collaborator is more than a cooperative\\nrespondent.\\n\\n\\nIn this paper, we present a plan-based architecture for response\\ngeneration in collaborative consultation dialogues, with emphasis on\\ncases in which the system and the user disagree. The model treats\\nutterances as proposals open for negotiation and only incorporates a\\nproposal into the shared plan under construction if both agents\\nbelieve the proposal to be appropriate. If the system does not accept\\na user proposal, the system attempts to modify it, and natural\\nlanguage utterances are generated as a part of this process. Since the\\nsystem's utterances are also treated as proposals, a recursive\\nnegotiation process can ensue. This response generation architecture\\nhas been implemented in a prototype system for a university advisement\\ndomain.\\n\\n\\n  Modeling Collaboration \\n\\nIn a collaborative planning process, conflicts in agents' beliefs must\\nbe resolved as soon as they arise in order to prevent the agents from\\nconstructing different plans. Hence, once a set of actions is proposed\\nby an agent, the other agent must first evaluate the proposal based on\\n his own private beliefs  and determine whether or not to accept the proposal. If an agent detects any conflict which leads\\nhim to reject the proposal, he should attempt to modify the proposal\\nto a form that will be accepted by both agents -- to do otherwise\\nis to fail in his responsibilities as a participant in collaborative\\nproblem-solving. Thus, we capture collaboration in a Propose-Evaluate-Modify cycle. This theory views the collaborative\\nplanning process as a sequence of proposals, evaluations, and\\nmodifications, which may result in a fully constructed shared plan\\nagreed upon by both agents. Notice that this model is essentially a\\nrecursive one: the Modify action in itself contains a full\\ncollaborative process -- an agent's proposal of a modification,\\nthe other agent's evaluation of the proposal, and potential\\nmodification to the modification!\\n\\n\\nWe capture this theory in a plan-based system for response generation\\nin collaborative task-oriented interactions. We assume that the\\ncurrent status of the interaction is represented by a tripartite\\n dialogue model  that captures intentions on three levels: domain, problem-solving, and discourse.  The domain level\\ncontains the domain plan being constructed for later execution. The\\nproblem-solving level contains the agents' intentions about how to\\nconstruct the domain plan, and the discourse level contains the\\ncommunicative plan initiated to further their joint problem-solving\\nintentions.\\n\\n\\nEach utterance by a participant constitutes a proposal intended\\nto affect the shared model of domain, problem-solving, and discourse\\nintentions. For example, relating a user's query such as Who is\\nteaching AI? to an existing tripartite model might require inferring\\na chain of domain actions that are not already part of the plan,\\nincluding Take-Course(User,AI). These inferred actions explain\\nwhy the user asked the question and are actions that the user is\\nimplicitly proposing be added to the plan. In order to capture the\\nnotion of proposals vs. shared plans in a collaborative\\nplanning process, we separate the dialogue model into an existing\\nmodel, which consists of a shared plan agreed upon by both agents,\\nand the proposed additions, which contain newly inferred\\nactions.  Furthermore, we augment Lambert's plan recognition algorithm\\n  with a simplified version of Eller's relaxation  algorithm  to recognize ill-formed plans. \\n\\n\\nWe adopt a plan-based mechanism because it is general and easily\\nextendable, allows the same declarative knowledge about collaborative\\nproblem-solving to be used both in generation and understanding, and\\nallows the recursive nature of our theory to be represented by\\nrecursive meta-plans.  This paper focuses on one component of our\\nmodel, the arbitrator, which performs the Evaluate and\\nModify actions in the Propose-Evaluate-Modify cycle of\\ncollaboration.\\n\\n\\n  The Arbitration Process \\n\\nA proposal consists of a chain of actions for addition to the\\nshared plan.  The arbitrator evaluates a proposal and determines\\nwhether or not to accept it, and if not, modifies the original\\nproposal to a form that will potentially be accepted by both\\nagents. The arbitrator has two subcomponents, the evaluator and the modifier, and has access to a library of\\n generic recipes for performing actions. \\n\\n  The Evaluator \\n\\n=-1000\\nA collaborative agent, when presented a proposal, needs to decide\\nwhether or not he believes that the proposal will result in a valid\\nplan and will produce a reasonably efficient way to achieve the\\nhigh-level goal. Thus, the evaluator should check for two types\\nof discrepancies in beliefs: one that causes the proposal to be viewed\\n by the system as invalid , and one in which the system believes that a better alternative to the user's proposal exists\\n ,. Based on this evaluation, the system determines whether it should accept the user's proposal,\\ncausing the proposed actions to be incorporated into the existing\\nmodel, or should reject the proposal, in which case a negotiation\\nsubdialogue will be initiated.\\n\\n\\nThe processes for detecting conflicts and better alternatives start at\\nthe top-level proposed action, and are interleaved because we intend\\nfor the system to address the highest-level action disagreed upon by\\nthe agents.  This is because it is meaningless to suggest, for\\nexample, a better alternative to an action when one believes that its\\nparent action is infeasible.\\n\\n  Detecting Conflicts About Plan Validity \\n\\nPollack argues that a plan can fail because of an infeasible\\naction or because the plan itself is ill-formed\\n . An action is infeasible if it cannot be performed by its agent; thus, the evaluator performs a feasibility check by examining whether the applicability conditions\\nof the action are satisfied and if its preconditions can be\\n satisfied. A plan is considered ill-formed if child actions do not contribute to their parent action as intended; hence, the\\nevaluator performs a well-formedness check to examine, for each\\n pair of parent-child actions in the proposal, whether the contributes relationship holds between them. The well-formedness check is performed before the feasibility check since it is reasonable to check the\\nrelationship between an action and its parent before examining the\\naction itself.\\n\\n\\n  Detecting Sub-Optimal Solutions \\n\\nIt is not sufficient for the system, as a collaborator, to accept or\\nreject a proposal merely based on its validity. If the system knows of\\na substantially superior alternative to the proposal, but does not\\nsuggest it to the user, it cannot be said to have fulfilled its\\nresponsibility as a collaborative agent; hence the system must model\\nuser characteristics in order to best tailor its identification of\\nsub-optimal plans to individual users. Our system maintains a user\\nmodel that includes the user's preferences. A preference\\nindicates, for a particular user, the preferred value of an attribute\\nassociated with an object and the strength of this preference. The\\npreferences are represented in the form, prefers(_user,\\n_attribute(_object, _value), _action, _strength), which indicates\\nthat _user has a _strength preference that the attribute _attribute\\nof _object be _value when performing _action. For instance, prefers(UserA, Difficulty(_course, easy), Take-Course, weak)\\nindicates that UserA has a weak preference for taking easy courses. A\\ncompanion paper describes our mechanism for recognizing user\\n preferences during the course of a dialogue . \\n\\n\\nSuppose that the evaluator must determine whether an action\\nAi (in a chain of proposed actions \\n\\n)\\nis\\nthe best way of performing its parent action Ai+1. We will limit\\nour discussion to the situation in which there is only one generic\\naction (such as Take-Course) that achieves Ai+1, but there\\nare several possible instantiations of the parameters of the action\\n(such as Take-Course(UserA,CS601) and Take-Course(UserA,CS621)).\\n\\n  The Ranking Advisor \\n\\nThe ranking advisor's task is to determine how best the parameters of\\nan action can be instantiated, based on the user's preferences.  For\\neach object that can instantiate a parameter of an action (such as\\nCS621 instantiating _course in Take-Course(UserA,_course)),\\nthe evaluator provides the ranking advisor with the values of\\nits attributes (e.g., Difficulty(CS621,difficult)) and the\\nuser's preferences for the values of these attributes (e.g., prefers(UserA, Difficulty(_course,moderate), Take-Course, weak)).\\n\\n\\nTwo factors should be considered when ranking the candidate\\n instantiations: the strength of the preference and the closeness of the match. The strength of a preference indicates the weight that should be assigned to the\\npreference. The closeness of the match (exact, strong, weak, or\\nnone) measures how well the actual and the preferred values of\\nan attribute match. It is measured based on the distance between\\nthe two values where the unit of measurement differs depending on the\\ntype of the attribute. For example, for attributes with discrete\\nvalues (difficulty of a course can be very-difficult,\\ndifficult, moderate, easy, or very-easy), the match between\\ndifficult and moderate will be strong, while that\\nbetween difficult and easy will be weak. The\\ncloseness of the match must be modeled in order to capture the fact\\nthat if the user prefers difficult courses, a moderate course will be\\nconsidered preferable to an easy one, even though neither of them\\nexactly satisfies the user's preference.\\n\\n\\nFor each candidate instantiation, the ranking advisor assigns\\nnumerical values to the strength of the preferences for the relevant\\nattributes and computes the closeness of each match. A weight is\\ncomputed for each candidate instantiation by summing the products of\\ncorresponding terms of the strength of a preference and the closeness\\nof a match. The instantiation with the highest weight is considered\\nthe best instantiation for the action under consideration. Thus,\\nthe selection strategy employed by our ranking advisor corresponds to\\n an additive model of human decision-making . \\n\\n\\n  Example \\n\\nWe demonstrate the ranking advisor by showing how two different\\ninstantiations, CS601 and CS621, of the Take-Course action are\\n ranked.  Figure  shows the relevant domain knowledge and user model information.\\n\\n\\nThe ranking advisor matches the user's preferences against the domain\\nknowledge for each of CS601 and CS621. The attributes that will be\\ntaken into account are the ones for which the user has indicated\\npreferences. For each attribute, the advisor records the strength\\nof the preference and the closeness of the match for each\\ninstantiation. For instance, in considering the attribute workload, the strength of the preference will be low-moderate,\\nand the closeness of the match will be strong and exact\\n for CS601 and CS621, respectively.  Table  shows a summary of the strength of the preferences and the closeness of the\\nmatches for the relevant attributes for both instantiations.\\nNumerical values are then assigned and used to calculate a final\\nweight for each candidate. In this example, the normalized weight for\\nCS601 is 43/48 and that for CS621 is 29/48; therefore, CS601 is\\nconsidered a substantially better instantiation than CS621 for the\\nTake-Course action for UserA.\\n\\n\\n\\n\\n  The Modifier \\n\\n=-1000\\nThe modifier is invoked when a proposal is rejected. Its task is\\nto modify the proposal to a form that will potentially be accepted by\\nboth agents. The process is controlled by the Modify-Proposal\\naction, which has four specializations: 1) Correct-Node, for\\nwhen the proposal is infeasible, 2) Correct-Relation, for when\\nthe proposal is ill-formed, 3) Improve-Action, for when a better\\ngeneric action is found, and 4) Improve-Parameter, for when a\\nbetter instantiation of a parameter is found. Each specialization\\neventually decomposes into some primitive action which modifies the\\nproposal. However, an agent will be considered uncooperative if he\\nmodifies a proposed shared plan without the collaborating agent's\\nconsent; thus, the four specializations share a common precondition\\n-- that the discrepancies in beliefs must be squared away\\n  before any modification can take place. It is the attempt to satisfy this precondition that causes the system to\\ngenerate natural language utterances to accomplish the change in the\\nuser's beliefs.\\n\\n\\n Figure  shows two problem-solving recipes, Correct-Relation and Modify-Relation, the latter being a subaction of the former. The applicability conditions of Correct-Relation indicate that it is applicable when the agents, _s1\\nand _s2, disagree on whether a particular relationship (such as contributes) holds between two actions (_node1 and _node2) in the\\nproposal. The applicability condition and precondition of Modify-Relation show that the action can only be performed if both\\n_s1 and _s2 believe that the relationship _rel does not hold\\nbetween _node1 and _node2; in other words, the conflict between _s1\\nand _s2 must have been resolved. The attempt to satisfy this\\nprecondition causes the system to invoke discourse actions to modify\\nthe user's beliefs, which can be viewed as initiating a negotiation\\nsubdialogue to resolve a conflict. If the user accepts the system's\\nbeliefs, thus satisfying the precondition of Modify-Relation,\\nthe original dialogue model can be modified; however, if the user\\nrejects the system's beliefs, he will invoke the Modify-Proposal\\naction to revise the system's suggested modification of his original\\nproposal.\\n\\n\\nIn order to retain as much of the original proposal as possible when\\nmodifying a proposal, Modify-Relation has two specializations:\\nRemove-Node and Alter-Node.  The former is selected if the\\naction itself is inappropriate, and will cause the action to be\\nremoved from the dialogue model. The latter is chosen if a parameter\\nis inappropriately instantiated, in which case the action will remain\\nin the dialogue model and the problematic parameter will be left\\nuninstantiated.\\n\\n\\n  Example of Correcting an Invalid Proposal \\n\\nSuppose earlier dialogue suggests that the user has the goal of\\ngetting a Master's degree in CS (Get-Masters(U,CS)).\\n Figure  illustrates the dialogue model that would result from the following utterances.\\n\\n\\n#1U:\\n   #2S:\\n    (4) #1\\n  I want to satisfy my seminar course requirement.\\n    (5)  \\n  Who is teaching AI? \\n\\n\\nThe evaluation process, which determines whether or not to accept the\\nproposal, starts at the top-level proposed domain action, Satisfy-Seminar-Course(U,CS). Suppose the system believes that Satisfy-Seminar-Course(U,CS) contributes to Get-Masters(U,CS),\\nthat U can perform Satisfy-Seminar-Course(U,CS), and that there\\nis no better alternative to the instantiation of Satisfy-Seminar-Course. The evaluator then checks its child\\naction Take-Course(U,AI). The system's recipe library indicates\\nthat Take-Course(U,AI) does not contribute to Satisfy-Seminar-Course(U,CS), since it believes that AI is not\\na seminar course, causing the proposal to be rejected.\\n\\n\\nThe modifier performs the Modify-Proposal action, which\\nselects as its specialization Correct-Relation, because the\\n rejected proposal is ill-formed.  Figure  shows the arbitration process and how Correct-Relation is expanded. Notice\\nthat the arbitration process (the problem-solving level in\\n Figure ) operates on the entire dialogue model in  Figure , and therefore is represented as meta-level problem-solving actions.  In order to satisfy the\\nprecondition of Modify-Relation, the system invokes the discourse\\naction Inform as an attempt to change the user's belief (in this\\ncase, to achieve believe(U,holds(contributes,\\nTake-Course(U,AI), Satisfy-Seminar-Course(U,CS)))). The Inform\\naction further decomposes into two actions, one which tells the user\\nof the belief, and one which provides support for the claim.  This\\nprocess will generate the following two utterances:\\n\\n\\n#1S:\\n   #2\\n    (6) #1\\nTaking AI does not contribute to satisfying the seminar\\ncourse requirement.\\n    (7)  \\nAI is not a seminar course.\\n\\n\\nIf the user accepts the system's utterances, thus satisfying the\\nprecondition that the conflict be resolved, Modify-Relation can\\nbe performed and changes made to the dialogue model. In this example,\\nthe proposal is rejected due to an inappropriate instantiation of the\\nparameter _course; thus Modify-Relation will select Alter-Node as a specialization to replace all instances of AI in\\nthe dialogue model with a variable. This variable can be\\nreinstantiated by Insert-Correction, the second subaction of\\nCorrect-Relation.\\n\\n\\nAssuming that the system and the user encounter no further conflict in\\nreinstantiating the variable, the arbitration process at the\\nmeta-level is completed and the original dialogue is returned to. The\\nproposed additions now consist of actions agreed upon by both agents\\nand will therefore be incorporated into the existing model. Notice\\nthat our model separates the negotiation subdialogue (captured at the\\nmeta level) from the original dialogue while allowing the same\\nplan-based mechanism to be used at both levels. It also accounts for\\nwhy the user's original question about the instructor of AI is never\\nanswered -- a conflict was detected that made the question\\nsuperfluous.  Thus certain situations in which questions fail to be\\nanswered can be accounted for by the collaborative process rather than\\nbeing viewed as a violation of cooperative behaviour.\\n\\n\\n  Example of Suggesting Better Alternatives \\n\\nConsider the following utterances, whose dialogue model has the same\\n structure as that for utterances () and ()  (Figure ). \\n\\n\\n#1U:\\n   #2\\n    (8) #1\\n  I want to satisfy my theory course requirement.\\n    (9)  \\n  Who is teaching CS621? \\n\\n\\nFor space reasons, we skip ahead in the evaluation process to the\\noptimality check for Take-Course(U,CS621).  There are two\\ninstantiations of _course that satisfy the constraints specified in\\nthe recipe for Satisfy-Theory-Course: CS601 and CS621. These are\\nranked by the ranking advisor based on the user's preferences,\\n summarized in Table , which suggests that CS601 is a substantially better alternative to CS621. Thus, Improve-Parameter is selected as a specialization of Modify-Proposal. Similar to the previous example, the Inform\\ndiscourse action will be invoked as an attempt to resolve the\\ndiscrepancies in beliefs between the two agents, which would lead to\\nthe generation of the following utterances:\\n\\n\\n#1S:\\n   #2\\n    (10) #1\\n  CS601 is a better alternative than CS621.\\n    (11)  \\n  CS601 meets at 2pm and involves formal languages and grammar.\\n\\n\\n Notice that utterance () provides supporting  evidence for the claim in (), and is obtained by comparing the sets of information used by the ranking advisor\\n (Table ) and selecting the features that contribute most to making CS601 preferable to CS621.\\n\\n\\n\\n  The Belief Level \\n\\nWe showed how our arbitrator detects and resolves conflicts at\\nthe domain level. Our goal, however, is to develop a mechanism that\\ncan handle negotiations at the domain, problem-solving, and discourse\\nlevels in a uniform fashion.  The process can be successfully applied\\nto the problem-solving level because both the domain and\\nproblem-solving levels represent actions that the agents propose to do\\n(at a later point in time for the domain level and at the current time\\nfor the problem-solving level); however, the discourse level actions\\nare actions that are currently being executed, instead of proposed for execution. This causes problems for the modification\\nprocess, as illustrated by the following example.\\n\\n\\n#1U:\\n   #2S:\\n    (12) #1\\n  I want to take AI.  \\n    (13)  \\n  Dr. Brown is teaching AI,  \\n    (14)  \\n  since he is a full professor. \\n\\n\\n Utterance () provides support for  (), which supports (). However, if the system believes that whether one is a full professor has no relation to\\nwhether or not he teaches AI, the system and the user have a conflict\\n as to whether () supports (). Problems will arise if the system convinces the user that Dr. Brown teaches AI\\nbecause that is his area of specialty, not because he is a full\\nprofessor, and attempts to modify the dialogue model by replacing the\\n Inform action that represents () with one that conveys specializes(Brown,AI).  This modification is\\ninappropriate because it indicates that the user informed the system\\nthat Dr. Brown specializes in AI, which never happened in the first\\nplace.  Therefore, we argue that instead of applying the arbitration\\nprocess to the discourse level, it should be applied to the beliefs\\nproposed by the discourse actions.\\n\\n\\nIn order to preserve the representation of the discourse level, and to\\nhandle the kind of conflict shown in the previous example, we expand\\nthe dialogue model to include a belief level. The belief level\\ncaptures domain-related beliefs proposed by discourse actions as well\\nas the relationship amongst them. For instance, an Inform action\\nproposes a mutual belief (MB) of a proposition and an Obtain-Info-Ref action proposes that both agents come to know the\\nreferent (Mknowref) of a parameter. Thus, information captured at the\\nbelief level consists not of actions, as in the other three levels,\\nbut of beliefs that are to be achieved, and belief relationships, such\\nas support, attack, etc.\\n\\n  Discourse Level Example Revisited \\n\\n Figure  outlines the dialogue model for utterances  ()-() with the additional belief level.  Note that each Inform action at the discourse level proposes a mutual\\nbelief, and that supports relationships (inferred from Address-Acceptance) are proposed between the mutual beliefs.\\n\\n\\nThe evaluation process starts at the proposed domain level. Suppose\\nthat the system believes that both Take-Course(U,AI) and Build-Plan(U,S,Take-Course(U,AI)) can be performed. However, an\\nexamination of the proposed belief level causes the proposal to be\\nrejected because the system does not believe that Dr. Brown being a\\nfull professor supports the fact that he teaches AI. Thus, Correct-Relation is selected as the specialization of Modify-Proposal in order to resolve the conflict regarding this supports relationship. Again in order to satisfy the precondition of\\nmodifying the proposal, the system invokes the Inform action\\nwhich would generate the following utterance:\\n\\n\\n#1S:\\n   #2\\n    (15) #1\\nDr. Brown being a full professor does not provide support\\nfor him teaching AI.\\n\\n\\nThus, with the addition of the belief level, the arbitrator is\\nable to capture the process of evaluating and modifying proposals in a\\nuniform fashion at the domain, problem-solving, and belief levels. An\\nadditional advantage of the belief level is that it captures the\\nbeliefs conveyed by the discourse level, instead of how they are\\nconveyed (by an Inform action, by expressing doubt, etc.).\\n\\n\\n\\n  Related Work \\n\\nAllen  proposed different plan modalities that\\ncapture the shared and individual beliefs during collaboration, and\\n Grosz, Sidner and Lochbaum , proposed a SharedPlan model for capturing intentions during a collaborative\\nprocess. However, they do not address response generation during\\ncollaboration. Litman and Allen  used\\ndiscourse meta-plans to\\nhandle correction subdialogues. However, their Correct-Plan only\\naddressed cases in which an agent adds a repair step to a pre-existing\\nplan that does not execute as expected. Thus their meta-plans do not\\nhandle correction of proposed additions to the dialogue model, since\\nthis generally does not involve adding a step to the proposal.\\nFurthermore, they were only concerned with understanding utterances, not\\nwith generating appropriate responses. Heeman and Hirst\\n and\\nEdmonds  use meta-plans to account for\\ncollaboration, but their mechanisms are limited to understanding and\\ngenerating referring expressions.  Although Heeman is extending his\\nmodel to account for collaboration in task-oriented dialogues\\n , his extension is limited to the recognition of actions in such dialogues. Guinn and Biermann\\n developed a model of collaborative\\nproblem-solving which attempts to resolve conflicts between agents\\nregarding the best path for achieving a goal. However, their work has\\nconcentrated on situations in which the user is trying to execute a\\ntask under the system's guidance rather than those where the system\\nand user are collaboratively developing a plan for the user to execute\\nat a later point in time.\\n\\n\\nResearchers have utilized plan-based mechanisms to generate natural\\nlanguage responses, including explanations\\n ,,. However, they only handle cases in which the user fails to understand the system, instead of\\ncases in which the user disagrees with the system. Maybury\\n developed plan operators for persuasive utterances,\\nbut does not provide a framework for negotiation of conflicting views.\\n\\n\\nIn suggesting better alternatives, our system differs from van Beek's\\n in a number of ways. The most significant are\\nthat our system dynamically recognizes user preferences\\n , takes into account both the strength of the preferences and the closeness of the matches in ranking\\ninstantiations, and captures the response generation process in an\\noverall collaborative framework that can negotiate proposals with the\\nuser.\\n\\n\\n  Conclusions and Future Work \\n\\nThis paper has presented a plan-based system that captures\\ncollaborative response generation in a Propose-Evaluate-Modify\\ncycle. Our system can initiate subdialogues to negotiate implicitly\\nproposed additions to the shared plan, can appropriately respond to\\nuser queries that are motivated by ill-formed or suboptimal solutions,\\nand handles in a unified manner the negotiation of proposed domain\\nactions, proposed\\nproblem-solving actions, and beliefs proposed by discourse actions.\\nIn addition, our system captures cooperative\\nresponses within an overall collaborative framework that allows for\\nnegotiation and accounts for why questions are sometimes never\\nanswered (even in the most cooperative of environments).\\n\\n\\nThis response generation architecture has been implemented in a\\nprototype system for a university advisement domain. The system is\\npresented with the existing dialogue model and the actions proposed by\\nthe user's new utterances. It then produces as output the logical form\\nfor the appropriate collaborative system response. In the future, we\\nwill extend our system to include various argumentation strategies\\n ,, for supporting its claims. \\n\\n\\n  Acknowledgments \\n\\nThe authors would like to thank Stephanie Elzer for her comments on\\nearlier drafts of this paper.\\n\\nBibliography \\n\\nAllen, J.\\n1991.\\nDiscourse structure in the TRAINS project.\\nIn Darpa Speech and Natural Language Workshop.\\n\\n\\nBratman, M.\\n1990.\\nWhat is intention?\\nIn Cohen, P.; Morgan, J.; and Pollack, M., eds., Intentions in\\n  Communication.\\nchapter 2,  15-31.\\n\\n\\nCawsey, A.\\n1993.\\nPlanning interactive explanations.\\nInternational Journal of Man-Machine Studies  169-199.\\n\\n\\nEdmonds, P.\\n1993.\\nA computational model of collaboration on reference in\\n  direction-giving dialogues.\\nTechnical Report CSRI-289, Univ. of Toronto.\\n\\n\\nEller, R., and Carberry, S.\\n1992.\\nA meta-rule approach to flexible plan recognition in dialogue.\\nUser Modeling and User-Adapted Interaction 2:27-53.\\n\\n\\nElzer, S.; Chu, J.; and Carberry, S.\\n1994.\\nRecognizing and utilizing user preferences in collaborative\\n  consultation dialogues.\\nIn Progress.\\n\\n\\nGrosz, B., and Sidner, C.\\n1990.\\nPlans for discourse.\\nIn Cohen, P.; Morgan, J.; and Pollack, M., eds., Intentions in\\n  Communication.\\nchapter 20,  417-444.\\n\\n\\nGuinn, C., and Biermann, A.\\n1993.\\nConflict resolution in collaborative discourse.\\nIn Proceedings of the IJCAI-93 Workshop:Computational Models of\\n  Conflict Management in Cooperative Problem Solving,  84-88.\\n\\n\\nHeeman, P., and Hirst, G.\\n1992.\\nCollaborating on referring expressions.\\nTechnical Report 435, Univ. of Rochester.\\n\\n\\nHeeman, P.\\n1993.\\nSpeech actions and mental states in task-oriented dialogues.\\nIn AAAI 1993 Spring Symposium on Reasoning About Mental States:\\n  Formal Theories and Applications.\\n\\n\\nJoshi, A.; Webber, B.; and Weischedel, R.\\n1984.\\nLiving up to expectations: Computing expert responses.\\nIn Proceedings of the AAAI,  169-175.\\n\\n\\nJoshi, A.\\n1982.\\nMutual beliefs in question-answer systems.\\nIn Smith, N., ed., Mutual Knowledge.\\nchapter 4,  181-197.\\n\\n\\nLambert, L., and Carberry, S.\\n1991.\\nA tripartite plan-based model of dialogue.\\nIn Proceedings of the ACL,  47-54.\\n\\n\\nLambert, L., and Carberry, S.\\n1992.\\nModeling negotiation dialogues.\\nIn Proceedings of the ACL,  193-200.\\n\\n\\nLitman, D., and Allen, J.\\n1987.\\nA plan recognition model for subdialogues in conversation.\\nCognitive Science 11:163-200.\\n\\n\\nLochbaum, K.\\n1991.\\nAn algorithm for plan recognition in collaborative discourse.\\nIn Proceedings of the ACL,  33-38.\\n\\n\\nMaybury, M.\\n1992.\\nCommunicative acts for explanation generation.\\nInternational Journal of Man-Machine Studies 37:135-172.\\n\\n\\nMaybury, M.\\n1993.\\nCommunicative acts for generating natural language arguments.\\nIn Proceedings of the AAAI,  357-364.\\n\\n\\nMoore, J., and Paris, C.\\n1993.\\nPlanning text for advisory dialogues: Capturing intentional,\\n  rhetorical and attentional information.\\nComputational Linguistics 19(4):651-694.\\n\\n\\nPollack, M.\\n1986.\\nA model of plan inference that distinguishes between the beliefs of\\n  actors and observers.\\nIn Proceedings of the ACL,  207-214.\\n\\n\\nQuilici, A.\\n1991.\\nThe Correction Machine: A computer Model of Recognizing and\\n  Producing Belief Justifications in Argumentative Dialogs.\\nPh.D. Dissertation, UCLA.\\n\\n\\nReed, S.\\n1982.\\nCognition: Theory and Applications.\\nchapter 14,  337-365.\\n\\n\\nSidner, C.\\n1992.\\nUsing discourse to negotiate in collaborative activity: An artificial\\n  language.\\nIn AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent\\n  Systems,  121-128.\\n\\n\\nSycara, K.\\n1989.\\nArgumentation: Planning other agents' plans.\\nIn Proceedings of the IJCAI,  517-523.\\n\\n\\nvan Beek, P.\\n1987.\\nA model for generating better explanations.\\nIn Proceedings of the ACL,  215-220.\\n\\nFootnotes\\n\\n  This material is\\nbased upon work supported by the National Science Foundation under\\nGrant No. IRI-9122026.\\n  A recipe\\n  is a template for performing an action. It encodes the preconditions for an action, the effects of an action,\\nthe subactions comprising the body of an action, etc.\\n  Applicability conditions are conditions that must\\nalready be satisfied in order for an action to be reasonable to\\npursue, whereas an agent can try to achieve unsatisfied preconditions.\\nOur evaluator considers a precondition satisfiable if there exists an\\naction which achieves the precondition and whose applicability\\nconditions are satisfied. Thus only a cursory evaluation of\\nfeasibility is pursued at this stage of the planning process, with\\nfurther details considered as the plan is worked out in depth. This\\nappears to reflect human interaction in naturally occuring\\ndialogues.\\n  Much of the\\ninformation needed for the feasibility and well-formedness checks will\\nbe provided by the plan-recognition system that identified the actions\\ncomprising the proposal.\\n  We\\nmodel six degrees each of positive and negative preferences based on\\nthe conversational circumstances and the semantic representation of\\n the utterance used to express the preferences . \\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper presents a plan-based architecture for response generation\\nin collaborative consultation dialogues, with emphasis on cases in\\nwhich the system (consultant) and user (executing agent) disagree.\\nOur work contributes to an overall system for collaborative\\nproblem-solving by providing a plan-based framework that captures the\\nPropose-Evaluate-Modify cycle of collaboration, and by allowing\\nthe system to initiate subdialogues to negotiate proposed additions to\\nthe shared plan and to provide support for its claims. In addition,\\nour system handles in a unified manner the negotiation of proposed\\ndomain actions, proposed problem-solving actions, and beliefs proposed\\nby discourse actions. Furthermore, it captures cooperative responses\\nwithin the collaborative framework and accounts for why questions are\\nsometimes never answered.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nIn conversation, a person sometimes has to refer to an object that is\\nnot previously known to the other participant.\\nOne particular situation in which this\\narises is in giving directions.  For example:\\nA B funny \\nGo straight ahead until you get to a funny-looking building.\\nThe hearer has to understand the reference well enough that when he\\nlater reaches the building, he will recognize it as the intended\\nreferent.\\n\\n\\nA reference of this sort is often achieved through a collaboration between the conversants.  In such cases, the speaker has\\nthe goal of having the hearer know how to identify an object.  The\\nspeaker attempts to achieve this goal by building a description of the\\nobject that she believes will give the hearer the ability to identify\\nit when it is possible to do so.  The hearer needs to be confident that the\\ndescription will be adequate as a means of identifying the referent,\\nbut because of the inevitable differences in beliefs about the\\nworld, he might not be.  When the hearer is not confident, the speaker\\nand hearer collaborate to make a new referring expression\\nthat the hearer believes is adequate.  This can be seen in the\\nfollowing portion of a telephone conversation recorded by\\nPsathas .\\nA B lowell Direction-giving\\nYa just stay on 2A, until ya get to Lowell Street.\\nIs it marked?\\nYeah, I think there's a street sign there, it's an intersection\\nwith lights.\\nOkay.\\nIn this dialogue, speaker B is not confident that he will be able to identify\\nthe intersection at Lowell Street, and so suggests that the intersection\\nmight be marked.\\nSpeaker A replies with an elaboration of the initial expression, and B finds\\nthat he is now confident, and so accepts the reference.\\n\\n\\nThis type of reference is different from the type that has been studied\\ntraditionally by researchers who have usually assumed that the agents\\nhave mutual knowledge of the\\n referent ,,,,, are copresent with the  referent ,, or have the referent in their focus of  attention .  In these theories, the speaker has the intention that the hearer either know the referent or identify it immediately.\\n\\n\\nAlthough the type of reference that we wish to model does not rely on\\nthese assumptions, we can nevertheless draw from these theories.  Thus,\\nwe base our model on the work of Clark and\\nWilkes-Gibbs , and Heeman and\\nHirst  who both modeled (the first\\npsychologically, and the second computationally) how people collaborate\\non reference to objects for which they have mutual knowledge.  We will\\nbriefly discuss these models, before we describe our own.\\n\\n\\n  Collaboration on reference \\n\\nIn their fundamental experiment, Clark and Wilkes-Gibbs \\ndemonstrated that conversants use a set of inherently collaborative\\nprocedures to establish the mutual belief that the hearer has\\nunderstood a reference.  In the experiment, two subjects were each\\ngiven a set of hard-to-describe tangram figures that were kept hidden\\nfrom the other.  One subject was required to get the other subject to\\nrearrange his set to match the ordering of her set, and to do so\\nthrough conversation alone.  Thus, the two subjects were obliged to\\ncollaborate on constructing descriptions of the figures that would\\nallow them to be unambiguously identified; for example, the one\\nthat looks like an angel with a stick.\\n\\n\\nClark and Wilkes-Gibbs developed the following process model to explain their\\nfindings.\\nTo initiate the process, speaker A presents an initial version of a referring\\nexpression on which speaker B passes judgment.  B can either accept it,\\nreject it, or postpone his decision until later.  If B\\nrejects or postpones, then the expression must be refashioned by either\\nA or B.  Refashionings are accomplished in three main ways:  repairing the expression by correcting speech errors, expanding\\nthe expression by adding more qualifications, or replacing part\\nor all of the expression with new qualifications.\\nEach judgment/refashioning pair\\noperates on the current referring expression, replacing it with a new\\none.\\nThis process continues until the expression, kept in the\\nparticipants' common ground, is mutually accepted.\\n\\n\\nThis excerpt from Clark and Wilkes-Gibbs's data illustrates rejection\\n(line 2), replacement (line 2), and acceptance (lines 3 and 4):\\nA B clark \\nOkay, and the next one is the person that looks like they're\\ncarrying something and it's sticking out to the left.  It looks\\nlike a hat that's upside down.\\nThe guy that's pointing to the left again?\\nYeah, pointing to the left, that's it!  [laughs]\\nOkay.\\n\\n\\nHeeman and Hirst  rendered Clark and Wilkes-Gibbs's\\nmodel computationally by casting it into the planning paradigm.  Their\\nmodel covers both the initiator of a referring action, and the recipient who tries to understand the reference.\\nIn this model, the initiator has the goal of\\nhaving the recipient identify the referent, and so constructs a\\nreferring plan\\ngiven a set of beliefs about what the recipient believes.\\nThe result of the initiator's plan is a set of surface speech actions,\\nand hearing only these actions, the recipient tries to infer a\\nplan in order to understand the reference.  Thus, referring expressions\\nare represented as plan derivations, and an unsuccessful referring\\nexpression is an invalid plan in whose repair the agents collaborate.\\n\\n\\nAn agent can infer a plan even if it is invalid in that agent's\\n view .  The evaluation process attempts to find an instantiation of the variables such that all of the constraints are\\nsatisfied and the mental actions executable with respect to the\\nhearer's beliefs about the speaker's beliefs.\\n\\n\\nIf the recipient finds the initial referring expression plan invalid, then\\nthe agents will collaborate in its repair. Heeman and Hirst used plan\\nrepair techniques to refashion an expression, and used discourse plans,\\nor meta-plans, to communicate the changes to it.  Thus, a collaborative\\ndialogue is modeled in terms of the evolution of the referring plan.\\n\\n\\nFirst, an agent must communicate that she has not understood a plan.\\nDepending on how the referring plan constrains the choice of referent,\\nshe constructs an instance of either reject-plan or postpone-plan, whose resulting surface speech actions are s-reject and s-postpone respectively.\\nNext, one agent or the other must refashion the referring expression\\nplan in the context of the judgment by either replacing some of its\\nactions (by using replace-plan) or by adding new actions to it\\n(by using expand-plan).  The result of both plans is the surface\\nspeech action s-actions.\\n\\n\\nBecause the model can play the role of both the initiator and the\\nrecipient, and because it can perform both plan construction and\\ninference, two copies of the model can converse with one another,\\nacting alternately as speaker and hearer.  Acting as hearer, one copy of\\nthe system\\nperforms plan inference on each set of surface speech actions that it\\nobserves, and updates the state of the collaboration.  It then switches\\nroles to become the speaker, and looks for a goal to adopt, and\\nconstructs a plan that achieves it.  After responding with the surface\\nactions of the plan, it updates the state of the collaboration,\\npresupposing that the other copy will accept the plan.  The system\\nrepeats the process until it can find no more goals to adopt, at which\\ntime it switches back to being the hearer and waits for a response from\\nthe other copy.\\n\\n\\n  Confidence and salience \\n\\nA crucial assumption of Clark and Wilkes-Gibbs's work--and of\\nHeeman and Hirst's model--is that the recipient of the initial\\nreferring expression already has some knowledge of the referent in\\nquestion.  In Clark and Wilkes-Gibbs's experiments, for example, it is\\none of the tangram figures.  In other words, the hearer can understand\\na referring expression if its content uniquely describes an object that\\nhe knows about.  Obviously, an agent cannot use this criterion to\\nunderstand the reference to the building in\\n Example --he has never heard of the building before.  What criteria, then, does he base his understanding on?\\n\\n\\nThe basis of our model is that the hearer can accept a referring\\nexpression plan if (1) the plan contains a description that is useful for making an identification plan that the hearer can\\nexecute to identify the referent, and (2) the hearer is confident\\nthat the identification plan is adequate.\\n\\n\\nThe first condition, originally described by Appelt , is\\nimportant because the success of the referring action depends on the hearer\\nformulating a useful identification plan.  We take the referring\\nexpression plan itself to be the identification plan.  The mental\\nactions in the plan will encode only useful descriptions.\\nFor the second condition to hold, the hearer must believe that the\\nidentification plan is good enough to uniquely identify the referent\\nwhen it becomes visible.  This involves giving enough information by\\nusing the most salient attributes of the referent.\\n\\n\\nIn our model, each agent associates a numeric confidence value\\nwith each of the attributes in the referring expression, and by\\ncomposing these, computes a level of confidence in the adequacy of the\\ncomplete referring expression plan that can be interpreted as ranging\\nfrom low confidence to high confidence.  The present composition\\nfunction is simple addition, but one could envision more complex\\nsystems to compute confidence, such as an algebra of confidence or a\\nnon-numeric system.  If the overall confidence value exceeds some set\\nvalue, the agent's confidence threshold, then the agent believes\\nthe plan is adequate.  That is, if the agent is the initiator, she\\nbelieves that the other will be able to understand the reference; if\\nthe agent is the other, he believes that he has understood the\\nreference.\\n\\n\\nNow, the confidence value of each attribute is equivalent to its salience within the context of the referring expression.  Salience,\\nfor our purposes in direction-giving, is primarily visual prominence,\\nbut can also involve identifiability, familiarity, and functional\\n importance ,.  One approach is to encode the salient properties in a static hierarchy as Davis , and Reiter and\\n Dale  have done. But, ideally, salience should depend on\\nthe context surrounding the referent.  For example, the height of a\\ntall building would normally be salient, but not if it were\\nsurrounded by other tall buildings.  This computation would be quite\\ncomplex, so we have adopted a middle ground between the simple\\ncontext-independent approaches, and a full-blown contextual analysis.\\nThe middle ground involves taking the type of object into account when\\nchoosing attributes and landmarks that relate to it.  For example,\\nheight and architectural style can be very salient features for\\ndescribing a building, but not for describing an intersection, for which\\nhaving a sign or traffic lights is important.  This approach still\\nallows us to encode salience in a hierarchy, but it is dependent on the\\nreferent.\\n\\n\\n Table  shows an example of a simple salience hierarchy that an agent might have.\\nThe hierarchy is actually a set of partial\\norderings of attributes, represented by lambda expressions, indexed by\\nobject type.\\nIn the table, the confidence value of using architectural style to\\ndescribe a building is 4.  The confidence value of a tall building is\\n3, and so this attribute is less salient than architectural style.  The\\nother rows (for describing intersections) follow\\n similarly. \\n\\n\\nEach agent has his own beliefs about salience.\\nIt is the difference in their beliefs that leads to the\\nnecessity for collaboration on reference.  Ideally, the initiator should\\nconstruct referring expressions with the recipients'\\n(believed) beliefs about salience in mind, but we have chosen to avoid this\\ncomplexity by making the simplifying assumption that the initiator is\\nan expert (and thus knows best what is salient).\\n\\n\\n  Plans for referring \\n\\nAn agent uses his salience hierarchy for two related purposes: the\\nfirst to determine what is salient in a particular situation, and the\\nsecond to determine the adequacy of a description.  So, the hierarchy\\nis accessed during both plan construction and plan inference.\\n\\n\\nIn plan construction, the hierarchy is used for constructing initial\\nreferring expression plans, and for elaborating on inadequate plans by\\nallowing an agent to choose the most salient properties of the referent\\nfirst.  The agent constructs an initial referring expression plan in\\nalmost the same way as in Heeman and Hirst's system.  Mental actions in\\nthe intermediate plans of a referring expression plan allow the speaker\\nto choose the most salient attributes that have not yet been chosen,\\nand constraints in the surface speech actions make sure the speaker\\n believes that each attribute is true. Other mental actions in the intermediate plans add up the confidence values of the attributes, and a final constraint makes sure\\nthat the sum exceeds the agent's confidence threshold.  So, for a\\nreferring plan to be valid, it must describe a unique object, and it\\nmust be adequate with respect to the speaker's beliefs.  This means\\nthat attributes beyond those required for a unique description could be\\n necessary.  For example, to construct the reference to the building in Example ) and determined that architectural style is salient.  Hence, she described the building as\\nfunny-looking.  This single attribute was enough to exceed her\\nconfidence threshold.\\n\\n\\nDuring plan inference, the salience hierarchy is used when evaluating a\\nrecognized plan.  The mental actions in the intermediate plans determine\\nthe confidence values of each attribute (from the hearer's salience\\nhierarchy), and add them up.  The final\\nconstraint in the plan makes sure that the hearer's confidence\\nthreshold is exceeded.\\nThus, judging the adequacy of a referring expression plan falls out\\nof the regular plan evaluation process.  If the final constraint does\\nnot hold, then the invalidity is noted so that the plan\\ncan be operated on appropriately by the discourse plans.\\n\\n\\nFor example, after recognizing the reference\\n in Example , he computes the confidence value of 4.  If this value exceeds his\\nconfidence threshold, then he will accept the plan.  If not, he will\\nbelieve that there is an error at the constraint that checks his\\nconfidence threshold.\\n\\n\\n  Suggestion and elaboration \\n\\nIf the recipient is not confident in the adequacy of the plan, he uses an\\ninstance of postpone-plan to inform the initiator that he is not\\nconfident of its adequacy, thereby causing the initiator to raise her own\\nconfidence threshold.  Now, although he cannot refashion the\\nexpression himself, he does have the ability to help the initiator by\\nsuggesting a good way to expand it; suggestion is a\\nconversational move in which an agent suggests a new attribute that he\\ndeems would increase his confidence in the expression's adequacy if the\\nexpression were expanded to include the attribute.  Continuing with the\\nexample, if the hearer were not confident about the adequacy of the funny-looking building, he might suggest that the initiator use\\nheight (as well as architectural style), by asking Is it tall?.\\nFrom this suggestion the initiator might expand her expression to the tall funny-looking building.  So, in our sense, a suggestion is an\\nillocutionary act of questioning; along with actually suggesting a way\\nto expand a plan, the agent is asking whether or not the referent has\\nthe suggested attribute.\\n\\n\\nTo decide what suggestion to make, the agent uses an instance of suggest-expand-plan, which has a mental action in its decomposition\\nthat chooses the attribute that he believes is the most salient\\nthat has not been used already.  The result of the plan is the surface\\nspeech action, s-suggest, that communicates the suggestion.\\n\\n\\nHowever, only the initiator of the referring expression can actually\\nelaborate a referring expression, because only she has the\\nknowledge to do so.  Depending on whether the hearer of the expression\\nmakes a suggestion or not, the initiator has two options when\\nelaborating a plan.  If no suggestion was made, then she can expand the\\nplan according to her own beliefs about the referent's attributes and\\ntheir salience.  On the other hand, if a suggestion was made, she could\\ninstead attempt to expand the plan by affirming or denying the attribute\\nsuggested.\\nIf possible, she should use the suggestion to elaborate the\\nplan, thus avoiding unwanted conversational implicature,\\nbut its use may not be enough to make the plan adequate.\\n\\n\\nThe decomposition of expand-plan calls the plan constructor with\\nthe goal of constructing a modifiers schema and with the\\nsuggested attribute as input--in a sense, continuing the construction\\nof the initial referring plan.  The plan constructor attempts to find a\\nplan with the surface speech actions for the suggested attribute in its\\nyield, but this might not be possible.  In any case, the speaker\\nconstructs an expansion that will make the plan adequate according to\\n her beliefs. \\n\\n\\nThe response to a suggestion depends, obviously, on whether or not the\\nsuggestion was used to expand the plan.\\nThe speaker can\\n(1) affirm that the plan was expanded with the suggestion by using the s-affirm speech act;\\n(2) affirm that the suggestion was used, along with additional\\nattributes that weren't suggested, by using s-affirm and s-actions;\\nor\\n(3) deny the suggestion with s-deny, and inform the other by s-actions as to how the plan was expanded.\\n\\n\\nBy repeatedly using the postponement, elaboration, and suggestion\\nmoves, the two agents collaborate through discourse on refashioning the\\nreferring expression until they mutually believe that the recipient is\\nconfident that it is adequate.\\n\\n\\n  Example \\n\\n We have implemented the model in Prolog.  Table  shows the input/output of two copies of the system engaging\\n in a simplified version of Example . Note that the system generates and understands\\nutterances in the form of descriptions of the surface speech actions, not\\nsurface natural language forms.  The existence of a parser and a\\ngenerator that can map between the two forms is assumed.  Complete\\ndetails of this example and of the model are given by\\nEdmonds my-thesis.\\n\\n\\n  Conclusion \\n\\nWhen an agent refers to a particular object that is not previously\\nknown to another agent, she has the intention that the agent be able to\\nidentify the object (when it is possible to do so) by means of the\\nreferring expression.  Because of the inevitable differences in their\\nbeliefs about the world--specifically about what is salient--the\\nagents may have to collaborate to make the expression adequate.\\n\\n\\nWe have implemented a computational plan-based model that accounts for\\nthe collaborative nature of reference in the domain of interactive\\ndirection-giving.  An agent constructs a referring expression plan by\\nusing the referent's most salient features.  An agent understands a\\nreference once he is confident in the adequacy of its (inferred) plan as\\na means of identifying the referent.  To collaborate, the agents use\\njudgment, suggestion, and elaboration moves to refashion the referring\\nexpression until they mutually believe that the recipient has\\nunderstood.\\n\\n\\n  Acknowledgments \\n\\nOur work is supported by the University of Toronto and by the Natural\\nSciences and Engineering Research Council of Canada.  We are grateful to\\nPeter Heeman, Graeme Hirst, and Jeffrey Siskind for many helpful\\ndiscussions.\\n\\nBibliography \\n\\nDouglas E. Appelt\\n(1985a).\\nPlanning English referring expressions.\\nArtificial Intelligence, 26(1):1-33.\\n\\n\\nDouglas E. Appelt\\n(1985b).\\nSome pragmatic issues in the planning of definite and indefinite noun\\n  phrases.\\nIn Proceedings of the 23rd Annual Meeting of the Association for\\n  Computational Linguistics, pages 198-203.\\n\\n\\nDouglas E. Appelt and Amichai Kronfeld\\n(1987).\\nA computational model of referring.\\nIn Proceedings of the Tenth International Joint Conference on\\n  Artificial Intelligence (IJCAI-87), pages 640-647.\\n\\n\\nHerbert H. Clark and Deanna Wilkes-Gibbs\\n(1986).\\nReferring as a collaborative process.\\nCognition, 22:1-39.\\nReprinted in Cohen, P. R., Morgan, J., and Pollack, M. E.,\\neditors. (1990). Intentions in Communication. MIT Press. pages 463-493.\\n\\n\\nPhilip R. Cohen\\n(1981).\\nThe need for referent identification as a planned action.\\nIn Proceedings of the Seventh International Joint Conference on\\n  Artificial Intelligence (IJCAI-81), pages 31-36.\\n\\n\\nJames Raymond Davis\\n(1989).\\nBack Seat Driver: Voice Assisted Automobile Navigation.\\nPh.D. thesis, Massachusetts Institute of Technology.\\n\\n\\nAnn S. Devlin\\n(1976).\\nThe ``small town'' cognitive map: Adjusting to a new environment.\\nIn G.T. Moore and R.G. Golledge, editors, Environmental Knowing:\\n  Theories, Research and Methods. Dowden, Hutchinson and Ross.\\n\\n\\nPhilip G. Edmonds\\n(1993).\\nA computational model of collaboration on reference in\\n  direction-giving dialogues.\\nM.Sc. thesis, published as technical report CSRI-289,\\nDepartment of Computer Science, University of Toronto.\\n\\n\\nPeter A. Heeman and Graeme Hirst\\n(1992).\\nCollaborating on referring expressions.\\nTechnical Report TR 435, Computer Science Dept., Univ. of Rochester,\\n  Rochester, New York.\\n\\n\\nKevin Lynch\\n(1960).\\nThe Image of the City.\\nMIT Press.\\n\\n\\nMartha Pollack\\n(1990).\\nPlans as complex mental attitudes.\\nIn P. R. Cohen, J. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 77-103. MIT Press.\\n\\n\\nGeorge Psathas\\n(1991).\\nThe structure of direction-giving in interaction.\\nIn Deirdre Boden and Don H. Zimmerman, editors, Talk and Social\\n  Structure, pages 195-216. Polity Press.\\n\\n\\nEhud Reiter and Robert Dale\\n(1992).\\nA fast algorithm for the generation of referring expressions.\\nIn Proceedings of the 14th International Conference on\\n  Computational Linguistics (COLING-92), pages 232-238.\\n\\n\\nJohn. R. Searle\\n(1969).\\nSpeech Acts: An Essay in the Philosophy of Language.\\nCambridge University Press.\\n\\nFootnotes\\n\\n   These models assume that all\\nagents have identical beliefs, which is clearly insufficient for modeling\\ncollaborative dialogue.\\n   Given information about salience, we could\\nconstruct such a hierarchy, but we do not presume that it would be easy\\nto know what is salient.\\n   In Heeman and Hirst's\\nmodel, an attribute has to be mutually believed to be used.  Here,\\nmutual belief is not possible because the hearer has no knowledge of\\nthe referent, but mutual belief is an intended effect of using this\\nplan.\\n   Recall that she raised her confidence threshold\\nas a result of the hearer's postponement move, so now she must meet the\\nnew threshold.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nIn conversation, a person sometimes has to refer to an object\\nthat is not previously known to the other participant.  We present a\\nplan-based model of how agents collaborate on reference of this sort.\\nIn making a reference, an agent uses the most salient attributes of the\\nreferent.  In understanding a reference, an agent determines his\\nconfidence in its adequacy as a means of identifying the referent.  To\\ncollaborate, the agents use judgment, suggestion, and elaboration moves\\nto refashion an inadequate referring expression.\\n\\n'],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nOver the  last  few  years  there  has   been a  growing  interest  in\\ncomputational morphology and phonology.  A number of systems have been\\ndeveloped that deal with word-level processing. A widely used approach\\nis finite-state morphology,  most notably two-level morphology (for an\\nintroduction, see ).  Morphological components are\\nsuccessfully used for a   wide range of stand-alone  applications like\\nspelling correction and hyphenation.   One obvious application  is the\\nuse    in NLP systems  geared   to   the analysis/generation of  text.\\nSurprisingly, they have not  been widely applied in  this domain up to\\nnow.\\n\\n\\nA major reason for this is  the problem of interfacing morphology with\\nsyntax.   Reflecting the current trend   in syntax towards lexicalism,\\nunification-based systems use highly  structured feature structures as\\ninput.  Translating the output of morphological components into such a\\nrepresentation has proved to be difficult. Reducing interface problems\\nis therefore crucial to success.\\n\\n\\nA  tight integration between word  and  sentence level processing also\\nhas linguistic advantages.  The boundary between morphology and syntax\\nis fuzzy.   When processing written  text the units morphology  has to\\ndeal with are, in a  technical sense, not  words but character strings\\nseparated by delimiters. While these strings roughly correspond to the\\nwords of a  sentence there  are  problematic cases.   In German, e.g.,\\nzu-infinitive or verbs with separable  prefixes are written as\\na single unit in some instances and separately in others.\\n\\n\\nThe problem  has been recognized  and some possible remedies have been\\nproposed.  They  all  try to minimize or   to  eliminate the interface\\nbetween word   and   sentence level  processing.    One  step   is the\\ndescription of word formation in  terms of a unification-based grammar\\nto  make the result of morphological  processing directly available to\\n syntax and   vice versa, an   approach already  taken in  X2MORF ,, an extension of two-level morphology. \\n\\n\\nThe harder  problem is  the  integration  of morphophonology  which is\\ntraditionally  formalized in a  way  not easily translatable into  the\\nfeature formalism.  We will show  how this  can be achieved by merging\\nthe word-level grammar of  X2MORF  into an HPSG-style grammar, and\\nby adopting a relational view of its two-level rules.\\n\\n\\nIn this paper we  assume basic familiarity with  unification-based NLP\\ntechniques and two-level morphology.\\n\\n\\n    Integrating Morphology into HPSG\\n\\n\\nHead-driven    Phrase   Structure   Grammar  (HPSG,   PS1,\\nPS2) can be viewed as a mono-level but multi-stratal theory\\nof  grammar, where  different strata  relate to  different  aspects of\\nlinguistic information,  but   are  represented uniformly  in  feature\\nlogics. As  such  it is well   suited as a  linguistic theory  for our\\nenterprise.\\n\\n\\nHPSG differentiates between three  strata-- PHON,  SYNSEM\\nand   DTRS.  Though morphology is  not considered in the standard\\napproach, it suggests  itself to  be  included as a fourth  stratum by\\nintroducing a feature      MORPH into  the type  sign.\\nMorphotactics are  easily  described  in  terms  of a   feature  based\\ngrammar.  The    problem is how  to  deal  with  morphophonology.  Two\\nproposals have been made to overcome this problem.\\n\\n\\n encode  finite state automata directly in the\\nfeature formalism.   Since two-level rules  can be compiled  into such\\nautomata, morphophonology can be straightforwardly integrated into the\\ngrammar.  While  this is formally   elegant  it seems  to be  no  good\\nsolution  for practical  considerations.  First,  it  is  not entirely\\nclear from  their  paper how the   problem of null characters   can be\\nhandled.  Second, encoding large automata  will result in a very large\\nand unwieldy type   hierarchy.  In general, introducing automata  into\\nfeature structures and encoding morphophonology directly at that level\\nseems to be too low-level.\\n\\n\\n argue  against the use of  two-level morphology\\nbecause of  linguistic  considerations.  The linguistic  background of\\ntwo-level  rules--main stream segmental  phonology--has  widely been\\nrejected as a   valid   linguistic model.  Instead, they    base their\\nimplementation      on   autosegmental             phonology      (cf.\\n).\\n\\n\\nThis is certainly linguistically appealing.  But there are reasons for\\nsticking to a more conservative  approach.  Finite-state morphology as\\na formalism is not necessarily tied to segmental phonology.  There are\\nvarious approaches to cope  with non-concatenative phenomena--one  of\\n them    X2MORF .   Also,   for a  number of languages complete sets  of  two-level rules  do  exist  and can immediately  be\\nbrought  to bear.  Finally, finite-state  morphology has  proven to be\\nefficient while the method proposed by  seems to\\nbe computationally costly.\\n\\n\\nLike the  other approaches ours is  also  based on  HPSG. However, we\\nemploy  a different approach to integration.    Our grammar is encoded\\nusing a   unification engine based   on constraint  logic  programming\\n(CLP).  Besides  conventional attribute-value descriptions this system\\nallows  for the direct  representation of  more general relations,  as\\nthey are required  by HPSG.  This  extension of the formalism is used\\nfor the integration of morphology.  Thus  X2MORF  is treated as one\\nspecial relation of  the grammar.  As  a result, our approach is  more\\nmodular than the others.   While being fully integrated morphology can\\nstill be viewed as an autonomous component leading  to a more flexible\\ndesign.\\n\\n\\nWe  will now  give an overview    of  X2MORF before  describing the\\nintegrated system and its implementation in detail.\\n\\n\\n    Word Level Processing --  X2MORF\\n\\n\\n X2MORF differs from standard two-level morphology in two important\\nrespects.  Continuation  classes are replaced  by a feature-based word\\ngrammar.  This allows for a  more fine-grained description of  morphs.\\nIt is also    a  prerequisite   for    a tight integration  with     a\\nunification-based grammar.   X2MORF uses a morph lexicon where each\\nmorph has one  or more feature structures  assigned.  The word grammar\\nitself is simple.  Morphs have  a functor-argument structure along the\\nlines of  .   Affixes are  unary  functors\\nwhile stems are arguments  without any further structure, resulting in\\na binary tree structure.\\n\\n\\nThe  other extension  concerns      the two-level rules,  which    are\\nsupplemented   with a morphological filter   consisting  of a  feature\\nstructure.   This is  important because  in  morphophonology only some\\nrules are purely phonologically motivated.   Others are triggered by a\\nmixture of phonological and morphological facts.  Such rules cannot be\\nproperly represented in the standard approach.\\n\\n\\nTake, e.g.,  umlaut and schwa  epenthesis in German:  The third person\\nsingular present tense suffix for German verbs is -t, e.g.,   sag-t \\n\\n sagt.  For  stems ending in  a dental, schwa is\\ninserted  before the  ending, e.g.,  bad-t \\n\\n badet.\\nThis  rule does not hold across  the whole vocabulary though. Stems of\\nthe  strong paradigm do exhibit  umlaut  in 3rdPersSgPres which blocks\\nschwa epenthesis.    The  final dental  of the   stem must  be omitted\\ninstead, e.g., rat-t \\n\\n rt.\\n\\n\\n The three  rules below)--produce the  required behavior. In particular, these rules relate surface   rt to lexical $rAt+t$. \\n X2MORF  can be seen  as a relation between   a surface string (the\\nword  form), a  lexical    string,  and  a feature structure      (the\\ninterpretation   of   the word  form).    Relevant for  sentence level\\nprocessing  is the morphosyntactic information  and the stem, found as\\nthe values of paths   MORPHMHEAD and  MORPHSTEM\\n respectively (cf. Fig.   below).  This is supplemented by lexeme specific  information in the value of    SYNSEM (for  a\\ndetailed description see ).\\n\\n\\n    Implementing HPSG in a CLP Framework\\n\\n\\nHPSG  employs  strongly typed    feature  structures together   with\\nprinciples  constraining    them  further.     Well-typedness\\nrequirements  restrict the space   of  valid feature structures   (cf.\\n):  Every  feature structure must  be associated\\nwith a type, and every type restricts its associated feature structure\\nin  that only  certain features are   allowed and the values of  these\\nfeatures must  be  of   a certain  type.  Appropriateness    and value\\nrestrictions are inherited along the type hierarchy.\\n\\n\\nThe   second   source   of  constraints,  in    order   to  admit only\\nlinguistically    valid feature  structures,   are  the  principles of\\ngrammar.   PS1   allow  general  implicative   and negative\\nconstraints   in the form    of  conditional feature   structures.  In\\nPS2 principles are given  only in verbal form.  Recent work\\non formalizing the basis of HPSG models them as constraints attached\\nto types (e.g., ).  However, these distinctions affect\\nonly how   the   applicability of a   principle  is  specified.   More\\nimportant for  our present purpose  is the  form which the constraints\\nexpressed  by a principle    may take.  Besides  constraints enforcing\\nsimple structure  sharing (e.g., the Head  Feature Principle  given in\\n Fig.)  there are   also complex  relational  dependencies  (e.g.,  in   the  Subcategorization Principle).     Constraints like these go beyond the  expressivity of pure feature formalisms  alone and need to\\nbe defined in a recursive manner.\\n\\n\\nIn   order  to integrate  such  complex  constraints  in  the  feature\\nunification framework  we   interpret  unification of   typed  feature\\nstructures under  the    restrictions of  principled  constraints   as\\n constraint  solving  in   the CLP  paradigm  . \\n\\n\\nIn   CLP the notion  of unification  is   replaced by the more general\\nnotion of constraint solving. Constraint solvers  may be embedded into\\na logic programming language either by writing a meta-interpreter or by\\nmaking   use of  a  system  which   allows  for  the implementation of\\nunification extensions.\\n\\n\\n The second approach  is  taken by DMCAI  CLP  , a  Prolog    system whose unification  mechanism is extended in  such a way  that the user\\nmay introduce interpreted terms and specify  their meaning with regard\\nto  unification  through Prolog predicates.   The  basic  mechanism to\\nachieve this behavior is   the  use of attributed  variables,\\nwhich  may be    qualified   by arbitrary    user-defined  attributes.\\nAttributed variables  behave  like ordinary Prolog  variables with two\\nnotable exceptions: when an attributed  variable is to be unified with\\na   non-variable term or another  attributed  variable the unification\\nextensions come into play.   For either case  the user has to supply a\\npredicate which explicitly specifies  how the attributes interact  and\\nhow they should  be interpreted with respect  to the semantics of  the\\napplication  domain.   Unification succeeds  only  if these constraint\\nsolving  clauses managing  the  combination--or verification--of the\\ninvolved attributes are successful.\\n\\n\\nThe implementation of typed feature structures in our system makes use\\nof  the CLP  facilities provided   by  this  enhanced Prolog   system.\\nFeature  structures  are  implemented   by   the  attribute   fs(Type,Dag,Goals),   where    Dag  is  a   list   of\\nfeature-value  pairs (which is  empty  in case of   atomic types) or a\\nmarker indicating uninstantiatedness   of the  substructure   (feature\\nstructures are instantiated  lazily).  Goals  is a list of\\ndelayed  constraints (see   below).   Well-typed   unification of  two\\nfeature  structures is implemented  via the constraint solving clauses\\nmentioned  above, taking   into  account type   hierarchy and  feature\\nappropriateness      (for        a      detailed   description     cf.\\n).\\n\\n\\nConstraints  imposed onto  feature structures   by  the principles  of\\ngrammar are  stated  in  a  conditional form where  the  antecedent is\\n restricted to  contain only typing requirements. In order  to account for these conditional constraints  we  adopt  a  licensing  view:  Every node  of  a feature\\nstructure has to be licensed by all principles of grammar.\\n\\n\\nA node  is  licensed by a   principle if either  (i) the\\nfeature structure  F rooted  in  that  node  satisfies  the\\napplicability conditions   of  the   principle and  the    constraints\\nexpressed  by   the principle  successfully unify  with  F,  or   (ii) the feature structure   F rooted in   that node is     incompatible with the  applicability conditions of the principle.\\nThe interesting  case arises  when a  feature  structure does not\\n  satisfy the  applicability conditions of  the principle but is   compatible with them.   Thus applicability of  the principle can be\\ndecided only  later, after further  instantiation or unification steps\\nhave restricted the (sub)structure rooted at  that node.  In precisely\\nthis case the application (or the abandoning) of the constraint has to\\nbe delayed.  The  delay mechanism utilizes  the Goals slot\\n in the fs/3 attribute,  which is dedicated to hold the delayed constraints.   As an example take the  well known Head Feature\\n Principle  of  HPSG (Fig.). \\nThe conditional  operator ===]   is translated at read  time via\\nterm_expansion/2 and  implements the  delay mechanism by\\ncompiling precondition   checks into the  principle.  These antecedent\\nchecks trigger  either   the  application   of  the principle,     its\\nabandonment,  or its delay (by  annotating the variables which are not\\nsufficiently constrained to decide  on the antecedent with the delayed\\ngoals).\\n\\n\\nTwo advantages  of  this approach to  implement principled constraints\\nare  especially important for   our  present purpose:  First,  stating\\nredundant  typing requirements for  embedded  structures  (i.e.   type\\nrestrictions that would follow  automatically from well-typing) forces\\ndelay of the  conditional  constraint  until these substructures   are\\ninstantiated.   This  device  can, e.g.,   be used  to  block infinite\\nrecursion in recursively defined  constraints.  Second, the right hand\\npart of    the   conditional is not   restricted    to feature logical\\nexpressions, but instead can contain  arbitrary Prolog goals.  In this\\nway  constraints involving    relational  dependencies (such   as  the\\nSubcategorization Principle and  the morphological  relation between a\\nlexical  and a  surface string)  can  be expressed within the  feature\\nformalism and there  is no need for  external devices controlling this\\ninteraction.  Furthermore, the  conditional  constraint syntax is  not\\nrestricted  to  unary licensing principles  but can   also be  used to\\nexpress  relations, such   as fs_append/3--needed  for\\nimplementing   the Subcat    Principle--which  appends two    feature\\n structure lists (Fig. ). \\nNote  that  disjunctive relations such  as append can  now be\\nwritten    as  the  conjunction of    two   specialized cases applying\\nconditionally.   Furthermore,  infinite  loops   due to uninstantiated\\nvariables can  never occur,  a  crucial  requirement when  integrating\\nrelational dependencies into a lazy instantiating feature formalism.\\n\\n\\n    Embedding  X2MORF into the Feature System\\n\\n\\nOriginally  X2MORF  was   realized  as  a   separate  morphological\\ncomponent interfaced  to  the  sentence analyzer/generator  only   via\\nsequential   data transfer.   In  the case  of   analysis, the feature\\nstructure representing  the word form was   transmitted to the parser.\\nFor generation,  X2MORF  expected  a  feature structure as    input\\nreproducing   one or  more   word    forms.  This  purely   sequential\\narchitecture was not satisfactory because of the problems mentioned in\\nthe introduction.\\n\\n\\nIn order to  achieve tight integration,  we adopt a relational view of\\n X2MORF and encode the relation  between surface string and lexical\\nstring directly  without using  finite  state automata  (for arguments\\nsupporting  this  approach  cf.  ).  However, our\\napproach    extends  in     that  it (i)\\nexplicitly accounts  for the  insertion  of null  characters and    (ii) introduces   the  filter  concept  of    X2MORF  into the\\nrelational approach.\\n\\n\\nThe general format of a two-level rule specification in our system is\\n  LCon [=] Transition [=] RCon\\n[:- Filter]\\nin the case  of equivalence  rules,  optional rules are written  using\\nonly single arrows (=] and  [=).  These rules are compiled\\n into Prolog  clauses relating the  lexical and  surface character  streams appropriately (see  Fig.    for an example of  the t-elision rule for German).\\n\\n\\nTo obtain a correct  relationship  between surface and  lexical string\\nevery  transition  has  to  be  licensed   by a   morphological  rule.\\nTransitions  not mentioned  by  rules are  handled  by a default rule.\\nInstantiation of contexts may not   be done by the rules   themselves,\\nsince  this   would make it  impossible   to obtain negation   via the\\ncut-operator. Instead,   it is handled  separately in  a backtrackable\\nfashion.\\n\\n\\nThe central  relation is  the morphology  predicate, (see\\n Fig.     ) mediating  between   lexical string, surface string (with inserted null  elements), the pure (denullified)  surface\\nstring  and the  feature   structure   of  the morphological     sign.\\nInstantiation of   pairs  is done  depending  on  the possible lexical\\ncontinuations (the lexicon being represented by a trie-structure). The\\namount of lookahead is determined  by the current pair  which is to be\\n licensed  by morphrule.  Synchronization  of surface and lexical string by insertion of null characters  is also handled at the\\ninstantiation level.\\n\\n\\nThe  integration of the  two-level relation into the general framework\\nof  the  feature based sentence-level  and  word-level grammars is now\\nperformed by  adding this relation  as a principled constraint  at the\\nappropriate level.\\n\\n\\nIn a definite   clause style AVM  notation this  could  be  written as\\nfollows (given that morphology/3 is a wrapper  around the\\nmorphology relation given above,  starting with empty left context and\\nhiding the nullified surface stream):\\nConcatenation is delayed until the argument's  MSTRING is\\ninstantiated. Thus, infinite loops when concatenating are avoided.\\n\\n\\nAs an example we demonstrate how these constraints interact in forming\\nthe third person  singular present tense form of  the German verb   raten (to guess). The lexical string is composed of the stem   rAt and the  suffix +t. The lexical\\n entries of these two morphs are given in Fig. . \\n\\n\\nInteraction between  syntactic and morphological processes takes place\\nat the  word level.  The  application of  the two-level rules relating\\nthe surface string (i.e the  PHON-value of  the word) and\\nthe lexical-string (i.e.    MORPHMSTRING) is also triggered\\nhere.  This  interaction is  completely   neutral with  respect to the\\ndirection of  processing  due to   its relational  nature. Parsing  is\\nperformed by simply instantiating the  PHON value. Generation can\\nbe achieved when  MORPHMSTRING is present, which in turn is\\nobtained by concatenating the   lexical strings of the msigns\\ninstantiated by the morph grammar.\\n\\n\\nAs a result of this constraint interaction the structure shown in Fig.\\n   is  obtained.  Features  relevant at  the  syntactic level (such as  PERSON and  TENSE)  are percolated from \\n  MORPHMHEAD  to   SYNSEMLOCCATHEAD  via\\nstructure sharing constraints attached  to the type word (this\\n interaction is not  shown in Fig.  ).  Information  on subcategorization and  semantic content for the  word is obtained from\\nthe lexeme lexicon  using     MORPHSTEM  as a  key.   These\\nconstraints  complete   the    interaction    between  syntactic   and\\nmorphological processing at the word-level.\\n\\n\\n    Conclusion\\n\\n\\nWe have presented a framework for  the tight integration of word level\\nand sentence  level processing in a   unification-based paradigm.  The\\nsystem is   built upon a   unification engine   implemented  in a  CLP\\nlanguage supporting types and definite   relations as part of  feature\\ndescriptions. Using   this  extended   feature  formalism, which    is\\nindependently motivated   by requirements    of  standard HPSG,     a\\nreimplementation of  X2MORF was  integrated  into the grammar as  a\\nspecialized relation.\\n\\n\\nThis architecture has computational as  well as linguistic advantages.\\nIntegrating morphology and  morphophonology directly into  the grammar\\nis in the  spirit of HPSG, which  views grammar as a relation between\\nthe phonological    (or  graphemic) form  of   an  utterance   and its\\nsyntactic/semantic   representation.    This  way  the   treatment  of\\nphenomena transcending  the boundary between  morphology and syntax is\\nalso made possible.\\n\\n\\nOn the implementation side, the practical problems of interfacing two\\ninherently different modules are eliminated. For applications this\\nmeans that using a morphological component is made easy. Nevertheless,\\nthis tight integration still leaves morphology and syntax/semantics as\\nautonomous components, enabling direct use of existing data sets\\ndescribing morphophonology in terms of the two-level paradigm.\\n\\nBibliography \\n\\n  Abramson  H.:  A Logic  Programming View  of Relational Morphology, in\\n  Proceedings of the 15th COLING, August 23-28, 1992, Vol.III,\\n  pp.850-854, 1992.\\n\\n\\n  Bird S., Klein E.:  Enriching HPSG Phonology, University of Edinburgh,\\n  UK, Research Paper EUCCS/RP-56, 1993.\\n\\n\\n  Carpenter B., Pollard C., Franz A.: The Specification and Implementation of\\n  Constraint-Based Unification Grammars, Proceedings of 2[nd] IWPT,\\n  Cancun, Mexico, 143-153, 1991.\\n\\n\\n  Carpenter B.: The Logic of Typed Feature Structures, Cambridge\\n  University Press, Cambridge Tracts in Theoretical Computer Science 32, 1992.\\n\\n\\n  Goldsmith J.A.: Autosegmental and Metrical Phonology, Basil\\n  Blackwell, Oxford, 1990.\\n\\n\\n  Holzbaur C.: Metastructures vs. Attributed Variables in the Context of\\n  Extensible Unification, in Bruynooghe M. and Wirsing M.(eds.), Programming\\n  Language Implementation and Logic Programming, Springer, LNCS 631,\\npp.260-268,\\n  1992.\\n\\n\\n  Jaffar J., Lassez J.L.: Constraint Logic Programming, in Proceedings 14th ACM\\n  POPL Conf., Munich, 1987.\\n\\n\\n  Krieger H.-U., Pirker H., Nerbonne J.: Feature-based Allomorphy, Proceedings\\n  of the 31st Annual Meeting of the ACL,  Columbus, Ohio, pp.140-147, 1993.\\n\\n\\n  Matiasek J., Heinz W.: A CLP Based Approach to HPSG, sterreichisches\\n  Forschungsinstitut fr Artificial Intelligence, Wien, TR-93-26, 1993.\\n\\n\\n  Pollard C.J., Sag I.A.: Information-Based Syntax and Semantics,\\n  University of Chicago Press, Chicago, 1987.\\n\\n\\nPollard,  C.J,  Sag I.A.: Head-Driven Phrase Structure Grammar,\\nUniversity of Chicago Press and CSLI Publications, in press.\\n\\n\\n  di Sciullo A.-M., Williams E.: On the Definition of Word, MIT Press,\\n  Cambridge, MA, 1987.\\n\\n\\n  Sproat R.: Morphology and Computation, MIT Press, Cambridge, MA,\\nACL-MIT\\n  Series in NLP, 1992.\\n\\n\\n  Trost H.: The Application of Two-Level Morphology to Non-Concatenative German\\n  Morphology, in Karlgren H.(ed.), Proceedings of the 13th COLING,\\n  Helsinki, Finland, pp.371-376, 1990.\\n\\n\\n  Trost H.: X2MORF: A Morphological Component Based on Augmented Two-Level\\n  Morphology, in Proceedings of the 12th IJCAI, Morgan Kaufmann, San\\n  Mateo, CA, pp.1024-1030, 1991.\\n\\n\\n  Trost H.: Coping with Derivation in a Morphological Component, in 6th\\n  Conference of the European Chapter of the ACL, Utrecht, pp.368-376, 1993.\\n\\nFootnotes\\n\\nFinancial support for the Austrian Research Institute for\\n          Artificial Intelligence is provided by the Austrian             Ministry of Science and Research. We would like to\\n          thank Wolfgang Heinz for valuable comments and\\n          suggestions\\n  These rules as well  as other data presented\\n  in  the examples are simplified   for the purpose of  demonstration\\nThe lexical  character A may  have  the surface realizations\\n  a  and  .  The  rule has   an empty phonological\\n  context  but a  morphological filter.  This  is an  example for  the\\n  treatment of non-concatenative phenomena in  X2MORF.\\n  ``In   a headed\\n  phrase (i.e., a phrasal sign whose  DTRS  value is of sort     head-struc), the  SUBCAT value of the head daughter is the\\n  concatenation of  the phrase's   SUBCAT list with the  list (in\\n  order of  increasing  obliqueness)  of   SYNSEM values   of the\\n   complement   daughters.'' \\n  DMCAI CLP  is an\\n  enhanced version of SICStus  Prolog, available by anonymous ftp from\\n  ftp.ai.univie.ac.at\\n  This is only\\n  a syntactic   variant   of attaching  constraints  solely  to  types\\n    and does not  permit general conditional  structures as   used in  PS1.\\n  pred/n   is the usual notation for  a\\n  n-ary  Prolog predicate.\\n  The   operators     ::=,   ::,  :, ===    are defined  for typing    of a node,  path\\n  restriction,  path  concatenation and  value  restriction (type     or coreference) respectively.\\n  Note   that left contexts   are encoded\\n  reversed to account for  the left to right traversal  of the pair of\\n  character streams. Left contexts can be remembered and checked most\\n  efficiently this  way.\\n  This  interaction and the\\n  lexicon lookup of the feature structure corresponding to the current\\n  morph, which takes  place when encountering  a morph boundary is not\\n  shown for  the sake of simplicity.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe   present  an    integrated architecture   for\\n  word-level   and  sentence-level processing  in  a unification-based\\n  paradigm. The  core  of the system  is  a  CLP implementation  of  a\\n  unification  engine   for feature  structures  supporting relational\\n  values.   In this framework   an HPSG-style grammar is implemented.\\n  Word-level   processing uses    X2MORF, a morphological  component\\n  based  on    an extended version    of  two-level  morphology.  This\\n  component is tightly integrated with the grammar as a relation.  The\\n  advantage of  this approach is that morphology  and syntax  are kept\\n  logically autonomous while at the   same time minimizing   interface\\n  problems.\\n\\n'],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nIn recent years there has been a common agreement in the NLP research\\ncommunity on the importance of having an extensive coverage of\\nselectional restrictions (SRs) tuned to the domain to work with.  SRs\\ncan be seen as semantic type constraints that a word sense imposes\\non the words with which it combines in the process of semantic\\ninterpretation.  SRs may have different applications in NLP, specifically,\\nthey may help a parser with Word Sense Selection (WSS, as in\\n ), with preferring certain structures out of several grammatical ones\\n  and finally with deciding the semantic role  played by a syntactic complement . Lexicography is also interested in the acquisition of SRs (both defining in\\ncontext approach and lexical semantics work\\n ). \\n\\n\\nThe aim of our work is to explore the feasibility of using an\\nstatistical method for extracting SRs from on-line\\ncorpora. Resnik  developed a method for automatically\\nextracting class-based SRs from on-line corpora.  Ribas \\nperformed some experiments using this basic technique and drew up some\\n limitations from the corresponding results.\\n\\n\\nIn this paper we will describe some substantial modifications to the\\nbasic technique and will report the corresponding experimental\\nevaluation. The outline of the paper is as follows: in section\\n  we summarize the basic methodology used in  , analyzing its limitations; in section   we explore some alternative statistical measures for ranking the hypothesized SRs; in section\\n  we propose some evaluation measures on the SRs-learning problem, and use them to test the experimental results obtained\\n by the different techniques; finally, in section  we draw up the final conclusions and establish future lines of research.\\n\\n\\n    The basic technique for learning SRs\\n\\n  Description \\n\\nThe technique functionality can be summarized as:\\n\\n\\nInput\\nThe training set, i.e. a list of complement co-occurrence triples,\\n(verb-lemma,\\nsyntactic-relationship, noun-lemma) extracted from the corpus.\\n\\n\\nPrevious knowledge used\\n\\n\\nA semantic\\n hierarchy (WordNet) where words are clustered in semantic classes,\\nand semantic classes are organized hierarchically. Polysemous words are\\nrepresented as instances of different classes.\\n\\n\\nOutput\\n\\n\\nA set of syntactic SRs, (verb-lemma, syntactic-relationship,\\nsemantic-class, weight).  The final SRs must be mutually disjoint. SRs are\\nweighted according to the statistical evidence found in the corpus.\\n\\n\\nLearning process\\n3 stages:\\n\\n\\n1.\\nCreation of the space of candidate classes.\\n2.\\nEvaluation of the appropriateness of the candidates by\\nmeans of a statistical measure.\\n3.\\nSelection of the most appropriate subset in the\\ncandidate space to convey the SRs.\\n\\n\\nThe appropriateness of a class for expressing SRs (stage 2) is\\nquantified from the strength of co-occurrence of verbs and classes of\\n nouns in the corpus .  Given the verb v, the syntactic-relationship s and the candidate class c, the\\nAssociation Score, Assoc, between v and c in s is defined:\\n\\n\\nAssoc(v,s,c)  =   p(c|v,s) I(v;c|s) \\\\nonumber \\\\\\\\\\n =  p(c|v,s) \\\\log \\\\frac{p(c|v,s)}{p(c|s) \\\\nonumber}\\n\\\\end{eqnarray} -->\\n\\n\\nThe two terms of Assoc try to capture different properties:\\n\\n\\n1.\\nMutual information ratio, I(v;c|s), measures the strength of the\\nstatistical association between the given verb v and the candidate\\nclass c in the given syntactic position s.  It compares the prior\\ndistribution, p(c|s), with the posterior distribution, p(c|v,s).\\n\\n\\n2.\\np(c|v,s) scales up the strength of the association by the\\nfrequency of the relationship.\\n\\n\\nProbabilities are estimated by Maximum Likelihood Estimation, counting\\n the relative frequency of events in the corpus. However, it is not obvious how to calculate class frequencies when the training corpus is not semantically tagged\\nas is the case. Nevertheless, we take a simplistic approach and\\ncalculate them in the following manner:\\n\\n\\n\\n\\n\\nWhere w is a constant factor used to normalize the\\n probabilities \\n\\n\\n\\n\\n\\nWhen creating the space of candidate classes (learning process, stage\\n1), we use a thresholding technique to ignore as much as possible\\nthe noise introduced in the training set.  Specifically, we consider\\nonly those classes that have a higher number of occurrences than the\\nthreshold. The selection of the most appropriate classes (stage 3) is\\nbased on a global search through the candidates, in such a way that\\nthe final classes are mutually disjoint (not related by hyperonymy).\\n\\n\\n    Evaluation\\n\\n\\nRibas  reported experimental results obtained from\\nthe application of the above technique to learn SRs. He performed an\\nevaluation of the SRs obtained from a training set of 870,000 words of\\nthe Wall Street Journal.  In this section we summarize the results and\\nconclusions reached in that paper.\\n\\n\\n For instance, table  shows the SRs acquired for the subject position of the verb seek. Type indicates a\\nmanual diagnosis about the class appropriateness (Ok: correct;\\nAbs: over-generalization; Senses: due to erroneous\\nsenses). Assoc corresponds to the association score (higher\\nvalues appear first). Most of the induced classes are due to incorrect\\nsenses. Thus, although suit was used in the WSJ articles only in the\\nsense of \\n\\n,\\nthe algorithm not only considered\\nthe other senses as well (\\n\\n,\\n\\n,\\n\\n)\\n, but the Assoc score\\nranked\\nthem higher than the appropriate sense. We can also notice that the\\nAbs class, \\n\\n,\\nseems too general for the\\nexample nouns, while one of its daughters, \\n\\n\\nseems to\\nfit the data much better.\\n\\n\\nAnalyzing the results obtained from different experimental evaluation\\nmethods, Ribas  drew up some conclusions:\\n\\n\\na.\\nThe technique achieves a good coverage.\\n\\n\\nb.\\nMost of the classes acquired result from the\\naccumulation of incorrect senses.\\n\\n\\nc.\\nNo clear co-relation between Assoc and the manual\\ndiagnosis is found.\\n\\n\\nd.\\nA slight tendency to over-generalization exists\\ndue to incorrect senses.\\n\\n\\nAlthough the performance of the presented technique seems to be quite\\ngood, we think that some of the detected flaws could possibly be\\naddressed. Noise due to polysemy of the nouns involved seems to be the\\nmain obstacle for the practicality of the technique. It makes the\\nassociation score prefer incorrect classes and jump on\\nover-generalizations. In this paper we are interested in exploring\\nvarious ways to make the technique more robust to noise, namely, (a)\\nto experiment with variations of the association score, (b) to\\nexperiment with thresholding.\\n\\n\\n\\n    Variations on the association statistical measure\\n\\n\\nIn this section we consider different variations on the association\\nscore in order to make it more robust. The different\\ntechniques are experimentally evaluated in section\\n . \\n\\n    Variations on the prior probability\\n\\n\\nWhen considering the prior probability, the more independent of the\\ncontext it is the better to measure actual associations. A sensible\\nmodification of the measure would be to consider p(c) as the prior\\ndistribution:\\n\\n\\n\\n\\n\\nUsing the chain rule on mutual information\\n , p. 22] we can mathematically relate the different versions of Assoc,\\n\\n\\n\\n\\n\\nThe first advantage of Assoc' would come from this (information\\ntheoretical) relationship. Specifically, the Assoc' takes into\\naccount the preference (selection) of syntactic positions for\\nparticular classes. In intuitive terms, typical subjects\\n(e.g. [person, individual, ...]) would be preferred (to atypical\\nsubjects as [suit_of_clothes]) as SRs on the subject in contrast\\nto Assoc. The second advantage is that as long as the prior\\nprobabilities, p(c), involve simpler events than those used in\\nAssoc, p(c|s), the estimation is easier and more accurate\\n(ameliorating data sparseness).\\n\\n\\nA subsequent modification would be to\\nestimate the prior, p(c), from the counts of all the nouns appearing\\nin the corpus independently of their syntactic positions (not\\nrestricted to be heads of verbal complements).  In this\\nway, the estimation of p(c) would be easier and more accurate.\\n\\n\\n    Estimating class probabilities from noun frequencies\\n\\n\\nIn the global weighting technique presented in equation\\n  very polysemous nouns provide the same amount of evidence to every sense as non-ambiguous nouns do -while less\\nambiguous nouns could be more informative about the correct classes as\\nlong as they do not carry ambiguity.\\n\\n\\n The weight introduced in () could alternatively be found in a local manner, in such a way that more\\npolysemous nouns would give less evidence to each one of their senses\\nthan less ambiguous ones. Local weight could be obtained using\\np(c|n). Nevertheless, a good estimation of this probability seems\\nquite problematic because of the lack of tagged training material. In\\nabsence of a better estimator we use a rather poor one as the uniform\\ndistribution,\\n\\n\\n\\n\\n\\nResnik  also uses a local normalization\\ntechnique but he normalizes by the total number of classes in the\\nhierarchy. This scheme seems to present two problematic features (see\\n  for more details). First, it doesn't take dependency relationships introduced by hyperonymy into\\naccount. Second, nouns categorized in lower levels in the taxonomy\\nprovide less weight to each class than higher nouns.\\n\\n\\n    Other statistical measures to score SRs\\n\\n\\nIn this section we propose the application of other measures apart\\nfrom Assoc for learning SRs: log-likelihood ratio\\n , relative entropy  , mutual information ratio  ,  .  In section () their experimental evaluation is presented.\\n\\n\\nThe statistical measures used to detect associations on the\\ndistribution defined by two random variables X and Y work by measuring\\nthe deviation of the conditional distribution, P(X|Y), from the\\nexpected distribution if both variables were considered independent,\\ni.e. the marginal distribution, P(X).  If P(X) is a good\\napproximation of P(X|Y), association measures should be low (near\\nzero), otherwise deviating significantly from zero.\\n\\n\\n Table  shows the cross-table formed by the conditional and marginal distributions in the case of \\n\\n\\nand \\n\\n.\\nDifferent association\\nmeasures use the information provided in the cross-table to different\\nextents. Thus, Assoc and mutual information ratio consider only the\\ndeviation of the conditional probability p(c|v,s) from the\\ncorresponding marginal, p(c).\\n\\n\\nOn the other hand, log-likelihood ratio and \\nmeasure\\nthe association between \\nand c considering the deviation of\\n the four conditional cells in table  from the corresponding marginals. It is plausible that the deviation of the\\ncells not taken into account by Assoc can help on extracting useful\\nSRs.\\n\\n\\nFinally, it would be interesting to only use the information related\\nto the selectional behavior of ,\\ni.e. comparing the conditional\\nprobabilities of c and \\ngiven \\nwith the corresponding\\nmarginals. Relative entropy, \\n\\n,\\ncould do this job.\\n\\n\\n\\n    Evaluation\\n\\n    Evaluation methods of SRs\\n\\n\\nEvaluation on NLP has been crucial to fostering research in\\nparticular areas. Evaluation of the SR learning task would provide\\ngrounds to compare different techniques that try to abstract SRs from\\n corpus using WordNet (e.g, section ). It would also permit measuring the utility of the SRs obtained using WordNet\\nin comparison with other frameworks using other kinds of\\nknowledge. Finally it would be a powerful tool for detecting flaws of\\na particular technique (e.g,\\n  analysis). \\n\\n\\nHowever, a related and crucial issue is which linguistic tasks are\\nused as a reference.  SRs are useful for both lexicography and NLP. On\\nthe one hand, from the point of view of lexicography, the goal of\\nevaluation would be to measure the quality of the SRs induced, (i.e., how\\nwell the resulting classes correspond to the nouns as they were used\\nin the corpus). On the other hand, from the point of view of NLP, SRs\\nshould be evaluated on their utility (i.e., how much they help on\\nperforming the reference task).\\n\\n    Lexicography-oriented evaluation\\n\\n\\nAs far as lexicography (quality) is concerned, we think the main\\ncriteria SRs acquired from corpora should meet are: (a) correct\\ncategorization -inferred classes should correspond to the correct\\nsenses of the words that are being generalized-, (b) appropriate\\ngeneralization level and (c) good coverage -the majority of the noun\\noccurrences in the corpus should be successfully generalized by the\\ninduced SRs.\\n\\n\\nSome of the methods we could use for assessing experimentally the\\naccomplishment of these criteria would be:\\n\\n\\n\\nIntrospection A lexicographer  checks if the SRs\\naccomplish the criteria (a) and (b) above (e.g., the manual diagnosis\\n in table ). Besides the intrinsic difficulties of this approach, it does not seem appropriate when comparing across\\ndifferent techniques for learning SRs, because of its qualitative\\nflavor.\\n\\nQuantification of generalization level appropriateness\\nA possible measure would be the percentage of sense occurrences\\nincluded in the induced SRs which are effectively correct (from now\\non called Abstraction Ratio).  Hopefully, a technique with a\\nhigher abstraction ratio learns classes that fit the set of\\nexamples better. A manual assessment of the ratio confirmed this behavior, as\\ntesting sets with a lower ratio seemed to be inducing less\\nAbs cases.\\n\\nQuantification of coverage It could be measured as the\\nproportion of triples whose correct sense belongs to one of the SRs.\\n\\n\\n\\n\\n  NLP evaluation tasks \\n\\nThe NLP tasks where SRs utility could be evaluated are\\ndiverse. Some of them have already been introduced in section\\n . In the recent literature  (, , ...) several task oriented schemes to test Selectional Restrictions (mainly on syntactic\\nambiguity resolution) have been proposed. However, we have tested SRs\\non a WSS task,\\n  using the following scheme.  For every triple in the testing set the algorithm selects as most appropriate that\\nnoun-sense that has as hyperonym the SR class with highest association\\nscore. When more than one sense belongs to the highest SR, a random\\nselection is performed. When no SR has been acquired, the algorithm\\nremains undecided.  The results of this WSS procedure are checked\\nagainst a testing-sample manually analyzed, and precision and recall\\nratios are calculated. Precision is calculated as the ratio of\\nmanual-automatic matches / number of noun occurrences disambiguated by\\nthe procedure. Recall is computed as the ratio of manual-automatic\\nmatches / total number of noun occurrences.\\n\\n\\n\\n    Experimental results\\n\\n\\nIn order to evaluate the different variants on the association score\\nand the impact of thresholding we performed several experiments. In\\nthis section we analyze the results. As training set we used the\\n870,000 words of WSJ material provided in the ACL/DCI version of\\nthe Penn Treebank. The testing set consisted of 2,658 triples\\ncorresponding to four average common verbs in the Treebank: rise, report, seek and present. We only considered\\nthose triples that had been correctly extracted from the Treebank and\\nwhose noun had the correct sense included in WordNet (2,165 triples\\nout of the 2,658, from now on, called the testing-sample).\\n\\n\\nAs evaluation measures we used coverage, abstraction ratio, and recall\\n and precision ratios on the WSS task (section ). In addition we performed some evaluation by hand comparing the SRs\\nacquired by the different techniques.\\n\\n  Comparing different techniques \\n\\nCoverage for the different techniques is\\nshown in table\\n . The higher the coverage, the better the technique succeeds in\\ncorrectly generalizing more of the input examples. The labels used for\\nreferring to the different techniques are as follows: ``Assoc \\np(c|s)'' corresponds to the basic association measure (section\\n ), ``Assoc  Head-nouns'' and ``Assoc  All nouns'' to the techniques introduced in section\\n , ``Assoc  Normalizing'' to the local normalization\\n (section ), and finally, log-likelihood, D (relative entropy) and I (mutual information\\nratio) to the techniques discussed in section\\n . \\n\\n\\nThe abstraction ratio  for the different\\n techniques is shown in table .  In principle, the higher abstraction ratio, the better the technique succeeds in\\nfiltering out incorrect senses (less Abs).\\n\\n\\nThe precision and recall ratios on the noun WSS task for the different\\ntechniques are shown in table\\n .  In principle, the higher the precision and recall ratios the better the technique succeeds in\\ninducing appropriate SRs for the disambiguation task.\\n\\n\\nAs far as the evaluation measures try to account for different\\nphenomena the goodness of a particular technique should be quantified\\nas a trade-off. Most of the results are very similar (differences are\\nnot statistically significative). Therefore we should be cautious when\\nextrapolating the results. Some of the conclusions from the tables\\nabove are:\\n\\n\\n1.\\n\\nand I get sensibly worse results than other\\nmeasures (although abstraction is quite good).\\n\\n\\n2.\\nThe local normalizing technique using the\\nuniform distribution does not help. It seems that by using the local\\nweighting we misinform the algorithm. The problem is the reduced\\nweight that polysemous nouns get, while they seem to be the most\\n informative. However, a better informed kind of local weight (section\\n ) should improve the technique significantly. \\n\\n\\n3.\\nAll versions of Assoc (except the local normalization) get\\ngood results. Specially the two techniques that exploit a simpler\\nprior distribution, which seem to improve the basic technique.\\n\\n\\n4.\\nlog-likelihood and D seem to get slightly worse\\nresults than Assoc techniques, although the results are very similar.\\n\\n\\n  Thresholding \\n\\nWe were also interested in measuring the impact of thresholding on the\\n SRs acquired. In figure  we can see the different evaluation measures of the basic technique when varying the\\nthreshold. Precision and recall coincide when no candidate classes are\\nrefused (\\n\\nthreshold = 1). However, as it might be expected, as the\\nthreshold increases (i.e. some cases are not classified) the two\\nratios slightly diverge (precision increases and recall diminishes).\\n\\n\\n Figure  also shows the impact of thresholding on coverage and abstraction ratios. Both decrease when\\nthreshold increases, probably because when the rejecting threshold is\\nlow, small classes that fit the data well can be induced, learning\\nover-general or incomplete SRs otherwise.\\n\\n\\nFinally, it seems that precision and abstraction ratios are in inverse\\nco-relation (as precision grows, abstraction decreases). In terms of\\nWSS, general classes may be performing better than classes that fit\\nthe data better. Nevertheless, this relationship should be further\\nexplored in future work.\\n\\n\\n\\n\\n    Conclusions and future work\\n\\n\\nIn this paper we have presented some variations affecting the\\nassociation measure and thresholding on the basic technique for\\nlearning SRs from on-line corpora.  We proposed some evaluation\\nmeasures for the SRs learning task. Finally, experimental results on\\nthese variations were reported. We can conclude that some of these\\nvariations seem to improve the results obtained using the basic\\ntechnique. However, although the technique still seems far from\\npractical application to NLP tasks, it may be most useful for providing\\nexperimental insight to lexicographers. Future lines of research will\\nmainly concentrate on improving the local normalization technique by\\nsolving the noun sense ambiguity. We have foreseen the application of\\nthe following techniques:\\n\\n\\n\\nSimple techniques to decide the best sense c given the target\\n        noun n using estimates of the n-grams: P(c), P(c|n),\\n         P(c|v,s) and \\n\\nP(c|v,s,n), obtained from supervised and\\n         un-supervised corpora.\\n\\nCombining the different n-grams by means of smoothing\\n         techniques.\\n\\nCalculating \\n\\nP(c|v,s,n) combining P(n|c) and\\n         P(c|v,s), and applying the EM Algorithm\\n           to improve the model. \\n\\nUsing the WordNet hierarchy as a source of backing-off\\n        knowledge, in such a way that if n-grams composed by c        aren't enough to decide the best sense (are equal to zero),\\n        the tri-grams of ancestor classes could be used instead.\\n\\n\\n\\nBibliography \\n\\nR. Basili, M.T. Pazienza, and P. Velardi.\\n1992.\\nComputational lexicons: the neat examples and the odd exemplars.\\nIn Procs 3rd ANLP, Trento, Italy, April.\\n\\n\\nK.W. Church and P. Hanks.\\n1990.\\nWord association norms, mutual information and lexicography.\\nComputational Linguistics, 16(1).\\n\\n\\nT.M. Cover and J.A. Thomas, editors.\\n1991.\\nElements of Information Theory.\\nJohn Wiley.\\n\\n\\nA. P. Dempster, N. M. Laird, and D. B. Rubin.\\n1977.\\nMaximum likelihood from incomplete data via the em algorithm.\\nJournal of the Royal Statistical Society, 39(B):1-38.\\n\\n\\nT. Dunning.\\n1993.\\nAccurate methods for the statistics of surprise and coincidence.\\nComputational Linguistics, 19(1):61-74.\\n\\n\\nW. Gale and K. W. Church.\\n1991.\\nIdentifying word correspondences in parallel texts.\\nIn DARPA Speech and Natural Language Workshop, Pacific Grove,\\n  California, February.\\n.\\n\\n\\nR. Grishman and J. Sterling.\\n1992.\\nAcquisition of selectional patterns.\\nIn COLING, Nantes, France, march.\\n\\n\\nG. Hirst.\\n1987.\\nSemantic interpretation and the resolution of ambiguity.\\nCambridge University Press.\\n\\n\\nB. Levin.\\n1992.\\nTowards a lexical organization of English verbs.\\nUniversity of Chicago Press.\\n\\n\\nG. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller.\\n1991.\\nFive papers on wordnet.\\nInternational Journal of Lexicography.\\n\\n\\nP. S. Resnik.\\n1992.\\nWordnet and distributional analysis: A class-based approach to\\n  lexical discovery.\\nIn AAAI Symposium on Probabilistic Approaches to NL, San Jose,\\n  CA.\\n\\n\\nP. S. Resnik.\\n1993.\\nSelection and Information: A Class-Based Approach to lexical\\n  relationships.\\nPh.D. thesis, Computer and Information Science Department,\\n  University of Pennsylvania.\\n\\n\\nF. Ribas.\\n1994a.\\nAn experiment on learning appropriate selectional restrictions from a\\n  parsed corpus.\\nIn COLING, Kyoto, Japan, August.\\n\\n\\nF. Ribas.\\n1994b.\\nLearning more appropriate selectional restrictions.\\nTechnical report, ESPRIT BRA-7315 ACQUILEX-II WP.\\n\\n\\nG. Whittemore, K. Ferrara, and H. Brunner.\\n1990.\\nEmpirical study of predictive powers of simple attachment schemes for\\n  post-modifier prepositional phrases.\\nIn Procs. ACL, Pennsylvania.\\n\\n\\nG. K. Zipf.\\n1945.\\nThe meaning-frequency relationship of words.\\nThe Journal of General Psychology, 33:251-256.\\n\\n\\n(Acquilex-II Working Papers can be obtained by sending a request to\\ncide@cup.cam.uk)\\n\\nFootnotes\\n\\n  Revised version prepared for the CMP-LG\\nE-Print Archive of the original paper published in the Proceedings of the 7th\\nConference of the European Chapter of the Association for\\nComputational Llinguistics, Dublin, Ireland, March 1995. The research\\nreported here has been made in the framework of the Acquilex-II Esprit\\nProject (7315), and has been supported by a grant of Departament\\nd'Ensenyament, Generalitat de Catalunya, 91-DOGC-1491.\\n  WordNet is a broad-coverage lexical\\n database, see . \\n  Utility of\\nsmoothing techniques on class-based distributions is dubious\\n . \\n  Resnik  and Ribas\\n used equation\\n  without introducing normalization. Therefore, the estimated function\\ndidn't accomplish probability axioms. Nevertheless, their results\\nshould be equivalent (for our purposes) to those introducing\\nnormalization because it shouldn't affect the relative ordering of\\nAssoc among rival candidate classes for the same (v,s).\\n  In some way, it conforms to Zipf-law\\n : noun frequency and polysemy are correlated. \\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe present some variations affecting the association measure and\\nthresholding on a technique for learning Selectional Restrictions from\\non-line corpora. It uses a wide-coverage noun taxonomy and a\\nstatistical measure to generalize the appropriate semantic\\nclasses. Evaluation measures for the Selectional Restrictions learning\\ntask are discussed. Finally, an experimental evaluation of these\\nvariations is reported.\\nSubject Areas: corpus-based language modeling, computational\\nlexicography\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nUnification-based grammar formalisms can be viewed as generalizations of\\nContext-Free Grammars (CFG) where the nonterminal symbols are replaced\\nby an infinite domain of feature structures.  Much of their popularity\\nstems from the way in which syntactic generalization may be elegantly\\nstated by means of constraints amongst features and their values.\\nUnfortunately, the expressivity of these formalisms can have undesirable\\nconsequences for their processing.  In naive implementations of\\nunification grammar parsers, feature structures play the same role as\\nnonterminals in standard context-free grammar parsers. Potentially large\\nfeature structures are stored at intermediate steps in the computation,\\nso that the space requirements of the algorithm are\\nexpensive. Furthermore, the need to perform non-destructive unification\\nmeans that a large proportion of the processing time is spent copying\\nfeature structures.\\n\\n\\nOne approach to this problem is to refine parsing algorithms by\\ndeveloping techniques such as restrictions, structure-sharing, and lazy\\nunification that reduce the amount of structure that is stored and hence\\nthe need for copying of features\\n structures ,,,,,,,,,). While these techniques can yield significant improvements in performance, the\\ngenerality of unification-based grammar formalisms means that there are\\nstill cases where expensive processing is unavoidable. This approach\\ndoes not address the fundamental issue of the tradeoff between the\\ndescriptive capacity of a formalism and its computational power.\\n\\n\\nIn this paper we identify a set of constraints that can be placed on\\nunification-based grammar formalisms in order to guarantee the existence\\nof polynomial time parsing algorithms.  Our choice of constraints is\\nmotivated by showing how they generalize constraints inherent in Linear\\nIndexed Grammar (lig). We begin by describing how constraints inherent\\nin LIG admit tractable processing algorithms and then consider how\\nthese constraints can be generalized to a formalism that manipulates\\ntrees rather than stacks. The constraints that we identify for the\\ntree-based system can be regarded equally well as constraints on\\n unification-based grammar formalisms such as PATR . \\n\\n\\n  From Stacks to Trees \\n\\nAn Indexed Grammar (ig) can be viewed as a cfg in which each\\nnonterminal is associated with a stack of indices.  Productions specify\\nnot only how nonterminals can be rewritten but also how their associated\\nstacks are modified.  lig, which were first described by\\nGazdar , are constrained such that stacks are passed\\nfrom the mother to at most a single daughter.\\n\\n\\nFor , the size of the domain of nonterminals and associated stacks\\n(the analogue of the nonterminals in cfg) is not bound by the grammar.\\nHowever, Vijay-Shanker and Weir  demonstrate that\\npolynomial time performance can be achieved through the use of\\nstructure-sharing made possible by constraints in the way that LIG use\\nstacks.  Although stacks of unbounded size can arise during a\\nderivation, it is not possible for a  to specify that two\\ndependent, unbounded stacks must appear at distinct places in the\\nderivation tree. Structure-sharing can therefore be used effectively\\nbecause checking the applicability of rules at each step in the\\nderivation involves the comparison of structures of limited size.\\n\\n\\nOur goal is to generalize the constraints inherent in LIG, to a\\nformalism that manipulates feature structures rather than stacks.  As a\\nguiding heuristic we will avoid formalisms that generate tree sets with\\nan unbounded number of unbounded, dependent branches.\\nIt appears that the structure-sharing techniques used with LIG cannot\\nbe generalized in a straightforward way to such formalisms.\\n\\n\\nSuppose that we generalize LIG to allow the stack to be passed from\\nthe mother to two daughters. If this is done recursion can be used to\\nproduce an unbounded number of unbounded, dependent branches.  An\\nalternative is to allow an unbounded stack to be shared between two (or\\nmore) daughters but not with the mother.  Thus, rules may\\nmention more than one unbounded stack, but the stack associated with the\\nmother is still associated with at most one daughter. We refer to this\\nextension as Partially Linear Indexed Grammars (plig).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIn plig, stacks shared amongst siblings cannot be passed to the\\nmother. As a consequence, there is no possibility that recursion can be\\nused to increase the number of dependent branches. In fact, the number\\nof dependent branches is bounded by the length of the right-hand-side of\\nproductions.  By the same token, however, plig may only generate\\nstructural descriptions in which dependent branches begin at nodes that\\nare siblings of one another. Note that the tree shown in\\n Figure  is unobtainable because the branch rooted at \\nis dependent on more than one of the branches originating at\\nits sibling .\\n\\n\\nThis limitation can be overcome by moving to a formalism that\\nmanipulates trees rather than stacks. We consider an extension of cfg\\nin which each nonterminal A is associated with a tree .\\nProductions now specify how the tree associated with the mother is\\nrelated to the trees associated with the daughters. We denote trees with\\nfirst order terms. For example, the following production requires that\\nthe x and y subtrees of the mother's tree are shared with the Band C daughters, respectively. In addition, the daughters have in\\ncommon the subtree z.\\n\\n\\n\\n\\n\\nThere is a need to incorporate some kind of generalized notion of\\nlinearity into such a system.  Corresponding to the linearity\\nrestriction in lig we require that any part of the mother's tree is\\npassed to at most one daughter. Corresponding to the partial\\nlinearity of plig, we permit subtrees that are not shared with the\\nmother to be shared amongst the daughters.  Under these conditions, the\\n tree set shown in Figure  can be generated.  The nodes \\nand \\nshare the tree ,\\nwhich occurs twice at the\\nnode .\\nAt \\nthe two copies of \\nare distributed\\nacross the daughters.\\n\\n\\nThe formalism as currently described can be used to simulate arbitrary\\nTuring Machine computations. To see this, note that an instantaneous\\ndescription of a Turing Machine can be encoded with a tree as shown in\\n Figure .  Moves of the Turing Machine can be simulated by unary productions. The following production may be glossed: ``if in\\nstate q and scanning the symbol X, then change state to q', write\\n the symbol Y and move left'' . \\n\\n\\n\\n\\n\\nOne solution to this problem is to prevent a single daughter sharing\\nmore than one of its subtrees with the mother. However, we do not impose\\nthis restriction because it still leaves open the possibility of\\ngenerating trees in which every branch has the same length, thus\\nviolating the condition that trees have at most a bounded number of\\n unbounded, dependent branches.  Figure  shows how a set of such trees can be generated by illustrating the effect of the\\nfollowing production.\\n\\n\\n\\n\\n\\nTo see this, assume (by induction) that all four of the daughter\\nnonterminals are associated with the full binary tree of height i().  All four of these trees are constrained to be equal by the\\nproduction given above, which requires that they have identical left\\n(i.e. z) subtrees (these subtrees must be the full binary tree\\n\\n).  Passing the right subtrees (x, y, x' and y') to\\nthe mother as shown allows the construction of a full binary tree with\\nheight i+1 (\\n\\n). This can be repeated an unbounded\\nnumber of times so that all full binary trees are produced.\\n\\n\\nTo overcome both of these problems we impose the following additional\\nconstraint on the productions of a grammar. We require that subtrees of\\nthe mother that are passed to daughters that share subtrees with one\\nanother must appear as siblings in the mother's tree.  Note that this\\ncondition rules out the production responsible for building full binary\\ntrees since the x, y, x' and y' subtrees are not siblings in the\\nmother's tree despite the fact that all of the daughters share a common\\nsubtree z.  Moreover, since a daughter shares subtrees with itself, a\\nspecial case of the condition is that subtrees occurring within some\\ndaughter can only appear as siblings in the mother.  This condition also\\nrules out the Turing Machine simulation.  We refer to this formalism as\\nPartially Linear Tree Grammars (pltg). As a further illustration of the\\n constraints places on shared subtrees, Figure  shows a local tree that could appear in a derivation tree. This local tree is\\nlicensed by the following production which respects all of the\\nconstraints on pltg productions.\\n\\n\\n\\n\\n\\n Note that in Figure  the daughter nodes labelled B and D share a common subtree and the subtrees shared between the mother and the B and D\\ndaughters appear as siblings in the tree associated with the mother.\\n\\n\\n\\n\\n\\n\\n\\n\\n  Trees to Feature Structures \\n\\nFinally, we note that acyclic feature structures without re-entrancy can\\nbe viewed as trees with branches labelled by feature names and atomic\\nvalues only found at leaf nodes (interior nodes being unlabelled). Based\\non this observation, we can consider the constraints we have formulated\\nfor the tree system pltg as constraints on a unification-based\\ngrammar formalism such as PATR. We will call this system Partially\\nLinear PATR (plpatr).  Having made the move from trees to feature\\nstructures, we consider the possibility of re-entrancy in plpatr.\\n\\n\\nNote that the feature structure at the root of a plpatr derivation\\ntree will not involve re-entrancy.  However, for the following reasons\\nwe believe that this does not constitute as great a limitation as it\\nmight appear.  In unification-based grammar, the feature structure\\nassociated with the root of the tree is often regarded as the structure\\nthat has been derived from the input (i.e., the output of a parser). As\\na consequence there is a tendency to use the grammar rules to accumulate\\na single, large feature structure giving a complete encoding of the\\nanalysis.  To do this, unbounded feature information is passed up the\\ntree in a way that violates the constraints developed in this paper.\\nRather than giving such prominence to the root feature structure, we\\nsuggest that the entire derivation tree should be seen as the object\\nthat is derived from the input, i.e., this is what the parser returns.\\nBecause feature structures associated with all nodes in the tree are\\navailable, feature information need only be passed up the tree when it\\nis required in order to establish dependencies within the derivation\\ntree. When this approach is taken, there may be less need for\\nre-entrancy in the root feature structure. Furthermore, re-entrancy in\\nthe form of shared feature structures within and across nodes will be\\n found in plpatr (see for example Figure ). \\n\\n\\n  Generative Capacity \\n\\nLIG are more powerful than cfg and are known to be weakly equivalent\\nto Tree Adjoining Grammar, Combinatory Categorial Grammar, and Head\\n Grammar . plig are more powerful than LIG since they can generate the k-copy language for any fixed k (see\\n Example ).  Slightly more generally, plig can generate the language \\n\\n\\n\\n\\n\\nfor any \\nand regular\\nlanguage R.  We believe that the language involving copies of strings\\n of matching brackets described in Example  cannot be  generated by plig but, as shown in Example , it can be generated by pltg and therefore plpatr.  Slightly more generally,\\npltg can generate the language \\n\\n\\n\\n\\n\\nfor any \\nand context-free language L. It appears that the class of languages\\ngenerated by pltg is included in those languages generated by Linear\\n Context-Free Rewriting Systems  since the construction involved in a proof of this underlies the recognition algorithm\\ndiscussed in the next section.\\n\\n\\nAs is the case for the tree sets of ig, LIG and Tree\\nAdjoining Grammar, the tree sets generated by pltg have path sets that\\nare context-free languages. In other words, the set of all strings\\nlabelling root to frontier paths of derivation trees is a context-free\\nlanguage.  While the tree sets of LIG and Tree Adjoining Grammars have\\nindependent branches, pltg tree sets exhibit dependent branches,\\nwhere the number of dependent branches in any tree is bounded by the\\ngrammar. Note that the number of dependent branches in the tree sets of\\nig is not bounded by the grammar (e.g., they generate\\nsets of all full binary trees).\\n\\n\\n  Tractable Recognition \\n\\nIn this section we outline the main ideas underlying a polynomial time\\nrecognition algorithm for plpatr that generalizes the CKY algorithm\\n ,.  The key to this algorithm is the use of structure sharing techniques similar to those used to process LIG efficiently\\n . To understand how these techniques are applied in the case of plpatr, it is therefore helpful to consider first the somewhat\\nsimpler case of lig.\\n\\n\\nThe CKY algorithm is a bottom-up recognition algorithm for cfg. For a\\ngiven grammar G and input string \\n\\n\\nthe algorithm\\nconstructs an array P, having n[2] elements, where element P[i,j]stores all and only those nonterminals of G that derive the substring\\n\\n.\\nA naive adaptation of this algorithm for LIG\\nrecognition would involve storing a set of nonterminals and their\\nassociated stacks. But since stack length is at least proportional to\\nthe length of the input string, the resultant algorithm would exhibit\\nexponential space and time complexity in the worst case.  Vijay-Shanker\\nand Weir  showed that the behaviour of the naive\\nalgorithm can be improved upon.  In LIG derivations the application of\\na rule cannot depend on more than a bounded portion of the top of the\\nstack. Thus, rather than storing the whole of the potentially unbounded\\nstack in a particular array entry, it suffices to store just a bounded\\nportion together with a pointer to the residue.\\n\\n\\n Consider Figure . Tree (a) shows a LIG derivation of the substring \\n\\n\\nfrom the object\\n\\n.\\nIn this derivation tree, the node labelled\\n\\n\\nis a distinguished descendant of the\\n root and is the first point below  \\n\\nat which\\nthe top symbol ()\\nof the (unbounded) stack \\n\\n\\nis\\nexposed. This node is called the terminator of the node labelled\\n\\n.\\nIt is not difficult to show that only that portion\\nof the derivation below the terminator node is dependent on more\\nthan the top of the stack \\n\\n.\\nIt follows that for any stack\\n\\n,\\nif there is a derivation of the substring \\n\\n\\nfrom \\n\\n\\n(see tree (b)), then there is a\\ncorresponding derivation of \\n\\n\\nfrom\\n\\n\\n(see tree (c)). This captures the sense in\\nwhich LIG derivations exhibit ``context-freeness''.  Efficient storage of\\nstacks can therefore be achieved by storing in P[i,j] just that\\nbounded amount of information (nonterminal plus top of stack) relevant\\nto rule application, together with a pointer to any entry in P[p,q]representing a subderivation from an object \\n\\n.\\n\\n\\nBefore describing how we adapt this technique to the case of plpatr we\\ndiscuss the sense in which plpatr derivations exhibit a\\n``context-freeness'' property.  The constraints on plpatr which we\\nhave identified in this paper ensure that these feature values can be\\nmanipulated independently of one another and that they behave in a\\nstack-like way.  As a consequence, the storage technique used\\neffectively for LIG recognition may be generalized to the case of\\nplpatr.\\n\\n\\n Suppose that we have the derived tree shown in Figure  where the nodes at the root of the subtrees \\nand \\nare\\nthe so-called f-terminator and g-terminator of the tree's root,\\nrespectively. Roughly speaking, the f-terminator of a node is the node\\nfrom which it gets the value for the feature f. Because of the\\nconstraints on the form of plpatr productions, the derivations between\\nthe root of \\nand these terminators cannot in general depend on\\nmore than a bounded part of the feature structures \\nand\\n.\\nAt the root of the figure the feature structures\\n\\nand \\nhave been expanded to show the extent of the\\ndependency in this example. In this case, the value of the feature fin \\nmust be a, whereas, the feature g is not\\nfixed. Furthermore, the value of the feature g in \\nmust be\\nb, whereas, the feature f is not fixed.  This means, for example,\\nthat the applicability of the productions used on the path from the root\\nof \\nto the root of \\ndepends on the feature f in\\n\\nhaving the value a but does not depend on the value of the\\nfeature g in .\\nNote that in this tree the value of the\\nfeature g in \\nis\\n\\n\\n\\n\\n\\nand the value of the feature f in is \\n\\n\\n\\n\\n\\n Suppose that, in addition to the tree shown in Figure  the grammar generates the pair of trees shown in\\n Figure . Notice that while the feature structures at the root of \\nand \\nare not compatible with \\nand\\n,\\nthey do agree with respect to those parts that are fully\\nexpanded at 's root node. The ``context-freeness'' of plpatr\\n means that given the three trees shown in Figures   and  the tree shown in Figure  will also be generated by the grammar.\\n\\n\\nThis gives us a means of efficiently storing the potentially unbounded\\nfeature structures associated with nodes in a derivation tree (derived\\nfeature structures). By analogy with the situation for , derived\\nfeature structures can be viewed as consisting of a bounded part\\n(relevant to rule application) plus unbounded information about the\\nvalues of features.  For each feature, we store in the recognition array\\na bounded amount of information about its value locally, together with a\\npointer to a further array element. Entries in this element of the\\nrecognition array that are compatible (i.e. unifiable) with the bounded,\\nlocal information correspond to different possible values for the\\nfeature. For example, we can use a single entry in the recognition array\\nto store the fact that all of the feature structures that can appear at\\n the root of the trees in Figure  derive the substring \\n\\n.\\nThis entry would be underspecified, for example, the\\nvalue of feature \\nwould be specified to be any feature stored\\nin the array entry for the substring \\n\\n\\nwhose feature fhad the value a.\\n\\n\\nHowever, this is not the end of the story. In contrast to lig, plpatr\\nlicenses structure sharing on the right hand side of productions. That\\nis, partial linearity permits feature values to be shared between\\ndaughters where they are not also shared with the mother.  But in that\\ncase, it appears that checking the applicability of a production at some\\npoint in a derivation must entail the comparison of structures of\\nunbounded size.  In fact, this is not so.  The plpatr recognition\\nalgorithm employs a second array (called the compatibility array), which\\nencodes information about the compatibility of derived feature\\nstructures.  Tuples of compatible derived feature structures are stored\\nin the compatibility array using exactly the same approach used to store\\nfeature structures in the main recognition array.  The presence of a\\ntuple in the compatibility array (the indices of which encode which\\ninput substrings are spanned) indicates the existence of derivations of\\ncompatible feature structures.  Due to the ``context-freeness'' of\\nplpatr, new entries can be added to the compatibility array in a\\nbottom-up manner based on existing entries without the need to\\nreconstruct complete feature structures.\\n\\n\\n  Conclusions \\n\\nIn considering ways of extending , this paper has introduced the\\nnotion of partial linearity and shown how it can be manifested in the\\nform of a constrained unification-based grammar formalism.  We have\\nexplored examples of the kinds of tree sets and string languages that\\nthis system can generate. We have also briefly outlined the sense in\\nwhich partial linearity gives rise to ``context-freeness'' in derivations\\nand sketched how this can be exploited in order to obtain a tractable\\nrecognition algorithm.\\n\\n\\n  Acknowledgements \\n\\nWe thank Roger Evans, Gerald Gazdar, Aravind Joshi, Bernard Lang,\\nFernando Pereira, Mark Steedman and K. Vijay-Shanker for their help.\\n\\nBibliography \\n\\nMartin Emele.\\n1991.\\nUnification with lazy non-redundant copying.\\nIn 29[th] meeting Assoc. Comput. Ling., pages 323--330,\\n  Berkeley, CA.\\n\\n\\nG. Gazdar.\\n1988.\\nApplicability of indexed grammars to natural languages.\\nIn U. Reyle and C. Rohrer, editors, Natural Language Parsing and\\n  Linguistic Theories, pages 69--94. D. Reidel, Dordrecht, Holland.\\n\\n\\nDale Gerdemann.\\n1989.\\nUsing restrictions to optimize unification parsing.\\nIn International Workshop of Parsing Technologies, pages\\n  8--17, Pittsburgh, PA.\\n\\n\\nKurt Godden.\\n1990.\\nLazy unification.\\nIn 28[th] meeting Assoc. Comput. Ling., pages 180--187,\\n  Pittsburgh, PA.\\n\\n\\nS. P. Harrison and T. M. Ellison.\\n1992.\\nRestriction and termination in parsing with feature-theoretic\\n  grammars.\\nComputational Linguistics, 18(4):519--531.\\n\\n\\nL. Karttunen and M. Kay.\\n1985.\\nStructure sharing with binary trees.\\nIn 23[th] meeting Assoc. Comput. Ling., pages 133--136.\\n\\n\\nT. Kasami.\\n1965.\\nAn efficient recognition and syntax algorithm for context-free\\n  languages.\\nTechnical Report AF-CRL-65-758, Air Force Cambridge Research\\n  Laboratory, Bedford, MA.\\n\\n\\nKiyoshi Kogure.\\n1990.\\nStrategic lazy incremental copy graph unification.\\nIn 13[th] International Conference on Comput. Ling., pages\\n  223--228, Helsinki.\\n\\n\\nF. C. N. Pereira.\\n1985.\\nA structure-sharing representation for unification-based grammar\\n  formalisms.\\nIn 23[th] meeting Assoc. Comput. Ling., pages 137--144.\\n\\n\\nS. M. Shieber.\\n1984.\\nThe design of a computer language for linguistic information.\\nIn 10[th] International Conference on Comput. Ling., pages\\n  363-366.\\n\\n\\nS. M. Shieber.\\n1985.\\nUsing restriction to extend parsing algorithms for\\n  complex-feature-based formalisms.\\nIn 23[rd] meeting Assoc. Comput. Ling., pages 82-93.\\n\\n\\nHideto Tomabechi.\\n1991.\\nQuasi-destructive graph unification.\\nIn 29[th] meeting Assoc. Comput. Ling., pages 315--322,\\n  Berkeley, CA.\\n\\n\\nK. Vijay-Shanker and D. J. Weir.\\n1993.\\nParsing some constrained grammar formalisms.\\nComputational Linguistics, 19(4):591--636.\\n\\n\\nK. Vijay-Shanker and D. J. Weir.\\n1994.\\nThe equivalence of four extensions of context-free grammars.\\nMath. Syst. Theory, 27:511-546.\\n\\n\\nK. Vijay-Shanker, D. J. Weir, and A. K. Joshi.\\n1987.\\nCharacterizing structural descriptions produced by various\\n  grammatical formalisms.\\nIn 25[th] meeting Assoc. Comput. Ling., pages 104-111.\\n\\n\\nDavid Wroblewski.\\n1987.\\nNondestructive graph unification.\\nIn 6[th] National Conference on Artificial Intelligence,\\n  pages 582--587, Seattle, WA.\\n\\n\\nD. H. Younger.\\n1967.\\nRecognition and parsing of context-free languages in time n[3].\\nInformation and Control, 10(2):189-208.\\n\\nFootnotes\\n\\n  There will be a set of such\\nproductions for each tape symbol W.\\n  The stack \\n\\n\\nassociated with B is\\n``inherited'' from the stack associated with A at the root of the\\ntree.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nVijay-Shanker and Weir  show that Linear Indexed\\nGrammars (lig) can be processed in polynomial time by exploiting\\nconstraints which make possible the extensive use of\\nstructure-sharing. This paper describes a formalism that is more\\npowerful than lig, but which can also be processed in polynomial time\\nusing similar techniques. The formalism, which we refer to as Partially\\nLinear PATR (plpatr) manipulates feature structures rather than\\nstacks.\\n\\n'],\n",
              " ['\\n\\n  Introduction \\n\\nSince online text becomes available in ever increasing volumes and an\\never increasing number of languages,\\nthere is a growing need for robust processing\\ntechniques that can analyze text without expensive and time-consuming\\nadaptation to new domains and genres. This need motivates research on\\nfully automatic text processing that may rely on general principles of\\nlinguistics and computation, but does not depend on knowledge about\\nindividual words.\\n\\n\\nIn this paper, we describe an experiment on fully automatic derivation\\nof the knowledge necessary for part-of-speech tagging. Part-of-speech\\ntagging is of interest for a number of applications, for example\\n access to text data bases , robust parsing ,  and general parsing ,.  The goal is to find an unsupervised method for tagging that relies on general distributional\\nproperties of text, properties that are invariant across languages and\\nsublanguages. While the proposed algorithm is not successful for all\\ngrammatical categories, it does show that fully automatic tagging is\\npossible when demands on accuracy are modest.\\n\\n\\nThe following sections discuss related work, describe the learning\\nprocedure and evaluate it on the Brown Corpus\\n . \\n\\n\\n  Related Work \\n\\nThe simplest part-of-speech taggers are bigram or trigram models\\n ,. They require a relatively large tagged training text. Transformation-based tagging as\\nintroduced by  also requires a hand-tagged text for\\ntraining.  No pretagged text is necessary for Hidden Markov Models\\n ,,. Still, a lexicon is needed that specifies the possible parts of speech for every word. \\nhave shown that the effort necessary to construct the part-of-speech\\nlexicon can be considerably reduced by combining learning procedures\\nand a partial part-of-speech categorization elicited from an\\ninformant.\\n\\n\\nThe present paper is concerned with tagging languages and sublanguages for\\nwhich\\nno a priori knowledge about grammatical categories\\nis available, a situation that occurs often in\\n practice . \\n\\n\\nSeveral researchers have worked on learning grammatical\\nproperties of words.  trains a connectionist net to\\npredict words, a process that generates internal representations that\\nreflect grammatical category.\\n try to infer grammatical category from bigram statistics.\\n and  use\\nvector models in which words are clustered according to the similarity\\nof their close neighbors in a corpus.  present a\\nprobabilistic model for entropy maximization that also relies on the\\nimmediate neighbors of words in a corpus.  applies\\nfactor analysis to collocations of two target words (``certain\\'\\' and\\n``right\\'\\') with their immediate neighbors.\\n\\n\\nWhat these approaches have in common is that they classify words instead of individual occurrences.\\nGiven the widespread\\npart-of-speech ambiguity of words this is\\n problematic. How should a\\nword like ``plant\\'\\' be categorized if it has uses both as a verb and\\nas a noun? How can a categorization be considered meaningful if the\\ninfinitive marker ``to\\'\\' is not distinguished from the homophonous\\npreposition?\\n\\n\\n In a previous paper , we trained\\na neural network to disambiguate\\npart-of-speech using context;\\nhowever,\\nno information about the word that is to be categorized was used. This\\nscheme fails for cases like ``The soldiers rarely come\\nhome.\\'\\' vs. ``The soldiers will come home.\\'\\'  where the context is\\nidentical and information about the lexical item in question\\n(``rarely\\'\\' vs. ``will\\'\\') is needed in combination with context for\\ncorrect classification. In this paper, we will compare\\ntwo tagging algorithms, one based on classifying word types,\\nand one based on classifying words-plus-context.\\n\\n\\n  Tag induction \\n\\nWe start by constructing representations of the syntactic behavior of\\na word with respect to its left and right context.\\nOur working hypothesis is\\nthat syntactic behavior is reflected in co-occurrence patterns.\\nTherefore, we will measure the similarity between two words with\\nrespect to their syntactic behavior to, say, their left side by the\\ndegree to which they share the same neighbors on the left. If the\\ncounts of neighbors are assembled into a vector (with one dimension\\nfor each neighbor), the cosine can be\\nemployed to measure similarity.  It will assign a value close to 1.0\\nif two words share many neighbors, and 0.0 if they share none. We\\nrefer to the vector of left neighbors of a word as its left\\ncontext vector, and to the vector of right neighbors as its right context vector.\\nThe unreduced context vectors in the experiment\\ndescribed here have 250 entries, corresponding to the 250 most\\nfrequent words in the Brown corpus.\\n\\n\\nThis basic idea of measuring distributional similarity in terms of\\nshared neighbors\\nmust be modified because of the\\nsparseness of the data. Consider\\ntwo infrequent adjectives that happen to modify different nouns in the\\ncorpus. Their right similarity according to the cosine measure would be\\nzero. This is clearly undesirable. But even with high-frequency words,\\nthe simple vector model can yield misleading similarity measurements. A\\ncase in point is ``a\\'\\' vs. ``an\\'\\'. These two articles do not share\\nany right neighbors since the former is only used before consonants\\nand the latter only before vowels. Yet intuitively, they are similar with\\nrespect to their right syntactic context despite the lack of common\\nright neighbors.\\n\\n\\nOur solution to these problems is the application of a singular value\\ndecomposition. We can represent the left vectors of all words in the\\ncorpus as a matrix C with n rows, one for each word whose left\\nneighbors are to be represented, and kcolumns, one for each of the possible neighbors.\\nSVD can be used to approximate the row and column vectors of C in a\\nlow-dimensional space. In more detail,\\nSVD decomposes  a matrix C, the matrix of left vectors\\nin our case, into\\nthree matrices T0, S0, and D0 such that:\\n\\n\\nC= T0 S0 D0\\'\\n\\n\\nS0 is a diagonal k-by-k matrix that contains the singular values of Cin descending order. The ith singular value can be interpreted\\nas indicating the strength of the ith principal component of\\nC.  T0 and D0 are orthonormal matrices\\nthat approximate the rows and columns of C, respectively.  By\\nrestricting the matrices T0, S0, and\\nD0 to\\ntheir first m[k columns (= principal components) one obtains the\\nmatrices T, S, and D. Their product \\nis the best least\\nsquare\\napproximation of C by a matrix of rank m: \\n\\n.\\nWe chose m=50 (reduction to a 50-dimensional space) for the SVD\\'s\\ndescribed in this paper.\\n\\n\\nSVD addresses the problems of generalization and sparseness because\\nbroad and stable generalizations are represented on dimensions\\nwith large values which will be retained in the dimensionality reduction.\\nIn contrast, dimensions corresponding to small singular values\\nrepresent idiosyncrasies,  like the phonological constraint on the usage\\nof ``an\\'\\' vs. ``a\\'\\', and will be dropped.\\nWe also gain efficiency since we can manipulate smaller vectors,\\nreduced to 50 dimensions.\\nWe used SVDPACK to compute the singular\\n value decompositions described in this paper . \\n\\n\\n Table  shows the nearest neighbors of two words (ordered according to closeness to the head word) after the dimensionality\\nreduction. Neighbors with highest similarity according to both left\\nand right context are listed. One can see clear differences between\\nthe nearest neighbors in the two spaces. The right-context neighbors\\nof ``onto\\'\\' contain verbs because both prepositions and verbs govern\\nnoun phrases to their right. The left-context neighborhood of ``onto\\'\\'\\nreflects the fact that prepositional phrases are used in the same\\nposition as adverbs like ``away\\'\\' and ``together\\'\\', thus making their\\nleft context similar. For ``seemed\\'\\', left-context neighbors are words\\nthat have similar types of noun phrases in subject position (mainly\\nauxiliaries).  The right-context neighbors all take ``to\\'\\'-infinitives\\nas complements. An adjective like ``likely\\'\\' is very similar to\\n``seemed\\'\\' in this respect although its left context is quite\\ndifferent from that of ``seemed\\'\\'.  Similarly, the generalization that\\nprepositions and transitive verbs are very similar if not identical in\\nthe way they govern noun phrases would be lost if ``left\\'\\' and\\n``right\\'\\' properties of words were lumped together in one\\nrepresentation.  These examples demonstrate the importance of\\nrepresenting generalizations about left and right context separately.\\n\\n\\nThe left and right context vectors are the basis for four different\\ntag induction experiments, which are described in detail below:\\n\\ninduction based on word type only\\n\\ninduction based on word type and context\\n\\ninduction based on word type and context, restricted to ``natural\\'\\'\\ncontexts\\n\\ninduction based on word type and context, using generalized left\\nand right context vectors\\n\\n\\n\\n  Induction based on word type only \\n\\nThe two context vectors of a word characterize the distribution of\\nneighboring words to its left and right.  The concatenation of left\\nand right context vector can therefore serve as a representation of a\\n word\\'s distributional behavior ,. We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus.\\nHere,\\nwe use the raw 250-dimensional context vectors and apply the SVD to\\nthe 47,025-by-500 matrix (47,025 words with two 250-dimensional\\ncontext vectors each). We obtained 47,025 50-dimensional reduced vectors from\\nthe SVD and clustered them into 200 classes using the fast clustering\\n algorithm Buckshot  (group average agglomeration applied to a sample). This classification constitutes the baseline performance\\nfor distributional part-of-speech tagging. All occurrences of a word\\nare assigned to one class. As pointed out above, such a procedure is\\nproblematic for ambiguous words.\\n\\n\\n  Induction based on word type and context \\n\\nIn order to exploit contextual information in the classification of a\\ntoken, we simply use context vectors of the two words occurring next\\nto the token.\\nAn occurrence of word w is represented by a concatenation of four\\ncontext vectors:\\n\\nThe right context vector of the preceding word.\\n\\nThe left context vector of w.\\n\\nThe right context vector of w.\\n\\nThe left context vector of the following word.\\n\\n\\nThe motivation is that a word\\'s syntactic role depends both on\\nthe syntactic properties of its neighbors and on\\nits own potential for entering into syntactic relationships with these\\nneighbors. The only properties of context that we consider\\nare the  right-context vector of the preceding\\nword and the left-context vector of the following word because they\\nseem to represent the contextual information most important for the\\ncategorization of w. For example, for the disambiguation of ``work\\'\\'\\nin ``her work seemed to be important\\'\\', only the fact that ``seemed\\'\\'\\nexpects noun phrases to its left is important, the right context\\nvector of ``seemed\\'\\' does not contribute to disambiguation.\\nThat only the immediate neighbors are crucial for categorization is\\nclearly a simplification, but as the results presented below show it\\nseems to work surprisingly well.\\n\\n\\nAgain, an SVD is applied to address the problems of sparseness and\\ngeneralization. We randomly selected 20,000 word triplets from the corpus\\nand formed concatenations of four context vectors as described above.\\nThe singular value decomposition of the resulting 20,000-by-1,000\\nmatrix defines a mapping from the 1,000-dimensional space of\\nconcatenated context vectors to a 50-dimensional reduced space.  Our\\ntag set was then induced by clustering the reduced vectors of the\\n20,000 selected occurrences into 200 classes. Each\\nof the 200 tags is defined by the centroid of the corresponding class\\n(the sum of its members).  Distributional tagging of an occurrence of\\na word w proceeds then by retrieving the four relevant context\\nvectors (right context vector of previous word, left context vector of\\nfollowing\\nword, both context vectors of w) concatenating them to one\\n1000-component vector, mapping this vector to 50 dimensions,\\ncomputing the correlations with the 200 cluster centroids and,\\nfinally, assigning the occurrence to the closest cluster. This\\nprocedure was applied to all tokens of the Brown corpus.\\n\\n\\nWe will see below that this method of distributional tagging, although\\npartially successful, fails for many tokens whose neighbors are\\npunctuation marks. The context vectors of punctuation marks contribute\\nlittle information about syntactic categorization since there are no\\ngrammatical dependencies between words and punctuation marks, in\\ncontrast to strong dependencies between neighboring words.\\n\\n\\nFor this reason, a second induction on the basis of word type and\\ncontext was performed, but only for those tokens with informative\\ncontexts. Tokens next to punctuation marks and tokens with rare words\\nas neighbors were not included. Contexts with rare words (less than\\nten occurrences) were also excluded for similar reasons: If a word\\nonly occurs nine or fewer times its left and right context vectors\\ncapture little information for syntactic categorization. In the\\nexperiment,\\n20,000\\nnatural contexts were randomly selected, processed by the SVD and\\nclustered into 200 classes. The classification was then applied to all\\nnatural contexts of the Brown corpus.\\n\\n\\n  Generalized context vectors \\n\\nThe context vectors used so far only capture information about\\ndistributional interactions with the 250 most frequent words.\\nIntuitively, it should be possible to gain accuracy in tag induction\\nby using information from more words. One way to do this is to let the\\nright context vector record which classes of left context\\nvectors occur to the right of a word.  The rationale is that words\\nwith similar left context characterize words to their right in a\\nsimilar way. For example, ``seemed\\'\\' and ``would\\'\\' have similar left\\ncontexts, and they characterize the right contexts of ``he\\'\\' and ``the\\nfirefighter\\'\\' as potentially containing an inflected verb form. Rather\\nthan having separate entries in its right context vector for\\n``seemed\\'\\', ``would\\'\\', and ``likes\\'\\', a word like ``he\\'\\' can now be\\ncharacterized by a generalized entry for ``inflected verb form occurs\\nfrequently to my right\\'\\'.\\n\\n\\nThis proposal was implemented by applying a singular value\\ndecomposition to the 47025-by-250 matrix of left context vectors and\\nclustering the resulting context vectors into 250 classes. A\\ngeneralized right context vector v for word w was then formed by\\ncounting how often words from these 250 classes occurred to the right of w.\\nEntry vi counts the number of times that a word from class ioccurs to the right of w in the corpus (as opposed to the number of\\ntimes that the word with frequency rank i occurs to the right of\\nw).  Generalized left context vectors were derived by an analogous\\nprocedure using word-based right context vectors.  Note that the\\ninformation about left and right is kept separate in this computation.\\n This differs from previous approaches , in which left and right context vectors of a word are always used in one\\nconcatenated vector. There are arguably fewer different types of right\\nsyntactic contexts than types of syntactic categories. For example,\\ntransitive verbs and prepositions belong to different syntactic\\ncategories, but their right contexts are virtually identical in that\\nthey require a noun phrase. This generalization could not be exploited\\nif left and right context were not treated separately.\\n\\n\\nAnother argument for the two-step derivation is that many\\nwords don\\'t have any of the 250 most frequent words as their left or\\nright neighbor. Hence, their vector would be zero in the word-based\\nscheme. The class-based scheme makes it more likely that\\nmeaningful\\nrepresentations are formed for all words in the vocabulary.\\n\\n\\nThe generalized context vectors were input to the\\ntag induction procedure described above for word-based context\\nvectors: 20,000 word triplets were selected from the corpus, encoded as\\n1,000-dimensional vectors (consisting of four generalized context\\nvectors), decomposed by a singular value decomposition and clustered\\ninto 200 classes. The resulting classification was applied to all\\ntokens in the Brown corpus.\\n\\n\\n\\n  Results \\n\\nThe results of the four experiments were evaluated by forming 16\\n classes of tags from the Penn Treebank as shown in Table . Preliminary experiments showed that distributional\\nmethods distinguish adnominal and predicative uses of\\nadjectives (e.g. ``the black cat\\'\\' vs. ``the cat is black\\'\\').\\nTherefore the tag ``ADN\\'\\' was introduced for\\nuses of adjectives, nouns, and participles as adnominal modifiers.\\nThe tag ``PRD\\'\\' stands for predicative uses of adjectives.\\nThe Penn Treebank parses of the Brown corpus were used to determine\\nwhether a token functions as an adnominal modifier.\\nPunctuation marks, special symbols, interjections, foreign words and\\ntags with fewer than 100 instances were excluded from the\\nevaluation.\\n\\n\\n Tables  present results for word type-based induction and induction based on word type and context.\\nFor each tag t, the table lists the frequency\\n of t in the corpus (``frequency\\'\\'), the number of induced tags\\n\\n,\\nthat were assigned to it (``# classes\\'\\'); the number\\nof times an occurrence of t was correctly labeled as belonging to\\none of \\n\\n\\n(``correct\\'\\'); the number of times\\nthat a token of a different tag \\n\\n\\nwas miscategorized as being an\\ninstance of \\n\\n\\n(``incorrect\\'\\'); and precision\\nand recall of the categorization of t. Precision is the number of\\ncorrect tokens divided by the sum of correct and incorrect tokens.\\nRecall is the number of correct tokens divided by the total number\\nof tokens of t(in the first column). The last column gives van Rijsbergen\\'s\\nF measure which computes an aggregate score from precision and\\n recall: \\n\\n.\\nWe chose \\n\\n\\nto give equal weight to precision and recall.\\n\\n\\nIt is clear from the tables that incorporating context improves\\nperformance considerably. The F score increases for all tags except\\nCD, with an average improvement of more than 0.20. The tag CD is\\nprobably\\nbetter thought of as describing a word class. There is a wide range of\\nheterogeneous\\nsyntactic functions of cardinals in particular contexts:\\nquantificational and adnominal uses, bare NP\\'s (``is one of\\'\\'), dates\\nand ages (``Jan 1\\'\\', ``gave his age as 25\\'\\'), and enumerations.\\nIn this light, it is not surprising that the word-type method does\\nbetter on cardinals.\\n\\n\\n Table  shows that performance for generalized context vectors is better than for word-based context vectors (0.74 vs. 0.72). However, since the number of tags with better and worse\\nperformance is about the same (7 and 5), one cannot conclude with\\ncertainty that generalized context vectors induce tags of higher\\nquality. Apparently, the 250 most frequent words capture most of the\\nrelevant distributional information so that the additional information\\nfrom less frequent words available from generalized vectors only has a\\nsmall effect.\\n\\n\\n Table  looks at results for ``natural\\'\\' contexts, i.e. those not containing punctuation marks and rare words. Performance is consistently better than for the evaluation on all contexts,\\nindicating that the low quality of the distributional information\\nabout punctuation marks and rare words is a difficulty for successful\\ntag induction.\\n\\n\\nEven for ``natural\\'\\' contexts,\\nperformance varies considerably. It is fairly good for prepositions,\\ndeterminers, pronouns, conjunctions, the infinitive marker, modals,\\nand the possessive marker. Tag induction fails for cardinals (for the reasons\\nmentioned above) and for ``-ing\\'\\' forms. Present participles and gerunds\\nare difficult because they exhibit both verbal and nominal properties\\nand occur in a wide variety of different contexts whereas other parts\\nof speech have a few typical and frequent contexts.\\n\\n\\nIt may seem worrying that some of the tags are assigned a high number\\nof clusters (e.g., 49 for N, 36 for ADN).\\nA closer look\\nreveals that many clusters embody finer distinctions. Some examples:\\nNouns in\\n cluster 0 are heads of larger noun phrases, whereas the nouns\\nin cluster 1 are full-fledged NPs.  The members of classes 29\\nand 111 function as subjects. Class 49 consists of proper nouns.\\nHowever, there are many pairs or triples of clusters that\\nshould be collapsed into one on linguistic grounds. They were\\nseparated on distributional criteria that don\\'t have linguistic\\ncorrelates.\\n\\n\\nAn analysis of the divergence between our classification and the\\nmanually assigned tags revealed three main sources of errors:\\nrare words and rare syntactic phenomena,\\n indistinguishable distribution,\\nand non-local dependencies.\\n\\n\\nRare words are difficult because of lack of distributional evidence.\\nFor example, ``ties\\'\\' is used as a verb only 2 times (out of 15\\noccurrences in the corpus). Both occurrences are miscategorized, since\\nits context vectors do not provide enough evidence for the verbal use.\\nRare\\nsyntactic constructions pose a related problem: There are not enough\\ninstances to justify the creation of a separate cluster. For example,\\nverbs taking bare infinitives were classified as adverbs since this\\nis too rare a phenomenon to provide strong distributional evidence\\n(``we do not DARE speak of\\'\\', ``legislation could HELP remove\\'\\').\\n\\n\\nThe case of the tags ``VBN\\'\\' and ``PRD\\'\\' (past participles and\\npredicative adjectives)\\ndemonstrates the difficulties of word classes\\nwith indistinguishable distributions. There are hardly any\\ndistributional clues for distinguishing ``VBN\\'\\' and ``PRD\\'\\' since both\\n are mainly used as complements of ``to be\\'\\'. A common tag class was created for\\n``VBN\\'\\' and ``PRD\\'\\' to show that they are reasonably well\\ndistinguished from other parts of speech, even if not from each other.\\nSemantic understanding is necessary to distinguish\\nbetween the states described by phrases of the form ``to be\\nadjective\\'\\'  and the processes described by\\nphrases of the form ``to be past participle\\'\\'.\\n\\n\\nFinally, the method fails if there are no local dependencies that\\ncould be used for categorization and only non-local dependencies are\\ninformative. For example,\\nthe adverb in ``Mc*N. Hester,\\nCURRENTLY Dean of ...\\'\\' and the conjunction in\\n``to\\nadd that, IF United States policies ...\\'\\' have similar immediate\\nneighbors (comma, NP).\\nThe decision to consider only immediate neighbors is\\nresponsible for this type of error since taking a wider context into account\\nwould disambiguate the parts of speech in question.\\n\\n\\n  Future Work \\n\\nThere are three avenues of future research we are interested in\\npursuing. First, we are planning to apply the algorithm to an as yet\\nuntagged language. Languages with a rich morphology may be\\nmore difficult than English since with fewer tokens per type, there\\nis less data on which to base a categorization decision.\\n\\n\\nSecondly, the error analysis suggests that considering non-local\\ndependencies would improve results. Categories that can be induced\\nwell (those characterized by local dependencies) could be input into\\n procedures that learn phrase structure (e.g. ,). These phrase constraints could then be incorporated into the\\ndistributional tagger to characterize non-local dependencies.\\n\\n\\nFinally, our procedure induces a ``hard\\'\\' part-of-speech\\nclassification of occurrences in context, i.e., each occurrence is\\nassigned to only one category. It is by no means generally accepted\\nthat such a classification is linguistically adequate. There is both\\n synchronic  and diachronic  evidence suggesting that words and their uses can inherit properties from\\nseveral prototypical syntactic categories. For example, ``fun\\'\\' in\\n``It\\'s a fun thing to do.\\'\\' has properties of both a noun and an\\nadjective (superlative ``funnest\\'\\' possible). We are planning to\\nexplore ``soft\\'\\' classification algorithms that can account for these\\nphenomena.\\n\\n\\n  Conclusion \\n\\nIn this paper, we have attempted to construct an algorithm for fully\\nautomatic distributional tagging, using unannotated corpora as the\\nsole source of information. The main innovation is that the algorithm\\nis able to deal with part-of-speech ambiguity, a pervasive phenomenon\\nin natural language that was unaccounted for in previous work on learning\\ncategories from corpora.  The method was systematically evaluated on\\nthe Brown corpus.  Even if no automatic procedure can rival the\\naccuracy of human tagging, we hope that the algorithm will facilitate\\nthe initial tagging of texts in new languages and sublanguages.\\n\\n\\n  Acknowledgments \\n\\nI am grateful for helpful comments\\nto Steve Finch, Jan Pedersen and\\ntwo anonymous reviewers (from ACL and EACL).\\nI\\'m also indebted to Michael Berry for SVDPACK and to the Penn\\nTreebank Project\\nfor the parsed Brown corpus.\\n\\nBibliography \\n\\nSteven Abney.\\n1991.\\nParsing by chunks.\\nIn Berwick, Abney, and Tenny, editors, Principle-Based Parsing.\\n  Kluwer Academic Publishers.\\n\\n\\nMichael W. Berry.\\n1992.\\nLarge-scale sparse singular value computations.\\nThe International Journal of Supercomputer Applications,\\n  6(1):13-49.\\n\\n\\nDouglas Biber.\\n1993.\\nCo-occurrence patterns among collocations: A tool for corpus-based\\n  lexical knowledge acquisition.\\nComputational Linguistics, 19(3):531-538.\\n\\n\\nEric Brill and Mitch Marcus.\\n1992a.\\nTagging an unfamiliar text with minimal human supervision.\\nIn Robert Goldman, editor, Working Notes of the AAAI Fall\\n  Symposium on Probabilistic Approaches to Natural Language. AAAI Press.\\n\\n\\nEric Brill and Mitchell Marcus.\\n1992b.\\nAutomatically acquiring phrase structure using distributional\\n  analysis.\\nIn Proceedings of the DARPA workshop \"Speech and Natural\\n  Language\", pages 155-159.\\n\\n\\nEric Brill, David Magerman, Mitch Marcus, and Beatrice Santorini.\\n1990.\\nDeducing linguistic structure from the statistics of large corpora.\\nIn Proceedings of the DARPA Speech and Natural Language\\n  Workshop, pages 275-282.\\n\\n\\nEric Brill.\\n1993.\\nAutomatic grammar induction and parsing free text: A\\n  transformation-based approach.\\nIn Proceedings of ACL 31, Columbus OH.\\n\\n\\nEugene Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz.\\n1993.\\nEquations for part-of-speech tagging.\\nIn Proceedings of the Eleventh National Conference on Artificial\\n  Intelligence, pages 784-789.\\n\\n\\nEugene Charniak, Glenn Carroll, John Adcock, Anthony Cassandra, Yoshihiko\\n  Gotoh, Jeremy Katz, Michael Littman, and John McCann.\\n1994.\\nTaggers for parsers.\\nTechnical Report CS-94-06, Brown University.\\n\\n\\nKenneth W. Church.\\n1989.\\nA stochastic parts program and noun phrase parser for unrestricted\\n  text.\\nIn Proceedings of ICASSP-89, Glasgow, Scotland.\\n\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun.\\n1991.\\nA practical part-of-speech tagger.\\nIn The 3rd Conference on Applied Natural Language Processing,\\n  Trento, Italy.\\n\\n\\nDouglas R. Cutting, Jan O. Pedersen, David Karger, and John W. Tukey.\\n1992.\\nScatter/gather: A cluster-based approach to browsing large document\\n  collections.\\nIn Proceedings of SIGIR \\'92, pages 318-329.\\n\\n\\nC. G. deMarcken.\\n1990.\\nParsing the LOB corpus.\\nIn Proceedings of the 28th Annual Meeting of the Association for\\n  Computational Linguistics, pages 243-259.\\n\\n\\nJeffrey L. Elman.\\n1990.\\nFinding structure in time.\\nCognitive Science, 14:179-211.\\n\\n\\nSteven Finch and Nick Chater.\\n1992.\\nBootstrapping syntactic categories using statistical methods.\\nIn Walter Daelemans and David Powers, editors, Background and\\n  Experiments in Machine Learning of Natural Language, pages 229-235, Tilburg\\n  University. Institute for Language Technology and AI.\\n\\n\\nSteven Paul Finch.\\n1993.\\nFinding Structure in Language.\\nPh.D. thesis, University of Edinburgh.\\n\\n\\nW.N. Francis and F. Kucera.\\n1982.\\nFrequency Analysis of English Usage.\\nHoughton Mifflin, Boston.\\n\\n\\nF. Jelinek.\\n1985.\\nRobust part-of-speech tagging using a hidden markov model.\\nTechnical report, IBM, T.J. Watson Research Center.\\n\\n\\nReinhard Kneser and Hermann Ney.\\n1993.\\nForming word classes by statistical clustering for statistical\\n  language modelling.\\nIn Reinhard Khler and Burghard B. Rieger, editors,   Contributions to Quantitative Linguistics, pages 221-226. Kluwer Academic\\n  Publishers, Dordrecht, The Netherlands.\\n\\n\\nJulian Kupiec.\\n1992.\\nRobust part-of-speech tagging using a hidden markov model.\\nComputer Speech and Language, 6:225-242.\\n\\n\\nJulian Kupiec.\\n1993.\\nMurax: A robust linguistic approach for question answering using an\\n  on-line encyclopedia.\\nIn Proceedings of SIGIR \\'93, pages 181-190.\\n\\n\\nJohn R. Ross.\\n1972.\\nThe category squish: Endstation Hauptwort.\\nIn Papers from the Eighth Regional Meeting. Chicago Linguistic\\n  Society.\\n\\n\\nHinrich Schtze.\\n1993.\\nPart-of-speech induction from scratch.\\nIn Proceedings of ACL 31, pages 251-258, Columbus OH.\\n\\n\\nWhitney Tabor.\\n1994.\\nSyntactic Innovation: A Connectionist Model.\\nPh.D. thesis, Stanford University.\\n\\n\\nC. J. van Rijsbergen.\\n1979.\\nInformation Retrieval.\\nButterworths, London.\\nSecond Edition.\\n\\nFootnotes\\n\\n  Although\\nbib93 classifies collocations, these can also be ambiguous.\\nFor example, ``for certain\\'\\' has both senses\\nof ``certain\\'\\': ``particular\\'\\' and ``sure\\'\\'.\\n  The small difference in\\noverall frequency in the tables is due to the fact that some\\nword-based context vectors consist entirely of zeros. There were about\\na hundred word triplets whose four context vectors did not have\\nnon-zero entries and could not be assigned a cluster.\\n  Because of\\nphrases like ``I had sweet potatoes\\'\\',\\nforms of ``have\\'\\' cannot serve as a reliable discriminator either.\\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nThis paper presents an algorithm for tagging words whose\\npart-of-speech properties are unknown. Unlike previous work,\\nthe algorithm categorizes word tokens in context\\ninstead of word types.  The algorithm is\\nevaluated on the Brown Corpus.\\n\\n'],\n",
              " [\"\\n\\n  Pragmatics and Defeasibility \\n\\nIt is widely acknowledged that a full account of natural language\\nutterances cannot be given in terms of only syntactic or semantic\\nphenomena. For example, Hirschberg  has shown\\nthat in order to understand a scalar implicature, one must analyze the\\nconversants' beliefs and intentions. To recognize normal state\\nimplicatures one must consider mutual beliefs and\\n plans . To understand conversational implicatures associated with indirect replies one must consider discourse\\nexpectations, discourse plans, and discourse\\n relations ,. Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or\\nsyntactic constructs (cleft and pseudo-cleft sentences) are used.\\nDespite all the complexities that individualize the recognition stage\\nfor each of these inferences, all of them can be defeated by\\ncontext, by knowledge, beliefs, or plans of the agents that constitute\\npart of the context, or by other pragmatic rules.\\n\\n\\nDefeasibility is a notion that is tricky to deal with, and\\nscholars in logics and pragmatics have learned to circumvent it or\\nlive with it. The first observers of the phenomenon preferred to keep\\ndefeasibility outside the mathematical world. For\\nFrege , Russell , and\\nQuine  ``everything exists''; therefore, in their\\nlogical systems, it is impossible to formalize the cancellation of the\\npresupposition that definite referents\\n exist ,. We can taxonomize previous approaches to defeasible pragmatic inferences into three categories\\n(we omit here work on defeasibility related to linguistic phenomena\\nsuch as discourse, anaphora, or speech acts).\\n\\n\\n1.  Most linguistic approaches account for the defeasibility of\\n  pragmatic inferences by analyzing them in a context that consists of\\n  all or some of the previous utterances, including the current one.\\n   Context ,, procedural    rules ,, lexical and syntactic    structure , intentions , or    anaphoric constraints , decide what   presuppositions or implicatures are projected as pragmatic\\n  inferences for the utterance that is analyzed. The problem with\\n  these approaches is that they assign a dual life to pragmatic\\n  inferences: in the initial stage, as members of a simple or complex\\n  utterance, they are defeasible.  However, after that utterance is\\n  analyzed, there is no possibility left of cancelling that inference.\\n  But it is natural to have implicatures and presuppositions that are\\n  inferred and cancelled as a sequence of utterances proceeds:\\n   research in conversation repairs  abounds in such   examples. We address this issue in more detail in\\n   section . \\n\\n\\n2. One way of accounting for cancellations that occur later in the\\n  analyzed text is simply to extend the boundaries within which\\n  pragmatic inferences are evaluated, i.e., to look ahead a few\\n  utterances. Green  assumes that implicatures are\\n  connected to discourse entities and not to utterances, but her\\n  approach still does not allow cancellations across discourse units.\\n\\n\\n3.  Another way of allowing pragmatic inferences to be cancelled is\\n  to assign them the status of defeasible information.\\n  Mercer mercerphd formalizes presuppositions in a logical\\n   framework that handles defaults , but this approach   is not tractable and it treats natural disjunction as an\\n  exclusive-or and implication as logical equivalence.\\n\\n\\nComputational approaches fail to account for the cancellation of\\n pragmatic inferences: once presuppositions  or  implicatures , are generated, they can never be cancelled. We are not aware of any formalism or computational\\napproach that offers a unified explanation for the cancellability of\\npragmatic inferences in general, and of no approach that handles\\ncancellations that occur in sequences of utterances.\\n\\n\\nIt is our aim to provide such an approach here.  In doing this, we\\nassume the existence, for each type of pragmatic inference, of a set of\\nnecessary conditions that must be true in order for that inference to\\nbe triggered. Once such a set of conditions is met, the corresponding\\ninference is drawn, but it is assigned a defeasible status. It is the\\nrole of context and knowledge of the conversants to ``decide'' whether\\nthat inference will survive or not as a pragmatic inference of the\\nstructure.  We put no boundaries upon the time when such a\\ncancellation can occur, and we offer a unified explanation for\\npragmatic inferences that are inferable when simple utterances,\\ncomplex utterances, or sequences of utterances are considered.\\n\\n\\nWe propose a new formalism, called ``stratified logic'', that correctly\\nhandles the pragmatic inferences, and we start by giving a very brief\\nintroduction to the main ideas that underlie it.  We give the main\\nsteps of the algorithm that is defined on the backbone of stratified\\nlogic. We then show how different classes of pragmatic inferences can\\nbe captured using this formalism, and how our algorithm computes the\\nexpected results for a representative class of pragmatic inferences.\\nThe results we report here are obtained using an implementation\\n written in Common Lisp that uses Screamer , a macro package that provides nondeterministic constructs.\\n\\n\\n  Stratified logic \\n  Theoretical foundations \\n\\nWe can offer here only a brief overview of stratified logic. The\\nreader is referred to Marcu  for a\\ncomprehensive study. Stratified logic supports one type of\\nindefeasible information and two types of defeasible information,\\nnamely, infelicitously defeasible and felicitously defeasible.  The\\nnotion of infelicitously defeasible information is meant to capture\\ninferences that are anomalous to cancel, as in:\\n(1) \\n* John regrets that Mary came to the party but she did not\\n come. The notion of felicitously defeasible information is meant to capture the inferences that can be cancelled without any abnormality, as in:\\n(2) \\nJohn does not regret that Mary came to the party because she\\n did not come.  \\n\\n\\n The lattice in figure  underlies the semantics of stratified logic.  The lattice depicts the three levels of strength\\nthat seem to account for the inferences that pertain to natural\\nlanguage semantics and pragmatics: indefeasible information belongs to\\nthe u layer, infelicitously defeasible information belongs to the\\ni layer, and felicitously defeasible information belongs to the dlayer. Each layer is partitioned according to its polarity in truth,\\n\\n,\\nand falsity, \\n\\n.\\nThe lattice\\nshows a partial order that is defined over the different levels of\\ntruth. For example, something that is indefeasibly false, ,\\nis\\nstronger (in a sense to be defined below) than something that is\\ninfelicitously defeasibly true, ,\\nor felicitously defeasibly\\nfalse, .\\nFormally, we say that the u level is stronger than\\nthe i level, which is stronger than the d level: \\n\\n.\\n\\n\\n\\n\\nAt the syntactic level, we allow atomic formulas to be labelled\\naccording to the same underlying lattice. Compound formulas are\\nobtained in the usual way. This will give us formulas\\nsuch as \\n\\n,\\nor \\n\\n.\\nThe satisfaction relation is split\\naccording to the three levels of truth into u-satisfaction,\\ni-satisfaction, and d-satisfaction:\\n\\n\\n    Assume \\nis an  \\nvaluation such that \\n\\n\\nand assume that \\nmaps n-ary predicates\\np to relations \\n\\n.\\nFor any atomic formula \\n\\n,\\nand any stratified\\nvaluation ,\\nwhere \\n\\n\\nand\\nti are terms, the x-satisfiability relations are\\ndefined as follows:\\n\\n\\n\\n\\n\\n\\niff \\n\\n\\n\\n\\n\\n\\niff   \\n\\n\\n\\n\\n\\n\\niff   \\n\\n\\n\\n\\n\\n\\niff \\n\\n\\n\\n\\n\\n\\niff \\n\\n\\n\\n\\n\\n\\niff   \\n\\n\\n\\n\\n\\n\\niff \\n\\n\\n\\n\\n\\n\\niff \\n\\n\\n\\n\\n\\n\\niff \\n\\n\\n\\n\\n\\n\\n Definition  extends in a natural way to negated and compound formulas. Having a satisfaction definition associated with\\neach level of strength provides a high degree of flexibility. The same\\ntheory can be interpreted from a perspective that allows more\\nfreedom (u-satisfaction), or from a perspective that is tighter\\nand that signals when some defeasible information has been cancelled (i-\\nand d-satisfaction).\\n\\n\\nPossible interpretations of a given set of utterances with respect to\\na knowledge base are computed using an extension of the semantic\\ntableau method. This extension has been proved to be both sound and\\n complete . A partial ordering, , determines\\nthe set of optimistic interpretations for a theory.  An\\ninterpretation m0 is preferred to, or is more optimistic than, an\\ninterpretation m1 (\\n\\n)\\nif it contains more information\\nand that information can be more easily updated in the future.  That means\\nthat if an interpretation m0 makes an utterance true by assigning\\nto a relation R a defeasible status, while another interpretation\\nm1 makes the same utterance true by assigning the same relation Ra stronger status, m0 will be the preferred or optimistic\\none, because it is as informative as m1 and it allows more options\\nin the future (R can be defeated).\\n\\n\\nPragmatic inferences are triggered by utterances. To differentiate\\nbetween them and semantic inferences, we introduce a new quantifier,\\n\\n,\\nwhose semantics is defined such that a pragmatic inference\\nof the form \\n\\n\\nis instantiated only for those objects from the universe of discourse that pertain to an utterance having the\\nform \\n\\n.\\nHence, only if the antecedent of a\\npragmatic rule has been uttered can that rule be applied. A\\nmeta-logical construct uttered applies to the logical translation of\\nutterances.  This theory yields the following definition:\\n\\n\\n    Let \\nbe a theory described in terms of stratified first-order\\nlogic that appropriately formalizes the semantics of lexical items and\\nthe necessary conditions that trigger pragmatic inferences. The\\nsemantics of lexical terms is formalized using the quantifier\\n,\\nwhile the necessary conditions that pertain to pragmatic\\ninferences are captured using \\n\\n.\\nLet \\n\\nuttered(u) be the\\nlogical translation of a given utterance or set of utterances. We say\\nthat utterance u \\n\\n\\np if and only\\nif p[d] or p[i] is derived using pragmatic inferences in at least\\none optimistic model of the theory \\n\\n,\\nand if pis not cancelled by any stronger information (\\n\\n)\\nin any optimistic model schema of the\\ntheory. Symmetrically, one can define what a negative pragmatic\\ninference is.  In both cases, \\n\\n\\nis u-consistent.\\n\\n\\n  The algorithm \\n\\nOur algorithm, described in detail by\\nMarcu , takes as input a set of\\nfirst-order stratified formulas \\nthat represents an adequate\\nknowledge base that expresses semantic knowledge and the necessary\\nconditions for triggering pragmatic inferences, and the translation of\\nan utterance or set of utterances \\n\\nuttered(u). The algorithm builds\\nthe set of all possible interpretations for a given utterance, using a\\ngeneralization of the semantic tableau technique.  The model-ordering\\nrelation filters the optimistic interpretations. Among them, the\\ndefeasible inferences that have been triggered on pragmatic grounds\\nare checked to see whether or not they are cancelled in any optimistic\\ninterpretation.  Those that are not cancelled are labelled as\\npragmatic inferences for the given utterance or set of utterances.\\n\\n\\n\\n  A set of examples \\n\\nWe present a set of examples that covers a representative group of\\npragmatic inferences. In contrast with most other approaches, we\\nprovide a consistent methodology for computing these inferences and\\nfor determining whether they are cancelled or not for all possible\\nconfigurations: simple and complex utterances and sequences of\\nutterances.\\n\\n  Simple pragmatic inferences \\n  Lexical pragmatic inferences \\n\\nA factive such as the verb regret presupposes its complement, but\\nas we have seen, in positive environments, the presupposition is\\nstronger: it is\\nacceptable to defeat a presupposition triggered in a negative\\n environment (), but is infelicitous to defeat  one that belongs to a positive environment ().  Therefore, an appropriate formalization of utterance ()  and the requisite pragmatic knowledge will be as shown in (). \\n(3) \\n John does not regret that Mary came to the party.  \\n(4) \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe stratified semantic tableau that corresponds to\\n theory () is given in figure . The tableau yields two model schemata (see\\n figure ); in both of them, it is defeasibly inferred that Mary came to the party. The model-ordering\\nrelation \\nestablishes m0 as the optimistic model for the\\ntheory because it contains as much information as m1 and is easier\\nto defeat. Model m0 explains why Mary came to the party is a\\n presupposition for utterance (). \\n\\n\\n  Scalar implicatures \\n\\n Consider utterance (), and its  implicatures (). \\n(5) \\nJohn says that some of the boys went to the theatre.\\n(6) \\nNot {many/most/all} of the boys went to the theatre.\\n An appropriate formalization is given in (   ), where the second formula captures the defeasible scalar\\nimplicatures and the third formula reflects the relevant semantic\\ninformation for all.\\n(7) \\n\\n\\n The theory provides one optimistic model schema (figure    ) that reflects the expected pragmatic inferences, i.e., (Not most/Not\\nmany/Not all) of the boys went to the theatre.\\n\\n\\n  Simple cancellation \\n\\nAssume now, that after a moment of thought, the same person utters:\\n(8) \\n John says that some of the boys went to the theatre. In fact\\nall of them went to the theatre.By adding the extra utterance to the initial\\n theory (),  \\nuttered(went(all(boys), theatre)), one\\nwould obtain one optimistic model schema in which the conventional\\n implicatures have been cancelled (see figure ). \\n\\n\\n\\n  Complex utterances \\n\\nThe Achilles heel for most theories of presupposition  has been their\\nvulnerability to the projection problem.  Our solution for the\\nprojection problem does not differ from a solution for individual\\nutterances. Consider the following utterances and some of their\\n associated presuppositions () (the symbol  precedes\\nan inference drawn on pragmatic grounds):\\n(9) \\nEither Chris is not a bachelor or he regrets that Mary came\\n to the party.  \\n(10) \\n Chris is a bachelor or a spinster.  \\n(11) \\n\\n Chris is a (male) adult. Chris is not a bachelor presupposes that Chris is a male adult; Chris regrets that Mary came to the party presupposes\\nthat Mary came to the party. There is no contradiction between\\nthese two presuppositions, so one would expect a conversant to infer\\nboth of them if she hears an utterance such\\n as (). However, when one examines  utterance (), one observes immediately that there is a contradiction between the presuppositions carried by the individual\\ncomponents. Being a bachelor presupposes that Chris is a male,\\nwhile being a spinster presupposes that Chris is a\\nfemale. Normally, we would expect a conversant to notice this\\ncontradiction and to drop each of these elementary presuppositions\\n when she interprets (). \\n\\n\\nWe now study how stratified logic and  the model-ordering\\nrelation capture one's intuitions.\\n\\n  Or -- non-cancellation \\n\\n An appropriate formalization for utterance () and the  necessary semantic and pragmatic knowledge is given in (). \\n(12) \\n\\n\\n Besides the translation of the utterance, the initial theory contains a formalization of the defeasible implicature that natural disjunction\\nis used as an exclusive or, the knowledge that Mary is not\\na name for males, the lexical semantics for the word\\nbachelor, and the lexical pragmatics for bachelor and regret.  The stratified semantic tableau generates 12 model\\nschemata. Only four of them are kept as optimistic models for the\\nutterance.  The models yield Mary came to the party; Chris is a\\nmale; and Chris is an adult as pragmatic inferences of\\n utterance (). \\n\\n\\n  Or -- cancellation \\n\\n Consider now utterance ().  The stratified semantic tableau that corresponds to its logical theory yields 16 models, but\\nonly Chris is an adult satisfies\\n definition  and is projected as presupposition for the utterance.\\n\\n\\n\\n    Pragmatic inferences in sequences of utterances\\n\\n\\nWe have already mentioned that speech repairs constitute a good\\nbenchmark for studying the generation and cancellation of pragmatic\\n inferences along sequences of utterances .  Suppose, for example, that Jane has two friends -- John Smith and John Pevler --\\nand that her roommate Mary has met only John Smith, a married fellow.\\nAssume now that Jane has a conversation with Mary in which Jane\\nmentions only the name John because she is not aware that Mary does\\nnot know about the other John, who is a five-year-old boy. In this\\ncontext, it is natural for Mary to become confused and to come to wrong\\nconclusions. For example, Mary may reply that John is not a\\n  bachelor. Although this is true for both Johns, it is more\\nappropriate for the married fellow than for the five-year-old boy.\\nMary knows that John Smith is a married male, so the utterance makes\\nsense for her. At this point Jane realizes that Mary misunderstands\\nher: all the time Jane was talking about John Pevler, the\\n five-year-old boy. The utterances in () constitute a possible answer that Jane may give to Mary in order to clarify the problem.\\n(13) \\n a. No, John is not a bachelor. \\nb. I regret that you have misunderstood me. \\n c. He is only five years old. The first utterance in the sequence presupposes (   ). \\n(14) \\n \\nJohn is a male adult.\\n )b warns Mary that is very likely she misunderstood a  previous utterance (   ). The warning is conveyed by implicature.\\n(15) \\n \\n The hearer misunderstood the speaker. At this point, the hearer, Mary, starts to believe that one of her previous utterances has been elaborated on a\\nfalse assumption, but she does not know which one. The third\\n utterance ()c comes to clarify the issue. It explicitly expresses that John is not an adult. Therefore, it\\n cancels the early presupposition (): \\n(16) \\n\\n\\nJohn is an adult.\\n Note that there is a gap of one statement between the generation and the cancellation of this presupposition. The behavior described is\\nmirrored both by our theory and our program.\\n\\n\\n  Conversational implicatures in indirect replies \\n\\nThe same methodology can be applied to modeling conversational\\n implicatures in indirect replies . Green's algorithm makes use of discourse expectations, discourse plans, and discourse\\n relations. The following dialog is considered , p. 68]: \\n(17) \\nQ:   Did you go shopping? \\nA: a. My car's not running. \\n              b. The timing belt broke. \\n               c. (So) I had to take the bus.  \\n\\n\\n Answer () conveys a ``yes'', but a reply  consisting  only of ()a would implicate a ``no''. As Green notices,  in previous models of implicatures ,,  processing ()a will block the implicature generated  by ()c. Green solves the problem by extending the boundaries of the analysis to discourse units. Our approach does not\\nexhibit these constraints. As in the previous example, the one dealing\\nwith a sequence of utterances, we obtain a different interpretation\\nafter each step. When the question is asked, there is no\\n conversational implicature. Answer ()a makes the necessary conditions for implicating ``no'' true, and the implication\\n is computed. Answer ()b reinforces a previous condition.  Answer ()c makes the preconditions for implicating a ``no'' false, and the preconditions for implicating a ``yes'' true.\\nTherefore, the implicature at the end of the dialogue is that the\\nconversant who answered went shopping.\\n\\n\\n\\n  Conclusions \\n\\nUnlike most research in pragmatics that focuses on certain types of\\npresuppositions or implicatures, we provide a global framework in\\nwhich one can express all these types of pragmatic inferences. Each\\npragmatic inference is associated with a set of necessary conditions\\nthat may trigger that inference.  When such a set of conditions is\\nmet, that inference is drawn, but it is assigned a defeasible status.\\nAn extended definition of satisfaction and a notion of ``optimism''\\nwith respect to different interpretations yield the preferred\\ninterpretations for an utterance or sequences of utterances. These\\ninterpretations contain the pragmatic inferences that have not been\\ncancelled by context or conversant's knowledge, plans, or intentions.\\nThe formalism yields an algorithm that has been implemented in Common\\nLisp with Screamer. This algorithm computes uniformly pragmatic\\ninferences that are associated with simple and complex utterances and\\nsequences of utterances, and allows cancellations of pragmatic\\ninferences to occur at any time in the discourse.\\n\\n\\nAcknowledgements\\n\\n\\nThis research was supported in part by a grant from the Natural\\nSciences and Engineering Research Council of Canada.\\n\\nBibliography \\n\\nG. Frege.\\n1892.\\nber sinn und bedeutung.\\nZeitschrift fr Philos. und Philos. Kritik, 100:373-394.\\nreprinted as: On Sense and Nominatum, In Feigl H. and Sellars W.,\\n  editors, Readings in Philosophical Analysis, pages 85-102,\\n  Appleton-Century-Croft, New York, 1947.\\n\\n\\nG.J.M. Gazdar.\\n1979.\\nPragmatics: Implicature, Presupposition, and Logical Form.\\nAcademic Press.\\n\\n\\nN. Green and S. Carberry.\\n1994.\\nA hybrid reasoning model for indirect answers.\\nIn Proceedings 32nd Annual Meeting of the Association for\\n  Computational Linguistics, pages 58-65.\\n\\n\\nN. Green.\\n1990.\\nNormal state implicature.\\nIn Proceedings 28th Annual Meeting of the Association for\\n  Computational Linguistics, pages 89-96.\\n\\n\\nN. Green.\\n1992.\\nConversational implicatures in indirect replies.\\nIn Proceedings 30th Annual Meeting of the Association for\\n  Computational Linguistics, pages 64-71.\\n\\n\\nJ.B. Hirschberg.\\n1985.\\nA theory of scalar implicature.\\nTechnical Report MS-CIS-85-56, Department of Computer and Information\\n  Science, University of Pennsylvania.\\nAlso published by Garland Publishing Inc., 1991.\\n\\n\\nG. Hirst, S. McRoy, P. Heeman, P. Edmonds, and D. Horton.\\n1994.\\nRepairing conversational misunderstandings and non-understandings.\\nSpeech Communication, 15:213-229.\\n\\n\\nG. Hirst.\\n1991.\\nExistence assumptions in knowledge representation.\\nArtificial Intelligence, 49:199-242.\\n\\n\\nL. Karttunen and S. Peters.\\n1979.\\nConventional implicature.\\nIn Oh C.K. and Dinneen D.A, editors, Syntax and Semantics,\\n  Presupposition, volume 11, pages 1-56. Academic Press.\\n\\n\\nL. Karttunen.\\n1974.\\nPresupposition and linguistic context.\\nTheoretical Linguistics, 1:3-44.\\n\\n\\nP. Kay.\\n1992.\\nThe inheritance of presuppositions.\\nLinguistics  Philosophy, 15:333-379.\\n\\n\\nD. Marcu.\\n1994.\\nA formalism and an algorithm for computing pragmatic inferences and\\n  detecting infelicities.\\nMaster's thesis, Dept. of Computer Science, University of Toronto,\\n  September.\\nAlso published as Technical Report CSRI-309, Computer Systems\\n  Research Institute, University of Toronto.\\n\\n\\nD. Marcu and G. Hirst.\\n1994.\\nAn implemented formalism for computing linguistic presuppositions and\\n  existential commitments.\\nIn H. Bunt, R. Muskens, and G. Rentier, editors, International\\n  Workshop on Computational Semantics, pages 141-150, December.\\n\\n\\nS. McRoy and G. Hirst.\\n1993.\\nAbductive explanation of dialogue misunderstandings.\\nIn Proceedings, 6th Conference of the European Chapter of the\\n  Association for Computational Linguistics, pages 277-286, April.\\n\\n\\nR.E. Mercer.\\n1987.\\nA Default Logic Approach to the Derivation of Natural Language\\n  Presuppositions.\\nPh.D. thesis, Department of Computer Science, University of British\\n  Columbia.\\n\\n\\nW.V.O. Quine.\\n1949.\\nDesignation and existence.\\nIn Feigl H. and Sellars W., editors, Readings in Philosophical\\n  Analysis, pages 44-51. Appleton-Century-Croft, New York.\\n\\n\\nR. Reiter.\\n1980.\\nA logic for default reasoning.\\nArtificial Intelligence, 13:81-132.\\n\\n\\nB. Russell.\\n1905.\\nOn denoting.\\nMind n.s., 14:479-493.\\nreprinted in: Feigl H. and Sellars W. editors, Readings in\\n  Philosophical Analysis, pages 103-115. Appleton-Century-Croft, New York,\\n  1949.\\n\\n\\nR.A. van der Sandt.\\n1992.\\nPresupposition projection as anaphora resolution.\\nJournal of Semantics, 9:333-377.\\n\\n\\nJ.M. Siskind and D.A. McAllester.\\n1993.\\nScreamer: A portable efficient implementation of nondeterministic\\n  Common Lisp.\\nTechnical Report IRCS-93-03, University of Pennsylvania, Institute\\n  for Research in Cognitive Science, July 1.\\n\\n\\nR.M. Weischedel.\\n1979.\\nA new semantic computation while parsing: Presupposition and\\n  entailment.\\nIn Oh C.K. and Dinneen D.A, editors, Syntax and Semantics,\\n  Presupposition, volume 11, pages 155-182. Academic Press.\\n\\n\\nH. Zeevat.\\n1992.\\nPresupposition and accommodation in update semantics.\\nJournal of Semantics, 9:379-412.\\n\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nDrawing appropriate defeasible inferences has been proven to be one\\n  of the most pervasive puzzles of natural language processing and a\\n  recurrent problem in pragmatics.  This paper provides a theoretical\\n  framework, called stratified logic, that can accommodate\\n  defeasible pragmatic inferences. The framework yields an algorithm\\n  that computes the conversational, conventional, scalar, clausal, and\\n  normal state implicatures; and the presuppositions that are\\n  associated with utterances. The algorithm applies equally to simple\\n  and complex utterances and sequences of utterances.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nIt is common knowledge that a rational agent is inclined to presuppose\\nthe existence of definite references that occur in utterances. Hearing\\nor uttering the examples below, a rational agent presupposes that the\\ncheese, children, and car physically exist.\\n(1) \\nThe cheese I bought yesterday is very bad.\\n(2) \\nI really don't know what to do with my children anymore.\\n(3) \\nSorry I couldn't make it; my car broke on my way.However, day-to-day English provides an impressive number of cases\\nwhen existential presuppositions are not inferred, or when they are\\ndefeated by some commonsense knowledge \\nfor a comprehensive study).  One can explicitly speak of\\n nonexistence (); events and actions that do not  occur (); existence at other times ();  or fictional and imaginary objects (). \\n(4) \\n No one got an A+ in this course.  \\n(5) \\n John's party is cancelled.  \\n(6) \\n Gdel was a brilliant mathematician.  \\n(7) \\nSherlock Holmes is smarter than any other detective.\\n Note that the simple dichotomy found in most approaches to presupposition between existent and nonexistent objects is not enough\\nfor a full account of natural language expressiveness.\\n\\n\\nThe study of presuppositions is primarily a study of commitment --\\ncommitment to the existence of presupposed definite referents or to\\nthe truth of factive complements.  The reduction of presupposition to\\nentailment is inadequate because presuppositions are implied,\\nnot specified; they are not part of the truth conditions of\\nnatural language sentences, and they can be cancelled in negative\\nenvironments.   Trying to explain the whole phenomenon and to provide\\nsolutions for the projection problem, linguists have often omitted any\\nexplanation for the existential commitment of definite references or\\ntheir explanation has been a superficial one.  Similarly, philosophers\\nwho have studied existence and nonexistence have been more concerned\\nwith providing formal tools for manipulation of nonexistent objects\\nthan tools to capture our commonsense commitment.  This puts us in a\\ndifficult position. From a linguistic perspective, the literature\\nprovides a good set of theories able to more or less explain the\\ncommitment to the presupposed truth of factives and the like but not\\nthe existential commitment of definite references.  From a\\nphilosophical perspective, we have quite a few theories which deal\\nwith existence and nonexistence, but they too offer no explanation for\\nexistential commitment.\\n\\n\\nOur aim here is to provide a formalism that has the\\nstrength of both perspectives. We achieve this using the following:\\n\\na set of methodological principles that unify\\nMeinong's  philosophy with\\nGrice's  conversational principles;\\n\\na rich ontology in the style of Hirst , \\nwhich  provides  the possibility of having consistent models that\\ncontain objects belonging to different ontological spaces;\\n\\nan extension of stratified logic  where  the quantifiers are read under\\nLejewski's    ``unrestricted interpretation'',\\nwhich provides us the formal tool for expressing the above layers.\\n\\n\\n\\n\\nThe implementation relies on an extension of the Beth semantic\\ntableaux to stratified logic. The code is written in Common Lisp and\\nmakes extensive use of the nondeterministic facilities of the Screamer\\n system ,. \\n\\n\\nWe first review the philosophical approaches in studying existence\\nand nonexistence and the linguistic approaches in studying\\npresuppositions, emphasizing their (in)ability to deal with\\npresuppositions and nonexistence respectively. We give a brief\\nintroduction to stratified logic, its implementation, and explain the\\nmethodological principles of our approach.  In section 4 we show how\\nthis approach is able not only to deal with nonexistence but also able to\\nexplain the existential commitment of definite reference. The rest of\\nthe paper is dedicated to a comparison with Parsons's and Hobbs's\\nwork.\\n\\n\\n  What philosophers and linguists have to say \\n   Nonexistence and commitment in philosophy and logic \\n\\nEarly works of Frege  and\\nRussell  tackled a very small subset of what\\ntoday is labelled with the name ``presupposition'': the\\npresuppositions introduced by definite references and proper names.\\nHirst  shows that classical logic, which embeds\\n Quine's metaphysical view , p.150] that ``everything exists'', is not able to deal adequately with nonexistent objects. For\\nexample, if one knows that dragons do not exist -- \\n\\n\\n-- it is impossible to distinguish between My dragon\\nhas blue eyes and My dragon does not have blue eyes because\\nboth translations in first-order logic are false: \\n\\n.\\nTherefore, first-order logic is doomed to fail in any attempt to\\nreflect the presuppositions of definite references.\\n\\n\\nSeveral approaches to nonexistence rely on Meinong's mental act\\n philosophy .  For example, Parsons  avoids Russell's paraphrase of the\\ndefinite description by using the predicate E!. For Parsons, \\n\\n\\nrefers to the unique object that satisfies \\nif there is\\nsuch an object. Otherwise, it does not refer to anything at all.  For\\na sentence such as ``The man in the doorway is clever'',\\nParsons  argues that the translation\\n\\n\\nis not\\nadequate because it does not reflect our commitment to the man's\\nexistence. Therefore, he proposes that the translation should be\\n\\n\\nwhere E! is the existential predicate. But the problem\\nwith this is that it embeds the existential commitment in the logical\\ntranslation -- not as something that is ``implied'' or presupposed,\\nbut as something ``said'' or specified.  This is not the case with\\nlinguistic presuppositions. Thus, the first translation is too weak\\n-- unable to capture the commitment, and the second one is too strong\\n-- the commitment becomes part of the translation and leaves no room\\nfor cancellation of presupposition.\\n\\n\\nOutside Meinong's world, we find other approaches that focus on the\\nappropriate reading of the quantifiers.\\nLejewski  and Hintikka \\nboth propose an ``unrestricted interpretation'' of the quantifiers,\\nwhich makes no commitment to the existence of the objects over which\\nthey range.  Under this interpretation, existence can be predicated\\n(Lejewski), or explicitly captured as \\n\\n\\n(Hintikka).\\nThe latter solution is nothing but a translation into logic of Quine's\\nslogan, ``to exist is to be the value of a variable''.  In these\\nuniverses, we are free now to talk about Pegasus and dragons but we\\ncannot explain our commitment to the existence of the definite\\nreferents.\\n\\n\\nAn interesting approach towards explaining the conditions in which\\nexistential presuppositions are generated is built by\\nAtlas  around the notions of ``aboutness'' and\\n``noun-phrase topicality''.  Instead of allowing all the noun-phrases\\nin a sentence to exhibit presupposition generation capabilities, only\\nthe topical ones enjoy this property. Atlas gives no hint of how this\\ntheory could be extended to deal with factives or verbs of judging, and\\ndefining the notions of aboutness and topicality for them is not\\ntrivial. Even if we did manage to do this, such presuppositions can\\nnever be cancelled. Either they are generated or they are not. This\\nleads us to believe that sentences such as John didn't stop\\nbeating the rug because he never started cannot be captured in this\\nmanner.\\n\\n\\nHobbs  uses the ``unrestricted interpretation'' of\\nthe quantifiers introduced by Lejewski . Hence,\\nin Hobbs's framework, the set of things we can talk about (including,\\ntherefore, nonexistent things) and the set of things we quantify over\\nare equal. The existential commitment is captured by a set of\\n``transparency axioms''. For example, the sentence Ross worships\\nZeus is represented as:\\n\\n\\n\\n\\n\\nThe first conjunct says that E is the event of worshipping Zeus by\\nRoss,  and the second says that E exists in the real world. Hobbs\\nassigns a transparency property to the predicates. For\\n\\n,\\nthis property entails the existence of its second\\nargument in the physical world:\\n\\n\\n\\n\\n\\nApparently, the commitment to Ross's existence is solved.\\n\\n\\nis transparent in its second argument but not in its\\nthird; so we may infer that Ross is existent, but draw no conclusions\\nabout Zeus.  The problem is that the transparency axioms are\\nassociated with the predicates and not with the objects, so that there\\nis no criterion to choose an appropriate translation for a sentence\\nlike The King of Buganda worships Zeus because the translation\\nshould be transparent if we know nothing about Buganda and opaque\\notherwise.\\n\\n\\n  Theories of linguistic presupposition  and their relation to  (non)existence \\n\\nThe vast majority of the linguistic approaches are more concerned with\\n``how presuppositions are inherited'' than with ``what presuppositions\\nare''. Presuppositions are defined in terms of plugs, holes, and\\n filters , consistency ,  uncontroversiality , or hypothetical and secondary  contexts , but nothing is said about the logical framework into which they may be expressed. An exception is Mercer's\\napproach mercerphd.  He abandons the projection method in\\nfavour of rules of inference in default logic. Our main objection is\\nto Mercer's use of natural disjunction as an exclusive disjunction,\\nand the reduction of natural implication to logical equivalence.\\nMercer  argued that this is a consequence of the\\nway he intended his ``proof by cases'', in which ``the cases are taken\\nfrom a conjunctive statement, where the conjuncts are the disjuncts in\\na classical proof''. He assumes that this non-standard notion is the\\none that must be used in nonmonotonic reasoning.  But this\\nnon-traditional analysis and the reduction of natural implication to\\nlogical equivalence are not representable within the logic itself.\\nHence, this method is also a procedural one.\\n\\n\\nA different perspective is given by Sandt  and\\nZeevat  for whom presuppositions are understood as\\nanaphoric expressions that have internal structure and semantic\\ncontent. Because they have more semantic content than other anaphors,\\npresuppositions are able to create an antecedent in the case that the\\ndiscourse does not provide one. Van der Sandt provides a computational\\nmethod for presupposition resolution in an enhanced discourse\\nrepresentation theory, while Zeevat gives a declarative account for it\\nusing update semantics, but neither of the methods is able to\\naccommodate the cancellation of presupposition that is determined by\\ninformation added later to the discourse. A simple ontology consisting\\nonly of existent and nonexistent objects is inadequate for dealing\\nwith fictions or objects that have unactualized existence. Therefore,\\nsentences such as Sherlock Holmes is smarter than any other\\ndetective or The strike was averted cannot be represented in\\ntheir theories.\\n\\n\\n\\n   Reasoning in stratified logic \\n\\n Stratified logic  reflects a different understanding of default reasoning phenomena from that found in the classic\\n literature . Instead of treating the notion of defeasibility on consistency and justification-based grounds, we\\nconjecture that defeasible inferences are ``weaker'' than classical\\nentailments. For the purpose of this paper, it is enough to consider\\nonly a subset of stratified logic.\\n\\n\\nIn first-order stratified logic, a stratified interpretation \\nconsists of an universe of objects U and a function mapping Fas in first-order logic, but the relation set is partitioned according\\nto the strength (undefeasible and defeasible relations) and polarity\\n(positive and negative relations).  Thus, the set of relations Rwill be given by the union \\n\\n\\nwhere R[u] stands for positive undefeasible\\nrelations, \\n\\n\\nfor negative undefeasible relations, R[d]for positive defeasible relations, and \\n\\n\\nfor negative\\ndefeasible relations. Positive atomic formulas and negative (negated)\\natomic formulas are labelled as defeasible (e.g. \\n\\n)\\nor undefeasible (e.g. \\n\\n)\\nand compound\\nformulas are obtained from positive and negative atomic formulas using\\nclassical logical connectors.  For example, one would formalize that\\nuttering that John does not regret that Mary came to the party\\npresupposes that Mary came to the party as\\n\\n\\n\\n\\n\\n\\n\\nbecause Mary came to the party is defeasible: John does not\\nregret that Mary came to the party because she did not come.\\n\\n\\nAt the semantic level, we extend the notion of satisfiability to the\\ntwo levels we have introduced; hence, we will have u-satisfiability, ,\\nand d-satisfiability, .\\n\\n\\n    Assume \\nis an  \\nvaluation such that \\n\\n\\nand assume that \\nmaps n-ary predicates\\np to relations \\n\\n.\\nFor any atomic formula \\n\\n,\\nand any stratified\\nvaluation ,\\nwhere \\n\\n\\nand\\nti are terms, the x-satisfiability relations are\\ndefined as follows:\\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\nFor any negation of an atomic formula \\n\\n,\\nand any stratified valuation ,\\nwhere \\n\\n\\nand ti are terms, the x-satisfiability relations are\\ndefined as follows:\\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\niff  \\n\\n\\n\\n\\n\\n\\nThe x-satisfiability relation for compound formulas is defined\\nin the usual way. One can see that this definition of satisfiability\\nhas two major advantages: on one hand, the \\nrelation\\nprovides a high degree of liberty in satisfying sets of formulas that\\ncontain positive and negative information of different strengths; on\\nthe other hand the \\nrelation is able to signal when such a\\ncontradiction occurs. For example, in accordance with the above\\ndefinition, the theory \\n\\n\\nis u-satisfiable but is not d-satisfiable. That means defeasible and undefeasible information are\\nallowed to co-exist because the satisfiability relations are able to\\nhandle them appropriately.\\n\\n\\nStratified logic uses an extension of semantic tableaux that is both\\nsound and complete to compute the models associated with a given\\ntheory. On a set of model schemata, we define a partially ordered\\nrelation ()\\nthat yields the most optimistic schemata for the\\ntheory, i.e., those that contain more information and whose\\ninformation is as defeasible as possible. For example, a translation\\nin stratified logic of the classical example involving Tweety\\n(represented by the constant T) will yield three model schemata.\\nSchema m1 does not cancel the fact that Tweety flies as schema\\nm2 does.  Moreover, m1 contains more information than m3.\\nTherefore, m1 is the most optimistic model schema.\\n\\n\\n\\n\\n\\n\\n\\nModel schema m1 corresponds to an \\nstructure defined\\nover an universe that contains only one object, T, and no\\nfunction symbols. The relations defined on the universe are \\n\\n,\\n\\n\\nand \\n\\n.\\nFor the sake of compactness and clarity we\\nrepresent stratified models as unions of relations partitioned according\\nto their strength:\\n\\n\\n\\n\\n\\nThe stratified semantic tableaux and the model-ordering relations have\\nbeen fully implemented in Common Lisp using the Screamer macro package\\nthat provides nondeterministic facilities\\n  ,. Our program takes as input a logical representation of the background knowledge and of an\\nutterance, computes the model schemata for the theory, and returns the set of\\nmost optimistic schemata and the presuppositions associated with a given\\nutterance in the case that they exist. Computing the model schemata for a\\nstratified theory can be done within the same complexity bounds as in\\nfirst-order logic. The algorithm for determining the most optimistic\\nschemata is O(n[2]).\\n\\n\\n   Presuppositions as defeasible information \\n  Methodological principles for our approach \\n\\nThe approach to nonexistent objects and presuppositions that we are going\\nto present is constructed on the basis of a modified  set of Meinongian\\nprinciples about nonexistence. They are embedded in a stratified logic\\nframework in which quantifiers are taken under Lejewski's unrestricted\\ninterpretation. The ontology is enhanced with the eight types of\\nexistence listed by Hirst , though in this paper, we\\nwill deal only with physical existence, represented as E!,\\nunactualized existence, represented as ,\\nexistence\\noutside the world but with causal interaction with that world,\\nEOW!, and existence in fiction, F!.\\n\\n\\nFollowing Rapaport's style , we propose a set\\nof methodological principles based on Meinong \\nthat are meant to capture the ability of an intelligent agent to deal\\nwith existence and nonexistence rather from a conversational\\nperspective than from a rational one.\\n\\n\\nMC1.\\nEvery uttered sentence is ``directed'' towards an\\n``object'', because every uttered sentence can be seen as a\\nmaterialization of a mental act.\\n\\n\\nMC2.\\nAll uttered sentences  exist (technically, ``have being'').\\nHowever, this does not imply the existence of their referents,  which\\nare ``ausserseiend'' (beyond being and non-being).\\n\\n\\nMC3.\\nIt is not self-contradictory to deny, nor tautologous to\\naffirm, the existence of a referent.\\n\\n\\nMC4.\\nEvery referent and every uttered sentence  has properties.\\n\\n\\nMC5.\\nThe principles MC2 and MC4 are not inconsistent.\\n\\n\\nCorollary: Even referents of an uttered sentence that do not\\nexist have properties.\\n\\n\\nMC6.\\n(a) Every set of properties (Sosein) corresponds to the\\nutterance of a sentence. \\n                    (b) Every object of thought can be uttered.\\n\\n\\nMC7.\\nSome referents of an utterance are incomplete (undetermined\\nwith\\nrespect to some properties).\\n\\n\\nIn accordance with Grice , we need two additional\\nprinciples:\\nGC1.\\nThe speaker is committed to the truth of the sentences he utters.\\n\\n\\nGC2.\\nUsing and deriving  presuppositions requires, from both speaker and\\nlistener, a sort of ``optimism''.\\nPrinciple GC1 is formalized by the translation of the uttered\\nsentences into classical logic formulas in which quantifiers are read\\nunder their unrestricted interpretation. Principle GC2 is formalized\\nby the rules containing defeasible information that exist in the\\nknowledge base of the speaker and the hearer, and the notion of\\noptimism in the model-ordering relation. For example, a factive\\nnegation weakly implies the truth of its complement (see\\n formula  above).  Note that a non-optimistic interpretation of utterances will never be able to account for any of\\nthe pragmatic inferences, because they are not explicitly uttered.\\n\\n\\n  Formalizing presuppositions \\n\\nWe assume that our inference process relies not only on core\\nknowledge as in ``all men are mortal'' or ``birds fly'', but also on\\nknowledge of language use as in ``a factive negation weakly implies\\n the truth of its complement'' as shown in formula . \\n\\n\\nThat definite references imply the existence of their referents\\nconstitutes another instance of defeasible inference (see\\n examples ()--()).  We can capture this either by adding a new formula \\n\\n\\n\\n\\n\\nto our knowledge\\nbase, and by embedding syntactic terms into the logical form, as Hobbs\\ndid , or by representing this defeasible commitment\\nexplicitly in the translation of each utterance containing a definite\\nreference or proper noun.  Both approaches exhibit the same semantic\\nbehavior, and due to the model-ordering relation they explain our\\ncommitment to a referent's existence (in the case that we do not know\\notherwise). Because \\n\\n\\nis\\nsyntactic information, we depict it using a different font, but the\\nreader should understand that \\n\\n\\nis bound by the same\\nquantifier as x is, and that  \\n\\nis used as a metalogical symbol that triggers pragmatic inferences.\\n\\n\\nAs a last step, we abandon the Fregean reading of the quantifiers\\nand we adopt Lejewski's unrestricted\\ninterpretation . This means that \\nand\\n\\ndo not mix quantification with ontological commitment:\\n\\n\\ndoes not entail the physical existence of x,\\nso the things we can talk about equals the things we can quantify\\nover.  This yields the following:\\n\\n\\nDefinition: Presuppositions are  defeasible information\\nthat is derived from knowledge of language use and that is included in\\nthe most optimistic models of a theory described in terms of stratified\\nlogic under an unrestricted interpretation of the quantifiers.\\n\\n\\n  What the approach can do with existent and nonexistent objects \\n\\nAssume that someone utters the sentence The king of Buganda is\\n(not) bald. If we know nothing about Buganda and its king, the\\ncomplete theory of this utterance and the available knowledge in\\nstratified logic is this:\\n\\n\\n\\n\\n\\nThis theory has one optimistic model that reflects  one's commitment to\\nthe king's existence. The king's existence has the status of defeasible\\ninformation; it is derived using knowledge of language use and is a\\npresupposition of the utterance.\\n\\n\\n\\n\\n\\nKnowledge about the political system of France can inhibit\\nthe inference regarding the existence of its king in a sentence such as\\nThe king of France is (not) bald. Assume that  we know\\nthere is no king of France \\n\\n.\\nA complete formalization follows:\\n\\n\\n\\n\\n\\nFor this theory, we obtain only one model schema:\\n\\n\\n\\n\\nOne can notice that the existential presupposition is now\\ncancelled by some background knowledge. The only way one can satisfy\\nthe initial theory is if she has a stratified structure where \\n\\n.\\nThus, the theory yields one model\\n\\n\\n\\n\\n\\nAsserting existence\\nor nonexistence affects defeasible inferences due to knowledge of\\nlanguage use and restricts some of the models.\\nIf someone utters The king of Buganda exists and  we know nothing\\nabout Buganda, the translation\\n\\n\\n\\n\\n\\ngives  one  model:\\n\\n\\n\\n\\n\\nIf we know that the king of Buganda does not exist, or in other\\nwords we evaluate the above sentence against a knowledge base that\\ncontains\\n\\n\\n\\n\\n\\nthere is no model for this theory, so the utterance is interpreted as\\nfalse.  It is noteworthy that the inconsistency appears due to\\nspecific knowledge about the king's physical existence and not because\\nof a quantification convention as in classical first-order logic. On\\nthe other hand, the negation, The king of Buganda does not\\nexist, is consistent with the knowledge base and provides this model:\\n\\n\\n\\n\\n\\nSo far, we have emphasized the way presuppositions of definite\\nreferences can be handled in this framework. However, the proposed\\nmethod is general in the sense that it captures the other\\npresuppositional environments as well.  Moreover, the cancellation can\\noccur at any moment in discourse. Consider for example the\\nutterance John does not regret that Mary came to the party. Its\\nformalization in stratified logic follows:\\n\\n\\n\\n\\n\\nThe optimistic model computed by our program is this:\\n\\n\\n\\n\\n\\nThis model reflects our intuitions that Mary came to the\\nparty and all definite references exist.\\n\\n\\nIf one utters now Of course he doesn't. Mary did not come to the\\nparty, the new model computed by our program will reflect the fact that a\\npresupposition has been cancelled, even though this cancellation\\noccurred later in the discourse. Thus, the new optimistic model will\\nbe this:\\n\\n\\n\\n\\n\\nOur approach correctly handles references to unactualized objects such\\nas averted strikes or the paper that we had wanted to submit to AAAI-94. The\\nutterance The strike was averted can be formalized thus:\\n\\n\\n\\n\\n\\nThis gives one optimistic model:\\n\\n\\n\\n\\n\\n\\n  A comparison with Parsons's and Hobbs's work \\n  On Parsons's evidence for his theory of nonexistence \\n\\nParsons argues that is impossible to distinguish between the shape of\\nthe logical form of two sentences like these, in which one subject is\\nfictional and the other is real:\\na. Sherlock Holmes is more famous than any other detective.\\n\\n\\nb. Pel is more famous than any other soccer player.\\nIn our approach, similar syntactic translations give different\\nsemantic models when interpreted against different knowledge\\nbases. A complete theory for the first sentence is this:\\n\\n\\n\\n\\n\\nThis theory gives only one model:\\n\\n\\n\\n\\n\\nThis corresponds to an object \\nthat does not exist in the\\nreal world but exists as a fiction, has the property of being Sherlock\\nHolmes, and for any other object y, real or fictional that has the\\nproperty of being a detective, the object \\nis more famous\\nthan object y.  Of course, in this model, it is impossible to commit\\nourselves to Holmes's physical existence, but is possible to talk\\nabout him.\\n\\n\\nThe theory for the second sentence is this:\\n\\n\\n\\n\\n\\nThis theory exhibits one optimistic model:\\n\\n\\n\\n\\n\\nModel m states that the object ,\\nbeing Pel, exists in a\\ndefeasible sense and this is the existential presupposition of the\\ninitial utterance.\\n\\n\\nAs seen, it is needless to mention the existence of specific objects\\nin the knowledge base.  The model-ordering relation rejects anyhow\\nmodels that are not optimistic. In this way, the commitment to Pel's\\nexistence is preserved, and appears as a presupposition of the\\nutterance. Parsons's theory provides different logical forms for the\\nabove sentences, but fails to avoid the commitment to nonexistent\\nobjects.\\n\\n\\n  On Russell's arguments against Meinong's nonexistent objects \\n\\nThe approach presented here is not subject to Russell's criticisms of\\nMeinong.  His first objection was concerned with impossible objects,\\nwhich ``are apt to infringe the law of contradiction''. Russell does\\nnot say explicitly how this is supposed to happen.\\nParsons  reconstructs a line of reasoning\\nfor round squares that yields the inconsistency of having a\\nround square that is both round and not round.  Our approach is not\\nsubject to this contradiction because the existential and universal\\nquantifier do not commit us to the physical existence of the objects\\nthey quantify over. The sentence Squares are not round is true\\nin the world of Euclidian geometry, and assuming this geometry\\ncorrectly formalizes the real world, it is valid for entities from\\nthis world. There is no reason to extend our judgments about physical\\nexistent objects to the realm of nonexistent ones. Therefore, being\\nsquare implies not being round only for the physical existent objects:\\n\\n\\n\\n\\n\\nEven if we agree now that Round squares are round and Round\\nsquares are square, we cannot apply Modus Ponens because the\\nexistence of round squares is not asserted; therefore, the\\ncontradiction vanishes.\\n\\n\\nThe second Russellian objection was this: consider the existent golden\\nmountain; by the satisfaction principle it is golden, it is a mountain,\\nand it exists. Therefore, some gold mountain exists, which is\\nempirically\\nfalse.  The apparent puzzle found in Meinong's answer, The\\nexistent golden mountain is existent, but it does not exist, is\\nconsistent with our theory, because the adjective existent\\nbrings nothing new to our knowledge -- we are already committed to\\nthe mountain's existence. The only difference is that this commitment\\nis defeasible, so the model in which it is not acceptable to believe\\nin the existence of the mountain will survive our core knowledge. The\\ntranslation is\\n\\n\\n\\n\\n\\nand its semantic model contains a golden mountain \\nwhich, even\\nif it does not exist, \\n\\n,\\nis existent, \\n\\n.\\n\\n\\n  A comparison with Hobbs's work \\n\\nWe have mentioned that Hobbs's transparency pertains to relations and\\nnot to objects. In our approach, a sentence such as Ross worships\\nZeus can be satisfied by a set of semantic models that correspond to\\neach possible combination of the existence and non-existence of Ross\\nand Zeus.\\n\\n\\n\\n\\n\\nAmong them, only one is minimal: the one that explains the commitment\\nto both  Ross's  and Zeus's existence.\\n\\n\\n\\n\\n\\nBut let us  assume we know that there is no  entity in the real world that\\nenjoys the property of being Zeus, but rather one who exists outside the real\\nworld as a god (EOW![u]).\\n\\n\\n\\n\\n\\nThis theory is no longer satisfiable by a model in which Zeus exists\\nas a physical entity. However, the optimistic model explains our\\ncommitment to Ross's existence.\\n\\n\\n\\n\\n\\n\\n  Conclusion \\n\\nJoining Meinong's philosophy of nonexistence with Grice's\\nconversational principles provides a very strong motivation for a\\nuniform treatment of linguistic presuppositions. Lejewski's\\nunrestricted interpretation of the quantifiers, Hirst's ontology, and\\nthe notion of reasoning with stratified tableaux and\\nmodel-ordering in stratified logic provide the formal\\ntools to implement the principles. This amounts to a model-theoretic\\ndefinition for presuppositions that is able to offer a uniform\\ntreatment for linguistic presuppositions and an explanation for the\\nexistential commitment. A computationally tractable method can be\\nderived from the formalism. Its implementation in Common Lisp finds\\nthe natural language presuppositions, including the existential ones,\\nand correctly reflects their cancellation.\\n\\n\\nAcknowledgements\\n\\n\\nThis research was supported in part by a grant from the Natural\\nSciences and Engineering Research Council of Canada.\\n\\nBibliography \\n\\nJ.D. Atlas.\\nWhat are negative existence statements about?\\nLinguistics and Philosophy, 11:373-394, 1988.\\n\\n\\nG. Frege.\\nber sinn und bedeutung.\\nZ. Philos. Philos. Kritik, 100:373-394, 1892.\\nreprinted as: On Sense and Nominatum, In Feigl H. and Sellars W.,\\n  editors, Readings in Philosophical Analysis, pages 85-102,\\n  Appleton-Century-Croft, New York, 1947.\\n\\n\\nG.J.M. Gazdar.\\nPragmatics: Implicature, Presupposition, and Logical Form.\\nAcademic Press, 1979.\\n\\n\\nH.P. Grice.\\nLogic and conversation.\\nIn Cole P. and Morgan J.L., editors, Syntax and Semantics,\\n  Speech Acts, volume 3, pages 41-58. Academic Press, 1975.\\n\\n\\nK.J.J. Hintikka.\\nExistential presuppositions and existential commitments.\\nJournal of Philosophy, 56:125-137, 1959.\\n\\n\\nG. Hirst.\\nExistence assumptions in knowledge representation.\\nArtificial Intelligence, 49:199-242, 1991.\\n\\n\\nJ.R. Hobbs.\\nOntological promiscuity.\\nIn Proceedings 23rd Annual Meeting of the Association for\\n  Computational Linguistics, pages 61-69, 1985.\\n\\n\\nL. Karttunen.\\nPresuppositions of compound sentences.\\nLinguistic Inquiry, 4(2):169-193, 1973.\\n\\n\\nP. Kay.\\nThe inheritance of presuppositions.\\nLinguistics  Philosophy, 15:333-379, 1992.\\n\\n\\nC. Lejewski.\\nLogic and existence.\\nBritish Journal for the Philosophy of Science, 5:104-119,\\n  1954.\\n\\n\\nD. Marcu.\\nA formalism and an algorithm for computing pragmatic inferences and\\n  detecting infelicities.\\nMaster's thesis, Dept. of Computer Science, University of Toronto,\\n  September 1994.\\nAlso published as Technical Report CSRI-309, Computer\\nSystems Research Institute, University of Toronto.\\n\\n\\nA. Meinong.\\nber gegenstandstheorie.\\nIn Meinong A., editor, Untersuchungen zur Gegenstandstheorie und\\n  Psychologie. Barth, Leipzig, 1904.\\nreprinted in: The theory of objects, Chisholm R.M. editor,   Realism and the Background of Phenomenology, pages 76-117. Free Press,\\n  Glencoe, IL, 1960.\\n\\n\\nR.E. Mercer.\\nA Default Logic Approach to the Derivation of Natural Language\\n  Presuppositions.\\nPhD thesis, Department of Computer Science, University of British\\n  Columbia, 1987.\\n\\n\\nR.E. Mercer.\\nPersonal communication, 1993.\\n\\n\\nT. Parsons.\\nNonexistent Objects.\\nYale University Press, New Haven, CT, 1980.\\n\\n\\nW.V.O. Quine.\\nDesignation and existence.\\nIn Feigl H. and Sellars W., editors, Readings in Philosophical\\n  Analysis, pages 44-51. Appleton-Century-Croft, New York, 1949.\\n\\n\\nW.J. Rapaport.\\nMeinongian semantics for propositional semantic networks.\\nIn Proceedings 23rd Annual Meeting of the Association for\\n  Computational Linguistics, pages 43-48, 1985.\\n\\n\\nR. Reiter.\\nA logic for default reasoning.\\nArtificial Intelligence, 13:81-132, 1980.\\n\\n\\nB. Russell.\\nOn denoting.\\nMind n.s., 14:479-493, 1905.\\nreprinted in: Feigl H. and Sellars W. editors, Readings in\\n  Philosophical Analysis, pages 103-115. Appleton-Century-Croft, New York,\\n  1949.\\n\\n\\nR.A. van der Sandt.\\nPresupposition projection as anaphora resolution.\\nJournal of Semantics, 9:333-377, 1992.\\n\\n\\nJ.M. Siskind and D.A. McAllester.\\nNondeterministic Lisp as a substrate for constraint logic\\n  programming.\\nIn Proceedings of the Twelfth National Conference on Artificial\\n  Intelligence, pages 133-138, 1993.\\n\\n\\nJ.M. Siskind and D.A. McAllester.\\nScreamer: A portable efficient implementation of nondeterministic\\n  Common Lisp.\\nTechnical Report IRCS-93-03, University of Pennsylvania, Institute\\n  for Research in Cognitive Science, July 1 1993.\\n\\n\\nS. Soames.\\nHow presuppositions are inherited: A solution to the projection\\n  problem.\\nLinguistic Inquiry, 13(3):483-545, Summer 1982.\\n\\n\\nH. Zeevat.\\nPresupposition and accommodation in update semantics.\\nJournal of Semantics, 9:379-412, 1992.\\n\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nWe rely on the strength of linguistic and philosophical perspectives\\nin constructing a framework that offers a unified explanation for\\npresuppositions and existential commitment.  We use a rich ontology\\nand a set of methodological principles that embed the essence of\\nMeinong's philosophy and Grice's conversational principles into a\\nstratified logic, under an unrestricted interpretation of the\\nquantifiers. The result is a logical formalism that yields a tractable\\ncomputational method that uniformly calculates all the presuppositions\\nof a given utterance, including the existential ones.\\n2\\n\\n\"],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nDATR was introduced by Evans and Gazdar  as a\\nsimple, declarative language for representing lexical knowledge in terms\\nof path/value equations.  The language lacks many of the constructs\\nfound in general purpose, knowledge representation formalisms, yet it has\\nsufficient expressive power to capture concisely the structure of\\nlexical information at a variety of levels of linguistic description.\\nAt the present time, DATR is probably the most widely-used formalism\\nfor representing natural language lexicons in the natural language\\nprocessing (NLP) community. There are around a dozen different\\nimplementations of the language and large DATR lexicons have been\\nconstructed for use in a variety of applications\\n ,,. DATR has been applied to problems in  inflectional and derivational morphology ,,,  lexical semantics , morphonology , prosody   and speech . In more recent work, the language has been used to provide a concise encoding of Lexicalised Tree\\n Adjoining Grammar ,. \\n\\n\\nA primary objective in the development of DATR has been the provision\\nof an explicit, mathematically rigorous semantics. This goal was\\naddressed in one of the first publications on the language\\n . The definitions given there deal with a subset of DATR that includes core features of the language such as the notions of local and global inheritance and DATR's default mechanism. However, they\\nexclude some important and widely-used constructs, most notably string\\n(or `list') values and evaluable paths. Moreover, it is by no means\\nclear that the approach can be generalized appropriately to cover these\\nfeatures.  In particular, the formal apparatus introduced by Evans and\\nGazdar in  provides no explicit model of DATR 's\\nnotion of global context. Rather, local and global inheritance\\nare represented by distinct semantic functions \\nand\\n.\\nThis approach is possible only on the (overly restrictive)\\nassumption that DATR statements involve either local or global\\ninheritance relations, but never both.\\n\\n\\nThe purpose of the present paper is to remedy the deficiencies of the\\n work described in  by furnishing DATR with a transparent, mathematical semantics. There is a standard view of DATR as a language\\nfor representing a certain class of non-monotonic inheritance networks\\n(`semantic nets').  While this perspective provides an intuitive and\\nappealing way of thinking about the structure and representation of\\nlexical knowledge, it is less clear that it provides an accurate or\\nparticularly helpful picture of the DATR language itself. In fact,\\nthere are a number of constructs available in DATR that are impossible\\nto visualize in terms of simple inheritance hierarchies. For this\\nreason, the work described in this paper reflects a rather different\\nperspective on DATR, as a language for defining certain kinds of\\npartial functions by cases. In the following sections this viewpoint is\\n made more precise. Section  presents the syntax of the DATR language and introduces the notion of a DATR theory.  An\\ninformal introduction to the DATR language is provided, by example, in\\n section .  The semantics of DATR is then covered in  two stages. Section  introduces DATR interepretations and describes the semantics of a restricted version of the language\\nwithout defaults. The treatment of implicit information is covered in\\n section , which provides a definition of a default model for a DATR theory.\\n\\n\\n    DATR Theories\\n\\n\\nLet \\n\\n\\nand \\n\\n\\nbe disjoint sets of symbols (the nodes\\nand atoms respectively). Nodes are denoted by N and atoms by\\na. The set  DESC of DATR value descriptors (or simply descriptors) is built up from the atoms and nodes as shown\\nbelow. Descriptors are denoted by d.\\n\\n\\n\\n\\n\\n\\nfor any \\n\\n\\n\\nFor any \\n\\n\\nand \\n\\n:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nValue descriptors are either atoms or inheritance descriptors,\\nwhere an inheritance descriptor is further distinguished as either local (unquoted) or global (quoted). There is just one kind\\nof local descriptor (node/path), but three kinds of global descriptor\\n (node/path, path and node) . \\n\\n\\nA path \\n\\n\\nis a (possibly empty)\\nsequence of atoms enclosed in angle brackets. Paths are denoted by P.\\nFor N a node, P a path and \\n\\n\\na (possibly empty)\\nsequence of atoms, an equation of the form \\n\\n\\nis\\ncalled an extensional sentence.  Intuitively, an extensional\\nsentence \\n\\n\\nstates that the value associated with the\\npath P at node N is .\\nFor \\na (possibly empty) sequence\\nof value descriptors, an equation of the form \\n\\n\\nis\\ncalled a definitional sentence. A definitional sentence\\n\\n\\nspecifies a property of the node N, namely that\\nthe path P is associated with the value defined by the sequence of\\nvalue descriptors .\\n\\n\\nA collection of equations can be used to specify the properties of\\ndifferent nodes in terms of one another, and a finite set of DATR sentences \\n\\n\\nis called a DATR theory.  In principle, a\\nDATR theory \\n\\n\\nmay consist of any combination of DATR sentences, either definitional or extensional, but in practice, DATR theories are more restricted than this.  The theory \\n\\n\\nis said to\\nbe definitional if it consists solely of definitional sentences\\nand it is said to be functional if it meets the following\\ncondition:\\n\\n\\n\\n\\n\\nThere is a pragmatic distinction between definitional and extensional\\nsentences akin to that drawn between the language used to define a database\\nand that used to query it.  DATR interpreters conventionally treat\\nall extensional sentences as `goal' statements, and evaluate them as soon\\nas they are encountered. Thus, it is not possible, in practice, to\\ncombine definitional and extensional sentences within a\\n theory. Functionality for DATR theories, as defined above, is really a syntactic notion. However, it approximates a deeper, semantic\\nrequirement that the nodes should correspond to (partial) functions from\\npaths to values.\\n\\n\\nIn the remainder of this paper we will use the term (DATR) theory always in the sense functional, definitional (DATR)\\ntheory.  For a given DATR theory \\n\\n\\nand node N of\\n\\n,\\nwe write \\n\\n\\nto denote that subset of the sentences in\\n\\n\\nthat relate to the node N. That is:\\n\\n\\n\\n\\n\\nThe set \\n\\n\\nis referred to as the definition of N(in \\n\\n).\\n\\n\\n    An Overview of DATR\\n\\n\\nAn example of (a fragment of) a DATR theory is shown in\\n figure .  The theory makes use of some standard abbreviatory devices that enable nodes and/or paths to be omitted in\\ncertain cases. For example, sets of sentences relating to the same node\\nare written with the node name implicit in all but the first-given\\nsentence in the set. Also, we write \\n\\nto abbreviate the definitional sentence\\n\\n,\\nand similarly elsewhere.\\n\\n\\nThe theory defines the properties of seven nodes: an abstract\\n\\n\\nnode, nodes \\n\\n,\\n\\n\\nand\\n\\n,\\nand three abstract lexemes \\n\\n,\\n\\nand \\n\\n.\\nEach node is associated with a collection of\\ndefinitional sentences that specify values associated with different\\npaths. This specification is achieved either explicitly, or implicitly. Values given explicitly are specified either directly, by exhibiting a particular value, or indirectly, in\\nterms of local and/or global inheritance. Implicit specification is\\nachieved via DATR 's default mechanism.\\n\\n\\nFor example, the definition of the \\n\\n\\nnode gives the values\\nof the paths \\n\\n\\nand \\n\\ndirectly, as \\n\\n\\nand \\n\\n,\\nrespectively.  Similarly,\\nthe definition of \\n\\n\\ngives the value of \\n\\n\\ndirectly as \\n\\n.\\nOn the other hand, the value of the\\nempty path at \\n\\n\\nis given indirectly, by local inheritance,\\nas the value of the empty path at \\n\\n.\\nNote that in itself,\\nthis might not appear to be particularly useful, since the theory does\\nnot provide an explicit value for the empty path in the definition of\\n\\n.\\nHowever, DATR's default mechanism permits any\\ndefinitional sentence to be applicable not only to the path specified in\\nits left-hand-side, but also for any rightward extension of that path\\nfor which no more specific definitional sentences exist.  This means\\nthat the statement \\n\\n\\nactually\\ncorresponds to a class of implicit definitional sentences, each\\nobtained by extending paths on the left- and the right-hand-sides of the\\nequation in the same manner. Examples include the following:\\n\\n\\n\\n\\n\\nThus, the value associated with \\n\\n\\nat \\n\\nis given (implicitly) as the value of \\n\\n\\nat\\n\\n,\\nwhich is given (explicitly) as \\n\\n.\\nAlso, the\\nvalues of \\n\\n\\nand \\n\\n,\\namongst many\\nothers, are inherited from \\n\\n.\\nIn the same way, the value of\\n\\n\\nat \\n\\n\\nis inherited locally from\\n\\n\\n(which in turn inherits locally from \\n\\n)\\nand\\nthe value of \\n\\n\\nat \\n\\n\\nis inherited locally\\nfrom \\n\\n\\n(which ultimately gets its value from \\n\\nvia \\n\\n).  Note however, that the following sentences do not follow by default from the specifications given at the relevant\\nnodes:\\n\\n\\n\\n\\n\\nIn each of the above cases, the theory provides an explicit statement\\nabout the value associated with the indicated path at the given node. As\\na result the default mechanism is effectively over-ridden.\\n\\n\\nIn order to understand the use of global (i.e. quoted) inheritance\\ndescriptors it is necessary to introduce DATR's notion of a global\\ncontext.  Suppose then that we wish to determine the value associated with\\nthe path \\n\\n\\nat the node \\n\\n.\\nIn this\\ncase, the global context will initially consist of the node/path pair\\n\\n.\\nNow, by default the value\\nassociated with \\n\\n\\nat \\n\\n\\nis inherited\\nlocally from \\n\\n\\nat \\n\\n.\\nThis, in turn,\\ninherits globally from the path \\n\\n.\\nThat\\nis:\\n\\n\\n\\n\\n\\nConsequently, the required value is that associated with \\n\\n\\nat the `global node' \\n\\n\\n(i.e. the node provided by\\nthe current global context), which is just \\n\\n.\\nIn a similar\\nfashion, the value associated with \\n\\n\\nat\\n\\n\\nis obtained as \\n\\n\\n(i.e. the string of atoms\\nformed by evaluating the specification \\n\\n\\nin the global context\\n\\n).\\n\\n\\nMore generally, the global context is used to fill in the missing node\\n(path) when a global path (node) is encountered. In addition however,\\nthe evaluation of a global descriptor results in the global context\\nbeing set to the new node/path pair. Thus in the preceding example,\\nafter the quoted descriptor \\n\\n\\nis\\nencountered, the global context effectively becomes \\n\\n\\n/\\n\\n\\n(i.e. the path component of the global context\\nis altered). Note that there is a real distinction between a local\\ninheritance descriptor of the form N:P and it's global counterpart\\n\\n.\\nThe former has no effect on the global context, while\\nthe latter effectively overwrites it.\\n\\n\\nFinally, the definition of \\n\\n\\nin the theory of\\n figure  illustrates a use of the `evaluable path' construct:\\n\\n\\n\\n\\n\\nThis states that the value of \\n\\n\\nat \\n\\nis inherited globally from the path \\n\\n,\\nwhere the\\ndots represent the result of evaluating the global path\\n\\n\\n(i.e. the value associated with\\n\\n\\nin the prevailing global context). Evaluable\\npaths provide a powerful means of capturing generalizations about the\\nstructure of lexical information.\\n\\n\\n    DATR Models\\n\\n\\nTo a first level of approximation, the DATR theory of\\n figure  can be understood as a representation of an inheritance hierarchy (a `semantic network') as shown in\\n figure . In the diagram, nodes are written as labelled boxes, and arcs correspond to (local) inheritance, or isa links.  Thus, the node \\n\\n\\ninherits from \\n\\nwhich inherits from \\n\\n\\nwhich in turn is a \\n\\n.\\nThe\\nhierarchy provides a useful means of visualising the overall structure\\nof the lexical knowledge encoded by the DATR theory.  However, the\\nsemantic network metaphor is of far less value as a way of thinking\\nabout the DATR language itself. Note that there is nothing inherent in\\nDATR to ensure that theories correspond to simple isa\\nhierarchies of the kind shown in the figure. What is more, the DATR language includes constructs that cannot be visualized in terms of\\nsimple networks of nodes connected by (local) inheritance links. Global\\ninheritance, for example, has a dynamic aspect which is difficult to\\nrepresent in terms of static links. Similar problems are presented by\\nboth string values and evaluable paths.  Our conclusion is that the\\nnetwork metaphor is of primary value to the DATR user. In order to\\nprovide a satisfactory, formal model of how the language `works' it is\\nnecessary to adopt a different perspective.\\n\\n\\nDATR theories can be viewed semantically as collections of\\ndefinitions of partial functions (`nodes' in DATR parlance) that map\\npaths onto values.  A model of a DATR theory is then an assignment of\\nfunctions to node symbols that is consistent with the definitions of\\nthose nodes within the theory. This picture of DATR as a formalism for\\ndefining partial functions is complicated by two features of the\\nlanguage however. First, the meaning of a given node depends, in\\ngeneral, on the global context of interpretation, so that nodes do not\\ncorrespond directly to mappings from paths to values, but rather to\\nfunctions from contexts to such mappings.  Second, it is\\nnecessary to provide an account of DATR's default mechanism.  It will\\nbe convenient to present our account of the semantics of DATR in two\\nstages.\\n\\n    DATR Interpretations\\n\\n\\nThis section considers a restricted version of DATR without the default mechanism.\\n Section  then shows how implicit information can be modelled by treating value descriptors as families of\\nvalues indexed by paths.\\n\\n\\nDefinition  4.1   \\nA DATR interpretation is a triple \\n\\n,\\nwhere\\n\\n\\n1.\\nU is a set;\\n2.\\n\\nis a function assigning to each element of the set \\n\\n\\na partial function from \\n\\n\\nto U[*].\\n3.\\nF is a valuation function assigning to each node N and atom\\na an\\nelement of U, such that distinct atoms are assigned distinct elements.\\n\\n\\nElements of the set U are denoted by u and elements of U[*] are\\ndenoted by v. Intuitively, U[*] is the domain of (semantic)\\nvalues/paths.  Elements of the set \\n\\n\\nare called contexts and denoted by c. The function \\ncan be thought of\\nas mapping global contexts onto (partial) functions from local contexts\\nto values.\\nThe function F is extended to paths, so that for \\n\\n\\n()\\nwe write F(P) to denote \\n\\n,\\nwhere \\n\\nui = F(ai) for each i (\\n\\n).\\n\\n\\nIntuitively, value descriptors denote elements of U[*] (as we\\nshall see, this will need to be revised later in order to account for\\nDATR's default mechanism). We associate with the interpretation \\n\\n\\na partial denotation function \\n\\n\\nand write \\n\\n\\nto denote the\\nmeaning (value) of descriptor d in the global context c. The\\n denotation function is defined as shown in figure . Note that an atom always denotes the same element of U, regardless of the\\ncontext. By contrast, the denotation of an inheritance descriptor is, in\\ngeneral, sensitive to the global context c in which it appears. Note\\nalso that in the case of a global inheritance descriptor, the global\\ncontext is effectively altered to reflect the new local context c'.\\nThe denotation function is extended to sequences of value descriptors in\\nthe obvious way. Thus, for \\n\\n\\n(), we write\\n\\n\\nto denote \\n\\n\\nif \\n\\n\\n(\\n\\n)\\nis defined (and \\n\\n\\nis\\nundefined otherwise).\\n\\n\\nNow, let \\n\\n\\nbe an\\ninterpretation and \\n\\n\\na theory. We will write\\n\\n\\nto denote that partial function from U[*] to\\nU[*] given by\\n\\n\\n\\n\\n\\nIt is easy to verify that \\n\\n\\ndoes indeed denote a\\npartial function (it follows from the functionality of the theory\\n\\n).  Let us also write \\n\\n\\nto denote that partial\\nfunction from U[*] to U[*] given by \\n\\n,\\nfor all .\\nThen, I models\\n\\n\\njust in case the following containment holds for each node Nand context c:\\n\\n\\n\\n\\n\\nThat is, an interpretation is a model of a DATR theory just in case\\n(for each global context) the function it associates with each node\\nrespects the definition of that node within the theory.\\n\\n\\n    Implicit Information and Default Models\\n\\n\\nThe notion of a model presented in the preceding section is too liberal\\nin that it takes no account of information implicit in a theory.\\nFor example, consider again the definition of the\\nnode \\n\\n\\n from the theory of figure , and repeated below.\\n\\n\\n\\n\\n\\nAccording to the definition of a model given previously, any model of the\\n theory of figure  will associate with the node \\n\\n\\na function from paths to values\\nwhich respects the above definition. This means that for\\nevery global context c, the following containment must\\n hold: \\n\\n\\n\\n\\n\\nOn the other hand, there is no guarantee that a given model will also\\nrespect the following containment:\\n\\n\\n\\n\\n\\nIn fact, this containment (amongst other things) should hold.\\nIt follows `by default' from the statements made about \\n\\nthat the path \\n\\n\\ninherits locally from \\n\\n\\nand\\nthat the value associated with any extension of \\n\\nis \\n\\n.\\n\\n\\nThere have been a number of formal treatments of defaults in\\nthe setting of attribute-value formalisms\\n ,,,. Each of these approaches formalizes a notion of default inheritance by defining appropriate operations\\n(e.g. default unification) for combining strict and default information.\\nStrict information is allowed to over-ride default information where the\\ncombination would otherwise lead to inconsistency (i.e. unification\\nfailure). In the case of DATR however, the formalism does not draw an\\nexplicit distinction between strict and default values for paths. In\\nfact, all of the information given explicitly in a DATR theory is\\nstrict. The non-monotonic nature of DATR theories\\narises from a general, default mechanism which\\n`fills in the gaps' by supplying values for paths not explicitly\\nspecified in a theory.\\nMore specifically,\\nDATR's default mechanism ensures that any path that is not\\nexplicitly specified for a given node will take its definition from the\\nlongest prefix of that path that is specified. Thus, the default\\nmechanism defines a class of implicit, definitional sentences with paths\\non the left that extend paths found on the left of explicit sentences.\\nFurthermore, this extension of paths is also carried over to paths\\noccurring on the right. In effect, each (explicit) path is associated\\nnot just with a single value specification, but with a whole family of\\nspecifications indexed by extensions of those paths.\\n\\n\\nThis suggests the following approach to the semantics of defaults in\\nDATR. Rather than interpreting node definitions (in a given global\\ncontext) as partial functions from paths to values (i.e. of type \\n\\n)\\nwe choose instead to interpret them as partial\\nfunctions from (explicit) paths, to functions from extensions of those\\npaths to values (i.e. of type \\n\\n). Now suppose that \\n\\n\\nis\\nthe function associated with the node definition \\n\\n\\nin a given\\nDATR interpretation.  We can define a partial function \\n\\n\\n(the default interpretation of \\n\\n)\\nas\\nfollows.  For each \\nset\\n\\n\\n\\n\\n\\nwhere \\n\\nv = v1v2 and v1 is the longest prefix of v such that\\nf(v1) is defined. In effect, the function \\nmakes explicit\\nthat information about paths and values that is only\\nimplicit in f, but just in\\nso far as it does not conflict with explicit information provided by\\nf.\\n\\n\\nIn order to re-interpret node definitions in the manner suggested above,\\nit is necessary to modify the interpretation of value descriptors.\\nIn a given global context c, a value descriptor\\nd now corresponds to a total function \\n\\n\\n(intuitively, a function from path extensions to values). For\\nexample, atoms now denote constant functions:\\n\\n\\n\\n\\n\\nMore generally, value descriptors will denote different values for\\n different paths. Figure  shows the revised clause for global node/path pairs, the other definitions being very similar.  Note\\nthe way in which the `path' argument v is used to extend \\n\\n\\nin order to define the new local (and in this case also, global)\\ncontext c'. On the other hand, the meaning of each of the di is\\nobtained with respect to the `empty path' \\n(i.e. path\\nextension does not apply to subterms of inheritance descriptors).\\n\\n\\nAs before, the interpretation function is extended to sequences of path\\ndescriptors, so that for \\n\\n\\n()\\nwe have\\n\\n,\\nif \\n\\n\\nis defined, for each i (\\n\\n)\\n(and\\n\\n\\nis undefined otherwise).  The definition of the\\ninterpretation of node definitions can be taken over unchanged from the\\nprevious section.  However, for a theory \\n\\n\\nand node N, the\\nfunction \\n\\n\\nis now of type \\n\\n.\\nAn interpretation \\n\\n\\nis a default model for theory \\n\\n\\njust in case for every context cand node N we have:\\n\\n\\n\\n\\n\\nAs an example, consider the default interpretation of the definition of\\nthe node \\n\\n\\ngiven above. By definition, any default model of\\n the theory of figure  must respect the following containment:\\n\\n\\n\\n\\n\\nFrom the definition of ,\\nit follows that for any path v, if\\nv extends \\n\\n,\\nthen it is mapped onto the value\\n\\n,\\nand otherwise it is mapped to the value given by\\n\\n.\\nWe have the following picture:\\n\\n\\n\\n\\n\\nThe default models of a theory \\n\\n\\nconstitute a proper subset of\\nthe models of \\n\\n:\\njust those that respect the default\\ninterpretations of each of the nodes defined within the theory.\\n\\n\\n\\n    Conclusions\\n\\n\\nThe work described in this paper fulfils one of the objectives of the\\nDATR programme: to provide the language with an explicit, declarative\\nsemantics. We have presented a formal model of DATR as a language for\\ndefining partial functions and this model has been contrasted with an\\ninformal view of DATR as a language for representing inheritance\\nhierarchies.  The approach provides a transparent treatment of DATR's\\nnotion of (local and global) context and accounts for DATR's default\\nmechanism by regarding value descriptors (semantically) as families of\\nvalues indexed by paths.\\n\\n\\nThe provision of a formal\\nsemantics for DATR is important for several reasons. First, it\\nprovides the DATR user with a concise, implementation-independent\\naccount of the meaning of DATR theories.  Second, it serves as a\\nstandard against which other, operational definitions\\nof the formalism can be judged.  Indeed, in the absence of such a\\nstandard, it is impossible to demonstrate formally the correctness of\\nnovel implementation strategies (for an example of such a strategy, see\\n ).  Third, the process of formalisation itself aids our understanding of the language and its relationship to other\\nnon-monotonic, attribute-value formalisms.  Finally, the semantics\\npresented in this paper provides a sound basis for subsequent\\ninvestigations into the mathematical and computational properties of\\nDATR.\\n\\n\\n  Acknowledgements \\n\\nThe author would like to thank Roger Evans, Gerald Gazdar, Bill Rounds\\nand David Weir for helpful discussions on the work described in this\\npaper.\\n\\nBibliography \\n\\nFrancois Andry, Norman Fraser, Scott McGlashan, Simon Thornton, and Nick Youd.\\n1992.\\nMaking DATR work for speech: lexicon compilation in SUNDIAL.\\nComputational Linguistics, 18(3):245-267.\\n\\n\\nGosse Bouma.\\n1992.\\nFeature structures and nonmonotonicity.\\nComputational Linguistics, 18(2):183-203.\\n\\n\\nLynne Cahill and Roger Evans.\\n1990.\\nAn application of DATR: the TIC lexicon.\\nIn Proceedings of the 9th European Conference on Artificial\\n  Intelligence, pages 120-125.\\n\\n\\nLynne Cahill.\\n1993.\\nMorphonology in the lexicon.\\nIn Proceedings of the 6th Conference of the European Chapter of\\n  the Association for Computational Linguistics, pages 87-96.\\n\\n\\nLynne Cahill.\\n1994.\\nAn inheritance-based lexicon for message understanding systems.\\nIn Proceedings of the 4th ACL Conference on Applied Natural\\n  Language Processing, pages 211-212.\\n\\n\\nBob Carpenter.\\n1993.\\nSkeptical and credulous default unification with applications to\\n  templates and inheritance.\\nIn Ted Briscoe, Valeria de Paiva, and Ann Copestake, editors,   Inheritance, Defaults and the Lexicon, pages 13-37. Cambridge University\\n  Press, Cambridge.\\n\\n\\nGreville Corbett and Norman Fraser.\\n1993.\\nNetwork morphology: a DATR account of Russian nominal inflection.\\nJournal of Linguistics, 29:113-142.\\n\\n\\nRoger Evans and Gerald Gazdar.\\n1989a.\\nInference in DATR.\\nIn Proceedings of the 4th Conference of the European Chapter of\\n  the Association for Computational Linguistics, pages 66-71.\\n\\n\\nRoger Evans and Gerald Gazdar.\\n1989b.\\nThe semantics of DATR.\\nIn Proceedings of AISB-89, pages 79-87.\\n\\n\\nRoger Evans, Gerald Gazdar, and David Weir.\\n1994.\\nUsing default inheritance to describe LTAG.\\nIn 3e Colloque International sur les Grammaires d'Arbres\\n  Adjoints (TAG+3), pages 79-87.\\n\\n\\nRoger Evans, Gerald Gazdar, and David Weir.\\n1995.\\nEncoding lexicalized tree adjoining grammars with a nonmonotonic\\n  inheritance hierarchy.\\nIn Proceedings of the 33rd Annual Meeting of the Association for\\n  Computational Linguistics.\\n\\n\\nGerald Gazdar.\\n1992.\\nParadigm function morphology in DATR.\\nIn Lynne Cahill and Richard Coates, editors, Sussex Papers in\\n  General and Computational Linguistics, number CSRP 239 in Cognitive\\n  Science Research Papers, pages 45-53. University of Sussex, Brighton.\\n\\n\\nDafydd Gibbon and Doris Bleiching.\\n1991.\\nAn ILEX model for German compound stress in DATR.\\nIn Proceedings of the FORWISS-ASL Workshop on Prosody in\\n  Man-Machine Communication.\\n\\n\\nJames Kilbury.\\n1992.\\nPardigm-based derivational morphology.\\nIn Guenther Goerz, editor, Proceedings of KONVENS 92, pages\\n  159-168. Springer, Berlin.\\n\\n\\nAdam Kilgariff.\\n1993.\\nInheriting verb alternations.\\nIn Proceedings of the 6th Conference of the European Chapter of\\n  the Association for Computational Linguistics, pages 213-221.\\n\\n\\nHagen Langer.\\n1994.\\nReverse queries in DATR.\\nIn Proceedings of the 15th International Conference on\\n  Computational Linguistics, volume II, pages 1089-1095, Kyoto.\\n\\n\\nGraham Russell, Afzal Ballim, John Carroll, and Susan Warwick-Armstrong.\\n1992.\\nA practical approach to multiple default inheritance for\\n  unification-based lexicons.\\nComputational Linguistics, 18(2):311-337.\\n\\n\\nMark Young and Bill Rounds.\\n1993.\\nA logical semantics for nonmonotonic sorts.\\nIn Proceedings of the 31st Annual Meeting of the Association for\\n  Computational Linguistics, pages 209-215.\\n\\nFootnotes\\n\\n  The syntax presented in\\n , permits nodes and paths to stand as local descriptors.  However, these additional forms can be viewed as\\nconventional abbreviations, in the appropriate syntactic context, for\\nnode/path pairs\\n  It is not clear why one would wish to do this anyway, but the\\npossibility is explicitly left open in the original definitions of\\n . \\n  In this and subsequent examples, syntactic objects\\n(e.g.\\n\\n,\\n\\n)\\nare used to stand for their\\nsemantic counterparts under F (i.e. \\n\\n,\\n\\n,\\nrespectively).\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\n Evans and Gazdar , introduced DATR as a simple, non-monotonic language for representing natural language lexicons.\\nAlthough a number of implementations of DATR exist, the full language\\nhas until now lacked an explicit, declarative semantics. This paper\\nrectifies the situation by providing a mathematical semantics for\\nDATR. We present a view of DATR as a language for defining certain\\nkinds of partial functions by cases. The formal model provides a\\ntransparent treatment of DATR's notion of global context. It is shown\\nthat DATR's default mechanism can be accounted for by interpreting\\nvalue descriptors as families of values indexed by paths.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nMany have argued that discourse has a global structure above\\nthe level of individual utterances,\\nand that linguistic phenomena\\nlike prosody,\\ncue phrases, and\\nnominal reference\\nare partly conditioned by and reflect this structure\\n (cf. [,,,   [,,,   [,,,). However, an obstacle\\nto exploiting the relation between global structure and\\nlinguistic devices in natural language systems\\nis that there is too little data\\nabout how they constrain one another.\\nWe have been engaged in a study addressing this gap.\\nIn previous work , we reported on a method for\\nempirically\\nvalidating global discourse units, and on our evaluation of\\nalgorithms to identify these units.  We found significant agreement\\namong naive subjects on a discourse segmentation task, which\\nsuggests that global discourse units have some objective reality.\\nHowever, we also found poor correlation of\\nthree untuned algorithms (based on features of\\nreferential noun phrases, cue words, and pauses, respectively)\\nwith the subjects' segmentations.\\n\\n\\nIn this paper, we discuss two methods for developing segmentation\\nalgorithms using multiple knowledge sources.\\n In section , we give a brief overview of related work and summarize our previous results.\\n In section , we discuss how linguistic features are coded and describe our evaluation.\\n In section , we present our analysis of the errors\\nmade by the best performing untuned algorithm, and a new algorithm that\\nrelies on enriched input features and multiple\\nknowledge sources.\\n In section , we discuss our use of machine learning tools to automatically construct decision trees for segmentation\\nfrom a large set of input features.\\nBoth the hand tuned and automatically derived algorithms improve\\nover our previous algorithms.\\nThe primary benefit of the hand tuning is to identify new input features\\nfor improving performance.  Machine learning tools make it\\nconvenient to perform numerous experiments, to use large feature\\nsets, and to evaluate results using\\ncross-validation.\\nWe discuss the significance of our results and briefly compare the two\\n methods in section . \\n\\n\\n    Discourse Segmentation\\n\\n    Related Work\\n\\n\\nSegmentation has played a significant role in much work on discourse.\\nThe linguistic structure of Grosz and Sidner's \\ntri-partite discourse model\\nconsists of multi-utterance segments\\nwhose hierarchical relations are isomorphic with intentional structure.\\n In other work (e.g., ,), segmental structure is an artifact of coherence relations among\\nutterances, and few if any specific claims are made regarding\\nsegmental structure per se.\\nRhetorical Structure\\nTheory (RST)  is another tradition of defining\\nrelations among utterances, and informs much work in generation.\\nIn addition, recent\\nwork  has addressed the\\nintegration of intentions and rhetorical relations.  Although all of\\nthese approaches have involved detailed analyses of individual\\ndiscourses or representative corpora, we believe there is a need\\nfor more rigorous empirical studies.\\n\\n\\nResearchers\\nhave begun to investigate the ability of humans to agree with one\\nanother on segmentation, and to propose methodologies for quantifying\\ntheir findings.  Several studies have used expert\\ncoders to locally and globally\\nstructure spoken discourse according to the model of Grosz and\\nSidner , including ,. Hearst  asked\\nsubjects to place boundaries between paragraphs of expository texts,\\nto indicate topic changes.  Moser and Moore  had\\nan expert coder assign segments and various segment features and\\nrelations based on RST.\\nTo quantify their findings,\\n these studies use notions of agreement  and/or reliability .\\n\\n\\nBy asking subjects to segment discourse using a non-linguistic\\ncriterion, the correlation of linguistic devices with independently\\nderived segments can then be investigated in a way that avoids\\ncircularity.   Together,  comprise an ongoing study using three corpora: professionally read AP news stories, spontaneous\\nnarrative, and read and spontaneous versions of task-oriented monologues.\\nDiscourse structures are derived from subjects' segmentations,\\nthen statistical measures are used to characterize these\\nstructures in terms of acoustic-prosodic features.\\nGrosz and Hirschberg's work\\nalso used the classification and regression tree system\\n CART  to automatically construct and evaluate decision\\ntrees for classifying\\naspects of discourse structure from intonational feature values.\\nMorris and\\nHirst  structured a set of magazine texts using the\\n theory of , developed a thesaurus-based lexical cohesion algorithm to segment text, then qualitatively compared their\\nsegmentations with the results.\\nHearst \\npresented two implemented segmentation algorithms based on term\\nrepetition, and compared the boundaries produced to the boundaries\\nmarked by at least 3 of 7 subjects, using information retrieval metrics.\\nKozima  had 16 subjects segment a simplified short story,\\ndeveloped an algorithm based on lexical cohesion, and qualitatively\\ncompared the results.  Reynar  proposed an algorithm\\nbased on lexical cohesion in conjunction with a graphical technique, and\\nused information retrieval metrics to evaluate the algorithm's\\nperformance in locating boundaries between concatenated news\\narticles.\\n\\n\\n    Our Previous Results\\n\\n\\nWe have been investigating a corpus of monologues collected\\nand transcribed by Chafe , known as the Pear stories.\\nAs reported in \\nwe first investigated whether units\\nof global structure consisting of sequences of utterances could be\\nreliably identified by naive subjects.\\nWe analyzed linear segmentations of 20 narratives\\nperformed by naive subjects (7 new subjects per narrative), where\\nspeaker intention was the segment criterion.\\nSubjects\\nwere given transcripts, asked to place a new segment boundary between\\n lines (prosodic phrases) wherever the speaker had a new communicative goal, and to briefly\\ndescribe the completed segment.  Subjects were free to assign\\nany number of boundaries.  The qualitative results were that\\nsegments varied in size from 1 to 49 phrases in length (Avg.=5.9),\\nand the rate at which subjects assigned boundaries ranged from\\n5.5% to 41.3%.\\nDespite this variation, we found statistically significant\\nagreement among subjects across all narratives\\non location of segment boundaries (\\n\\n).\\n\\n\\nWe then looked at the predictive power of\\nlinguistic cues for identifying the segment boundaries agreed upon by a\\nsignificant number of subjects.  We used three distinct algorithms\\nbased on the distribution of referential noun phrases, cue\\nwords, and pauses, respectively.  Each algorithm\\n(NP-A, CUE-A, PAUSE-A) was designed to\\nreplicate the subjects' segmentation task\\n(break up a narrative into contiguous segments, with segment\\nbreaks falling between prosodic phrases).\\nNP-A used three\\nfeatures,\\nwhile CUE-A and PAUSE-A\\neach made use of a single feature.\\n The features are a subset of those described in section . \\n\\n\\nTo evaluate how well an algorithm predicted segmental structure, we\\nused the information retrieval (IR) metrics described in\\n section .  As reported in , we also evaluated a simple additive method for combining algorithms in\\nwhich a boundary is proposed if each separate algorithm proposes a\\nboundary.\\nWe tested all pairwise combinations,\\nand the combination of all three\\nalgorithms.\\nNo algorithm or\\ncombination of algorithms performed as well as humans.\\nNP-A performed better than the other unimodal algorithms, and a\\ncombination of NP-A and PAUSE-A performed best.  We felt\\nthat significant improvements could be gained by\\ncombining the input features in more complex ways\\nrather than by simply combining the outputs of independent algorithms.\\n\\n\\n\\n    Methodology\\n\\n  Boundary Classification \\n\\nWe represent each narrative in our corpus as a sequence of potential\\nboundary sites, which occur between prosodic phrases.\\nWe classify a potential boundary site as boundary if it was\\nidentified as such by at least 3 of the 7 subjects in our earlier\\nstudy.  Otherwise it is classified as non-boundary.\\nAgreement\\namong subjects on boundaries was significant at below the .02%\\nlevel for values of j \\n3, where j is the number of subjects (1\\n to 7), on all 20 narratives. \\n\\n\\n Fig.  shows a typical segmentation of one of the narratives in our corpus.  Each line corresponds to a prosodic phrase, and each\\nspace between the lines corresponds to a potential boundary\\nsite.  The bracketed numbers will be explained below.  The boxes in\\nthe figure show the subjects' responses at each\\npotential boundary site, and the resulting boundary classification.\\nOnly 2 of the 7 possible boundary\\nsites are classified as boundary.\\n\\n\\n\\n\\n  Coding of Linguistic Features \\n\\nGiven a narrative of n prosodic phrases, the\\nn-1 potential boundary sites are between each pair of\\nprosodic phrases Piand Pi+1, i from 1 to n-1.\\nEach potential boundary site\\nin our corpus is coded using the set of linguistic features\\n shown in Fig. . \\n\\n\\n\\n\\nValues for the prosodic features are obtained by automatic\\nanalysis of the transcripts, whose conventions\\n are defined in  and illustrated in Fig. : ``.'' and ``?''  indicate sentence-final\\nintonational contours; ``,'' indicates phrase-final but not sentence final\\nintonation;\\n``[X]'' indicates a pause lasting X\\nseconds;  ``..'' indicates a break in timing too short to\\nbe measured.\\nThe features before and after\\ndepend on the final punctuation of the phrases Pi and\\nPi+1, respectively.\\nThe value is `+sentence.final.contour' if ``.''\\nor ``?'', `-sentence.final.contour' if ``,''.  Pause\\nis assigned `true' if Pi+1 begins with [X], `false' otherwise.\\nDuration is assigned X if pause is `true', 0\\notherwise.\\n\\n\\nThe cue phrase features are also obtained by automatic analysis of the\\ntranscripts.  Cue1 is assigned `true' if the first lexical\\nitem in Pi+1 is a member of the set of cue words summarized\\n in .  Word1 is assigned this lexical item if cue1 is true, `NA' (not applicable)\\n otherwise. Cue2 is assigned `true' if cue1 is true and the second lexical item is also a cue word.\\nWord2 is assigned the second lexical item if cue2 is true, `NA' otherwise.\\n\\n\\nTwo of the noun phrase (NP) features are\\nhand-coded, along with functionally independent clauses (FICs),\\n following . The two authors coded independently and merged\\ntheir results.\\nThe third feature, global.pro, is computed from the hand coding.\\nFICs are tensed clauses that are neither\\nverb arguments nor restrictive relatives.\\nIf a new FIC\\n(Cj) begins in prosodic phrase Pi+1, then NPs in Cj are\\ncompared with NPs in previous clauses and the feature values assigned as\\n follows: \\n1.\\ncoref = `+coref' if Cj contains an NP that corefers with an NP in\\nCj-1; else coref = `-coref'\\n2.\\ninfer = `+infer' if Cj contains an NP whose referent can be\\ninferred from an NP in Cj-1 on the basis of a pre-defined set of inference\\nrelations; else infer = `-infer'\\n3.\\nglobal.pro = `+global.pro' if Cj contains a definite pronoun whose\\nreferent is mentioned in a previous clause up to the last boundary\\nassigned by the algorithm; else global.pro = `-global.pro'\\nIf a new FIC is not initiated in Pi+1,\\nvalues for all three features are `NA'.\\n\\n\\n\\n\\nCue-prosody, which encodes a combination of\\nprosodic and cue word features, was motivated by\\nan analysis of IR errors on our training data, as\\n described in section . Cue-prosody is\\n`complex' if:\\n1.\\nbefore = `+sentence.final.contour'\\n2.\\npause = `true'\\n3.\\nAnd either:\\n(a)\\ncue1 = `true', word1 \\n`and'\\n(b)\\ncue1 = `true', word1 = `and',\\ncue2 = `true', word2 \\n`and'\\nElse, cue-prosody has the same values\\nas pause.\\n\\n\\n Fig.  illustrates how the first boundary site in  Fig.  would be coded using the features in  Fig. . \\n\\n\\nThe prosodic and cue phrase features were motivated by previous\\nresults in the literature.  For example, phrases beginning discourse\\nsegments were correlated with preceding pause duration\\nin .  These and other\\n studies (e.g., ) also found it useful to distinguish between sentence and non-sentence final intonational contours.\\nInitial phrase position was correlated with discourse signaling uses\\n of cue words in ; a potential correlation between discourse signaling uses of cue words and adjacency patterns between cue words\\n was also suggested.  Finally,  found that treating cue phrases individually rather than as a class enhanced the results\\n of . \\n\\n\\nPassonneau  examined some of the\\nfew claims relating discourse anaphoric noun phrases to global\\ndiscourse structure in the Pear corpus.\\nResults included an absence of correlation of\\n segmental structure with centering ,, and poor correlation with the contrast between full noun phrases and\\npronouns.\\nAs noted in ,\\nthe NP features largely reflect Passonneau's hypotheses\\nthat adjacent utterances are more likely to contain expressions that\\ncorefer, or that are inferentially linked, if they occur within the\\nsame segment; and that a definite pronoun is more likely than a full\\nNP to refer to an entity that was mentioned in the current segment, if\\nnot in the previous utterance.\\n\\n\\n  Evaluation \\n\\nThe segmentation algorithms presented in the next two\\nsections were developed\\nby examining only a training set of narratives.  The algorithms\\nare then evaluated by examining their performance in predicting\\nsegmentation on a separate test set.  We currently use 10\\nnarratives for training and 5 narratives for testing.\\n(The remaining 5 narratives are reserved for future research.)\\nThe 10 training\\nnarratives\\nrange in length from 51 to 162 phrases (Avg.=101.4), or from 38 to 121\\nclauses (Avg.=76.8).\\nThe 5 test\\nnarratives\\nrange in length from 47 to 113 phrases (Avg.=87.4), or from\\n37 to 101 clauses  (Avg.=69.0).\\nThe ratios of test to training data measured in\\nnarratives, prosodic phrases and clauses, respectively, are 50.0%, 43.1% and\\n44.9%.\\nFor the machine learning algorithm we also estimate\\n performance using cross-validation , as detailed in  Section . \\n\\n\\nTo quantify algorithm performance, we use the information\\n retrieval metrics shown in Fig. . \\n\\n\\n\\n\\nRecall\\n= Precision\\n= \\n\\n\\nFallout\\n= Error\\n= \\nRecall is the ratio of correctly hypothesized boundaries to target\\nboundaries.\\nPrecision is the ratio of hypothesized\\nboundaries that are correct to the total hypothesized boundaries.\\n (Cf. Fig.  for fallout and error.) Ideal behavior would be to identify all and only the target\\nboundaries: the values for\\n b and c in Fig.  would thus both equal 0, representing no errors. The ideal values for recall, precision, fallout, and error are 1, 1, 0, and 0,\\nwhile the worst values are 0, 0, 1, and 1.\\nTo get an intuitive summary of overall performance,\\nwe also sum the deviation of the observed value from the ideal value for each\\nmetric:\\n(1-recall) + (1-precision) + fallout + error.\\nThe summed deviation for perfect performance is thus 0.\\n\\n\\nFinally, to interpret our quantitative results, we use the\\nperformance of our human subjects as a target goal for the performance\\n of our algorithms . Table  shows the average human performance for both the training and test sets of narratives.\\n\\n\\n\\n\\nNote that human performance is basically the same for both sets\\nof narratives. However, two factors prevent this performance from being closer\\nto ideal (e.g., recall and precision of 1).  The first is the wide\\nvariation in the number of boundaries that subjects used, as discussed\\nabove. The second is the inherently fuzzy nature of boundary location.\\nWe discuss this second issue at length in ,\\nand present relaxed IR metrics that penalize near misses less heavily\\n in . \\n\\n\\n\\n    Hand Tuning\\n\\n\\nTo improve performance,\\nwe analyzed the two types of IR errors\\nmade by the original NP\\nalgorithm  on the training data.\\n Type ``b'' errors (cf. Fig. ), mis-classification of non-boundaries, were reduced by\\nchanging the coding features pertaining to clauses and NPs.  Most ``b''\\nerrors correlated with two conditions used in the NP algorithm,\\nidentification of clauses and of inferential links.  The revision led to fewer\\nclauses (more assignments of `NA' for the three NP features) and more\\ninference relations.\\nOne example of a change to clause coding is that formulaic\\nutterances having the structure of clauses, but which function like\\ninterjections, are no longer recognized as independent clauses.\\nThese include the phrases let's see, let me see, I\\ndon't know, you know when they occur with no verb phrase argument.\\nOther changes pertained to sentence fragments, unexpected clausal\\narguments, and embedded speech.\\n\\n\\nThree types of inference relations linking successive clauses\\n(Ci-1, Ci) were added (originally there were 5\\n types ). Now, a pronoun (e.g., it, that, this) in Ci referring to an\\naction, event or fact inferrable from Ci-1 links the two clauses.\\n So does an implicit argument, as in Fig. , where the missing argument of notice is inferred to be the event of the pears\\nfalling.\\nThe third case is where an NP in Ci is described as\\npart of an event that results directly from an event mentioned in\\nCi-1.\\n\\n\\n\\n\\n ``C'' type errors (cf. Fig. ), mis-classification of boundaries, often occurred where prosodic and cue features conflicted with NP\\nfeatures.  The original NP algorithm assigned boundaries wherever the\\nthree values `-coref', `-infer', `-global.pro' (defined in\\n section ) co-occurred, represented as the first conditional  statement of Fig. .  Experiments led to the hypothesis that the most improvement came by assigning a boundary if the cue-prosody feature had the value `complex', even if the algorithm\\n would not otherwise assign a boundary, as shown in Fig. . \\n\\n\\n\\n\\n\\n\\n\\n\\nWe refer to the original NP algorithm applied to the initial coding as\\nCondition 1, and the tuned algorithm applied to the enriched coding\\n as Condition 2.  Table  presents the average IR scores across the narratives in the training set for both conditions.\\nReduction of ``b'' type errors\\nraises precision, and lowers fallout and error rate.  Reduction of ``c''\\ntype errors raises recall, and lowers fallout and error rate.  All scores\\nimprove in Condition 2, with precision and fallout\\nshowing the greatest relative improvement.\\nThe major difference from human performance is relatively poorer precision.\\n\\n\\nThe standard\\n deviations in Table  are often close to 1/4 or 1/3 of the reported averages.\\nThis indicates a large amount\\nof variability in the data, reflecting wide differences across\\nnarratives (speakers) in the training set\\nwith respect to the distinctions recognized by the\\nalgorithm.  Although the high standard\\ndeviations show that the tuned algorithm is not well fitted to each\\nnarrative, it is likely that it is overspecialized to the training sample\\nin the sense that test narratives are likely to exhibit further\\nvariation.\\n\\n\\n Table  shows the results of the hand tuned algorithm on the 5 randomly selected test narratives on both Conditions 1 and 2.\\nCondition 1 results, the untuned algorithm with the initial feature set,\\nare very similar to the training set except for worse precision.\\nThus, despite the high standard deviations, 10 narratives seems to have\\nbeen a sufficient sample size for evaluating the initial NP algorithm.\\n Condition 2 results are better than condition 1 in Table .  This is strong evidence that the tuned algorithm is a better predictor of segment\\nboundaries than the original NP algorithm.  Nevertheless, the test\\nresults of condition 2 are much worse than the corresponding training results,\\nparticularly for precision (.44 versus .62). This confirms that the\\ntuned algorithm is over calibrated to the training set.\\n\\n\\n    Machine Learning\\n\\n\\n We use the machine learning program C4.5  to automatically develop segmentation algorithms\\nfrom our corpus of coded\\nnarratives, where each potential boundary site has been classified and\\nrepresented as a set of linguistic features.  The\\nfirst input to C4.5 specifies the names of the classes to be learned\\n(boundary and non-boundary), and the names and potential\\n values of a fixed set of coding features (Fig. ).  The second input is the training data, i.e., a set of examples for which\\n the class and feature values (as in Fig. ) are specified.  Our training set of 10 narratives provides 1004 examples\\nof potential boundary sites. The output of C4.5 is a classification\\nalgorithm expressed as a decision tree, which predicts the class of a\\npotential boundary given its set of feature values.\\n\\n\\nBecause machine learning makes it convenient to\\ninduce decision trees under a wide variety of conditions, we\\nhave performed numerous experiments, varying\\nthe number of features used to code the training data, the definitions used for\\nclassifying a potential boundary site as boundary or\\n non-boundary and the options available for running the C4.5 program.\\n Fig.  shows one of the highest-performing learned decision trees from\\nour experiments.\\n\\n\\n\\n\\nThis decision tree was learned under the following\\n conditions: all of the features shown in Fig.  were used to code the training data, boundaries were classified as discussed in\\n section , and C4.5 was run using only the default options. The decision tree predicts the class of a potential boundary site\\nbased on the features before, after, duration, cue1, word1, coref, infer, and global.pro.  Note that although not all available features are used\\nin the tree, the included features represent 3 of the 4 general types\\nof knowledge (prosody, cue phrases and noun phrases).  Each level of\\nthe tree specifies a test on a single feature, with a branch for every\\n possible outcome of the test.   A branch can either lead to the assignment of a class, or to another test.\\nFor example, the tree\\ninitially branches based on the value of the feature before.  If\\nthe value is `-sentence.final.contour' then the first branch is taken\\nand the potential boundary site is assigned the class non-boundary.  If the value of before is\\n`+sentence.final.contour' then the second branch is taken and the\\nfeature coref is tested.\\n\\n\\nThe performance of this learned decision tree averaged over the 10\\n training narratives is shown in Table , on the line labeled ``Learning 1''.  The line labeled ``Learning 2'' shows the results\\nfrom\\nanother machine learning experiment, in which one of the default\\nC4.5 options used in ``Learning 1'' is overridden.\\nThe ``Learning 2'' tree (not shown due to space restrictions) is more\\n complex than the tree of Fig. , but has slightly better performance.  Note that ``Learning 1'' performance\\n is comparable to human performance (Table ), while ``Learning 2'' is slightly better than humans.\\nThe results obtained via\\nmachine learning are also somewhat better than the results obtained\\nusing hand tuning--particularly with respect to precision (``Condition 2''\\n in Table ). \\n\\n\\nThe performance of the learned decision trees averaged over the 5 test\\n narratives is shown in Table  shows that, as with the hand tuning results (and as expected), average performance is worse when\\napplied to the testing rather than the training data particularly with\\nrespect to precision.  However, performance is an improvement over our\\nprevious best\\nresults (``Condition 1'' in\\n Table ), and is comparable to (``Learning 1'') or very slightly\\nbetter than (``Learning 2'') the hand tuning results (``Condition 2'' in\\n Table ). \\n\\n\\n We also use the resampling method of cross-validation  to estimate performance, which averages results over multiple\\npartitions of a sample into test versus training data.\\nWe performed 10 runs of the learning program, each using 9 of the 10\\ntraining narratives for that run's training set (for learning the\\ntree) and the remaining narrative for testing.  Note that for each iteration of the cross-validation, the learning process begins\\nfrom scratch and thus each training and testing set are still\\ndisjoint.  While this method does not make sense for humans, computers\\ncan truly ignore previous iterations.\\nFor sample sizes in the hundreds (our\\n10 narratives provide 1004 examples) 10-fold cross-validation often\\nprovides a better performance estimate than the hold-out\\n method .  Results using cross-validation are shown in Table , and are better than the estimates obtained using the hold-out\\n method (Table ), with the major improvement coming from precision.  Because a different tree is learned on\\neach iteration, the cross-validation evaluates the learning method,\\nnot a particular decision tree.\\n\\n\\n\\n\\n\\n\\n\\n\\n    Conclusion\\n\\n\\nWe have presented two methods for developing segmentation\\nhypotheses using multiple linguistic features.  The first method\\nhand tunes features and algorithms\\nbased on analysis of training errors.\\nThe second method,\\nmachine learning, automatically induces decision trees from\\ncoded corpora.\\nBoth methods rely on an\\nenriched set of input features compared to our previous work.  With\\neach method, we have achieved marked improvements in performance\\ncompared to our previous work and are approaching human performance.\\nNote that quantitatively, the machine learning results are slightly\\nbetter than the hand tuning results.  The main difference on average\\nperformance is the higher precision of the automated algorithm.\\nFurthermore, note that the machine learning algorithm used the changes\\nto the coding features that resulted from the tuning methods.  This\\nsuggests that hand tuning is a useful method for understanding how to\\nbest code the data, while machine learning provides an effective (and\\nautomatic) way to produce an algorithm given a good feature\\nrepresentation.\\n\\n\\nOur results lend further support to the hypothesis that linguistic devices\\n correlate with discourse structure (cf.  section ), which itself has practical import.  Understanding systems could\\ninfer segments as a step towards producing summaries,\\nwhile generation systems could signal segments to increase\\n comprehensibility. Our results also suggest that to best identify or convey segment\\nboundaries, systems will need to exploit multiple signals\\nsimultaneously.\\n\\n\\nWe plan to continue our experiments by further merging the automated\\nand analytic techniques, and evaluating new algorithms on our final test\\ncorpus.  Because we have already used cross-validation, we do not\\nanticipate significant degradation on new test narratives.  An important\\narea for future research is to develop principled methods\\nfor identifying distinct speaker strategies pertaining to\\nhow they signal segments.  Performance of\\nindividual speakers varies widely as shown by the high standard\\ndeviations in our tables.  The original NP,\\nhand tuned, and machine learning algorithms all do relatively\\npoorly on narrative 16 and relatively well on 11 (both in the test set)\\nunder all conditions.\\nThis lends support to the hypothesis that there may be consistent\\ndifferences among speakers regarding strategies for\\nsignaling shifts in global discourse structure.\\n\\nBibliography \\n\\nLeo Breiman, Jerome Friedman, Richard Olshen, and C. Stone.\\n1984.\\nClassification and Regression Trees.\\nWadsworth and Brooks, Monterey, CA.\\n\\n\\nWallace L. Chafe.\\n1980.\\nThe Pear Stories.\\nAblex Publishing Corporation, Norwood, NJ.\\n\\n\\nWilliam Gale, Ken W. Church, and David Yarowsky.\\n1992.\\nEstimating upper and lower bounds on the performance of word-sense\\n  disambiguation programs.\\nIn Proc. of the 30th ACL, pages 249-256.\\n\\n\\nBarbara Grosz and Julia Hirschberg.\\n1992.\\nSome intonational characteristics of discourse structure.\\nIn Proc. of the International Conference on Spoken Language\\n  Processing.\\n\\n\\nBarbara Grosz and Candace Sidner.\\n1986.\\nAttention, intentions and the structure of discourse.\\nComputational Linguistics, 12:175-204.\\n\\n\\nBarbara J. Grosz, Aaravind K. Joshi, and Scott Weinstein.\\n1983.\\nProviding a unified account of definite noun phrases in discourse.\\nIn Proc. of the 21st ACL, pages 44-50.\\n\\n\\nMarti A. Hearst.\\n1994.\\nMulti-paragraph segmentation of expository text.\\nIn Proc. of the 32nd ACL.\\n\\n\\nJulia Hirschberg and Barbara Grosz.\\n1992.\\nIntonational features of local and global discourse structure.\\nIn Proc. of the Darpa Workshop on Spoken Language.\\n\\n\\nJulia Hirschberg and Diane Litman.\\n1993.\\nEmpirical studies on the disambiguation of cue phrases.\\nComputational Linguistics, 19(3):501-530.\\n\\n\\nJulia Hirschberg and Janet Pierrehumbert.\\n1986.\\nThe intonational structuring of discourse.\\nIn Proc. of the 24th ACL.\\n\\n\\nJerry R. Hobbs.\\n1979.\\nCoherence and coreference.\\nCognitive Science, 3(1):67-90.\\n\\n\\nAmy Isard and Jean Carletta.\\n1995.\\nReplicability of transaction and action coding in the map task\\n  corpus.\\nIn AAAI 1995 Spring Symposium Series: Empirical Methods in\\n  Discourse Interpretation and Generation, pages 60-66.\\n\\n\\nMegumi Kameyama.\\n1986.\\nA property-sharing constraint in centering.\\nIn Proc. of the 24th ACL, pages 200-206.\\n\\n\\nH. Kozima.\\n1993.\\nText segmentation based on similarity between words.\\nIn Proc. of the 31st ACL (Student Session), pages 286-288.\\n\\n\\nAlex Lascarides and Jon Oberlander.\\n1992.\\nTemporal coherence and defeasible knowledge.\\nTheoretical Linguistics.\\n\\n\\nCharlotte Linde.\\n1979.\\nFocus of attention and the choice of pronouns in discourse.\\nIn Talmy Givon, editor, Syntax and Semantics: Discourse and\\n  Syntax, pages 337-354. Academic Press, New York.\\n\\n\\nDiane J. Litman and Rebecca J. Passonneau.\\n1995.\\nDeveloping algorithms for discourse segmentation.\\nIn AAAI 1995 Spring Symposium Series: Empirical Methods in\\n  Discourse Interpretation and Generation, pages 85-91.\\n\\n\\nDiane J. Litman.\\n1994.\\nClassifying cue phrases in text and speech using machine learning.\\nIn Proc. of the 12th AAAI, pages 806-813.\\n\\n\\nWilliam C. Mann and Sandra Thompson.\\n1988.\\nRhetorical structure theory.\\nTEXT, pages 243-281.\\n\\n\\nJohanna D. Moore and Cecile Paris.\\n1993.\\nPlanning text for advisory dialogues: Capturing intentional and\\n  rhetorical information.\\nComputational Linguistics, 19:652-694.\\n\\n\\nJohanna D. Moore and Martha E. Pollack.\\n1992.\\nA problem for RST: The need for multi-level discourse analysis.\\nComputational Linguistics, 18:537-544.\\n\\n\\nJane Morris and Graeme Hirst.\\n1991.\\nLexical cohesion computed by thesaural relations as an indicator of\\n  the structure of text.\\nComputational Linguistics, 17:21-48.\\n\\n\\nMegan Moser and Julia D. Moore.\\n1995.\\nUsing discourse analysis and automatic text generation to study\\n  discourse cue usage.\\nIn AAAI 1995 Spring Symposium Series: Empirical Methods in\\n  Discourse Interpretation and Generation, pages 92-98.\\n\\n\\nChristine H. Nakatani, Julia Hirschberg, and Barbara J. Grosz.\\n1995.\\nDiscourse structure in spoken language: Studies on speech corpora.\\nIn AAAI 1995 Spring Symposium Series: Empirical Methods in\\n  Discourse Interpretation and Generation, pages 106-112.\\n\\n\\nRebecca J. Passonneau and Diane J. Litman.\\n1993.\\nIntention-based segmentation: Human reliability and correlation with\\n  linguistic cues.\\nIn Proc. of the 31st ACL, pages 148-155.\\n\\n\\nRebecca J. Passonneau and D. Litman.\\nto appear.\\nEmpirical analysis of three dimensions of spoken discourse.\\nIn E. Hovy and D. Scott, editors, Interdisciplinary Perspectives\\n  on Discourse. Springer Verlag, Berlin.\\n\\n\\nRebecca J. Passonneau.\\n1994.\\nProtocol for coding discourse referential noun phrases and their\\n  antecedents.\\nTechnical report, Columbia University.\\n\\n\\nRebecca J. Passonneau.\\nto appear.\\nInteraction of the segmental structure of discourse with explicitness\\n  of discourse anaphora.\\nIn E. Prince, A. Joshi, and M. Walker, editors, Proc. of the\\n  Workshop on Centering Theory in Naturally Occurring Discourse. Oxford\\n  University Press.\\n\\n\\nLivya Polanyi.\\n1988.\\nA formal model of discourse structure.\\nJournal of Pragmatics, pages 601-638.\\n\\n\\nJohn R. Quinlan.\\n1993.\\nC4.5 : Programs for Machine Learning.\\nMorgan Kaufmann Publishers, San Mateo, Calif.\\n\\n\\nRachel Reichman.\\n1985.\\nGetting Computers to Talk Like You and Me: Discourse Context,\\n  Focus, and Semantics.\\nBradford. MIT, Cambridge.\\n\\n\\nJ. C. Reynar.\\n1994.\\nAn automatic method of finding topic boundaries.\\nIn Proc. of the 32nd ACL (Student Session), pages 331-333.\\n\\n\\nLisa J. Stifleman.\\n1995.\\nA discourse analysis approach to structured speech.\\nIn AAAI 1995 Spring Symposium Series: Empirical Methods in\\n  Discourse Interpretation and Generation, pages 162-167.\\n\\n\\nBonnie L. Webber.\\n1991.\\nStructure and ostension in the interpretation of discourse deixis.\\nLanguage and Cognitive Processes, pages 107-135.\\n\\n\\nSholom M. Weiss and Casimir Kulikowski.\\n1991.\\nComputer systems that learn: classification and prediction\\n  methods from statistics, neural nets, machine learning, and expert systems.\\nMorgan Kaufmann.\\n\\nFootnotes\\n\\n  Bellcore did not support the second author's\\nwork.\\n  We used Chafe's \\nprosodic analysis.\\n  We previously used\\nagreement by 4 subjects as the threshold for boundaries; for j , agreement was significant at the .01%\\nlevel. \\n  The cue phrases that occur in the corpus are\\n shown as potential values in Fig. . \\n  The NP algorithm can assign multiple boundaries\\nwithin one prosodic phrase if the phrase contains multiple clauses;\\nthese very rare cases are\\nnormalized [].\\n    varies the number of subjects used to\\ndetermine boundaries.\\n  The actual tree branches on every\\nvalue of word1; the figure merges these branches\\nfor clarity.\\n  Cf.  who\\nargue that\\ncomprehensibility improves if units are prosodically signaled.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe predict discourse segment boundaries from linguistic\\nfeatures of utterances, using a corpus of spoken narratives as data.\\nWe present two methods for developing segmentation algorithms from\\ntraining data: hand tuning and machine learning.  When multiple types\\nof features are used, results approach human\\nperformance on an independent test set (both\\nmethods), and using cross-validation (machine learning).\\n\\n'],\n",
              " ['\\n\\n  Introduction \\n\\nThe Tree Adjoining Grammar (TAG) formalism was first introduced two\\n decades ago , and since then there has been a steady stream of theoretical work using the formalism. But it is only more recently\\nthat grammars of non-trivial size have been developed: Abeille, Bishop,\\nCote  Schabes  describe a feature-based Lexicalized\\nTree Adjoining Grammar (LTAG) for English which subsequently became the\\n basis for the grammar used in the XTAG system, a wide-coverage LTAG parser ,,. The advent of such large grammars gives rise to questions of efficient representation, and the fully\\nlexicalized character of the LTAG formalism suggests that recent\\nresearch into lexical representation might be a place to look for\\nanswers (see for example Briscoe et al. ;\\nDaelemans  ). In this paper we explore this\\nsuggestion by showing how the lexical knowledge representation\\n language ( LKRL) DATR , can be used to formulate a compact, hierarchical encoding of an LTAG.\\n\\n\\n The issue of efficient representation for LTAG is discussed by Vijay-Shanker  Schabes , who draw attention to the\\nconsiderable redundancy inherent in LTAG lexicons that are expressed\\nin a flat manner with no sharing of structure or properties across the\\nelementary trees. For example, XTAG currently includes over 100,000\\nlexemes, each of which is associated with a family of trees (typically\\naround 20) drawn from a set of over 500 elementary trees.  Many of these\\ntrees have structure in common, many of the lexemes have the same tree\\nfamilies, and many of the trees within families are systematically\\nrelated in ways which other formalisms capture using transformations or\\nmetarules.  However, the LTAG formalism itself does not provide any\\ndirect support for capturing such regularities.\\n\\n\\nVijay-Shanker  Schabes address this problem by introducing a\\nhierarchical lexicon structure with monotonic inheritance and lexical\\nrules, using an approach loosely based on that of\\nFlickinger  but tailored for LTAG trees rather than\\nHPSG subcategorization lists. Becker , \\nproposes a slightly different solution, combining an inheritance\\n component and a set of metarules. We share their perception of the problem and agree that adopting a hierarchical approach provides\\nthe best available solution to it. However, rather than creating a\\nhierarchical lexical formalism that is specific to the LTAG problem,\\nwe have used DATR, an  LKRL that is already quite widely known and\\nused. From an LTAG perspective, it makes sense to use an already\\navailable  LKRL that was specifically designed to address these kinds\\nof representational issues. From a DATR perspective, LTAG presents\\ninteresting problems arising from its radically lexicalist character:\\nall grammatical relations, including unbounded dependency\\nconstructions, are represented lexically and are thus open to lexical\\ngeneralization.\\n\\n\\nThere are also several further benefits to be gained from using an\\nestablished general purpose  LKRL such as DATR. First, it makes it\\neasier to compare the resulting LTAG lexicon with those associated\\nwith other types of lexical syntax: there are existing DATR lexicon\\nfragments for HPSG, PATR and Word Grammar, among others. Second,\\nDATR is not restricted to syntactic description, so one can take\\nadvantage of existing analyses of other levels of lexical description,\\nsuch as phonology, prosody, morphology, compositional semantics and\\n lexical semantics. Third, one can exploit existing formal  and implementation work on the language. \\n\\n\\n  Representing LTAG trees \\n\\nThe principal unit of (syntactic) information associated with an LTAG entry is a tree structure in which the tree nodes are labeled with\\nsyntactic categories and feature information and there is at least one\\nleaf node labeled with a lexical category (such lexical leaf\\nnodes are known as anchors). For example, the canonical tree for\\n a ditransitive verb such as give is shown in figure . Following LTAG conventions (for the time being), the node labels here\\nare gross syntactic category specifications to which additional featural\\n information may be added, and are annotated to indicate node type: \\nindicates an anchor\\nnode, and \\n\\n\\nindicates a substitution node (where a fully\\nspecified tree with a compatible root label may be\\n attached). \\n\\n\\nIn representing such a tree in DATR, we do two things. First, in\\nkeeping with the radically lexicalist character of LTAG, we describe\\n the tree structure from its (lexical) anchor upwards, using a variant of Kilbury\\'s  bottom-up encoding of trees.  In this\\nencoding, a tree is described relative to a particular distinguished\\nleaf node (here the anchor node), using binary relations parent,\\nleft and right, relating the node to the subtrees associated with\\nits parent, and immediate-left and -right sisters, encoded in the same\\nway. Second, we embed the resulting tree structure (i.e., the node\\nrelations and type information) in the feature structure, so that the\\ntree relations (left, right and parent) become features.  The\\nobvious analogy here is the use of first/rest features to\\nencode subcategorisation lists in frameworks like HPSG.\\n\\n\\nThus the syntactic feature information directly associated with the\\nentry for give relates to the label for the  V node (for example,\\nthe value of its cat feature is v, the value of type is anchor),\\nwhile specifications of subfeatures of parent relate to the label of\\nthe  VP node. A simple bottom-up DATR representation for the whole tree\\n(apart from the node type information) follows:\\nGive:\\n    [cat] = v\\n    [parent cat] = vp\\n    [parent left cat] = np\\n    [parent parent cat] = s\\n    [right cat] = np\\n    [right right cat] = p\\n    [right right parent cat] = pp\\n    [right right right cat] = np.\\n This says that Give is a verb, with  VP as its parent, an  S as its grandparent and an  NP to the left of its parent. It also has an\\n NP to its right, and a tree rooted in a  P to the right of that,\\nwith a  PP parent and  NP right sister.  The implied bottom-up tree\\n structure is shown graphically in figure .  Here the nodes are  laid out just as in figure , but related via parent, left and right links, rather than the more usual (implicitly ordered) daughter links. Notice in particular that the right link from the\\nobject noun-phrase node points to the preposition node, not its\\nphrasal parent - this whole subtree is itself encoded\\nbottom-up. Nevertheless, the full tree structure is completely and\\naccurately represented by this encoding.\\n\\n\\nOnce we adopt this representational strategy, writing an LTAG lexicon in DATR becomes similar to writing any other type\\nof lexicalist grammar\\'s lexicon in an inheritance-based  LKRL. In HPSG,\\nfor example, the subcategorisation frames are coded as lists of\\ncategories, whilst in LTAG they are coded as trees. But, in both cases,\\nthe problem is one of concisely describing feature structures\\nassociated with lexical entries and relationships between lexical\\nentries. The same kinds of generalization arise\\nand the same techniques are applicable.\\nOf course, the presence of complete trees and the fully\\nlexicalized approach provide scope for capturing generalizations\\nlexically that are not available to approaches that only identify\\nparent and sibling nodes, say, in the lexical entries.\\n\\n\\n    Encoding lexical entries\\n\\n\\nFollowing conventional models of lexicon organisation, we would expect\\nGive to have a minimal syntactic specification itself, since\\nsyntactically it is\\na completely regular ditransitive verb. In fact none of the\\ninformation introduced so far is specific to Give. So rather than\\nproviding a completely explicit DATR definition for Give, as we\\ndid above, a more plausible account uses an inheritance hierarchy\\ndefining abstract intransitive, transitive and ditransitive verbs to\\n support Give (among others), as shown in figure . \\n\\n\\nThis basic organisational structure can be expressed as the following\\n DATR fragment: VERB:\\n    [] == TREENODE\\n    [cat] == v\\n    [type] == anchor\\n    [parent] == VPTREE:[].\\nVERB+NP:\\n    [] == VERB\\n    [right] == NPCOMP:[].\\nVERB+NP+PP:\\n    [] == VERB+NP\\n    [right right] == PTREE:[]\\n    [right right root] == to.\\nVERB+NP+NP:\\n    [] == VERB+NP\\n    [right right] == NPCOMP:[].\\nDie:\\n    [] == VERB\\n    [root] == die.\\nEat:\\n    [] == VERB+NP\\n    [root] == eat.\\nGive:\\n    [] == VERB+NP+PP\\n    [root] == give.\\nSpare:\\n    [] == VERB+NP+NP\\n    [root] == spare.\\nIgnoring for the moment the references to TREENODE, VPTREE, NPCOMP\\nand PTREE (which we shall define shortly), we see that VERB defines basic features for all verb entries (and can be used directly\\nfor intransitives such as Die), VERB+NP inherits from VERB but adds an  NP complement to the right of the verb (for transitives),\\nVERB+NP+PP inherits from VERB+NP but adds a further PP complement\\nand so on. Entries for regular verb lexemes are then minimal -\\nsyntactically they just inherit everything from the abstract\\ndefinitions.\\n\\n\\nThis DATR fragment is incomplete, because it neglects to define the\\ninternal structure of the TREENODE and the various\\nsubtree nodes in the lexical hierarchy.\\nEach such node is a\\ndescription of an LTAG tree at some degree of\\n abstraction. The following DATR statements complete the fragment, by providing definitions for this internal\\nstructure:\\nTREENODE:\\n    [] == undef\\n    [type] == internal.\\nSTREE:\\n    [] == TREENODE\\n    [cat] == s.\\nVPTREE:\\n    [] == TREENODE\\n    [cat] == vp\\n    [parent] == STREE:[]\\n    [left] == NPCOMP:[].\\nNPCOMP:\\n    [] == TREENODE\\n    [cat] == np\\n    [type] == substitution.\\nPPTREE:\\n    [] == TREENODE\\n    [cat] == pp.\\nPTREE:\\n    [] == TREENODE\\n    [cat] ==  p\\n    [type] == anchor\\n    [parent] == PPTREE:[]\\nHere, TREENODE represents an abstract node in an LTAG tree and\\nprovides a (default) type of internal. Notice that VERB is\\nitself a TREENODE (but with the nondefault type anchor),\\nand the other definitions here define the remaining tree nodes that\\narise in our small lexicon: VPTREE is the node for VERB\\'s\\nparent, STREE for VERB\\'s grandparent, NPCOMP defines the\\n structure needed for  NP complement substitution nodes, etc. \\n\\n\\nTaken together, these definitions provide a specification for Give\\njust as we had it before, but with the addition of type and root\\nfeatures. They also support some other verbs too, and it should be\\nclear that the basic technique extends readily to a wide range of other\\nverbs and other parts of speech. Also, although the trees we have\\ndescribed are all initial trees (in LTAG terminology), we can\\ndescribe auxiliary trees, which include a leaf node of type\\nfoot just as easily. A simple example is provided by the following\\ndefinition for auxiliary verbs:\\nAUXVERB:\\n    [] == TREENODE\\n    [cat] == v\\n    [type] == anchor\\n    [parent cat] == vp\\n    [right cat] == vp\\n    [right type] == foot.\\n\\n\\n  Lexical rules \\n\\nHaving established a basic structure for our LTAG lexicon, we now turn\\nour attention towards capturing other kinds of relationship among trees.\\nWe noted above that lexical entries are actually associated with\\ntree families, and that these group together trees that\\nare related to each other. Thus in the same family as a standard\\nditransitive verb, we might find the full passive, the agentless\\npassive, the dative alternation, the various relative clauses, and so\\nforth. It is clear that these families correspond closely to the outputs\\nof transformations or metarules in other frameworks, but the XTAG system currently has no formal component for describing the\\nrelationships among families nor mechanisms for generating them. And so\\nfar we have said nothing about them either - we have only characterized\\nsingle trees.\\n\\n\\nHowever, LTAG\\'s large domain of locality means that all such\\nrelationships can be viewed as directly lexical, and thus expressible\\nby lexical rules.  In fact we can go further than this: because we\\nhave embedded the domain of these lexical rules, namely the LTAG tree structures, within the feature structures, we can view such\\nlexical rules as covariation constraints within feature structures,\\nin much the same way that the covariation of, say, syntactic and\\nmorphological form is treated. In particular, we can use the\\nmechanisms that DATR already provides for feature covariation,\\nrather than having to invoke in addition some special purpose lexical\\nrule machinery.\\n\\n\\nWe consider six construction types found in the XTAG grammar: passive, dative, subject-auxiliary inversion, wh-questions, relative clauses and topicalisation. Our basic\\napproach to each of these is the same. Lexical rules are specified by\\ndefining a derived output tree structure in terms of an input tree structure, where each of these structures is a set of\\nfeature specifications of the sort defined above. Each lexical rule\\nhas a name, and the input and output tree structures for rule foo are referenced by prefixing feature paths of the sort given\\nabove with [input foo ..] or [output foo ..]. So for\\nexample, the category of the parent tree node of the output of the\\npassive rule might be referenced as [output passive parent\\ncat]. We define a very general default, stating that the output is the same as the input, so that lexical relationships\\nneed only concern themselves with components they modify.  This\\napproach to formulating lexical rules in DATR is quite general and\\nin no way restricted to LTAG: it can be readily adapted for\\napplication in the context of any feature-based lexicalist grammar\\nformalism.\\n\\n\\nUsing this approach, the dative lexical rule can be given a\\nminimalist implementation by the addition of the following single\\nline to VERB+NP+PP, defined above.\\nVERB+NP+PP:\\n    [output dative right right] == NPCOMP:[].\\nThis causes the second complement to a ditransitive verb in the dative\\nalternation to be an  NP, rather than a  PP as in the unmodified case.\\nSubject-auxiliary inversion can be achieved similarly by just specifying\\nthe output tree structure without reference to the input\\nstructure (note the addition here of a form feature specifying\\nverb form):\\nAUXVERB:\\n    [output auxinv form] == finite-inv\\n    [output auxinv parent cat] == s\\n    [output auxinv right cat] == s.\\n\\n\\nPassive is slightly more complex, in that it has to modify the given\\ninput tree structure rather than simply overwriting part of it.\\nThe definitions for passive occur at the VERB+NP node, since by\\ndefault, any transitive or subclass of transitive has a passive form.\\nIndividual transitive verbs, or whole subclasses, can override\\nthis default, leaving their passive tree structure undefined if\\nrequired. For agentless passives, the necessary additions to the\\n VERB+NP node are as follows: VERB+NP:\\n    [output passive form] == passive\\n    [output passive right] ==\\n            \"[input passive right right]\".\\nHere, the first line stipulates the form of the verb in the output tree\\nto be passive, while the second line redefines the complement structure:\\nthe output of passive has as its first complement the second\\ncomplement of its input, thereby discarding the first complement\\nof its input. Since complements are daisy-chained, all the\\nothers move up too.\\n\\n\\nWh-questions, relative clauses and topicalisation are slightly\\ndifferent, in that the application of the lexical rule causes structure\\nto be added to the top of the tree (above the  S node). Although\\nthese constructions involve unbounded dependencies, the unboundedness is\\ntaken care of by the LTAG adjunction mechanism: for lexical purposes\\nthe dependency is local. Since the relevant lexical rules can apply to\\nsentences that contain any kind of verb, they need to be stated at the\\nVERB node. Thus, for example, topicalisation and wh-questions\\ncan be defined as follows:\\nVERB:\\n    [output topic parent parent parent cat]\\n                                           == s\\n    [output topic parent parent left cat] == np\\n    [output topic parent parent left form]\\n                                      == normal\\n    [output whq] == \"[output topic]\"\\n    [output whq parent parent left form] == wh.\\nHere an additional  NP and  S are attached above the original  S node\\nto create a topicalised structure. The wh-rule inherits from the\\ntopicalisation rule, changing just one thing: the form of the new\\n NP is marked as wh, rather than as normal.\\n In the full fragment, the  NP added by these rules is also syntactically cross-referenced to\\na specific  NP marked as null in the input tree.\\nHowever, space does not permit presentation or discussion of the\\nDATR code that achieves this here.\\n\\n\\n  Applying lexical rules \\n\\nAs explained above, each lexical rule is defined to operate on its own\\nnotion of an input and produce its own output. In order for\\nthe rules to have an effect, the various input and output\\npaths have to be linked together using inheritance, creating a chain of\\ninheritances between the base, that is, the canonical definitions\\n we introduced in section , and surface tree structures of the lexical entry. For example, to `apply\\' the dative rule\\nto our Give definition, we could construct a definition such as\\nthis:\\nGive-dat:\\n    [] == Give\\n    [input dative] == []\\n    [surface] == [output dative].\\nValues for paths prefixed with surface inherit from the output of\\nthe dative rule. The input of the dative rule inherits from the base\\n(unprefixed) case, which inherits from Give. The dative rule\\ndefinition (just the one line introduced above, plus the default that\\noutput inherits from input) thus mediates between Give and the\\nsurface of Give-dat. This chain can be\\nextended by inserting additional inheritance specifications (such as\\npassive). Note that surface defaults to the base\\ncase, so all entries have a surface defined.\\n\\n\\nHowever, in our full fragment, additional support is provided to achieve\\nand constrain this rule chaining. Word definitions include boolean\\nfeatures indicating which rules to apply, and the presence of these\\nfeatures trigger inheritance between appropriate input and output paths and the base and surface specifications at the\\nends of the chain. For example, Word1 is an alternative way of\\nspecifying the dative alternant of Give, but results in\\ninheritance linking equivalent to that found in Give-dat above:\\nWord1:\\n    [] == Give\\n    [alt dative] == true.\\nMore interestingly, Word2 properly describes a wh-question based on the agentless passive of the dative of Give.\\n Word2:\\n    [] == Give\\n    [alt whq] == true\\n    [alt dative] == true\\n    [alt passive] == true.\\n    [parent left form] == null\\nNotice here the final line of Word2 which specifies the location\\nof the `extracted\\'  NP (the subject, in this case), by marking it\\nas null. As noted above, the full version of the whq lexical\\nrule uses this to specify a cross-reference relationship between the\\nwh- NP and the null  NP.\\n\\n\\nWe can, if we wish, encode constraints on the applicability of\\nrules in the mapping from boolean flags to actual inheritance\\nspecifications. Thus, for example, whq, rel, and topic are\\nmutually exclusive.  If such constraints are violated, then no value for\\nsurface gets defined.\\nThus Word3 improperly attempts topicalisation in addition to wh-question formation, and, as a result, will fail to define a surface tree structure at all:\\n Word3:\\n    [] == Give\\n    [alt whq] == true\\n    [alt topic] == true\\n    [alt dative] == true\\n    [alt passive] == true\\n    [parent left form] == null.\\n\\n\\nThis approach to lexical rules allows them to be specified at the\\nappropriate point in the lexical hierarchy, but overridden or\\nmodified in subclasses or lexemes as appropriate. It also allows\\ndefault generalisation over the lexical rules themselves, and control\\nover their application. The last section showed how the whq\\nlexical rule could be built by a single minor addition to that for\\ntopicalisation. However, it is worth noting that, in common with\\nother DATR specifications, the lexical rules presented here are\\nrule instances which can only be applied once to any given\\nlexeme - multiple application could be supported, by making multiple\\ninstances inherit from some common rule specification, but in our\\ncurrent treatment such instances would require different rule names.\\n\\n\\n    Comparison with related work\\n\\n\\nAs noted above, Vijay-Shanker  Schabes  have also\\nproposed an inheritance-based approach to this problem. They use\\nmonotonic inheritance to build up partial descriptions of trees: each\\ndescription is a finite set of dominance, immediate dominance and linear\\nprecedence statements about tree nodes in a tree description language\\ndeveloped by Rogers  Vijay-Shanker , and category\\ninformation is located in the node labels.\\n\\n\\nThis differs from our approach in a number of ways. First, our use of\\nnonmonotonic inheritance allows us to manipulate total instead of\\npartial descriptions of trees. The abstract verb class in the Vijay-Shanker\\n Schabes account subsumes both intransitive and transitive verb classes\\nbut is not identical to either - a minimal-satisfying-model step is\\nrequired to map partial tree descriptions into actual trees.\\nIn our analysis,\\nVERB is the intransitive verb class, with complements\\nspecifically marked as undefined: thus VERB:[right] == undef is\\ninherited from TREENODE and VERB+NP just\\noverrides this complement specification to add an  NP complement.\\nSecond, we describe trees using only local tree relations (between\\nadjacent nodes in the tree), while Vijay-Shanker  Schabes also use a\\nnonlocal dominance relation.\\n\\n\\nBoth these properties are crucial to our embedding of the tree structure\\nin the feature structure. We want the category information at each tree\\nnode to be partial in the conventional sense, so that in actual use such\\ncategories can be extended (by unification or whatever). So the feature\\nstructures that we associate with lexical entries must be viewed as\\npartial. But we do not want the tree structure to be extendible in\\nthe same way: we do not want an intransitive verb to be applicable in a\\ntransitive context, by unifying in a complement  NP. So the tree\\n structures we define must be total descriptions.  And of course, our use of only local relations allows a direct mapping from\\ntree structure to feature path, which would not be possible at all if\\nnonlocal relations were present.\\n\\n\\nSo while these differences may seem small, they allow us to take this\\nsignificant representational step - significant because it is the tree\\nstructure embedding that allows us to view lexical rules as feature\\ncovariation constraints. The result is that while Vijay-Shanker \\nSchabes use a tree description language, a category description\\nlanguage and a further formalism for lexical rules, we can capture\\neverything in one framework all of whose components (nonmonotonicity,\\ncovariation constraint handling, etc.) have already been independently\\n motivated for other aspects of lexical description. \\n\\n\\nBecker\\'s recent work , is also directed at\\nexactly the problem we address in the present paper.  Like him, we have\\nemployed an inheritance hierarchy.  And, like him, we have employed a\\nset of lexical rules (corresponding to his metarules).  The key\\ndifferences between our account and his are (i) that we have been able\\nto use an existing lexical knowledge representation language, rather\\nthan designing a formal system that is specific to LTAG, and (ii) that\\nwe have expressed our lexical rules in exactly the same language as that\\nwe have used to define the hierarchy, rather than invoking two quite\\ndifferent formal systems.\\n\\n\\nBecker\\'s sharp distinction between his metarules and his hierarchy\\ngives rise to some problems that our approach avoids.  Firstly, he\\nnotes that his metarules are subject to lexical exceptions and\\nproposes to deal with these by stating ``for each entry in the\\n(syntactic) lexicon .. which metarules are applicable for this entry\\'\\'\\n(1993,126).  We have no need to carry over this use of (meta)rule\\nfeatures since, in our account, lexical rules are not distinct from\\nany other kind of property in the inheritance hierarchy.  They can be\\nstated at the most inclusive relevant node and can then be overridden\\nat the exceptional descendant nodes.  Nothing specific needs to be\\nsaid about the nonexceptional nodes.\\n\\n\\nSecondly, his metarules may themselves be more or less similar to\\neach other and he suggests (1994,11) that these similarities could\\nbe captured if the metarules were also to be organized in a\\nhierarchy.  However, our approach allows us to deal with any such\\n similarities in the main lexical hierarchy itself rather than by setting up a separate hierarchical component just for metarules\\n(which appears to be what Becker has in mind).\\n\\n\\nThirdly, as he himself notes (1993,128), because his metarules map\\nfrom elementary trees that are in the inheritance hierarchy to\\nelementary trees that are outside it, most of the elementary trees\\nactually used are not directly connected to the hierarchy (although\\ntheir derived status with respect to it can be reconstructed).  Our\\napproach keeps all elementary trees, whether or not they have been\\npartly defined by a lexical rule, entirely within the lexical\\nhierarchy.\\n\\n\\nIn fact, Becker himself considers the possibility of capturing all\\nthe significant generalizations by using just one of the two\\nmechanisms that he proposes: ``one might want to reconsider the usage\\nof one mechanism for phenomena in both dimensions\\'\\' (1993,135).\\nBut, as he goes on to point out, his existing type of inheritance\\nnetwork is not up to taking on the task performed by his metarules\\nbecause the former is monotonic whilst his metarules are not.\\nHowever, he does suggest a way in which the hierarchy could be\\ncompletely replaced by metarules but argues against adopting it\\n(1993,136).\\n\\n\\nAs will be apparent from the earlier sections of this paper, we\\nbelieve that Becker\\'s insights about the organization of an LTAG lexicon can be better expressed if the metarule component is replaced\\nby an encoding of (largely equivalent) lexical rules that are an\\nintegral part of a nonmonotonic inheritance hierarchy that stands as\\na description of all the elementary trees.\\n\\n\\n  Acknowledgements \\n\\nA precursor of this paper was presented at the September 1994 TAG+\\nWorkshop in Paris.  We thank the referees for that event and the ACL-95\\nreferees for a number of helpful comments.  We are also grateful to\\nAravind Joshi, Bill Keller, Owen Rambow K. Vijay-Shanker and The XTAG\\nGroup.  This research was partly supported by grants to Evans from\\nSERC/EPSRC (UK) and to Gazdar from ESRC (UK).\\n\\nBibliography \\n\\nAnne Abeill, Kathleen Bishop, Sharon Cote,  Yves Schabes.\\n1990.\\nA lexicalized tree adjoining grammar for English.\\nTechnical Report MS-CIS-90-24, Department of Computer  Information\\n  Science, Univ. of Pennsylvania.\\n\\n\\nFrancois Andry, Norman Fraser, Scott McGlashan, Simon Thornton,  Nick Youd.\\n1992.\\nMaking DATR work for speech: lexicon compilation in\\nSUNDIAL.\\nComput. Ling., 18(3):245-267.\\n\\n\\nPetra Barg.\\n1994.\\nAutomatic acquisition of DATR theories from\\nobservations.\\nTheories des Lexicons: Arbeiten des Sonderforschungsbereichs 282,\\n  Heinrich-Heine Univ. of Dsseldorf, Dsseldorf.\\n\\n\\nTilman Becker.\\n1993.\\nHyTAG: A new type of tree adjoining grammar for hybrid\\n  syntactic representation of free word order languages.\\nPh.D. thesis, Univ. des Saarlandes.\\n\\n\\nTilman Becker.\\n1994.\\nPatterns in metarules.\\nIn Proceedings of the Third International Workshop on Tree\\n  Adjoining Grammars, 9-11.\\n\\n\\nDoris Bleiching.\\n1992.\\nProsodisches Wissen in Lexicon.\\nIn G. Grz, ed., KONVENS-92, 59-68. Springer-Verlag.\\n\\n\\nDoris Bleiching.\\n1994.\\nIntegration von Morphophonologie und Prosodie in ein hierarchisches\\n  Lexicon.\\nIn H. Trost, ed., Proceedings of KONVENS-94, 32-41.\\n\\n\\nTed Briscoe, Valeria de Paiva,  Ann Copestake.\\n1993.\\nInheritance, Defaults,  the Lexicon.\\nCUP.\\n\\n\\nDunstan Brown  Andrew Hippisley.\\n1994.\\nConflict in Russian genitive plural assignment: A solution\\n  represented in DATR.\\nJ. of Slavic Linguistics, 2(1):48-76.\\n\\n\\nLynne Cahill  Roger Evans.\\n1990.\\nAn application of DATR: the TIC lexicon.\\nIn ECAI-90, 120-125.\\n\\n\\nLynne Cahill.\\n1990.\\nSyllable-based morphology.\\nIn COLING-90, volume 3, 48-53.\\n\\n\\nLynne Cahill.\\n1993.\\nMorphonology in the lexicon.\\nIn EACL-93, 87-96.\\n\\n\\nGreville Corbett  Norman Fraser.\\n1993.\\nNetwork morphology: a DATR account of Russian nominal inflection.\\nJ. of Linguistics, 29:113-142.\\n\\n\\nWalter Daelemans  Gerald Gazdar, eds.\\n1992.\\nSpecial issues on inheritance.\\nComput. Ling., 18(2  3).\\n\\n\\nChristy Doran, Dania Egedi, Beth Ann Hockey,  B. Srinivas.\\n1994a.\\nStatus of the XTAG system.\\nIn Proceedings of the Third International Workshop on Tree\\n  Adjoining Grammars, 20-23.\\n\\n\\nChristy Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas,  Martin Zaidel.\\n1994b.\\nXTAG system -- a wide coverage grammar for English.\\nIn COLING-94, 922-928.\\n\\n\\nMarkus Duda  Gunter Gebhardi.\\n1994.\\nDUTR - a DATR-PATR interface formalism.\\nIn H. Trost, ed., Proceedings of KONVENS-94, 411-414.\\n\\n\\nRoger Evans  Gerald Gazdar.\\n1989a.\\nInference in DATR.\\nIn EACL-89, 66-71.\\n\\n\\nRoger Evans  Gerald Gazdar.\\n1989b.\\nThe semantics of DATR.\\nIn AISB-89, 79-87.\\n\\n\\nDaniel P. Flickinger.\\n1987.\\nLexical Rules in the Hierarchical Lexicon.\\nPh.D. thesis, Stanford Univ.\\n\\n\\nNorman Fraser  Greville Corbett.\\nin press.\\nGender, animacy,  declensional class assignment: a unified account\\n  for Russian.\\nIn Geert Booij  Jaap van Marle, ed., Yearbook of Morphology\\n  1994. Kluwer, Dordrecht.\\n\\n\\nDafydd Gibbon.\\n1992.\\nILEX: a linguistic approach to computational lexica.\\nIn Ursula Klenk, ed., Computatio Linguae: Aufstze zur\\n  algorithmischen und quantitativen Analyse der Sprache (Zeitschrift\\n  fr Dialektologie und Linguistik, Beiheft 73), 32-53. Franz\\n  Steiner Verlag, Stuttgart.\\n\\n\\nA. K. Joshi, L. S. Levy,  M. Takahashi.\\n1975.\\nTree adjunct grammars.\\nJ. Comput. Syst. Sci., 10(1):136-163.\\n\\n\\nJames Kilbury, Petra [Barg] Nrger,  Ingrid Renz.\\n1991.\\nDATR as a lexical component for PATR.\\nIn EACL-91, 137-142.\\n\\n\\nJames Kilbury, Petra Barg,  Ingrid Renz.\\n1994.\\nSimulation lexicalischen Erwerbs.\\nIn Christopher Habel  Gert Rickheit Sascha W. Felix, ed,   Kognitive Linguistik: Reprsentation und Prozesse, 251-271.\\n  Westdeutscher Verlag, Opladen.\\n\\n\\nJames Kilbury.\\n1990.\\nEncoding constituent structure in feature structures.\\nUnpublished manuscript, Univ. of Dsseldorf, Dsseldorf.\\n\\n\\nAdam Kilgarriff  Gerald Gazdar.\\n1995.\\nPolysemous relations.\\nIn Frank Palmer, ed., Grammar  Meaning: essays in honour\\n  of Sir John Lyons, 1-25. CUP.\\n\\n\\nAdam Kilgarriff.\\n1993.\\nInheriting verb alternations.\\nIn EACL-93, 213-221.\\n\\n\\nHagen Langer.\\n1994.\\nReverse queries in DATR.\\nIn COLING-94, 1089-1095.\\n\\n\\nMarc Light, Sabine Reinhard,  Marie Boyle-Hinrichs.\\n1993.\\nINSYST: an automatic inserter system for hierarchical\\n    lexica.\\nIn EACL-93, page 471.\\n\\n\\nMarc Light.\\n1994.\\nClassification in feature-based default inheritance hierarchies.\\nIn H. Trost, ed., Proceedings of KONVENS-94, 220-229.\\n\\n\\nSabine Reinhard  Dafydd Gibbon.\\n1991.\\nProsodic inheritance  morphological generalisations.\\nIn EACL-91, 131-136.\\n\\n\\nJames Rogers  K. Vijay-Shanker.\\n1992.\\nReasoning with descriptions of trees.\\nIn ACL-92, 72-80.\\n\\n\\nK. Vijay-Shanker  Yves Schabes.\\n1992.\\nStructure sharing in lexicalized tree-adjoining grammar.\\nIn COLING-92, 205-211.\\n\\n\\nThe XTAG Research Group.\\n1995.\\nA lexicalized tree adjoining grammar for English.\\nTechnical Report IRCS Report 95-03, The Institute for Research in\\n  Cognitive Science, Univ. of Pennsylvania.\\n\\nFootnotes\\n\\n  As with all\\nfully lexicalized grammar formalisms, there is really no conceptual\\ndistinction to be drawn in LTAG between the lexicon and the grammar:\\nthe grammatical rules are just lexical properties.\\n   See Section     for further discussion of these approaches.\\n  See, for example,\\nBleiching , Brown \\nHippisley , Corbett  Fraser ,\\nCahill , Cahill  Evans ,\\nFraser  Corbett , Gibbon ,\\nKilgarriff , Kilgarriff \\nGazdar , Reinhard \\nGibbon .\\n  See, for example, Andry\\net al.  on compilation, Kilbury et\\nal.  on coding  DAGs, Duda \\nGebhardi  on dynamic querying,\\nLanger  on reverse querying, and Barg ,\\nLight , Light et al.  and\\nKilbury et al.  on automatic acquisition.\\nAnd there are at least a dozen different DATR implementations\\navailable, on various platforms and programming languages.\\n  In fact, LTAG commonly distinguishes\\ntwo sets of features at each node (top and bottom), but for\\nsimplicity we shall assume just one set in this paper.\\n  LTAG\\'s other tree-building operation is adjunction, which allows a tree-fragment to be spliced into the body of\\na tree.  However, we only need to concern ourselves here with the representation of the trees involved, not with the\\nsubstitution/adjunction distinction.\\n  The tree\\n in figure  has more than one anchor - in such cases it is generally easy to decide which anchor is the most\\nappropriate root for the tree (here, the verb anchor).\\n  To gain the intuitive sense of this fragment,\\nread a line such as [] == VERB as ``inherit everything from\\nthe definition of VERB\\'\\', and a line such as [parent] ==\\nPPTREE:[] as ``inherit the parent subtree from the definition\\nof PPTREE\\'\\'. Inheritance in DATR is always by default -\\nlocally defined feature specifications take priority over inherited\\nones.\\n  Even the lexeme nodes are abstract - individual\\nword forms might be represented by further more specific nodes attached\\nbelow the lexemes in the hierarchy.\\nOur example makes much use of multiple inheritance (thus, for example,\\nVPTREE inherits from TREENODE, STREE and NPCOMP)\\nbut all such multiple inheritance is orthogonal in DATR: no path\\ncan inherit from more than one node.\\n  Oversimplifying slightly, the\\ndouble quotes in \"[input passive right right]\" mean that that\\nDATR path will not be evaluated locally (i.e., at the VERB+NP\\nnode), but rather at the relevant lexeme node (e.g., Eat or Give).\\n  The full version of this DATR fragment includes\\nall\\nthe components discussed above in a single coherent, but slightly more\\ncomplex account. It is available on request from the authors.\\n  Note\\nthat simplified fragment presented here does not get this right. It\\nmakes all feature specifications total descriptions. To correct this we\\nwould need to change TREENODE so that only the values of [right], [left] and [parent] default to undef.\\n  As in the work\\ncited in footnote 3, above.\\n  As\\nillustrated by the way in which the whq lexical rule inherits\\nfrom that for topicalisation in the example given above.\\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nThis paper shows how DATR, a widely used formal language for lexical\\nknowledge representation, can be used to define an LTAG lexicon as\\nan inheritance hierarchy with internal lexical rules.  A bottom-up\\nfeatural encoding is used for LTAG trees and this allows lexical\\nrules to be implemented as covariation constraints within feature\\nstructures.  Such an approach eliminates the considerable redundancy\\notherwise associated with an LTAG lexicon.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nText chunking involves dividing sentences into\\nnonoverlapping segments\\non the basis of fairly superficial analysis.\\nAbney  has proposed this\\nas a useful and relatively tractable precursor\\nto full parsing,\\nsince it provides a foundation\\nfor further levels of analysis including verb-argument identification,\\nwhile still allowing more complex attachment decisions\\nto be postponed to a later phase.\\nSince chunking includes identifying\\nthe non-recursive portions of noun phrases,\\nit can also be useful for other purposes\\nincluding index term generation.\\n\\n\\nMost efforts at superficially extracting segments from sentences\\nhave focused on identifying low-level noun groups, either\\nusing hand-built grammars and finite state techniques or\\nusing statistical models like HMMs trained from corpora.\\nIn this paper, we target a somewhat higher level of chunk structure\\nusing Brill's \\ntransformation-based learning mechanism,\\nin which a sequence of transformational rules\\nis learned from a corpus;\\nthis sequence iteratively improves upon a baseline model\\nfor some interpretive feature of the text.\\nThis technique has previously been used not only for\\n part-of-speech tagging , but also for  prepositional phrase attachment disambiguation , and assigning unlabeled binary-branching tree structure\\n to sentences . Because transformation-based learning uses\\npattern-action rules based on selected features of the local context,\\nit is helpful for the values being predicted\\nto also be encoded locally.\\nIn the text-chunking application,\\nencoding the predicted chunk structure\\nin tags attached to the words,\\nrather than as brackets between words,\\navoids many of the difficulties with unbalanced bracketings\\nthat would result if such local rules were allowed\\nto insert or alter inter-word brackets directly.\\n\\n\\nIn this study,\\ntraining and test sets marked\\nwith two different types of chunk structure\\nwere derived algorithmically from the parsed data\\nin the Penn Treebank corpus\\n of Wall Street Journal text . The source texts were then run through Brill's\\n part-of-speech tagger , and, as a baseline heuristic,\\nchunk structure tags were assigned to each word\\nbased on its part-of-speech tag.\\nRules were then automatically learned\\nthat updated these chunk structure tags\\nbased on neighboring words\\nand their part-of-speech and chunk tags.\\nApplying transformation-based learning to text chunking\\nturns out to be different in interesting ways\\nfrom its use for part-of-speech tagging.\\nThe much smaller tagset calls for a different organization\\nof the computation, and the fact that\\npart-of-speech assignments as well as word identities are fixed\\nsuggests different optimizations.\\n\\n\\n  Text Chunking \\n\\nAbney  has proposed text chunking\\nas a useful preliminary step to parsing.\\nHis chunks are inspired in part by psychological studies\\nof Gee and Grosjean \\nthat link pause durations in reading\\nand naive sentence diagraming to\\ntext groupings that they called -phrases,\\nwhich very roughly correspond\\nto breaking the string after each syntactic\\nhead that is a content word.\\nAbney's other motivation for chunking\\nis procedural, based on the hypothesis that the identification of chunks\\ncan be done fairly dependably by finite state methods,\\npostponing the decisions that require higher-level analysis\\nto a parsing phase that chooses how to combine the chunks.\\n\\n  Existing Chunk Identification Techniques \\n\\nExisting efforts at identifying chunks in text have been focused\\nprimarily\\non low-level noun group identification,\\nfrequently as a step in deriving index terms,\\nmotivated in part by the limited coverage of present broad-scale parsers\\nwhen dealing with unrestricted text.\\nSome researchers have applied grammar-based methods,\\ncombining lexical data with finite state or other grammar constraints,\\nwhile others have worked on inducing statistical models\\neither directly from the words or from automatically assigned\\npart-of-speech classes.\\n\\n\\nOn the grammar-based side,\\nBourigault \\ndescribes a system for extracting\\n``terminological noun phrases'' from French text.\\nThis system first uses heuristics\\nto find ``maximal length noun phrases'',\\nand then uses a grammar to extract\\n``terminological units.''\\nFor example, from the maximal NP\\nle disque dur de la station de travail\\nit extracts the two terminological phrases\\ndisque dur, and station de travail.\\nBourigault claims that the grammar can parse\\n``around 95% of the maximal length noun phrases''\\nin a test corpus into possible terminological phrases,\\nwhich then require manual validation.\\nHowever, because its goal is terminological phrases,\\nit appears that this system ignores NP chunk-initial\\ndeterminers and\\nother initial prenominal modifiers,\\nsomewhat simplifying the parsing task.\\n\\n\\nVoutilainen ,\\nin his impressive NPtool system,\\nuses an approach that is in some ways similar\\nto the one used here,\\nin that he adds to his part-of-speech tags\\na new kind of tag that shows chunk structure;\\nthe chunk tag ``@]N'', for example,\\nis used for determiners and premodifiers,\\nboth of which group with the following noun head.\\nHe uses a lexicon that lists all the possible chunk tags\\nfor each word\\ncombined with hand-built constraint grammar patterns.\\nThese patterns eliminate impossible readings\\nto identify a somewhat idiosyncratic kind of target noun group\\nthat does not include initial determiners but does include\\npostmodifying prepositional phrases (including determiners).\\nVoutilainen claims recall rates of 98.5% or better with\\nprecision of 95% or better.\\nHowever, the sample NPtool analysis\\n given in the appendix of , appears to be less accurate than claimed in general,\\nwith 5 apparent mistakes (and one unresolved ambiguity) out of the 32 NP\\nchunks in that sample, as listed in\\n Table . These putative errors, combined with the claimed high performance, suggest that NPtool's definition of NP chunk\\nis also tuned for extracting terminological phrases, and thus\\nexcludes many kinds of NP premodifiers, again simplifying the chunking\\ntask.\\n\\n\\n\\n\\nKupiec  also briefly mentions the use of\\nfinite state NP recognizers for both English and French\\nto prepare the input for a program that identified\\nthe correspondences between NPs in bilingual corpora,\\nbut he does not directly discuss their performance.\\n\\n\\nUsing statistical methods,\\nChurch's Parts program ,\\nin addition to identifying parts of speech,\\nalso inserted brackets identifying core NPs.\\nThese brackets were placed using a statistical model\\ntrained on Brown corpus\\nmaterial in which NP brackets had been inserted\\nsemi-automatically.\\nIn the small test sample shown,\\nthis system achieved 98% recall for correct brackets.\\nAt about the same time, Ejerhed ,\\nworking with Church, performed comparisons\\nbetween finite state methods and Church's stochastic models\\nfor identifying both non-recursive clauses\\nand non-recursive NPs in English text.\\nIn those comparisons, the stochastic methods\\noutperformed the hand built finite-state models,\\nwith claimed accuracies of 93.5% (clauses)\\nand 98.6% (NPs) for the statistical models\\ncompared to\\nto 87% (clauses) and 97.8% (NPs) for\\nthe finite-state methods.\\n\\n\\nRunning Church's program on test material,\\nhowever, reveals that the definition of NP\\nembodied in Church's program is quite simplified\\nin that it does not include, for example,\\nstructures or words conjoined within NP by either explicit conjunctions\\nlike ``and'' and ``or'', or implicitly by commas.\\nChurch's chunker thus assigns\\nthe following NP chunk structures:\\n\\n\\n[a Skokie] , [Ill.] , [subsidiary] \\n[newer] , [big-selling prescriptions drugs] \\n[the inefficiency] , [waste] and [lack] of [coordination]\\n[Kidder] , [Peabody]   [Co]\\n\\n\\nIt is difficult to compare performance figures between studies;\\nthe definitions of the target chunks\\nand the evaluation methodologies differ widely\\nand are frequently incompletely specified.\\nAll of the cited performance figures above also appear to derive from\\nmanual checks by the investigators\\nof the system's predicted output, and it\\nis hard to estimate the impact of the system's suggested chunking\\non the judge's determination.\\nWe believe that the work reported here\\nis the first study\\nwhich has attempted to find NP chunks\\nsubject only to the limitation\\nthat the structures recognized\\ndo not include recursively embedded NPs,\\nand which has measured performance by automatic\\ncomparison with a preparsed corpus.\\n\\n\\n  Deriving Chunks from Treebank Parses \\n\\nWe performed experiments using two different chunk structure targets,\\none that tried to bracket non-recursive ``baseNPs''\\nand one that partitioned sentences into non-overlapping\\nN-type and V-type chunks,\\nloosely following Abney's model.\\nTraining and test materials with chunk tags encoding\\neach of these kinds of structure\\nwere derived automatically\\nfrom the parsed Wall Street Journal text\\n in the Penn Treebank . While this automatic derivation process introduced\\na small percentage of errors of its own,\\nit was the only practical way both to provide the amount\\nof training data required and to allow for fully-automatic\\ntesting.\\n\\n\\nThe goal of the ``baseNP'' chunks was to identify essentially\\nthe initial portions of non-recursive noun phrases up to the head,\\nincluding determiners but not including postmodifying\\nprepositional phrases or clauses.\\nThese chunks were extracted from the Treebank parses,\\nbasically by selecting NPs that\\n contained no nested NPs. The handling of conjunction followed that of the Treebank annotators\\nas to whether to show separate baseNPs\\nor a single baseNP spanning\\n the conjunction. Possessives were treated as a special case,\\nviewing the possessive marker\\nas the first word of a new baseNP,\\nthus flattening the recursive structure in a useful way.\\nThe following sentences give examples of this baseNP\\nchunk structure:\\nDuring [N the third quarter N] ,\\n[N Compaq N] purchased\\n[N a former Wang Laboratories\\nmanufacturing facility N] in\\n[N Sterling N] ,\\n[N Scotland N] ,\\nwhich will be used for\\n[N international service and repair\\noperations N] .\\n\\n\\n[N The government N] has [N other agencies and\\ninstruments N] for pursuing [N these other objectives N] .\\n\\n\\nEven [N Mao Tse-tung N] [N 's China N] began in [N 1949 N] with [N a partnership N] between [N the communists N] and [N a number N] of [N smaller , non-communist parties N] .\\n\\n\\nThe chunks in the partitioning chunk experiments\\nwere somewhat closer to Abney's model,\\nwhere the prepositions in prepositional phrases are included\\nwith the object NP up to the head in a single N-type chunk.\\nThis created substantial additional ambiguity for the system,\\nwhich had to distinguish prepositions from particles.\\nThe handling of conjunction again follows the Treebank parse\\nwith nominal conjuncts parsed in the Treebank\\nas a single NP forming a single N chunk,\\nwhile those parsed as conjoined NPs\\nbecome separate chunks, with any coordinating\\nconjunctions attached like prepositions to the following N chunk.\\n\\n\\nThe portions of the text\\nnot involved in N-type chunks\\nwere grouped as chunks termed V-type,\\nthough these ``V'' chunks\\nincluded many elements that were not verbal,\\nincluding adjective phrases.\\nThe internal structure of these V-type chunks\\nloosely followed the Treebank parse,\\nthough V chunks often group together elements\\nthat were sisters in the underlying parse tree.\\nAgain, the possessive marker was viewed as initiating a new N-type chunk.\\nThe following sentences are annotated with these\\npartitioning N and V chunks:\\n[N Some bankers N]  [V are reporting V] [N more inquiries than usual N] [N about CDs N] [N since Friday N] .\\n\\n\\n[N Eastern Airlines N] [N ' creditors N] [V have begun exploring V] [N alternative approaches N] [N to a Chapter 11 reorganization N] [V because V] [N they N]\\n[V are unhappy V] [N with the carrier N] [N 's latest proposal N] .\\n\\n\\n[N Indexing N] [N for the most part N] [V has involved simply buying V] [V and then holding V] [N stocks N] [N in the correct mix N] [V to mirror V] [N a stock market barometer N] .\\n\\n\\nThese two kinds of chunk structure derived from the Treebank data\\nwere encoded as chunk tags attached to each word and\\nprovided the targets for the transformation-based learning.\\n\\n\\n\\n  The Transformation-based Learning Paradigm \\n\\n As shown in Fig. , transformation-based learning\\nstarts with a supervised training corpus\\nthat specifies the correct values for some linguistic feature of interest,\\na baseline heuristic for predicting initial values for that feature,\\nand a set of rule templates\\nthat determine a space of possible transformational rules.\\nThe patterns of the learned rules match to particular\\ncombinations of features in the neighborhood surrounding a word,\\nand their action is to change the system's current guess\\nas to the feature for that word.\\n\\n\\n\\n\\nTo learn a model,\\none first applies the baseline heuristic to produce\\ninitial hypotheses for each site in the training corpus.\\nAt each site where this baseline prediction is not correct,\\nthe templates are then used to form instantiated candidate rules\\nwith patterns that test selected features in the neighborhood of the word\\nand actions that correct the currently incorrect tag assignment.\\nThis process eventually identifies all the rule candidates\\ngenerated by that template set that would have a positive effect\\non the current tag assignments anywhere in the corpus.\\n\\n\\nThose candidate rules are then tested against the rest of corpus,\\nto identify at how many locations they would cause negative changes.\\nOne of those rules whose net score (positive changes minus negative changes)\\nis maximal is then\\nselected, applied to the corpus, and also written out as the first rule\\nin the learned sequence.\\nThis entire learning process is then repeated on the transformed corpus:\\nderiving candidate rules, scoring them, and selecting one\\nwith the maximal positive effect.\\nThis process is iterated,\\nleading to an ordered sequence of rules,\\nwith rules discovered first\\nordered before those discovered later.\\nThe predictions of the model on new text\\nare determined by beginning with the baseline heuristic prediction\\nand then applying each rule in the learned rule sequence in turn.\\n\\n\\n  Transformational Text Chunking \\n\\nThis section discusses how text chunking can be encoded\\nas a tagging problem that can be conveniently addressed using\\ntransformational learning.\\nWe also note some related adaptations\\nin the procedure for learning rules\\nthat improve its performance,\\ntaking advantage of ways in which this task differs\\nfrom the learning of part-of-speech tags.\\n\\n  Encoding Choices \\n\\nApplying transformational learning to text chunking requires\\nthat the system's current hypotheses about chunk structure\\nbe represented in a way\\nthat can be matched against the pattern parts of rules.\\nOne way to do this would be to have patterns match tree fragments and\\nactions modify tree geometries, as in Brill's\\ntransformational parser .\\nIn this work, we have found it convenient\\nto do so  by encoding the chunking\\nusing an additional set of tags,\\nso that each word carries both a part-of-speech tag\\nand also a ``chunk tag''\\nfrom which the chunk structure can be derived.\\n\\n\\nIn the baseNP experiments aimed at\\nnon-recursive NP structures,\\nwe use the chunk tag set\\n{I, O, B}, where words marked I are\\ninside some baseNP, those marked O are outside,\\nand the B tag is  used to mark\\nthe left most item of a baseNP\\nwhich immediately follows another baseNP.\\nIn these tests, punctuation marks were tagged\\nin the same way as words.\\n\\n\\nIn the experiments that partitioned text into N and V chunks,\\nwe use the chunk tag set {BN, N, BV, V, P},\\nwhere BN marks the first word and N the succeeding words\\nin an N-type group while BV and V\\nplay the same role for V-type groups.\\nPunctuation marks,\\nwhich are ignored in Abney's chunk grammar,\\nbut which the Treebank data treats as normal lexical items\\nwith their own part-of-speech tags,\\nare unambiguously assigned the chunk tag P.\\nItems tagged P are allowed to appear within N or V chunks;\\nthey are irrelevant as far as chunk boundaries are concerned,\\nbut they are still available to be matched against\\nas elements of the left hand sides of rules.\\n\\n\\nEncoding chunk structure\\nwith tags attached to words\\nrather than non-recursive bracket markers inserted\\nbetween words\\nhas the advantage\\nthat it limits the dependence\\nbetween different elements of the encoded representation.\\nWhile brackets must be correctly paired\\nin order to derive a chunk structure,\\nit is easy to define a mapping\\nthat can produce a valid chunk structure\\nfrom any sequence of chunk tags;\\nthe few hard cases that arise\\ncan be handled completely locally.\\nFor example, in the baseNP tag set,\\nwhenever a B tag immediately follows an O, it\\nmust be treated as an I,\\nand, in the partitioning chunk tag set,\\nwherever a V tag immediately follows an N tag\\nwithout any intervening BV,\\nit must be treated as a BV.\\n\\n\\n  Baseline System \\n\\nTransformational learning begins with some initial ``baseline''\\nprediction, which here means\\na baseline assignment of chunk tags to words.\\nReasonable suggestions for baseline heuristics\\nafter a text has been tagged for part-of-speech\\nmight\\ninclude assigning to each word the chunk tag that it carried\\nmost frequently in the training set,\\nor assigning each part-of-speech tag\\nthe chunk tag that was most frequently associated\\nwith that part-of-speech tag in the training.\\nWe tested both approaches, and the baseline heuristic\\nusing part-of-speech tags turned out to do better,\\nso it was the one used in our experiments.\\nThe part-of-speech tags used by this baseline heuristic,\\nand then later also matched against by transformational rule patterns,\\nwere derived by running the raw texts in a prepass through\\n Brill's transformational part-of-speech tagger . \\n\\n\\n  Rule Templates \\n\\nIn transformational learning, the space of candidate rules\\nto be searched is defined by a set of rule templates\\nthat each specify a small number of particular feature sets\\nas the relevant factors that a rule's\\nleft-hand-side pattern should examine,\\nfor example, the part-of-speech tag\\nof the word two to the left\\ncombined with the actual word one to the left.\\nIn the preliminary scan of the corpus for each learning pass,\\nit is these templates that are applied\\nto each location whose current tag is not correct,\\ngenerating a candidate rule that would apply at least at that one location,\\nmatching those factors and correcting the chunk tag assignment.\\n\\n\\nWhen this approach is applied to part-of-speech tagging,\\nthe possible sources of evidence for templates\\ninvolve the identities of words\\nwithin a neighborhood of some appropriate size\\nand their current part-of-speech tag assignments.\\nIn the text chunking application,\\nthe tags being assigned are chunk structure tags,\\nwhile the part-of-speech tags are a fixed part of the environment,\\nlike the lexical identities of the words themselves.\\nThis additional class of available information causes a significant\\nincrease in the number of reasonable templates\\nif templates for a wide range of the possible combinations of evidence\\nare desired.\\n The distributed version of Brill's tagger  makes use of 26 templates, involving\\nvarious mixes of word and part-of-speech tests on neighboring words.\\nOur tests were performed using 100 templates;\\nthese included almost all of Brill's combinations,\\nand extended them to include references to chunk tags\\nas well as to words and part-of-speech tags.\\n\\n\\nThe set of 100 rule templates used here\\nwas built from repetitions of 10 basic patterns, shown\\non the left side of\\n Table  as they apply to words. \\n\\n\\n\\n\\nThe same 10 patterns can also be used to match against\\npart-of-speech tags, encoded as P0, P-1, etc.\\n(In other tests, we have explored mixed templates,\\nthat match against both word and part-of-speech values,\\nbut no mixed templates were used in these experiments.)\\nThese 20 word and part-of-speech patterns were then combined with\\neach of the 5 different chunk tag patterns\\nshown on the right side of the table.\\nThe cross product of the 20 word and part-of-speech patterns\\nwith the 5 chunk tag patterns\\ndetermined the full set of 100 templates used.\\n\\n\\n\\n  Algorithm Design Issues \\n\\nThe large increase in the number of rule templates\\nin the text chunking application\\nwhen compared to part-of-speech tagging\\npushed the training process against the available limits\\nin terms of both space and time,\\nparticularly when combined with the desire\\nto work with the largest possible training sets.\\nVarious optimizations proved to be crucial\\nto make the tests described feasible.\\n\\n  Organization of the Computation \\n\\nOne change in the algorithm is related\\nto the smaller size of the tag set.\\n In Brill's tagger , an initial calculation in each pass computes the confusion matrix\\nfor the current tag assignments and\\nsorts the entries of that [old-tag \\nnew-tag] matrix,\\nso that candidate rules can then be processed\\nin decreasing order of the maximum possible benefit\\nfor any rule changing, say, old tag I to new tag J.\\nThe search for the best-scoring rule can then be halted\\nwhen a cell of the confusion matrix is reached\\nwhose maximum possible benefit is less\\nthan the net benefit of some rule already encountered.\\n\\n\\nThe power of that approach is dependent on the fact\\nthat the confusion matrix for part-of-speech tagging\\npartitions the space of candidate rules\\ninto a relatively large number of classes,\\nso that one is likely to be able to exclude\\na reasonably large portion of the search space.\\nIn a chunk tagging application, with only 3 or 4 tags\\nin the effective tagset,\\nthis approach based on the confusion matrix offers much less benefit.\\n\\n\\nHowever, even though the confusion matrix does not usefully subdivide\\nthe space of possible rules when the tag set is this small,\\nit is still possible to apply a similar optimization by sorting the entire list\\nof candidate rules on the basis of their positive scores,\\nand then processing the candidate rules\\n(which means determining their negative scores and thus their net scores)\\nin order of decreasing positive scores.\\nBy keeping track of the rule with maximum benefit seen so far,\\none can be certain of having found one of the globally best rules\\nwhen one reaches candidate rules in the sorted list\\nwhose positive score is\\nnot greater than the net score of the best rule so far.\\n\\n\\n  Indexing Static Rule Elements \\n\\nIn earlier work on transformational part-of-speech\\n tagging , we noted that it is possible to greatly speed up the learning process\\nby constructing a full, bidirectional index\\nlinking each candidate rule to those locations in the corpus\\nat which it applies\\nand each location in the corpus to those candidate rules\\nthat apply there.\\nSuch an index allows the process of applying rules\\nto be performed without having to search through the corpus.\\nUnfortunately, such complete indexing proved to be\\ntoo costly in terms of physical memory to be feasible\\nin this application.\\n\\n\\nHowever, it is possible to construct a limited index\\nthat lists for each candidate rule those locations in the corpus\\nat which the static portions of its left-hand-side pattern match.\\nBecause this index involves only the stable word identity and\\npart-of-speech tag values, it does not require updating;\\nthus it can be stored more compactly, and\\nit is also not necessary to maintain back pointers\\nfrom corpus locations to the applicable rules.\\nThis kind of partial static index proved to be a significant advantage\\nin the portion of the program where candidate rules\\nwith relatively high positive scores are being tested\\nto determine their negative scores,\\nsince it avoids the necessity of testing such rules against\\nevery location in the corpus.\\n\\n\\n  Heuristic Disabling of Unlikely Rules \\n\\nWe also investigated a new heuristic to\\nspeed up the computation:\\nAfter each pass, we disable\\nall rules whose positive score is significantly lower\\nthan the net score of the best rule for the current pass.\\nA disabled rule is then reenabled whenever enough other changes\\nhave been made to the corpus that it seems possible that the score\\nof that rule might have changed enough to bring it back into contention\\nfor the top place.\\nThis is done by adding some fraction of the changes made in each pass\\nto the positive scores of the disabled rules,\\nand reenabling rules whose adjusted positive scores came within a threshold\\nof the net score of the successful rule on some pass.\\n\\n\\nNote that this heuristic technique introduces some risk of missing\\nthe actual best rule in a pass, due to its being incorrectly disabled\\nat the time.\\nHowever, empirical comparisons between runs with and without rule disabling\\nsuggest that conservative use of this technique\\ncan produce an order of magnitude speedup\\nwhile imposing only a very slight cost in terms of suboptimality\\nof the resulting learned rule sequence.\\n\\n\\n\\n  Results \\n\\nThe automatic derivation of training and testing data from\\nthe Treebank analyses\\nallowed for fully automatic scoring,\\nthough the scores are naturally subject to any remaining\\nsystematic errors in the data derivation process\\nas well as to bona fide parsing errors\\nin the Treebank source.\\n Table  shows the results for the baseNP tests, and\\n Table  shows the results for the partitioning chunks task.\\nSince training set size has a significant effect\\non the results,\\nvalues are shown for three different training set sizes.\\n(The test set in all cases was 50K words.\\nTraining runs were halted after the first 500 rules;\\nrules learned after that point affect relatively few locations\\nin the training set and have only a very slight effect\\nfor good or ill on test set performance.)\\n\\n\\n\\n\\n\\n\\nThe first line in each table gives the performance of the baseline system,\\nwhich assigned a baseNP or chunk tag to each word\\non the basis of the POS tag assigned in the prepass.\\nPerformance is stated in terms of recall\\n(percentage of correct chunks found)\\nand precision\\n(percentage of chunks found that are correct),\\nwhere both ends of a chunk had to match exactly for it to be counted.\\nThe raw percentage of correct chunk tags is also given\\nfor each run, and for each performance measure,\\nthe relative error reduction compared to the baseline is listed.\\nThe partitioning chunks do appear\\nto be somewhat harder to predict than baseNP chunks.\\nThe higher error reduction for the former\\nis partly due to the fact that the part-of-speech baseline\\nfor that task is much lower.\\n\\n  Analysis of Initial Rules \\n\\nTo give a sense of the kinds of rules being learned,\\nthe first 10 rules from the 200K baseNP run\\n are shown in Table . It is worth glossing the rules,\\nsince one of the advantages of transformation-based learning is\\nexactly that the resulting model is easily interpretable.\\nIn the first of the baseNP rules,\\nadjectives (with part-of-speech tag JJ)\\nthat are currently tagged I\\nbut that are followed by words tagged O\\nhave their tags changed to O.\\nIn Rule 2, determiners that are preceded by two words\\nboth tagged I have their own tag changed to B,\\nmarking the beginning of a baseNP\\nthat happens to directly follow another.\\n(Since the tag B is only used when baseNPs abut,\\nthe baseline system tags determiners as I.)\\nRule 3 takes words which immediately follow\\ndeterminers\\ntagged I that in turn follow something tagged O\\nand changes their tag to also be I.\\nRules 4-6 are similar to Rule 2, marking the initial words\\nof baseNPs that directly follow another baseNP.\\nRule 7 marks conjunctions (with part-of-speech tag CC) as I\\nif they follow an I and precede a noun,\\nsince such conjunctions are more likely to be embedded in a single baseNP\\nthan to separate two baseNPs,\\nand Rules 8 and 9 do the same.\\n(The word `'' in rule 8 comes mostly from company names\\nin the Wall St. Journal source data.)\\nFinally, Rule 10 picks up cases like ``including about four million shares''\\nwhere ``about'' is used\\nas a quantifier rather than preposition.\\n\\n\\n\\n\\nA similar list of the first ten rules for the chunk task\\n can be seen in Table . To gloss a few of these,\\nin the first rule here, determiners (with part-of-speech tag DT),\\nwhich usually begin N chunks and thus are assigned the baseline\\ntag BN, have their chunk tags\\nchanged to N if they follow a word whose tag is also BN.\\nIn Rule 2, sites currently tagged N but which fall\\nat the beginning of a sentence have their tags switched to BN.\\n(The dummy tag Z and word ZZZ\\nindicate that the locations one to the left are beyond the\\nsentence boundaries.)\\nRule 3 changes N to BN after a comma (which is tagged P), and\\nin Rule 4, locations tagged BN are switched to BV if the following\\nlocation is tagged V and has the part-of-speech tag VB.\\n\\n\\n\\n\\n  Contribution of Lexical Templates \\n\\nThe fact that this system includes lexical rule templates\\nthat refer to actual words sets it apart from approaches\\nthat rely only on part-of-speech tags to predict chunk structure.\\nTo explore how much difference in performance those lexical rule\\ntemplates make, we repeated the above test runs omitting\\ntemplates that refer to specific words.\\n The results for these runs, in Tables   and , suggest that the lexical rules improve performance\\non the baseNP chunk task by about 1%\\n(roughly 5% of the overall error reduction)\\nand on the partitioning chunk task by about 5%\\n(roughly 10% of the error reduction).\\nThus lexical rules appear to be making a limited contribution\\nin determining baseNP chunks,\\nbut a more significant one for the partitioning chunks.\\n\\n\\n\\n\\n\\n\\n  Frequent Error Classes \\n\\nA rough hand categorization of a sample of the errors from a baseNP run\\nindicates that many fall into\\nclasses that are understandably difficult for any process\\nusing only local word and part-of-speech patterns to resolve.\\nThe most frequent single confusion involved words tagged VBG and VBN,\\nwhose baseline prediction given their part-of-speech tag\\nwas O, but which also occur frequently inside baseNPs.\\nThe system did discover some rules that allowed it\\nto fix certain classes of VBG and VBN mistaggings,\\nfor example, rules that retagged VBNs as I\\nwhen they preceded an NN or NNS tagged I.\\nHowever, many also remained unresolved, and many of those\\nappear to be cases that would require more than local word and\\npart-of-speech patterns to resolve.\\n\\n\\nThe second most common class of errors involved conjunctions,\\nwhich, combined with the former class,\\nmake up half of all the errors in the sample.\\nThe Treebank tags the words ``and'' and frequently ``,''\\nwith the part-of-speech tag CC,\\nwhich the baseline system again predicted would fall most often\\n outside of a baseNP. However, the Treebank parses do also frequently classify\\nconjunctions of Ns or NPs as a single baseNP,\\nand again there appear to be insufficient clues in the word and tag\\ncontexts for the current system to make the distinction.\\nFrequently, in fact, the actual choice of structure\\nassigned by the Treebank annotators seemed largely dependent on\\nsemantic indications unavailable to the transformational learner.\\n\\n\\n\\n  Future Directions \\n\\nWe are planning to explore several different paths\\nthat might increase the system's power to distinguish\\nthe linguistic contexts in which particular changes would be useful.\\nOne such direction is to expand the template set\\nby adding templates that are sensitive to the chunk structure.\\nFor example, instead of referring to the word two to the left,\\na rule pattern could refer to the first word in the current chunk,\\nor the last word of the previous chunk.\\nAnother direction would be to enrich the vocabulary of chunk tags,\\nso that they could be used during the learning process to encode\\ncontextual features for use by later rules in the sequence.\\n\\n\\nWe would also like to explore applying these same kinds of techniques\\nto building\\nlarger scale structures, in which larger units are assembled\\nor predicate/argument structures derived by combining chunks.\\nOne interesting direction here would be to explore the use of\\nchunk structure tags that encode a form of dependency grammar,\\nwhere the tag ``N+2'' might mean that the current word is to be taken\\nas part of the unit headed by the N two words to the right.\\n\\n\\n  Conclusions \\n\\nBy representing text chunking as a kind of tagging problem,\\nit becomes possible to easily apply transformation-based learning.\\nWe have shown that this approach\\nis able to automatically induce\\na chunking model from supervised training\\nthat achieves recall and precision of 92% for baseNP chunks\\nand 88% for partitioning N and V chunks.\\nSuch chunking models provide a useful and feasible next step\\nin textual interpretation that goes beyond part-of-speech tagging,\\nand that serve as a foundation both for larger-scale grouping\\nand for direct extraction of subunits like index terms.\\nIn addition, some variations in the transformation-based learning algorithm\\nare suggested by this application that may also be useful in other settings.\\n\\n\\n  Acknowledgments \\n\\nWe would like to thank Eric Brill for making his system widely available,\\nand Ted Briscoe and David Yarowsky for helpful\\ncomments, including the suggestion to test the system's\\nperformance without lexical rule templates.\\n\\nBibliography \\n\\nAbney, Steven.\\n1991.\\nParsing by chunks.\\nIn Berwick, Abney, and Tenny, editors, Principle-Based Parsing.\\n  Kluwer Academic Publishers.\\n\\n\\nBourigault, D.\\n1992.\\nSurface grammatical analysis for the extraction of terminological\\n  noun phrases.\\nIn Proceedings of the Fifteenth International Conference on\\n  Computational Linguistics, pages 977-981.\\n\\n\\nBrill, Eric.\\n1993a.\\nAutomatic grammar induction and parsing free text: A\\n  transformation-based approach.\\nIn Proceedings of the DARPA Speech and Natural Language\\n  Workshop, 1993, pages 237-242.\\n\\n\\nBrill, Eric.\\n1993b.\\nA Corpus-Based Approach to Language Learning.\\nPh.D. thesis, University of Pennsylvania.\\n\\n\\nBrill, Eric.\\n1993c.\\nRule based tagger, version 1.14.\\nAvailable from ftp.cs.jhu.edu in the directory /pub/brill/programs/.\\n\\n\\nBrill, Eric.\\n1994.\\nSome advances in transformation-based part of speech tagging.\\nIn Proceedings of the Twelfth National Conference on Artificial\\n  Intelligence, pages 722-727.\\n(cmp-lg/9406010).\\n\\n\\nBrill, Eric and Philip Resnik.\\n1994.\\nA rule-based approach to prepositional attachment disambiguation.\\nIn Proceedings of the Sixteenth International Conference on\\n  Computational Linguistics.\\n(cmp-lg/9410026).\\n\\n\\nChurch, Kenneth.\\n1988.\\nA stochastic parts program and noun phrase parser for unrestricted\\n  text.\\nIn Second Conference on Applied Natural Language Processing.\\n  ACL.\\n\\n\\nEjerhed, Eva I.\\n1988.\\nFinding clauses in unrestricted text by finitary and stochastic\\n  methods.\\nIn Second Conference on Applied Natural Language Processing,\\n  pages 219-227. ACL.\\n\\n\\nGee, James Paul and Franois Grosjean.\\n1983.\\nPerformance structures: A psycholinguistic and linguistic appraisal.\\nCognitive Psychology, 15:411-458.\\n\\n\\nKupiec, Julian.\\n1993.\\nAn algorithm for finding noun phrase correspondences in bilingual\\n  corpora.\\nIn Proceedings of the 31st Annual Meeting of the ACL, pages\\n  17-22.\\n\\n\\nMarcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann\\n  Bies, Mark Ferguson, Karen Katz, and Britta Schasberger.\\n1994.\\nThe Penn Treebank: A revised corpus design for extracting\\n  predicate argument structure.\\nIn Human Language Technology, ARPA March 1994 Workshop. Morgan\\n  Kaumann.\\n\\n\\nRamshaw, Lance A. and Mitchell P. Marcus.\\n1994.\\nExploring the statistical derivation of transformational rule\\n  sequences for part-of-speech tagging.\\nIn Proceedings of the ACL Balancing Act Workshop on Combining\\n  Symbolic and Statistical Approaches to Language, pages 86-95.\\n(cmp-lg/9406011).\\n\\n\\nVoutilainen, Atro.\\n1993.\\nNPTool, a detector of English noun phrases.\\nIn Proceedings of the Workshop on Very Large Corpora, pages\\n  48-57. ACL, June.\\n(cmp-lg/9502010).\\n\\nFootnotes\\n\\n  This heuristic fails in some cases.\\nFor example, Treebank uses the label NAC for some NPs\\nfunctioning as premodifiers, like ``Bank of England''\\nin ``Robin Leigh-Pemberton, Bank of England governor, conceded..'';\\nin such cases, ``governor'' is not included in any baseNP chunk.\\n  Non-constituent NP conjunction,\\nwhich Treebank labels NX,\\nis another example that still causes problems.\\n  Note that this is\\none of the cases where Church's chunker\\nallows separate NP fragments to count as chunks.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nEric Brill introduced\\ntransformation-based learning\\nand showed that it can do part-of-speech tagging\\nwith fairly high accuracy.\\nThe same method can be applied\\nat a higher level of textual interpretation\\nfor locating chunks in the tagged text,\\nincluding non-recursive ``baseNP'' chunks.\\nFor this purpose, it is convenient\\nto view chunking as a tagging problem\\nby encoding the chunk structure in new tags attached to each word.\\nIn automatic tests using Treebank-derived data,\\nthis technique achieved recall and precision rates of roughly 92%\\nfor baseNP chunks and 88%\\nfor somewhat more complex chunks that partition the sentence.\\nSome interesting adaptations\\nto the transformation-based learning approach\\nare also suggested by this application.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nTo predict and track the center of attention in discourse, theories of\\ncentering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and\\ngrammatical features of the text such as pronominalization and surface\\nsentence position. This may be sufficient for written discourse.\\nFor oral discourse, however, we must also consider the\\nway intonation affects\\nthe interpretation of a sentence, especially the cases\\nin which it alters the predictions of centering theories.\\nI investigate this via a phenomenon that, by the strictest\\ninterpretation of either centering or intonation theories, should not occur\\n-- the case of pitch accented pronominals.\\n\\n\\nCentering theories would be hard pressed to predict pitch accents on\\npronominals, on grounds of redundancy.  To bestow an intonational marker\\nof salience (the pitch accent) on a textual marker of salience (the\\npronominal) is unnecessarily redundant and especially when textual\\nfeatures correctly predict the focus of attention.\\n\\n\\nIntonational theories would be similarly hard pressed, but on grounds of\\ninformation quality and efficient use of limited resources. Given the\\nserial and ephemeral nature of speech and the limits of working\\nmemory, it is most expedient to mark as salient the information-rich\\nnonpronominals, rather than their semantically impoverished pronominal\\nstand-ins. To do otherwise is an injudicious use of an attentional cue.\\n\\n\\nHowever, when uttered with  contrastive stress on the pronouns,\\n (1) John\\n introduced Bill as a psycholinguist\\n  and then HE insulted HIM.\\n(after Lakoff, 1971) is felicitously understood to mean that after a\\nslanderous introduction, Bill retaliated in kind against John.\\n\\n\\nWhat makes (1) felicitous is that the pitch accents on the pronominals\\ncontribute attentional information that cannot be gleaned from text alone.\\nThis suggests an attentional component to pitch accents, in addition to\\nthe propositional component explicated in Pierrehumbert and Hirschberg\\n(1990).\\nIn this paper, I combine their account of\\npitch accent semantics with Grosz,  Joshi and Weinstein's (1989) account\\nof centering to yield insights into the phenomenon\\nof pitch accented pronominals, and the attentional consequences of pitch\\naccents in general.\\nThe relevant claims in  and  are reviewed in the next two sections.\\n\\n\\n  Pitch accent semantics \\n\\nA pitch accent is a distinctive intonational contour\\napplied to a word to convey sentential stress (Bolinger, 1958;\\nPierrehumbert, 1980).  catalogues six pitch accents, all combinations\\nof high (H) and low (L) pitch targets, and structured as a main tone\\nand an optional leading or trailing tone. The form of\\nthe accent -- L, H, L+H or H+L -- informs about the operation\\nthat would relate the salient item to the\\n mutual beliefs of the conversants; the main tone either commits (H*) or\\nfails to commit (L*) to the salience of the proposition itself, or the\\nrelevance of the operation.\\n\\n\\n\\nH* predicates a proposition as mutually believed, and proclaims\\nits addition to the set of mutual beliefs; L* fails to\\npredicate a proposition as mutually believed. As  points out,\\nfailure to predicate has contradictory sources: the proposition\\nhas already been predicated as mutually believed; or, the speaker, but\\nnot the hearer, is prevented from predication (perhaps by social\\nconstraints); or the speaker actively believes the salient proposition to be\\nfalse.\\n\\nH+L evokes an inference path. H*+L commits to the existence\\nof inference path that would support the proposition as mutually\\nbelieved, indicates that it can\\nbe found or derived from the set of mutual beliefs; H+L* conveys\\nuncertainty about the existence of such a path.\\n\\nL+H evokes a scale or ordered set to which the accented constituent\\nbelongs: L+H* commits to the salience of the scale, and is typically used\\nto convey contrastive stress; L*+H also evokes a scale but fails to commit to\\nits salience, e.g., conveying uncertainty about the salience of the\\nscale with regard to the accented constituent.\\n\\n\\n\\n\\n  Centering structures and operations \\n\\nTo explain how speakers move an entity in and out of the center of [mutual]\\nattention,  formalizes attentional operations with two computational\\nstructures -- the forward-looking center list (Cf) and the backward-looking center (the Cb). Cf is a partially ordered list of\\n centering candidates; the Cb, at the head of Cf, is the current center of attention.\\n\\n\\nAfter each utterance, one of three operations are possible:\\n\\n\\n\\nThe Cb retains both its position at the head of Cf and its status as\\nthe Cb; therefore it continues as the center in the next utterance.\\n\\nThe Cb retains its centered status for the current utterance\\nbut its rank is lowered -- it no longer resides at the head of Cf and\\ntherefore ceases to be the center in the next utterance.\\n\\nThe Cb loses both its centered status and ranking in the current\\nutterance as attention shifts to a new center.\\n\\n\\n\\n\\nIn addition,  constrains pronominalization such that no element in an\\nutterance can be realized as a pronoun unless the Cb is also realized as a\\npronoun, and imposes a preference ordering for operations on Cf, such that\\nthe least reordering is always preferred. That is, a sequence of\\ncontinuations is preferred over a sequence of retentions, which is\\npreferred over a sequence of shifts.\\n\\n\\n  When intonation and centering collide \\n\\nMy synthesis of the claims in  and   produces an\\nattentional interpretation of pitch accents,\\nmodeled by operations on Cf, and  derived for\\neach accent from their corresponding propositional effect as\\ndescribed in .\\n\\n\\nThe corollaries for pitch accented pronominals are:\\n(1) when a pitch accent is applied to a pronominal, its main effect\\nis attentional, on the order of items in Cf;  (2)  the obligation to accent a\\npronominal\\nfor attentional reasons depends on the variance between what the\\ntext  predicts and what the speaker would like to assert\\nabout the order of items in Cf.\\n\\n\\nThese hypotheses arise from the following chain of assumptions:\\n\\n\\n(1) To analyze the effects of pitch accents on pronominals, it is\\nnecessary to distinguish between attentional and propositional salience.\\nAttentional salience  measures\\nthe degree to which an item is salient, expressible as\\na partial ordering,  e.g., its ranking in Cf.   It is a quantitative\\nfeature.\\nIn contrast, propositional salience, addressing an item's status in relation\\nto mutual beliefs, is qualitative.\\nIt is  calculated through\\ninference chains that link semantic and pragmatic propositions.\\n\\n\\nBoth attentional (Cf) and propositional (mutual beliefs) structures\\nare updated throughout. However, unlike attentional structures\\nwhich are ephemeral in various time scales and empty at the end of the\\ndiscourse (Grosz and Sidner, 1986), mutual beliefs persist throughout the\\nconversation, preserving at the end the semantic and pragmatic outcome of\\nthe discourse.\\n\\n\\nIn addition, while propositions can be excluded from the mutual beliefs\\nbecause they fail to meet some inclusion criterion, no lexical denotation\\nis excluded from Cf regardless of its propositional value. This is because\\nthe  salience most relevant to the attentional state is the proximity of\\na discourse entity to the head of Cf -- the closer it is, the more it\\nis centered and therefore, attentionally salient.\\n\\n\\n(2) Pitch accents on pronominals are primarily interpreted for what\\nthey say about attentional salience. One determiner of whether attentional\\nor propositional effects are dominant is the type of information provided\\nby the accented constituent. Because nonpronominals contribute\\ndiscourse content, pitch accented nonpronominals are mainly interpreted\\nwith respect to the mutual beliefs, that is, for their propositional\\ncontent. However, pronominals, with little intrinsic semantics, perform\\nprimarily an attentional function. Therefore pitch accented pronominals are\\nmainly interpreted with respect to Cf, for their attentional content.\\n\\n\\n(3) The specific attentional consequences of each pitch\\naccent on pronominals can be extrapolated by analogy from\\nthe propositional interpretations in , by replacing\\nmutual beliefs with Cf as the salient set. Thus,\\n\\n\\n\\nH* indicates instantiation of the pronominal's cospecifier as the\\nCb, while L* fails to instantiate it as the Cb;\\n\\nThe partially ordered set (salient scale) invoked by L+H\\nis Cf;\\n\\nThe inference path evoked by H+L is, for attentional purposes, a\\ntraversal of Cf.\\n\\n\\n\\n\\n(4) And therefore, the attentional effect of pitch accents can be\\nformally expressed as an effect on the order of items in Cf.\\n\\n\\nFrom these assumptions, I derive the following attentional consequences\\nfor pitch accented pronominals:\\n\\n\\n\\nOnly one pitch accent, L+H*, selects a Cb other than that predicted by\\ncentering theory and thereby reorders Cf.\\n\\nL*+H appears to support an impending reordering but does not compel\\nit.\\n\\nBy analogy, the remaining pitch accents, seem to\\neither weaken or strengthen the current center's Cb status, but\\ndo not force a reordering.\\n\\n\\n\\n\\n  Availability of cospecifiers \\n\\nThe attentional interpretations are constrained by what has\\nbeen mutually established in the prior discourse, or is situationally\\nevident.\\nTherefore, while contrastive stress may be\\nmandated when grammatical features select the wrong cospecifier, the\\naccenting is only felicitous when there is an alternate referent\\navailable.\\n\\n\\nFor example, in\\n (2) John introduced Bill as a psycholinguist\\n  and then heL+H* insulted him.\\nL+H* indicates that he no longer cospecifies with John.\\nIf the hearer is hasty, she might\\nselect Bill as the new Cb. However, this is not borne out by the\\nunaccented him, which continues to cospecify with  Bill. Since he and\\nhim cannot select the same referent,\\nhe requires a cospecifier that is neither John\\nnor Bill.  Because, the utterance itself does not provide a\\nany other alternatives,\\nheL+H*\\nis only felicitous (and coherent) if an alternate cospecifier has been placed\\nin Cf by prior discourse,  or by\\nthe speaker's concurrent\\ndeictic gesture towards a discourteous male.\\n\\n\\n  Conclusion and Future Work \\n\\nBy combining Pierrehumbert and Hirschberg's (1990) analysis\\nof intonational meaning with Grosz, Joshi and Weinstein's (1989)\\ntheory of centering in discourse,  the attentional affect of pitch accents\\nbecomes evident, and the paradox of pitch accented pronominals\\nunravels.  My goal here is  to develop an analysis and a line of inquiry\\nand to suggest\\nthat my derivative\\nclaims are plausible, and even\\nextensible to an attentional analysis of pitch accents on\\nnonpronominals.\\nThe proof, of course, will come from investigation\\nby multiple means -- constructed examples (e.g., Cahn, 1990), computer\\nsimulation, empirical analysis of speech data (e.g., Nakatani, 1993),\\nand psycholinguistic experiments.\\n\\n\\n  References \\n\\nDwight Bolinger. A Theory of Pitch Accent in English. Word,\\n14(2-3):109-149, 1958.\\n\\n\\nSusan E. Brennan, Marilyn W. Friedman, and Carl J. Pollard. A Centering\\nApproach to Pronouns. Proceedings of the 25th Conference of the\\nAssociation for Computational Linguistics, 1987.\\n\\n\\nJanet Cahn. The Effect of Intonation on Pronoun Referent Resolution.\\nDraft, 1990.  Available as:\\nLearning and Common Sense TR 94-06, M.I.T. Media Laboratory.\\n\\n\\nHerbert H. Clark and Catherine R. Marshall. Definite Reference and Mutual\\nKnowledge.  In Webber, Joshi and Sag, editors, Elements of Discourse\\nUnderstanding. Cambridge University Press, 1981.\\n\\n\\nBarbara Grosz, Aravind K. Joshi, and Scott Weinstein. Providing a unified\\naccount of definite noun phrases in discourse. Proceedings of the 21st\\nConference of the Association for Computational Linguistics, 1983.\\n\\n\\nBarbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. Towards a\\nComputational Theory of Discourse Interpretation. Draft,\\n1989.\\n\\n\\nBarbara J. Grosz and Candace L. Sidner. Attention, Intentions, and the\\nStructure of Discourse. Computational Linguistics, 12(3):175-204,\\n1986.\\n\\n\\nGeorge Lakoff. Presupposition and relative well-formedness.  In Danny D.\\nSteinberg and Leon A. Jakobovits, editors, Semantics: An\\nInterdisciplinary Reader in Philosophy, Linguistics and Psychology,\\nCambridge University Press, 1971.\\n\\n\\nChristine Nakatani. Accenting on Pronouns and Proper Names in Spontaneous\\nNarrative. Proceedings of the European Speech Communication Association\\nWorkshop on Prosody, 1993.\\n\\n\\nJanet B. Pierrehumbert. The Phonology and Phonetics of English\\nIntonation. Ph.D. thesis, Massachusetts Institute of Technology, 1980.\\n\\n\\nJanet B. Pierrehumbert and Julia Hirschberg. The Meaning of Intonation\\nContours in the Interpretation of Discourse. In Philip R. Cohen, Jerry\\nMorgan, and Martha E. Pollack, editors, Intentions in Communication,\\nMIT Press, 1990.\\n\\n\\nCandace L. Sidner. Focusing in the Comprehension of Definite Anaphora. In\\nBarbara J. Grosz, Karen Sparck-Jones, and Bonnie Lynn Webber, editors, Readings in Natural Language Processing, Morgan Kaufman\\nPublishers, Inc., 1986.\\n\\nFootnotes\\n\\n  Mutual beliefs:\\npropositions expressed or implied by the discourse, and which all\\nconversants believe each other to accept as true and relevant\\nsame (Clark and Marshall, 1981).\\n  For simplicity's sake, we assume the items\\nin Cf to be words and phrases; in actuality, they may be\\nnonlexical representations of concepts, or some hybrid of lexical,\\nconceptual and sensory data.\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nBy strictest interpretation, theories of both centering and\\n  intonational meaning fail to predict the existence of pitch accented\\n  pronominals.  Yet they occur felicitously in spoken discourse.  To\\n  explain this, I emphasize the dual functions served by pitch\\n  accents, as markers of both propositional (semantic/pragmatic) and\\n  attentional salience.  This distinction underlies my proposals about\\n  the attentional consequences of pitch accents when applied to\\n  pronominals, in particular, that while most pitch accents may weaken\\n  or reinforce a cospecifier's status as the center of attention, a\\n  contrastively stressed pronominal may force a shift, even when\\n  contraindicated by textual features.\\n\\n\"],\n",
              " [\"\\n\\n    Introduction\\n\\n\\nEvaluating semantic relatedness using network representations is a problem\\nwith a long history in artificial intelligence and psychology, dating back to\\nthe spreading activation approach of Quillian  and\\nCollins and Loftus .  Semantic similarity represents a\\nspecial case of semantic relatedness: for example, cars and gasoline would\\nseem to be more closely related than, say, cars and bicycles, but the latter\\npair are certainly more similar.  Rada et al.  suggest that\\nthe assessment of similarity in semantic networks can in fact be thought of as\\ninvolving just taxonomic ( IS-A) links, to the exclusion of other link\\ntypes; that view will also be taken here, although admittedly it excludes some\\npotentially useful information.\\n\\n\\nA natural way to evaluate semantic similarity in a taxonomy is to evaluate the\\ndistance between the nodes corresponding to the items being compared -- the\\nshorter the path from one node to another, the more similar they are.  Given\\nmultiple paths, one takes the length of the shortest one\\n ,,. \\n\\n\\nA widely acknowledged problem with this approach, however, is that it relies\\non the notion that links in the taxonomy represent uniform distances.\\nUnfortunately, this is difficult to define, much less to control.  In real\\ntaxonomies, there is wide variability in the ``distance'' covered by a single\\ntaxonomic link, particularly when certain sub-taxonomies (e.g. biological\\ncategories) are much denser than others.  For example, in WordNet\\n , a broad-coverage semantic network for English  constructed by George Miller and colleagues at Princeton, it is not at all\\ndifficult to find links that cover an intuitively narrow distance (\\nRABBIT EARS  IS-A  TELEVISION ANTENNA) or an intuitively wide one\\n( PHYTOPLANKTON  IS-A  LIVING THING).  The same kinds of\\n examples can be found in the Collins COBUILD Dictionary , which identifies superordinate terms for many words (e.g.   SAFETY VALVE\\n IS-A  VALVE seems a lot narrower than  KNITTING MACHINE \\nIS-A  MACHINE).\\n\\n\\nIn this paper, I describe an alternative way to evaluate semantic similarity\\nin a taxonomy, based on the notion of information content.  Like the edge\\ncounting method, it is conceptually quite simple.  However, it is not\\nsensitive to the problem of varying link distances.  In addition, by combining\\na taxonomic structure with empirical probability estimates, it provides a way\\nof adapting a static knowledge structure to multiple contexts.\\n Section  sets up the probabilistic framework and defines the measure of semantic similarity in information-theoretic terms;\\n Section  presents an evaluation of the similarity measure against human similarity judgments, using the simple edge-counting method as a\\n baseline; and Section  discusses related work. \\n\\n\\n    Similarity and Information Content\\n\\n\\nLet \\nbe the set of concepts in an  IS-A taxonomy, permitting\\nmultiple inheritance.\\nIntuitively, one key to the similarity of two concepts is the extent to which\\nthey share information in common, indicated in an  IS-A taxonomy by a\\nhighly specific concept that subsumes them both.  The edge counting method\\ncaptures this indirectly, since if the minimal path of  IS-A links\\nbetween two nodes is long, that means it is necessary to go high in the\\ntaxonomy, to more abstract concepts, in order to find a least upper bound.\\nFor example, in WordNet,  NICKEL and  DIME are both subsumed\\nby  COIN, whereas the most specific superclass that  NICKEL\\n and  CREDIT CARD share is  MEDIUM OF EXCHANGE. (See Figure    .) \\n\\n\\nBy associating probabilities with concepts in the taxonomy, it is possible to\\ncapture the same idea, but avoiding the unreliability of edge distances.  Let\\nthe taxonomy be augmented with a function \\n\\n,\\nsuch that for any \\n\\n,\\n\\n\\nis the probability of encountering\\nan instance of concept c.  This implies that \\nis monotonic as one\\nmoves up the taxonomy: if \\n\\n,\\nthen \\n\\n.\\nMoreover, if the taxonomy has a unique top node then its\\nprobability is 1.  \\n\\n\\nFollowing the standard argumentation of information theory\\n , the information content of a  concept c can be quantified as negative the log likelihood, \\n\\n.\\nNotice that quantifying information content in this way makes intuitive sense\\nin this setting: as probability increases, informativeness decreases, so the\\nmore abstract a concept, the lower its information content.  Moreover, if\\nthere is a unique top concept, its information content is 0.\\n\\n\\nThis quantitative characterization of information provides a new way to\\nmeasure semantic similarity.  The more information two concepts share in\\ncommon, the more similar they are, and the information shared by two concepts\\nis indicated by the information content of the concepts that subsume them in\\nthe taxonomy.   Formally, define\\n\\nwhere \\n\\nS(c1,c2) is the set of concepts that subsume both c1 and c2.\\nNotice that although similarity is computed by considering all upper bounds\\nfor the two concepts, the information measure has the effect of identifying\\nminimal upper bounds, since no class is less informative than its\\n superordinates.  For example, in Figure ,  COIN,  CASH, etc. are all members of\\n\\n,\\nbut the concept that is\\nstructurally the minimal upper bound,  COIN, will also be the most\\ninformative.  This can make a difference in cases of multiple inheritance; for\\n example, in Figure ,  METAL and  CHEMICAL ELEMENT are not structurally distinguishable as \\nupper bounds of  NICKEL' and  GOLD', but their information\\ncontent may in fact be quite different.\\n\\n\\nIn practice, one often needs to measure word similarity, rather than concept\\nsimilarity.  Using s(w) to represent the set of concepts in the taxonomy\\nthat are senses of word w, define\\n\\nwhere c1 ranges over s(w1) and c2 ranges over s(w2).  This is\\nconsistent with Rada et al.'s  treatment of ``disjunctive\\nconcepts'' using edge counting: they define the distance between two\\ndisjunctive sets of concepts as the minimum path length from any element of\\nthe first set to any element of the second.  Here, the word similarity is\\njudged by taking the maximal information content over all concepts of which\\nboth words could be an instance.\\n For example, Figure  illustrates how the similarity of words nickel and gold would be computed: the information content\\nwould be computed for all classes subsuming any pair in the cross product of\\n{ NICKEL, NICKEL'} and { GOLD, GOLD'}, and\\nthe information content of the most informative class used to quantify the\\nsimilarity of the two words.\\n\\n\\n    Evaluation\\n\\n  Implementation \\n\\nThe work reported here used WordNet's (50,000-node) taxonomy of concepts\\n represented by nouns (and compound nominals) in English. Frequencies of concepts in the taxonomy were estimated using noun frequencies from the Brown Corpus of\\n American English , a large (1,000,000 word) collection of text across genres ranging from news articles to science fiction.  Each noun\\nthat occurred in the corpus was counted as an occurrence of each taxonomic\\n class containing it. For example, in Figure    , an occurrence of the noun dime would be counted toward the frequency of  DIME,\\n COIN, and so forth.  Formally,\\n\\nwhere \\n\\n\\nis the set of words subsumed by concept c.  Concept\\nprobabilities were computed simply as relative frequency:\\n\\nwhere N was the total number of nouns observed (excluding those not subsumed\\nby any WordNet class, of course).\\n\\n\\n  Task \\n\\nAlthough there is no standard way to evaluate computational measures of\\nsemantic similarity, one reasonable way to judge would seem to be agreement\\nwith human similarity ratings.  This can be assessed by using a computational\\nsimilarity measure to rate the similarity of a set of word pairs, and looking\\nat how well its ratings correlate with human ratings of the same pairs.\\n\\n\\nAn experiment by Miller and Charles  provided\\nappropriate human subject data for the task.  In their study, 38 undergraduate\\nsubjects were given 30 pairs of nouns that were chosen to cover high,\\nintermediate, and low levels of similarity (as determined using a previous\\n study ), and asked to rate ``similarity of meaning'' for each pair on a scale from 0 (no similarity) to 4 (perfect synonymy).  The\\naverage rating for each pair thus represents a good estimate of how similar\\nthe two words are, according to human judgments.\\n\\n\\nIn order to get a baseline for comparison, I replicated Miller and Charles's\\nexperiment, giving ten subjects the same 30 noun pairs.  The subjects were all\\ncomputer science graduate students or postdocs at the University of\\nPennsylvania, and the instructions were exactly the same as used by Miller and\\nCharles, the main difference being that in this replication the subjects\\ncompleted the questionnaire by electronic mail (though they were instructed to\\ncomplete the whole thing in a single uninterrupted sitting).  Five subjects\\nreceived the list of word pairs in a random order, and the other five received\\nthe list in the reverse order.  The correlation between the Miller and Charles\\nmean ratings and the mean ratings in my replication was .96, quite close to\\nthe .97 correlation that Miller and Charles obtained between their results and\\nthe ratings determined by the earlier study.\\n\\n\\nFor each subject in my replication, I computed how well his or her ratings\\ncorrelated with the Miller and Charles ratings.  The average correlation over\\nthe 10 subjects was \\n\\nr = 0.8848, with a standard deviation of\\n 0.08. This value represents an upper bound on what one should expect from a computational attempt to perform the same task.\\n\\n\\nFor purposes of evaluation, three computational similarity measures were used.\\nThe first is the similarity measurement using information content proposed in\\nthe previous section.  The second is a variant on the edge counting method,\\nconverting it from distance to similarity by subtracting the path length from\\nthe maximum possible path length:\\n\\n\\n\\n\\n\\nwhere c1 ranges over s(w1), c2 ranges over s(w2),  MAX is the\\nmaximum depth of the taxonomy, and \\n\\n\\nis the length of the\\nshortest path from c1 to c2 . (Recall that s(w) denotes the set of\\nconcepts in the taxonomy that represent senses of word w.)  Note that the\\nconversion from a distance to a similarity can be viewed as an expository\\nconvenience, and does not affect the evaluation: although the sign of the\\ncorrelation coefficient changes from positive to negative, its magnitude turns\\nout to be just the same regardless of whether or not the minimum path length\\nis subtracted from \\n\\n.\\n\\n\\nThe third point of comparison is a measure that simply uses the probability of\\na concept, rather than the information content:\\n\\\\mbox{\\\\rm sim}_{{\\\\rm p}(c)}(c_1, c_2)  =   \\\\raisebox{-1.5ex}{\\\\shortstack{{\\\\rm max}\\\\\\\\{\\\\small$c \\\\in S(c_1,c_2)$ }}}\\n\\\\left[ 1-{\\\\rm p}(c) \\\\right] \\\\\\\\\\n\\\\mbox{\\\\rm sim}_{{\\\\rm p}(c)}(w_1, w_2)  =   \\n                        \\\\raisebox{-1.5ex}{\\\\shortstack{{\\\\rm max}\\\\\\\\{\\\\small$c_1,c_2$ }}}  \\n                          \\\\left[ \\\\mbox{\\\\rm sim}_{{\\\\rm p}(c)}(c_1,c_2) \\\\right],\\n\\\\end{eqnarray} -->\\n where c1 ranges over s(w1) and c2 ranges over s(w2)in ().  Again, the difference between maximizing  \\n\\nand minimizing \\n\\n\\nturns out not to affect the magnitude of\\nthe correlation.  It simply ensures that the value can be interpreted as a\\nsimilarity value, with high values indicating similar words.\\n\\n\\n  Results \\n\\n Table  summarizes the experimental results, giving the correlation between the similarity ratings and the mean ratings reported by\\nMiller and Charles.  Note that, owing to a noun missing from the WordNet\\ntaxonomy, it was only possible to obtain computational similarity ratings for\\n28 of the 30 noun pairs; hence the proper point of comparison for human\\njudgments is not the correlation over all 30 items (r = .8848), but rather\\nthe correlation over the 28 included pairs (r = .9015).\\n The similarity ratings by item are given in Table . \\n\\n\\n    Discussion\\n\\n\\nThe experimental results in the previous section suggest that measuring\\nsemantic similarity using information content provides quite reasonable\\nresults, significantly better than the traditional method of simply counting\\nthe number of intervening  IS-A links.  \\n\\n\\nThe measure is not without its problems, however.  One problem is that, like\\nsimple edge counting, the measure sometimes produces spuriously high\\nsimilarity measures for words on the basis of inappropriate word senses.  For\\n example, Table  shows the word similarity for several words with tobacco.  Tobacco and alcohol are similar, both\\nbeing drugs, and tobacco and sugar are less similar, though\\nnot entirely dissimilar, since both can be classified as substances.\\nThe problem arises, however, in the similarity rating for tobacco with\\nhorse: the word horse can be used as a slang term for heroin, and as a result information-based similarity is maximized, and path\\nlength minimized, when the two words are both categorized as narcotics.  This\\nis contrary to intuition.\\n\\n\\nCases like this are probably relatively rare. However, the example illustrates\\na more general concern: in measuring similarity between words, it is really\\nthe relationship among word senses that matters, and a similarity\\nmeasure should be able to take this into account.\\n\\n\\nIn the absence of a reliable algorithm for choosing the appropriate word\\nsenses, the most straightforward way to do so in the information-based setting\\nis to consider all concepts to which both nouns belong rather than\\ntaking just the single maximally informative class.  This suggests redefining\\nsimilarity as follows:\\n{\\\\rm sim}(c_1,c_2)  =  \\\\sum_{i}\\\\alpha(c_i)[ -\\\\log{\\\\rm p}(c_i) ],\\n\\\\end{eqnarray} -->\\nwhere \\nis the set of concepts dominating both c1 and c2, as\\nbefore, and \\n\\n.\\nThis measure of similarity takes more\\ninformation into account than the previous one: rather than relying on the\\nsingle concept with maximum information content, it allows each class to contribute information content according to the value of\\n\\n.\\nIntuitively, these \\nvalues measure relevance -- for\\nexample, \\n\\n\\nmight be low in general usage but\\nhigh in the context of a newspaper article about drug dealers.  In work on\\nresolving syntactic ambiguity using semantic information\\n , I have found that local syntactic information can be used successfully to set values for the .\\n\\n\\n\\n    Related Work\\n\\n\\nAlthough the counting of edges in  IS-A taxonomies seems to be something\\nmany people have tried, there seem to be few published descriptions of\\nattempts to directly evaluate the effectiveness of this method.  A number of\\nresearchers have attempted to make use of conceptual distance in information\\nretrieval.  For example, Rada et al.  and Lee et\\nal.  report experiments using conceptual distance,\\nimplemented using the edge counting metric, as the basis for ranking documents\\nby their similarity to a query.  Sussna  uses semantic\\nrelatedness measured with WordNet in word sense disambiguation, defining a\\nmeasure of distance that weights different types of links and also explicitly\\ntakes depth in the taxonomy into account.\\n\\n\\nThe most relevant related work appears in an unpublished manuscript by Leacock\\nand Chodorow .  They have defined a measure\\nresembling information content, but using the normalized path length between\\nthe two concepts being compared rather than the probability of a subsuming\\nconcept.  Specifically, they define\\n\\\\mbox{\\\\rm sim}_{\\\\mbox{ndist}}(w_1, w_2)  = \\n- \\\\log \\\\left[ \\\\frac{\\\\raisebox{-1.5ex}{\\\\shortstack{{\\\\rm min}\\\\\\\\{\\\\small$c_1,c_2$ }}}\\\\mbox{len}(c_1,c_2)}{(2\\\\times\\\\mbox{\\\\sc max})}\\n        \\\\right].\\n\\\\end{eqnarray} -->\\n\\n\\n(The notation above is the same as for equation ().)  In addition to this definition, they also include several special cases, most\\nnotably to avoid infinite similarity when c1 and c2 are exact synonyms\\nand thus have a path length of 0. Leacock and Chodorow have experimented with\\nthis measure and the information content measure described here in the context\\nof word sense disambiguation, and found that they yield roughly similar\\nresults.  More significantly, I recently implemented their method and tested\\nit on the task reported in the previous section, and found that it actually\\noutperforms the information-based measure.  This led me to do a followup\\nexperiment using a different and larger set of noun pairs, and in the followup\\n study the information-based measure performed better. The relationship between the two algorithms will thus require further study.  For now, however,\\nwhat seems most significant is that both approaches take the form of a\\nlog-based (and hence information-like) measure, as originally proposed in\\n . \\n\\n\\nFinally, in the context of current research in computational linguistics, the\\napproach to semantic similarity taken here can be viewed as a hybrid,\\ncombining corpus-based statistical methods with knowledge-based taxonomic\\ninformation.  The use of corpus statistics alone in evaluating word similarity\\n-- without prior taxonomic knowledge -- is currently an active area of\\nresearch in the natural language community.  This is largely a reaction to\\nsparse data problems in training statistical language models: it is difficult\\nto come up with an accurate statistical characterization of the behavior of\\nwords that have been encountered few times or not at all.  Word similarity\\nappears to be one promising way to solve the problem: the behavior of a word\\nis approximated by smoothing its observed behavior together with the behavior\\nof words to which it is similar.  For example, a speech recognizer that has\\nnever seen the phrase ate a peach can still conclude that John\\nate a peach is a reasonable sequence of words in English if it has seen\\nother sentences like Mary ate a pear and knows that peach and\\npear have similar behavior.\\n\\n\\nThe literature on corpus-based determination of word similarity has recently\\nbeen growing by leaps and bounds, and is too extensive to discuss in detail\\n here (for a review, see ), but most approaches to the problem share a common assumption: semantically similar words have similar\\ndistributional behavior in a corpus.  Using this assumption, it is common to\\ntreat the words that co-occur near a word as constituting features, and to\\ncompute word similarity in terms of how similar their feature sets are.  As in\\ninformation retrieval, the ``feature'' representation of a word often takes\\nthe form of a vector, with the similarity computation amounting to a\\ncomputation of distance in a highly multidimensional space.  Given a distance\\nmeasure, it is not uncommon to derive word classes by hierarchical clustering.\\nA difficulty with most distributional methods, however, is how the measure of\\nsimilarity (or distance) is to be interpreted.  Although word classes\\nresulting from distributional clustering are often described as ``semantic,''\\nthey often capture syntactic, pragmatic, or stylistic factors as well.\\n\\n\\n    Conclusions\\n\\n\\nThis paper has presented a new measure of semantic similarity in an  IS-A\\ntaxonomy, based on the notion of information content.  Experimental evaluation\\nwas performed using a large, independently constructed corpus, an\\nindependently constructed taxonomy, and previously existing human subject\\ndata.  The results suggest that the measure performs encouragingly well (a\\ncorrelation of r = 0.79 with a benchmark set of human similarity judgments,\\nagainst an upper bound of r = 0.90 for human subjects performing the same\\ntask), and significantly better than the traditional edge counting approach\\n(r = 0.66).\\n\\n\\nIn ongoing work, I am currently exploring the application of\\ntaxonomically-based semantic similarity in the disambiguation of word senses\\n .  The idea behind the approach is that when polysemous words appear together, the appropriate word senses to assign are often those\\nthat share elements of meaning.  Thus doctor can refer to either a\\nPh.D. or an M.D., and nurse can signify either a health professional\\nor someone who takes care of small children; but when doctor and nurse are seen together, the Ph.D. sense and the childcare sense go by the\\nwayside.  In a widely known paper, Lesk  exploits\\ndictionary definitions to identify shared elements of meaning -- for example,\\n in the Collins COBUILD Dictionary , the word ill can be found in the definitions of the correct senses.  More recently, Sussna\\n has explored using similarity of word senses based on\\nWordNet for the same purpose.  The work I am pursuing is similar in spirit to\\nSussna's approach, although the disambiguation algorithm and the similarity\\nmeasure differ substantially.\\n\\nBibliography \\n\\nA. Collins and E. Loftus.\\nA spreading activation theory of semantic processing.\\nPsychological Review, 82:407-428, 1975.\\n\\n\\nW. N. Francis and H. Kucera.\\nFrequency Analysis of English Usage: Lexicon and Grammar.\\nHoughton Mifflin, 1982.\\n\\n\\nClaudia Leacock and Martin Chodorow.\\nFilling in a sparse training space for word sense identification.\\nms., March 1994.\\n\\n\\nJoon Ho Lee, Myoung Ho Kim, and Yoon Joon Lee.\\nInformation retrieval based on conceptual distance in IS-A\\n  hierarchies.\\nJournal of Documentation, 49(2):188-207, June 1993.\\n\\n\\nMichael Lesk.\\nAutomatic sense disambiguation using machine readable dictionaries:\\n  how to tell a pine cone from an ice cream cone.\\nIn Proceedings of the 1986 SIGDOC Conference, pages 24-26,\\n  1986.\\n\\n\\nGeorge A. Miller and Walter G. Charles.\\nContextual correlates of semantic similarity.\\nLanguage and Cognitive Processes, 6(1):1-28, 1991.\\n\\n\\nGeorge Miller.\\nWordNet: An on-line lexical database.\\nInternational Journal of Lexicography, 3(4), 1990.\\n(Special Issue).\\n\\n\\nM. Ross Quillian.\\nSemantic memory.\\nIn M. Minsky, editor, Semantic Information Processing. MIT\\n  Press, Cambridge, MA, 1968.\\n\\n\\nRoy Rada and Ellen Bicknell.\\nRanking documents with a thesaurus.\\nJASIS, 40(5):304-310, September 1989.\\n\\n\\nRoy Rada, Hafedh Mili, Ellen Bicknell, and Maria Blettner.\\nDevelopment and application of a metric on semantic nets.\\nIEEE Transaction on Systems, Man, and Cybernetics,\\n  19(1):17-30, February 1989.\\n\\n\\nPhilip Resnik.\\nSelection and Information: A Class-Based Approach to Lexical\\n  Relationships.\\nPhD thesis, University of Pennsylvania, December 1993.\\n\\n\\nPhilip Resnik.\\nSemantic classes and syntactic ambiguity.\\nIn Proceedings of the 1993 ARPA Human Language Technology\\n  Workshop. Morgan Kaufmann, March 1993.\\n\\n\\nPhilip Resnik.\\nDisambiguating noun groupings with respect to WordNet senses.\\nIn Third Workshop on Very Large Corpora. Association for\\n  Computational Linguistics, 1995.\\n\\n\\nSheldon Ross.\\nA First Course in Probability.\\nMacmillan, 1976.\\n\\n\\nHerbert Rubenstein and John Goodenough.\\nContextual correlates of synonymy.\\nCACM, 8(10):627-633, October 1965.\\n\\n\\nJohn Sinclair (ed.).\\nCollins COBUILD English Language Dictionary.\\nCollins: London, 1987.\\n\\n\\nMichael Sussna.\\nWord sense disambiguation for free-text indexing using a massive\\n  semantic network.\\nIn Proceedings of the Second International Conference on\\n  Information and Knowledge Management (CIKM-93), Arlington, Virginia, 1993.\\n\\n\\nA. Tversky.\\nFeatures of similarity.\\nPsychological Review, 84:327-352, 1977.\\n\\n\\nSholom M. Weiss and Casimir A. Kulikowski.\\nComputer systems that learn: classification and prediction\\n  methods from statistics, neural nets, machine learning, and expert systems.\\nMorgan Kaufmann, San Mateo, CA, 1991.\\n\\nFootnotes\\n\\n  Appears in Proceedings of\\nIJCAI-95. Portions of this research \\nwere supported by an IBM Graduate Fellowship\\nand grants ARO DAAL 03-89-C-0031, DARPA N00014-90-J-1863, NSF IRI 90-16592,\\nand Ben Franklin 91S.3078C-1.\\n  In a\\n feature-based setting (e.g. ), this would be reflected by explicit shared features: nickels and dimes are both small, round, metallic,\\nand so on.  These features are captured implicitly by the taxonomy in\\ncategorizing  NICKEL and  DIME as subordinates of\\n COIN.\\n  Concept as used here refers to what Miller et al.  call\\na synset, essentially a node in the taxonomy.\\n  Plural nouns counted as instances of their\\nsingular forms.\\n  Inter-subject correlation in the replication, estimated using\\n leaving-one-out resampling , was  \\n.\\n  In the followup\\nstudy, I used netnews archives to gather highly frequent nouns within related\\ntopic areas, and then selected noun pairings at random, in order to avoid\\nbiasing the followup study in favor of either algorithm.\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper presents a new measure of semantic similarity in an  IS-A\\ntaxonomy, based on the notion of information content.  Experimental evaluation\\nsuggests that the measure performs encouragingly well (a correlation of r =\\n0.79 with a benchmark set of human similarity judgments, with an upper bound\\nof r = 0.90 for human subjects performing the same task), and significantly\\nbetter than the traditional edge counting approach (r = 0.66).\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nGenerative grammar and formal language theory share a common origin in\\na procedural notion of grammars: the grammar formalism provides\\na general mechanism for recognizing or generating languages while the\\ngrammar itself specializes that mechanism for a specific language.  At least\\ninitially there was hope that this relationship would be informative\\nfor linguistics, that by characterizing the natural languages in terms\\nof language-theoretic complexity one would gain insight into the\\nstructural regularities of those languages.  Moreover, the fact\\nthat language-theoretic complexity classes have dual automata-theoretic\\ncharacterizations offered the prospect that such results might provide abstract\\nmodels of the human language faculty, thereby  not just identifying these\\nregularities, but actually accounting for them.\\n\\n\\nOver time, the two disciplines have\\ngradually become estranged, principally due to \\na realization that the structural properties of languages\\nthat characterize natural languages may well not be those that can be\\ndistinguished by existing language-theoretic complexity classes.  Thus the\\ninsights offered by formal language theory might actually be misleading in\\nguiding theories of syntax.  As a\\nresult, the emphasis in generative grammar has turned from formalisms\\nwith restricted generative capacity to those that\\nsupport more natural expression of the observed regularities of\\nlanguages.  While a variety of distinct approaches have developed, most\\nof them can be characterized as constraint based--the formalism\\n(or formal framework) provides a class of structures and a means of\\nprecisely stating constraints on their form, the linguistic theory is\\nthen expressed as a system of constraints (or principles) that characterize the\\n class of well-formed analyses of the strings in the language. \\n\\n\\nAs the study of the formal properties of classes of structures defined in such\\na way falls within domain of Model Theory, it's not surprising that\\ntreatments of the meaning of these systems of constraints are typically couched\\nin terms of formal \\n logic ,,,,,,,,,. \\n\\n\\nWhile this provides a model-theoretic interpretation of the systems of\\nconstraints produced by these formalisms, those systems are\\ntypically built by derivational processes that employ extra-logical mechanisms\\nto combine constraints.\\nMore recently, it has become clear that in many cases these mechanisms can\\nbe replaced with ordinary logical operations.  (See, for\\ninstance: \\n, , , , , , , ,\\nand, anticipating all of these, .) \\nThis approach abandons\\nthe notions of grammar mechanism and derivation in favor of defining \\nlanguages as classes of more or less ordinary mathematical structures\\naxiomatized by sets of more or less ordinary logical formulae.  A grammatical\\ntheory expressed within such a framework is just \\nthe set of logical consequences of those axioms.  This step completes the\\ndetachment of generative grammar from its procedural roots.  Grammars, in this\\napproach, are purely declarative definitions of a class of structures,\\ncompletely independent of mechanisms to generate or check them.\\nWhile it is unlikely that every theory of syntax with an explicit derivational\\n component can be captured in this way, for those that can the logical re-interpretation frequently offers\\na simplified statement of the theory and clarifies its consequences.\\n\\n\\nBut the accompanying loss of language-theoretic complexity results is\\nunfortunate.  While such results may not be useful in guiding syntactic theory,\\nthey are not irrelevant.  The nature of language-theoretic complexity\\nhierarchies is to classify languages on the basis of their structural\\nproperties.  The languages in a class, for instance, will typically exhibit\\ncertain closure properties (e.g., pumping lemmas) and the classes themselves\\nadmit normal forms (e.g., representation theorems).  While the linguistic\\nsignificance of individual \\nresults of this sort is open to debate, they at least loosely parallel typical\\nlinguistic concerns:  closure properties state regularities that are exhibited\\nby the languages in a class, normal forms express generalizations about their\\nstructure.  So while these may not be the right results, they are not entirely\\nthe wrong kind of results.  Moreover, since these classifications are\\nbased on structural properties and the structural properties of natural\\nlanguage can be studied more or less directly, there is a reasonable\\nexpectation of finding empirical evidence falsifying a hypothesis about\\nlanguage-theoretic complexity of natural languages if such evidence exists.\\n\\n\\nFinally, the fact that these complexity classes have automata-theoretic\\ncharacterizations means that results concerning the complexity of\\nnatural languages will have implications for the nature of the human language\\nfaculty.  These automata-theoretic characterizations determine, along one axis,\\nthe types of resources required to generate or recognize the languages in a\\nclass.  The regular languages, for instance, can be characterized by\\nfinite-state (string) automata--these languages can be processed\\nusing a fixed amount of memory.  The context-sensitive languages, on the other\\nhad, can be characterized by linear-bounded automata--they can be\\nprocessed using \\nan amount of memory proportional to the length of the input.  The context-free\\nlanguages are probably best characterized by finite-state tree\\nautomata--these \\ncorrespond to recognition by a collection of processes, each with a fixed\\namount of memory,  where the number of processes is linear in the length of the\\ninput and all communication between processes is completed at the time they are\\nspawned.  As a result, while these results do not necessarily offer abstract\\nmodels of the human language faculty (since the complexity results do not\\nclaim to characterize the human languages, just to classify them), they do\\noffer lower bounds on certain abstract properties of that faculty.  In this\\nway, generative grammar in concert with formal language theory offers\\ninsight into a deep aspect of human \\ncognition--syntactic processing--on the basis of observable behavior--the\\nstructural properties of human languages.\\n\\n\\nIn this paper we discuss an approach to defining theories of syntax based on\\n\\n , a monadic second-order  language that has well-defined generative capacity: sets of finite trees are\\ndefinable within \\n\\n\\niff they are strongly context-free in a particular\\nsense.  While originally introduced as a means of establishing\\nlanguage-theoretic complexity results for constraint-based theories, this\\nlanguage has much to recommend it as a general framework for theories\\nof syntax in its own right.  Being a monadic second-order language it can\\ncapture the (pure) modal languages of much of the existing model-theoretic\\nsyntax literature directly; \\nhaving a signature based on the traditional linguistic relations of domination,\\nimmediate domination, linear precedence, etc. it can express most linguistic\\nprinciples transparently; and having a clear characterization in terms of\\ngenerative capacity, it serves to re-establish the close connection between\\ngenerative grammar and formal language theory that was lost in the move away\\nfrom phrase-structure grammars.  Thus,\\nwith this framework we get both the advantages of the model-theoretic\\napproach with respect to naturalness and clarity in expressing linguistic\\nprinciples and the advantages of the grammar-based approach with respect to\\nlanguage-theoretic complexity results.  \\n\\n\\nWe look, in particular, at the\\ndefinitions of a single aspect of each of GPSG and GB.  The first of these,\\nFeature Specification Defaults in GPSG, are widely assumed\\nto have an inherently dynamic character.  In addition to being purely\\ndeclarative, our reformalization is considerably simplified wrt the definition\\n in , and does not share its   misleading dynamic flavor.  We offer this as an example of \\nhow re-interpretations of this sort can inform the original theory.  In the\\nsecond example we sketch a definition of chains in GB.  This, again,\\ncaptures a \\npresumably dynamic aspect of the original theory in a static way.  Here,\\nthough, the main significance of the definition is that it forms a component of\\na full-scale treatment of a GB theory of English S- and D-Structure within\\n\\n.\\nThis full definition establishes that the theory we capture licenses a\\nstrongly context-free language.  More importantly, by examining the limitations\\nof this definition of \\nchains, and in particular the way it fails for examples of non-context-free\\nconstructions, we develop a characterization of the context-free languages that\\nis quite natural in the realm of GB.  This suggests that the apparent mismatch\\nbetween formal language theory and natural languages may well have more to do\\nwith the unnaturalness of the traditional diagnostics than a lack of relevance\\nof the underlying structural properties.\\n\\n\\nFinally, while GB and GPSG are fundamentally distinct, even antagonistic,\\napproaches to syntax, their translation into the model-theoretic terms of\\n\\n\\nallows us to explore the similarities between the theories they express\\nas well as to delineate actual distinctions between them.  We look briefly\\nat two of these issues.\\n\\n\\nTogether these examples are chosen to illustrate the main strengths of\\nthe model-theoretic approach, at least as embodied in \\n\\n,\\nas a\\nframework for studying theories of syntax: a focus on structural\\nproperties themselves, rather than on mechanisms for specifying them\\nor for generating or checking structures that exhibit them, and a\\nlanguage that is expressive enough to state most linguistically\\nsignificant properties in a natural way, but which is restricted\\nenough to have well-defined strong generative capacity.\\n\\n\\n  The Monadic Second-Order Language of  Trees \\n\\n\\n\\nis the monadic second-order language over the signature including a set\\nof individual constants (K), a set of monadic predicates (P), and binary\\npredicates for immediate domination (\\n\\n), domination (\\n\\n), linear\\nprecedence (\\n\\n)\\nand equality (\\n\\n).  The predicates in P can be\\nunderstood both as picking out particular subsets of the tree and as\\n(non-exclusive) labels or features decorating the tree.  Models for the\\n language are labeled tree domains  with the natural interpretation of the binary predicates.  In  we\\nhave shown that \\nthis language is equivalent in descriptive power to SS--the monadic\\nsecond-order theory of the complete infinitely branching tree--in the sense\\nthat sets of trees are definable in SS iff they are definable in\\n\\n.\\nThis places it within a hierarchy of results relating\\nlanguage-theoretic complexity classes to the descriptive complexity of their\\nmodels: the sets of strings definable in S1S are exactly the regular\\n sets , the sets of finite trees definable in SnS, for finite n, are the recognizable sets (roughly the sets of derivation trees of\\n CFGs) , and, it can be shown, the sets of finite trees definable in SS are those generated by generalized CFGs in which regular\\nexpressions may occur on the rhs of rewrite\\n rules .  Consequently, languages are definable in \\n\\n\\niff they are strongly context-free in the mildly generalized sense of\\nGPSG grammars.\\n\\n\\nIn restricting ourselves to the language of \\n\\n\\nwe are restricting ourselves\\nto reasoning in terms of just the predicates of its signature.  We can expand\\nthis by defining new predicates, even higher-order predicates that express, for\\ninstance, properties of or relations between sets, and in doing so we can use\\nmonadic predicates and individual constants freely since we can interpret these\\nas existentially bound variables.  But the fundamental restriction of \\n\\n\\nis\\nthat all predicates other than monadic first-order predicates must be\\nexplicitly defined, that is, their definitions must resolve, via syntactic\\nsubstitution, into formulae involving only the signature of \\n\\n.\\n\\n\\n  Feature Specification Defaults in GPSG \\n\\nWe now turn to our first application--the definition of Feature Specification\\n Defaults (FSDs) in GPSG.  Since GPSG is presumed to license (roughly) context-free languages, we are not concerned here with establishing\\nlanguage-theoretic complexity but rather with clarifying the linguistic theory\\nexpressed by GPSG.  FSDs specify conditions on feature values that must hold at\\na node in a licensed tree unless they are overridden by some other component of\\nthe grammar; in particular, unless they are incompatible with either a feature\\nspecified by the ID rule licensing the node (inherited features) or a\\nfeature required by one of the agreement principles--the Foot Feature\\nPrinciple (FFP), Head Feature Convention (HFC), or Control Agreement Principle\\n(CAP).  It is the fact that the default holds just in case it is incompatible\\nwith these other components that gives FSDs their dynamic flavor.  Note,\\nthough, in contrast to typical applications of default logics, a GPSG grammar\\nis not an evolving theory.  The exceptions to the defaults are fully determined\\nwhen the grammar is written.  If\\nwe ignore for the moment the effect of the agreement principles, the defaults\\nare roughly the converse of the ID rules: a non-default feature occurs iff it\\nis licensed by an ID rule.\\n\\n\\nIt is easy to capture ID rules in \\n\\n.\\nFor instance the rule:\\n\\n\\n\\n\\n\\ncan be expressed:\\n\\n\\n\\n\\n\\nwhere \\n\\n\\nholds iff the set of nodes that are\\nchildren of x are just the yi and  VPVP  VP , \\n\\n,\\netc. are\\n all members of P.  A sequence of nodes will satisfy \\n\\n\\niff they form a local tree that, in the\\nterminology of GKPS, is induced by the corresponding ID rule.  Using\\nsuch encodings we can define a predicate \\n\\n\\nwhich is true at a\\nnode x iff the feature f is compatible with the inherited features of x.\\n\\n\\nThe agreement principles require pairs of nodes occurring in certain\\nconfigurations in local trees to agree on certain classes of features.  Thus\\nthese principles do not introduce features into the trees, but rather propagate\\nfeatures from one node to another, possibly in many steps.  Consequently, these\\nprinciples cannot override FSDs by themselves; rather every violation of a\\ndefault must be licensed by an inherited feature somewhere in the tree.  In\\norder to account for this propagation of features, the definition of FSDs in\\nGKPS is based on \\nidentifying pairs of nodes that co-vary wrt the relevant features in all\\npossible extensions of the given tree.  As a result, although the treatment in\\nGKPS is actually declarative, this fact is far from obvious.\\n\\n\\nAgain, it is not difficult to define the configurations of local trees in which\\nnodes are required to agree by FFP, CAP, or HFC in \\n\\n.\\nLet the predicate\\n\\n\\nhold for a pair of nodes x and y iff they are\\nrequired to agree on f by one of these principles (and are, thus, in the\\nsame local tree).  Note that \\n\\n\\nis symmetric.  Following the\\nterminology of GKPS, we can identify the set of nodes that are prohibited \\nfrom taking feature f by the combination of the ID rules, FFP, CAP, and HFC\\nas the set of nodes that are privileged wrt f.  This includes all\\nnodes that are not Free for f as well as any node connected to such a node by\\na sequence of \\n\\n\\nlinks.  We, in essence, define this\\ninductively.  \\n\\n\\nis true of a set iff it includes all nodes not Free\\nfor f and is closed wrt \\n\\n.\\n\\n\\nis true of\\nthe smallest such set.\\n\\n\\n\\n\\n\\n\\n\\n\\nThere are two things to note about this definition.  First, in any tree there\\nis a unique set satisfying \\n\\n\\nand this contains exactly those\\nnodes not Free for f or connected to such a node by \\n\\n.\\nSecond, while this is a first-order inductive property, the definition is a\\nsecond-order explicit definition.  In fact, the second-order quantification of\\n\\n\\nallows us to capture any monadic first-order inductively or implicitly\\ndefinable property explicitly.\\n\\n\\nArmed with this definition, we can identify individuals that are privileged wrt\\nf simply as the members of \\n\\n. \\n\\n\\n\\n\\n\\nOne can define \\n\\n\\nwhich holds whenever x is\\nrequired to take the feature f along similar lines.\\n\\n\\nThese, then, let us capture FSDs.  For the default \\n  [_-INV] \\n\\n\\n, for instance, we get:\\n\\n\\n\\n\\n\\nFor \\n\\n\\n(which says that \\n  [_Bar 0] \\n\\n\\nnodes are, by default, not marked passive), we get:\\n\\n\\n\\n\\n\\nThe key thing to note about this treatment of FSDs is its simplicity relative\\nto the treatment of GKPS.  The second-order quantification allows us to\\nreason directly in terms of the sequence of nodes extending from the privileged\\nnode to the local tree that actually licenses the privilege.  The immediate\\nbenefit is the fact that it is clear that the property of satisfying a set of\\nFSDs is a static property of labeled trees and does not depend on the\\nparticular strategy employed in checking the tree for compliance.\\n\\n\\n  Chains in GB \\n\\nThe key issue in capturing GB theories within \\n\\n\\nis the fact that the\\nmechanism of free-indexation is provably non-definable.  Thus definitions of\\nprinciples that necessarily employ free-indexation have no direct\\ninterpretation in \\n\\n\\n(hardly surprising, as we expect GB to be capable of\\nexpressing non-context-free languages).  In many cases, though, references to\\nindices can be eliminated in favor of the underlying structural relationships\\n they express. The most prominent example is the definition of the chains\\nformed by move-.\\nThe fundamental problem here is identifying each\\ntrace with its antecedent without referencing their index.  Accounts of the\\nlicensing of traces that, in many cases of movement, replace co-indexation with\\ngovernment relations have been offered by both \\nand .  The key element of these accounts, from our point \\nof view, is that the antecedent of a trace must be the closest\\nantecedent-governor of the appropriate type.  These relationships are easy to\\ncapture in \\n\\n.\\nFor \\n\\n\\n\\n\\n  A Comparison and a Contrast \\n\\nHaving interpretations both of GPSG and of a GB account of English\\nin \\n\\n\\nprovides a certain amount of insight into the distinctions\\nbetween these approaches.  For example, while the explanations of\\nfiller-gap relationships in GB and GPSG are quite\\ndramatically dissimilar, when one focuses on the structures these\\naccounts license one finds some surprising parallels.  In the light of\\nour interpretation of antecedent-government, one can understand the\\nrole of minimality in Rizzi's and Manzini's accounts as eliminating\\nambiguity from the sequence of relations connecting the gap with its\\nfiller.  In GPSG this connection is made by the sequence of agreement\\nrelationships dictated by the Foot Feature Principle.  So while both\\ntheories accomplish agreement between filler and gap through marking a\\nsequence of elements falling between them, the GB account marks\\nas few as possible while the GPSG account marks every node of the\\nspine of the tree spanning them.  In both cases, the complexity of the\\nset of licensed structures can be limited to be strongly context-free\\niff the number of relationships that must be distinguished in a given\\ncontext can be bounded.\\n\\n\\nOne finds a strong contrast, on the other hand, in the way in which GB\\nand GPSG encode language universals.  In GB it is presumed that\\nall principles are universal with the theory being specialized to\\nspecific languages by a small set of finitely varying parameters.\\nThese principles are simply properties of trees.  In terms of models,\\none can understand GB to define a universal language--the set of all\\nanalyses that can occur in human languages.  The principles then\\ndistinguish particular sub-languages--the head-final or the pro-drop\\nlanguages, for instance.  Each realized human language is just the\\nintersection of the languages selected by the settings of its\\nparameters.  In GPSG, in contrast, many universals are, in\\nessence, closure properties that must be exhibited by human\\nlanguages--if the language includes trees in which a particular\\nconfiguration occurs then it includes variants of those trees in which\\ncertain related configurations occur.  Both the ECPO principle and\\nthe metarules can be understood in this way.  Thus while\\nuniversals in GB are properties of trees, in GPSG they tend to be\\nproperties of sets of trees.  This makes a significant\\ndifference in capturing these theories model-theoretically; in the GB\\ncase one is defining sets of models, in the GPSG case one is defining\\nsets of sets of models.  It is not at all clear what the linguistic\\nsignificance of this distinction is; one particularly interesting question is\\nwhether it has empirical consequences.  It is only from the\\nmodel-theoretic perspective that the question even arises.\\n\\n\\n  Conclusion \\n\\nWe have illustrated a general formal framework for expressing theories of\\nsyntax based on axiomatizing classes of models in \\n\\n.\\nThis approach has a\\nnumber of strengths.  First, as should be clear from our brief explorations of\\naspects of GPSG and GB, re-formalizations of existing theories \\nwithin \\n\\n\\ncan offer a clarifying perspective on those theories, and, in\\nparticular, on the consequences of individual components of those theories.\\nSecondly, the framework is purely declarative and focuses on those aspects of\\nlanguage that are more or less directly observable--their structural\\nproperties.  It allows us to reason about the consequences of a theory without\\nhypothesizing a specific mechanism implementing it.  The abstract properties of\\nthe mechanisms that might implement those theories, however, are not beyond our\\nreach.  The key virtue of descriptive complexity results like the\\ncharacterizations of language-theoretic complexity classes discussed here and\\nthe more typical characterizations of computational complexity\\n classes , is that  they allow us to determine the complexity of checking properties independently\\nof how that checking is implemented.  Thus we can use such descriptive\\ncomplexity results to draw conclusions about those\\nabstract properties of such mechanisms that are actually inferable from their\\nobservable behavior.  Finally, by providing a uniform representation for a\\nvariety of linguistic theories, it offers a framework for comparing their\\nconsequences.  Ultimately it has the potential to reduce distinctions between\\nthe mechanisms underlying those theories to distinctions between the properties\\nof the sets of structures they license.  In this way one might hope to\\nilluminate the empirical consequences of these distinctions, should any, in\\nfact, exist.\\n\\nBibliography \\n\\nBlackburn, Patrick, Claire Gardent, and Wilfried Meyer-Viol.\\n1993.\\nTalking about trees.\\nIn EACL 93, pages 21-29. European Association for\\n  Computational Linguistics.\\n\\n\\nBlackburn, Patrick and Wilfried Meyer-Viol.\\n1994.\\nLinguistics, logic, and finite trees.\\nBulletin of the IGPL, 2(1):3-29, March.\\n\\n\\nBchi, J. R.\\n1960.\\nWeak second-order arithmetic and finite automata.\\nZeitschrift fr mathematische Logik und Grundlagen der\\n  Mathematik, 6:66-92.\\n\\n\\nCarpenter, Bob.\\n1992.\\nThe Logic of Typed Feature Structures; with Applications to\\n  Unification Grammars, Logic Programs and Constraint Resolution.\\nNumber 32 in Cambridge Tracts in Theoretical Computer Science.\\n  Cambridge University Press.\\n\\n\\nCornell, Thomas Longacre.\\n1992.\\nDescription Theory, Licensing Theory, and Principle-Based\\n  Grammars and Parsers.\\nPh.D. thesis, University of California Los Angeles.\\n\\n\\nDawar, Anuj and K. Vijay-Shanker.\\n1990.\\nAn interpretation of negation in feature structure descriptions.\\nComputational Linguistics, 16(1):11-21.\\n\\n\\nDoner, John.\\n1970.\\nTree acceptors and some of their applications.\\nJournal of Computer and System Sciences, 4:406-451.\\n\\n\\nGazdar, Gerald, Ewan Klein, Geoffrey Pullum, and Ivan Sag.\\n1985.\\nGeneralized Phrase Structure Grammar.\\nHarvard University Press.\\n\\n\\nGazdar, Gerald, Geoffrey Pullum, Robert Carpenter, Ewan Klein, T. E. Hukari,\\n  and R. D. Levine.\\n1988.\\nCategory structures.\\nComputational Linguistics, 14:1-19.\\n\\n\\nGorn, Saul.\\n1967.\\nExplicit definitions and linguistic dominoes.\\nIn John F. Hart and Satoru Takasu, editors, Systems and Computer\\n  Science, Proceedings of the Conference held at Univ. of Western Ontario,\\n  1965. Univ. of Toronto Press.\\n\\n\\nGurevich, Yuri.\\n1988.\\nLogic and the challenge of computer science.\\nIn E. Brger, editor, Current Trends in Theoretical Computer\\n  Science. Computer Science Press, chapter 1, pages 1-57.\\n\\n\\nImmerman, Neil.\\n1989.\\nDescriptive and computational complexity.\\nIn Proceedings of Symposia in Applied Mathematics, pages\\n  75-91. American Mathematical Society.\\n\\n\\nJohnson, David E. and Paul M. Postal.\\n1980.\\nArc Pair Grammar.\\nPrinceton University Press, Princeton, New Jersey.\\n\\n\\nJohnson, Mark.\\n1988.\\nAttribute-Value Logic and the Theory of Grammar.\\nNumber 16 in CSLI Lecture Notes. Center for the Study of Language and\\n  Information, Stanford, CA.\\n\\n\\nJohnson, Mark.\\n1989.\\nThe use of knowledge of language.\\nJournal of Psycholinguistic Research, 18(1):105-128.\\n\\n\\nKasper, Robert T. and William C. Rounds.\\n1986.\\nA logical semantics for feature structures.\\nIn Proceedings of the 24th Annual Meeting of the Association for\\n  Computational Linguistics.\\n\\n\\nKasper, Robert T. and William C. Rounds.\\n1990.\\nThe logic of unification in grammar.\\nLinguistics and Philosophy, 13:35-58.\\n\\n\\nKeller, Bill.\\n1993.\\nFeature Logics, Infinitary Descriptions and Grammar.\\nNumber 44 in CSLI Lecture Notes. Center for the Study of Language and\\n  Information.\\n\\n\\nKracht, Marcus.\\n1995.\\nSyntactic codes and grammar refinement.\\nJournal of Logic, Language, and Information, 4:41-60.\\n\\n\\nManzini, Maria Rita.\\n1992.\\nLocality: A Theory and Some of Its Empirical Consequences.\\nMIT Press, Cambridge, Ma.\\n\\n\\nMoshier, M. Drew and William C. Rounds.\\n1987.\\nA logic for partially specified data structures.\\nIn ACM Symposium on the Principles of Programming Languages.\\n\\n\\nRizzi, Luigi.\\n1990.\\nRelativized Minimality.\\nMIT Press.\\n\\n\\nRogers, James.\\n1994.\\nStudies in the Logic of Trees with Applications to Grammar\\n  Formalisms.\\nPh.D. dissertation, Univ. of Delaware.\\n\\n\\nRogers, James.\\n1995.\\nOn descriptive complexity, language complexity, and GB.\\nIn Patrick Blackburn and Maarten de Rijke, editors, Specifying\\n  Syntactic Structures. In Press.\\nAlso available as IRCS Technical Report 95-14. cmp-lg/9505041.\\n\\n\\nRogers, James.\\n1996a.\\nA Descriptive Approach to Language-Theoretic Complexity.\\nStudies in Logic, Language, and Information. CSLI Publications.\\nTo appear.\\n\\n\\nRogers, James.\\n1996b.\\nThe descriptive complexity of local, recognizable, and generalized\\n  recognizable sets.\\nTechnical report, IRCS, Univ. of Pennsylvania.\\nIn Preparation.\\n\\n\\nRogers, James.\\n1996c.\\nGrammarless phrase-structure grammar.\\nUnder Review.\\n\\n\\nRogers, James and K. Vijay-Shanker.\\n1994.\\nObtaining trees from their descriptions: An application to\\n  tree-adjoining grammars.\\nComputational Intelligence, 10:401-421.\\n\\n\\nSmolka, Gert.\\n1989.\\nA feature logic with subsorts.\\nLILOG Report 33, IBM Germany, Stuttgart.\\n\\n\\nStabler, Jr., Edward P.\\n1992.\\nThe Logical Approach to Syntax.\\nBradford.\\n\\nFootnotes\\n\\n  This \\nnotion of constraint-based includes not only the obvious\\nformalisms, but the formal framework of GB as well.\\n  Whether there are theories\\nthat cannot be captured, at least without explicitly encoding the derivations,\\nis an open question of considerable theoretical interest, as is the question of\\nwhat empirical consequences such an essential dynamic character might have.\\n  We will refer to  as\\nGKPS\\n  We should note that the definition of FSDs\\nin GKPS is, in fact, declarative although this is obscured by the fact that\\nit is couched in terms of an algorithm for checking models.\\n  There is reason to believe that this\\nhierarchy can be extended to encompass, at least, a variety of mildly\\ncontext-sensitive languages as well.\\n  A more complete treatment of GPSG in \\n\\ncan be found in .\\n  We will not elaborate here on the encoding of\\ncategories in \\n\\n,\\nnor on non-finite ID schema like the iterating\\nco-ordination schema.  These present no significant problems.\\n  We could, of course,\\nskip the definition of \\n\\n\\nand define \\n\\n\\nas\\n\\n,\\nbut we prefer to emphasize the inductive\\nnature of the definition.\\n  More detailed\\nexpositions of the interpretation of GB in \\n\\n\\ncan be found in\\n, , and .\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nA natural next step in the evolution of constraint-based grammar\\nformalisms from rewriting formalisms is to abstract fully away from the details\\nof the grammar mechanism--to express syntactic theories purely in terms of the\\nproperties of the class of structures they license.  By focusing on the\\nstructural properties of languages rather than on mechanisms for generating or\\nchecking structures that exhibit those properties, this model-theoretic\\napproach can offer simpler and \\nsignificantly clearer expression of theories and can potentially provide a\\nuniform formalization, allowing disparate theories to be compared on\\nthe basis of those properties.  We discuss \\n\\n,\\na monadic second-order\\nlogical framework for such an approach to syntax that has the\\ndistinctive virtue of being superficially expressive--supporting direct\\nstatement of most linguistically significant syntactic properties--but having\\nwell-defined strong generative capacity--languages are definable in \\n\\n\\niff\\nthey are strongly context-free.  We draw examples from the realms of GPSG and\\nGB.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\n The efficiency of LR(k) parsing techniques   is very attractive from the perspective of natural language \\nprocessing applications.  \\nThis has stimulated the computational linguistics community to \\ndevelop extensions of these techniques to general context-free \\ngrammar parsing. \\nThe best-known example is \\ngeneralized LR parsing, also known as\\nTomita's algorithm, described by \\nand further investigated by, for example,\\n and . \\nDespite appearances, the graph-structured stacks used to describe \\nTomita's algorithm differ very little from parse tables, \\nor in other words,\\ngeneralized LR parsing is one of the\\nso called\\ntabular parsing algorithms, among which also the \\n CYK algorithm   and Earley's algorithm  can be found. (Tabular parsing is also known as chart parsing.)\\n\\n\\nIn this paper we investigate the extension of LR parsing to general \\ncontext-free grammars from a more general viewpoint: tabular\\nalgorithms can often be described by the composition\\nof two constructions. One example is given by \\nand ;\\nthe construction of pushdown automata from grammars and the\\nsimulation of these automata by means of tabulation yield different\\ntabular algorithms for different such constructions.\\nAnother example, on which our presentation is based, was first\\nsuggested by : a grammar is first transformed\\nand then a standard tabular algorithm \\nalong with some filtering condition \\nis applied\\nusing the transformed grammar. In our case, the transformation\\nand the subsequent application of the tabular algorithm result in a new form\\nof tabular LR parsing.\\n\\n\\nOur method is more efficient than Tomita's algorithm in two respects.\\nFirst, reduce operations are implemented in an efficient way,\\nby splitting them into several, more primitive, operations\\n(a similar idea has been proposed by  for Tomita's algorithm). \\nSecond, several paths in the computation that must be \\nsimulated separately by Tomita's algorithm are collapsed into a single \\ncomputation path, using state minimization techniques. \\nExperiments on practical grammars have indicated that there is a \\nsignificant gain in efficiency, with regard to both space and time requirements.\\n\\n\\nOur grammar transformation produces a so called cover for \\nthe input \\ngrammar, which together with the filtering condition \\nfully captures the specification of the method, \\nabstracting away from algorithmic details such as data structures \\nand control flow. \\nSince this cover can be easily precomputed,  \\nimplementing our LR parser simply amounts to\\nrunning the standard tabular algorithm.\\nThis is very attractive from an application-oriented perspective,\\nsince many actual systems for natural language processing are \\nbased on these kinds of parsing algorithm.\\n\\n\\nThe remainder of this paper is organized as follows.\\n In Section  some preliminaries are discussed.  We review the notion of LR automaton in Section   and introduce the notion of 2LR automaton in Section .  Then we specify our tabular LR method in Section ,  and provide an analysis of the algorithm in Section .  Finally, some empirical results are given in Section ,  and further discussion of our method is provided in Section .   \\n\\n\\n    Definitions\\n\\n\\nThroughout this paper we use standard formal language notation.\\nWe assume that the reader is familiar with context-free\\n grammar parsing theory .   \\n\\n\\nA context-free grammar ()\\nis a 4-tuple \\n\\n,\\nwhere \\nand are two finite disjoint sets of terminal \\nand nonterminal symbols, respectively, \\n\\n\\nis the start symbol, \\nand P is a finite set of rules. Each rule has the form\\n\\n\\nwith \\n\\n\\nand \\n\\n,\\nwhere \\ndenotes \\n\\n.\\nThe size of G, written ,\\nis defined\\nas \\n\\n;\\nby  we\\nmean the length of a string of symbols .\\n\\n\\nWe generally use symbols \\n\\n\\nto range over \\n\\n,\\nsymbols \\n\\n\\nto range over ,\\nsymbols X, Y, Z to range over ,\\nsymbols \\n\\n\\nto range over \\n\\n,\\nand symbols \\n\\n\\nto range over \\n\\n.\\nWe write \\nto denote the empty string.\\n\\n\\nA \\nis said to be in binary form if \\n\\nfor all of its rules\\n\\n.\\n(The binary form does not limit\\nthe (weak) generative capacity of context-free \\n grammars .) For technical reasons, we sometimes use the augmented grammar \\nassociated with G, defined as \\n\\n,\\nwhere ,\\n\\nand \\nare fresh symbols, \\n\\n,\\n\\n\\nand \\n\\n.\\n\\n\\nA pushdown automaton ()\\nis a 5-tuple\\n\\n,\\nwhere ,\\n\\nand T are finite sets of input symbols, stack symbols\\nand transitions, respectively;\\n\\n\\nis the initial stack symbol and\\n\\n\\nis the final stack symbol.\\n  Each transition has the form \\n\\n,\\nwhere \\n\\n,\\n\\n,\\n\\n,\\nand \\nor z=a.\\nWe generally use symbols \\n\\n\\nto range over ,\\nand\\nthe symbol \\nto range over .\\n\\n\\nConsider a fixed input string \\n\\n.\\nA configuration of the automaton is a pair \\n\\n\\nconsisting\\nof a stack \\n\\n\\nand the remaining input w,\\nwhich is a suffix of the input string v.\\nThe rightmost symbol of \\nrepresents the top of the stack.\\nThe initial configuration has the form\\n\\n,\\nwhere the stack is formed by the initial\\nstack symbol.  \\nThe final configuration has the form\\n\\n,\\nwhere the stack is formed by the final\\nstack symbol stacked upon the initial stack symbol. \\n\\n\\nThe application of a transition \\n\\nis described as follows.\\nIf the top-most symbols of the stack are ,\\nthen these \\nsymbols may be replaced by ,\\nprovided that either ,\\nor z=a and a is\\nthe first symbol of the remaining input. Furthermore, if z=a then a is\\nremoved from the remaining input. \\nFormally, for a fixed \\n\\nwe define the binary relation \\n\\non configurations as the least relation satisfying\\n\\n\\nif there is a transition\\n\\n,\\nand\\n\\n\\nif there is a transition\\n\\n.\\nThe recognition of a certain input v is obtained if starting from the\\ninitial configuration for that input we can reach the final\\nconfiguration by repeated application of transitions, or,\\nformally, if \\n\\n,\\nwhere \\ndenotes the reflexive and transitive closure of\\n.\\n\\n\\nBy a computation of a PDA we mean a sequence\\n\\n\\n\\n\\n\\n... \\n\\n,\\n.\\nA \\nis called deterministic \\nif for all possible configurations at most one transition is applicable. \\nA \\nis said to be in binary form\\nif, for all transitions \\n\\n,\\nwe have \\n\\n.\\n\\n\\n    LR automata\\n\\n\\nLet \\n\\n\\nbe a .\\nWe recall the notion of LR automaton, which is\\na particular kind of .\\nWe make use of the augmented grammar \\n\\n\\n introduced in Section . \\n\\n\\nLet \\n\\n.\\nWe introduce the function \\nfrom \\n\\n\\nto \\n\\n\\nand the function \\nfrom \\n\\n\\nto \\n\\n.\\nFor any \\n\\n,\\n\\n\\nis the smallest set such that\\n1.\\n\\n;\\nand\\n2.\\n\\n\\nand \\n\\n\\ntogether imply \\n\\n.\\nWe then define \\n\\n\\n\\n\\n\\nWe construct a finite set \\nas the smallest collection of sets \\nsatisfying the conditions:\\n1.\\n\\n;\\nand\\n2.\\nfor every \\n\\n\\nand \\n\\n,\\nwe have\\n\\n,\\nprovided \\n\\n.\\nTwo elements from \\ndeserve special attention:\\n\\n,\\nand ,\\nwhich is defined to be\\nthe unique set in \\ncontaining \\n\\n;\\nin other words, \\n\\n.\\n\\n\\nFor \\n\\n,\\nan A-redex is a string \\n\\n,\\n,\\nof elements from ,\\nsatisfying the following conditions: \\n1.\\n\\n,\\nfor some \\n\\n;\\nand\\n2.\\n\\n,\\n        for \\n\\n.\\nNote that in such an A-redex, \\n\\n\\n\\n\\n,\\nand \\n\\n\\nqk, for .\\n\\n\\nThe LR automaton associated with G is now introduced.\\n  Transitions in (i) above are called shift, \\ntransitions in (ii) are called reduce. \\n\\n\\n    2LR Automata\\n\\n\\nThe automata \\ndefined in the previous section\\nare deterministic only for a subset of the s, \\n called the LR(0) grammars , and behave nondeterministically in the general case.\\nWhen designing tabular methods that \\nsimulate nondeterministic computations of ,\\ntwo main difficulties are encountered: \\n\\nA reduce transition in \\nis an elementary operation \\nthat removes from the stack a number of elements bounded by \\nthe size of the underlying grammar.  Consequently, \\nthe time requirement of tabular\\nsimulation of \\ncomputations can be onerous, for \\nreasons pointed out by  and .\\n\\nThe set \\ncan be exponential in the size \\n of the grammar .   If in such a case the computations of \\ntouch upon each state, \\nthen time and space requirements of tabular simulation are \\nobviously onerous. \\n\\n\\nThe first issue above is solved here by recasting \\nin binary form. This is done by considering each reduce transition \\nas a sequence of ``pop'' operations which affect at most two stack symbols \\nat a time. (See also , , and ,\\nand for LR parsing specifically  and .)\\nThe following definition introduces this new kind of automaton.\\n\\n\\n\\n\\n\\nTransitions in (i) above are again called shift, transitions\\nin (ii) are called initiate, those in (iii) are called \\ngathering, and transitions in (iv) are called goto.\\nThe role of a reduce step in \\nis taken over in by an initiate step, a number of gathering steps, and a goto step.\\nObserve that these steps involve the new stack symbols\\n\\n\\nthat are distinguishable from possible stack symbols\\n\\n.\\n\\n\\nWe now turn to the second above-mentioned problem, \\nregarding the size of set .\\nThe problem is in part solved here as follows. \\nThe number of states in \\nis considerably reduced by identifying two states if they become identical\\nafter items \\n\\n\\nfrom \\nhave been\\nsimplified to only the suffix of the right-hand side .\\nThis is reminiscent of\\n techniques of state minimization for finite automata ,  as they have been applied before to LR parsing, e.g., \\nby  and .\\n\\n\\nLet \\nbe the augmented grammar associated with \\na \\nG, and let \\n\\n.\\nWe define variants of the \\nand \\nfunctions from the\\nprevious section as follows.\\nFor any set \\n\\n,\\n\\n\\nis the smallest collection of sets such that\\n1.\\n\\n;\\nand\\n2.\\n\\n\\nand\\n\\n\\ntogether imply \\n\\n.\\nAlso, we define \\n\\n\\n\\n\\n\\nWe now construct a finite set \\n\\n\\nas the smallest set \\nsatisfying the conditions:\\n1.\\n\\n;\\nand \\n2.\\nfor every\\n\\n\\nand \\n\\n,\\nwe have\\n\\n,\\nprovided \\n\\n.\\n\\n\\nAs stack symbols, we take the elements from \\nand a subset\\nof elements from \\n\\n:\\n\\n\\n\\n\\n\\nIn a stack symbol of the form (X, q),\\nthe X serves to record the grammar symbol\\nthat has been recognized last, cf. the symbols that formerly were found\\nimmediately before the dots. \\n\\n\\nThe 2LR automaton associated with G can now be introduced. \\n  Note that in the case of a reduce/reduce conflict with two grammar\\nrules sharing some suffix in the right-hand side, the gathering steps\\nof \\nwill treat both rules simultaneously,\\nuntil the parts of the right-hand sides are reached where the two rules differ.\\n(See  for a similar sharing of computation for\\ncommon suffixes.)\\n\\n\\nAn interesting fact is that the automaton \\nis very similar\\nto the automaton \\nconstructed for a grammar transformed\\nby the transformation \\n\\n\\ngiven by .\\n\\n\\n    The algorithm\\n\\n\\nThis section presents a tabular LR parser, \\nwhich is the main result of this paper. \\nThe parser is derived from the 2LR automata \\nintroduced in the previous section.  Following the general approach \\npresented by , we simulate computations of these devices \\nusing a tabular method, a grammar transformation and a filtering\\nfunction.\\n\\n\\nWe make use of a tabular parsing algorithm which is basically \\nan asynchronous version of the CYK algorithm, \\nas presented by , \\nextended to productions of the forms \\nand \\n\\n\\nand \\nwith a left-to-right filtering condition. \\nThe algorithm uses a parse table consisting in a 0-indexed \\nsquare array U. The indices represent positions in the input string.\\nWe define Ui to be \\n\\n.\\n\\n\\nComputation of the entries of U is moderated by a filtering process.\\nThis process makes use of a function \\nfrom\\n\\n\\nto \\n\\n,\\nspecific to a certain\\ncontext-free grammar.\\nWe have a certain nonterminal \\n\\n\\nwhich is initially\\ninserted in U0,0 in order to start the recognition process.\\n\\n\\nWe are now ready to give a formal specification of the tabular algorithm.\\n\\n\\n The string has been accepted when \\n\\n.\\n\\n\\nWe now specify a grammar transformation, based on the \\ndefinition of .\\n\\n\\n Observe that there is a direct, one-to-one correspondence between \\ntransitions of \\nand productions of .\\n\\n\\nThe accompanying function \\nis defined as follows\\n(q,q',q'' range over the stack elements):\\n\\n\\n\\n\\n\\nThe above definition implies that only the tabular equivalents of the\\nshift, initiate and goto transitions are subject to actual filtering; \\nthe simulation\\nof the gathering transitions does not depend on elements in .\\n\\n\\nFinally, the distinguished nonterminal from the cover used to initialize \\nthe table is .\\nThus we start with \\n\\n.\\n\\n\\nThe 2LR cover introduces spurious ambiguity: where some grammar\\nG would allow a certain number of parses to be found for a certain\\ninput, the grammar \\nin general allows more parses.\\nThis problem is in part solved by the filtering function .\\nThe remaining spurious ambiguity is avoided by a particular\\nway of constructing the parse trees, described in what follows.\\n\\n\\n After Algorithm  has recognized a given input, the set of all parse trees can be computed as \\n\\n\\nwhere the function ,\\nwhich \\ndetermines sets of either parse trees or lists of parse trees\\nfor entries in U, is recursively defined by:\\n1.\\n\\n\\nis the set .\\nThis set contains a single\\nparse tree consisting of a single node labelled a.\\n2.\\n\\n\\nis the set \\n\\n.\\nThis set consists of\\nan empty list of trees.\\n3.\\n\\n\\nis the union of the sets \\n\\n,\\nwhere \\n\\n,\\n\\n,\\nand there is at least one\\n\\n\\nand\\n\\n\\nin ,\\nfor some q.\\nFor each such k, select one such q. We define\\n\\n.\\nEach \\n\\n\\nis a list of\\ntrees, with head t and tail .\\n4.\\n\\n\\nis the union of the sets \\n\\n,\\nwhere \\n\\n\\nis such that \\n\\n\\nin .\\nWe define \\n\\n.\\nThe function \\nconstructs a tree from a fresh root node labelled\\nA and the trees in list \\nas immediate subtrees.\\nWe emphasize that in the third clause above, \\none should not consider more than one \\nq for given k in order to prevent spurious ambiguity.\\n(In fact, for fixed X,i,k and for different q \\nsuch that \\n\\n,\\n\\n\\nyields \\nthe exact same set of trees.)\\nWith this proviso, the degree of ambiguity, i.e. the number of parses\\nfound by the algorithm for any input, is reduced to exactly that of \\nthe source grammar.\\n\\n\\nA practical implementation would construct the parse trees on-the-fly,\\nattaching them to the table entries, allowing packing and sharing\\nof subtrees (cf. the literature on parse forests \\n ,).  Our algorithm \\nactually only needs one (packed) subtree for several \\n\\n\\nwith \\nfixed X,i,k but different q.\\nThe resulting parse forests would then be optimally\\ncompact, contrary to some other LR-based tabular algorithms, as pointed\\nout by ,  and . \\n\\n\\n    Analysis of the algorithm\\n\\n\\nIn this section, we investigate how the steps performed \\n by Algorithm  (applied to the 2LR cover) relate to those performed by ,\\nfor the same input.\\n\\n\\nWe define a subrelation \\nof \\nas:\\n\\n\\nif and only if\\n\\n,\\nfor some ,\\nwhere \\n\\n\\nfor all k, \\n\\n.\\nInformally, we have \\n\\n\\nif configuration\\n\\n\\ncan be reached from \\n\\n\\nwithout the\\nbottom-most part \\nof the intermediate stacks being affected by\\nany of the transitions;\\nfurthermore, at least one element is pushed on top of .\\n\\n\\nThe following characterization relates  \\nthe automaton \\n and Algorithm  applied to the 2LR cover.\\nSymbol \\n\\n\\nis eventually \\nadded to Ui,j if and only if for some :\\n\\n\\n\\n\\n\\nIn words, q is found in entry Ui,j if and only if, \\nat input position j, the automaton would push some \\nelement q on top of some lower-part of the stack \\nthat \\nremains unaffected while the input from i to j is being read.\\n\\n\\nThe above characterization, whose proof is not reported here,\\nis the justification for calling the resulting algorithm \\ntabular LR parsing. In particular, for\\na grammar for which \\nis deterministic, i.e. for an\\nLR(0) grammar, the number of steps performed by \\nand\\nthe number of steps performed by the above algorithm \\nare exactly the same. In the case\\nof grammars which are not LR(0), the tabular LR algorithm\\nis more efficient than for example a backtrack realisation of .\\n\\n\\nFor determining the order of the time complexity of\\nour algorithm, we look at the most\\nexpensive step, which\\nis the computation of an element \\n\\n\\nfrom \\ntwo elements \\n\\n\\nand \\n\\n,\\nthrough\\n\\n.\\nIn a straightforward realisation of the algorithm,\\nthis step can be applied \\n\\n\\ntimes (once for each i,k,j and each transition), each step taking\\na constant amount of time.\\nWe conclude that the time complexity of our algorithm\\nis \\n\\n.\\n\\n\\nAs far as space requirements are concerned, each set Ui,jor Ui contains at most \\n\\n\\nelements.\\n(One may assume an auxiliary table storing each Ui.)\\nThis results in a space complexity \\n\\n.\\n\\n\\nThe entries in the table represent single stack elements, as opposed to\\npairs of stack elements following  and . \\nThis has been investigated before \\nby  and .\\n\\n\\n    Empirical results\\n\\n\\n We have performed some experiments with Algorithm  applied to \\nand ,\\nfor 4 practical context-free grammars.\\nFor \\na cover was used analogous to \\nthe one in \\n Definition ; the filtering function remains the same.\\n\\n\\nThe first grammar \\ngenerates a subset of the programming language ALGOL 68\\n . The second and third  grammars generate a fragment of Dutch, and are \\nreferred to \\n as the CORRie grammar  and the Deltra  grammar ,  respectively. \\nThese grammars were stripped of their arguments\\nin order to \\nconvert them into context-free grammars.  \\nThe fourth grammar, referred to as the\\n Alvey grammar ,  generates a fragment of English and was\\nautomatically generated from a unification-based grammar.\\n\\n\\nThe test sentences\\nhave been obtained by automatic generation from the grammars,\\n using the Grammar Workbench , which uses a random generator to select rules; \\ntherefore these sentences do \\nnot necessarily represent input typical \\nof the applications \\nfor which the grammars were written. \\n Table  summarizes the test material.  \\n\\n\\nOur implementation is merely a prototype, which means that\\nabsolute duration of the parsing process is little indicative of\\nthe actual efficiency of more sophisticated implementations.\\nTherefore, our measurements have been restricted to implementation-independent\\nquantities, viz. the number of elements stored\\nin the parse table and the number of elementary steps\\nperformed by the algorithm. In a\\npractical implementation, such quantities will strongly influence the \\nspace and time complexity, although they do not represent the only\\ndetermining factors. Furthermore, all optimizations\\nof the time and space efficiency have been left out of consideration.\\n\\n\\n Table  presents the costs of parsing the  test sentences.\\nThe first and third columns \\ngive the number of entries stored in table\\nU, the second \\nand fourth \\ncolumns give the number of elementary steps that were\\nperformed. \\n\\n\\nAn elementary step consists of the derivation of one element\\nin \\nor \\nfrom one or two \\nother elements. \\nThe elements that are used in the filtering process are counted\\nindividually. \\nWe give an example for the case of .\\nSuppose we derive an element \\n\\n\\nfrom\\nan element \\n\\n,\\nwarranted by\\ntwo elements \\n\\n,\\n\\n,\\nthrough ,\\nin the presence of\\n\\n\\nand\\n\\n.\\nWe then count \\ntwo parsing steps, one for q1 and one for q2.\\n\\n\\n Table  shows that there is a significant gain in space and time efficiency \\nwhen moving \\nfrom \\nto .\\n\\n\\nApart from the dynamic costs of parsing, we have also measured some\\nquantities relevant to the construction and storage of the two types\\nof tabular LR parser. \\n These data are given in Table . \\n\\n\\nWe see that the number of states is strongly reduced\\nwith regard to traditional LR parsing. In the case of the Alvey\\ngrammar, \\nmoving from \\n\\n\\nto \\n\\n\\namounts\\nto a reduction to 20.3 %. \\nWhereas time- and space-efficient computation \\nof \\nfor this grammar is a serious problem,\\ncomputation of \\n\\n\\nwill not be difficult on any modern\\ncomputer. Also significant is the reduction from \\nto\\n,\\nespecially for the larger grammars. These quantities \\ncorrelate with the amount of storage needed for naive representation\\nof the respective automata.\\n\\n\\n    Discussion\\n\\n\\nOur treatment of tabular LR parsing has two important advantages over\\nthe one by Tomita: \\n\\nIt is conceptually simpler, because we make use of simple\\nconcepts such as a grammar transformation and the well-understood\\nCYK algorithm, instead of a complicated mechanism working on \\ngraph-structured stacks.\\n\\nOur algorithm requires fewer LR states.\\nThis leads to faster parser generation, to smaller parsers, \\nand to reduced time and space complexity of parsing itself.\\n\\n\\n\\n\\nThe conceptual simplicity of our formulation of tabular\\nLR parsing allows comparison with other tabular parsing techniques,\\nsuch as\\n Earley's algorithm  and tabular left-corner  parsing , based on implementation-independent criteria. This is in contrast to experiments reported before (e.g. by\\n),\\nwhich treated tabular LR parsing differently from the other\\ntechniques.\\n\\n\\nThe reduced time and space complexities reported in the previous\\nsection pertain to the tabular realisation of two\\nparsing techniques, expressed by the automata \\nand \\n.\\nThe tabular realisation of the former automata is\\nvery close to a variant of Tomita's algorithm \\nby . \\nThe objective of our experiments was to show that the\\nautomata \\nprovide a better basis than \\nfor \\ntabular LR parsing with regard to space and time complexity. \\n\\n\\nParsing algorithms that are not based on the LR technique\\nhave however been left out of consideration, and so\\nwere techniques for unification grammars and techniques incorporating\\nfinite-state processes.\\n\\n\\nTheoretical \\n considerations ,, have suggested that for natural language parsing, LR-based techniques may not\\nnecessarily be superior to other parsing techniques, although\\nconvincing empirical data to this effect has never been shown.\\nThis issue is difficult to resolve because so much of the \\nrelative efficiency of the different parsing techniques depends\\non particular grammars and particular input, as well as\\non particular implementations of the techniques. We hope\\nthe conceptual framework presented in this paper may\\nat least partly alleviate this problem.\\n\\n\\n  Acknowledgements \\n\\nThe first author is supported by the Dutch Organization\\nfor Scientific Research (NWO), under grant 305-00-802.\\nPart of the present research was done\\nwhile the second author was visiting the\\nCenter for Language and Speech Processing,\\nJohns Hopkins University, Baltimore, MD.\\n\\n\\nWe received kind help from \\nJohn Carroll, Job Honig, Kees Koster, Theo Vosse and Hans de Vreught\\nin finding the grammars mentioned in this paper.\\nGenerous help with locating relevant literature was provided by Anton\\nNijholt, Rockford Ross, and Arnd Rumann.\\n\\nBibliography \\n\\nBillot, S. and B. Lang.\\n1989.\\nThe structure of shared forests in ambiguous parsing.\\nIn 27th Annual Meeting of the ACL, pages 143-151.\\n\\n\\nBooth, T.L.\\n1967.\\nSequential Machines and Automata Theory.\\nWiley, New York.\\n\\n\\nCarroll, J.A.\\n1993.\\nPractical unification-based parsing of natural language.\\nTechnical Report No. 314, University of Cambridge, Computer\\n  Laboratory, England.\\nPhD thesis.\\n\\n\\nEarley, J.\\n1970.\\nAn efficient context-free parsing algorithm.\\nCommunications of the ACM, 13(2):94-102.\\n\\n\\nHarrison, M.A.\\n1978.\\nIntroduction to Formal Language Theory.\\nAddison-Wesley.\\n\\n\\nJohnson, M.\\n1991.\\nThe computational complexity of GLR parsing.\\nIn , chapter 3, pages 35-42.\\n\\n\\nKipps, J.R.\\n1991.\\nGLR parsing in time \\n\\n.\\nIn , chapter 4, pages 43-59.\\n\\n\\nLang, B.\\n1974.\\nDeterministic techniques for efficient non-deterministic parsers.\\nIn Automata, Languages and Programming, 2nd Colloquium,\\n  LNCS 14, pages 255-269,\\n  Saarbrcken. Springer-Verlag.\\n\\n\\nLeermakers, R.\\n1989.\\nHow to cover a grammar.\\nIn 27th Annual Meeting of the ACL, pages 135-142.\\n\\n\\nLeermakers, R.\\n1992a.\\nA recursive ascent Earley parser.\\nInformation Processing Letters, 41(2):87-91.\\n\\n\\nLeermakers, R.\\n1992b.\\nRecursive ascent parsing: from Earley to Marcus.\\nTheoretical Computer Science, 104:299-312.\\n\\n\\nNederhof, M.J.\\n1993.\\nGeneralized left-corner parsing.\\nIn Sixth Conference of the European Chapter of the ACL, pages\\n  305-314.\\n\\n\\nNederhof, M.J.\\n1994a.\\nLinguistic Parsing and Program Transformations.\\nPh.D. thesis, University of Nijmegen.\\n\\n\\nNederhof, M.J.\\n1994b.\\nAn optimal tabular parsing algorithm.\\nIn 32nd Annual Meeting of the ACL, pages 117-124.\\n\\n\\nNederhof, M.J. and K. Koster.\\n1992.\\nA customized grammar workbench.\\nIn J. Aarts, P. de Haan, and N. Oostdijk, editors, English\\n  Language Corpora: Design, Analysis and Exploitation, Papers from the\\n  thirteenth International Conference on English Language Research on\\n  Computerized Corpora, pages 163-179, Nijmegen. Rodopi.\\n\\n\\nNederhof, M.J. and J.J. Sarbo.\\n1993.\\nIncreasing the applicability of LR parsing.\\nIn Third International Workshop on Parsing Technologies, pages\\n  187-201.\\n\\n\\nNederhof, M.J. and G. Satta.\\n1994.\\nAn extended theory of head-driven parsing.\\nIn 32nd Annual Meeting of the ACL, pages 210-217.\\n\\n\\nPager, D.\\n1970.\\nA solution to an open problem by Knuth.\\nInformation and Control, 17:462-473.\\n\\n\\nRekers, J.\\n1992.\\nParser Generation for Interactive Environments.\\nPh.D. thesis, University of Amsterdam.\\n\\n\\nSchabes, Y.\\n1991.\\nPolynomial time and space shift-reduce parsing of arbitrary\\n  context-free grammars.\\nIn 29th Annual Meeting of the ACL, pages 106-113.\\n\\n\\nSchauerte, R.\\n1973.\\nTransformationen von LR(k)-grammatiken.\\nDiplomarbeit, Universitt Gttingen, Abteilung Informatik.\\n\\n\\nSchoorl, J.J. and S. Belder.\\n1990.\\nComputational linguistics at Delft: A status report.\\nReport WTM/TT 90-09, Delft University of Technology, Applied\\n  Linguistics Unit.\\n\\n\\nShann, P.\\n1991.\\nExperiments with GLR and chart parsing.\\nIn TO91a, chapter 2, pages 17-34.\\n\\n\\nSheil, B.A.\\n1976.\\nObservations on context-free parsing.\\nStatistical Methods in Linguistics, pages 71-109.\\n\\n\\nSippu, S. and E. Soisalon-Soininen.\\n1990.\\nParsing Theory, Vol. II: LR(k) and LL(k) Parsing.\\nSpringer-Verlag.\\n\\n\\nTomita, M.\\n1986.\\nEfficient Parsing for Natural Language.\\nKluwer Academic Publishers.\\n\\n\\nTomita, M., editor.\\n1991.\\nGeneralized LR Parsing.\\nKluwer Academic Publishers.\\n\\n\\nvan Wijngaarden, A. et al.\\n1975.\\nRevised report on the algorithmic language ALGOL 68.\\nActa Informatica, 5:1-236.\\n\\n\\nVillemonte de la Clergerie, E.\\n1993.\\nAutomates  Piles et Programmation Dynamique -- DyALog:\\n  Une application  la Programmation en Logique.\\nPh.D. thesis, Universit Paris VII.\\n\\n\\nVosse, T.G.\\n1994.\\nThe Word Connection.\\nPh.D. thesis, University of Leiden.\\n\\nFootnotes\\n\\nWe dispense with the notion of state, \\ntraditionally incorporated in the definition of .\\nThis does not affect the power of these devices, since\\nstates can be encoded within stack symbols and transitions.\\n  For the earliest mention of this transformation, we have\\nencountered pointers to . Regrettably, we have as yet not been\\nable to get hold of a copy of this paper.\\n  As remarked before by , the algorithms by\\n and  are not really related to LR parsing, although\\nsome notation used in these papers suggests otherwise.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nWe give a new treatment of tabular LR parsing,\\nwhich is an alternative to Tomita's generalized LR algorithm.\\nThe advantage is twofold. Firstly, our treatment is conceptually more\\nattractive because it uses simpler concepts, such as grammar\\ntransformations and standard tabulation techniques also\\nknow as chart parsing. Secondly, the static and dynamic\\ncomplexity of parsing, both in space and time, is significantly reduced.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nWord vectors reflecting word meanings are expected to enable\\nnumerical approaches to semantics.\\nSome early attempts at vector representation in psycholinguistics\\n were the semantic differential approach   and the associative distribution approach . However, they were derived manually through psychological experiments.\\nAn early attempt at automation was made by Wilks et al.\\n using co-occurrence statistics.\\nSince then, there have been some promising results from using\\nco-occurrence vectors, such as word sense disambiguation\\n , and word clustering . \\n\\n\\nHowever, using the co-occurrence statistics requires a huge corpus\\nthat covers even most rare words.\\nWe recently developed word vectors that are derived from an ordinary\\ndictionary by measuring the inter-word distances in the word definitions\\n . This method, by its nature, has no problem handling rare words.\\nIn this paper we examine the usefulness of these distance vectors\\nas semantic representations by comparing them with co-occurrence vectors.\\n\\n\\n  Distance Vectors \\n\\nA reference network of the words in a dictionary (Fig. )\\nis used to measure the distance between words.\\nThe network is a graph that shows which words are used in\\n the definition of each word . The network shown in Fig. is for a very small portion\\nof the reference network for the Collins English Dictionary\\n (1979 edition) in the CD-ROM I , with  head words +  definition words.\\n\\n\\n\\nFor example, the definition for dictionary is ``a book in which the\\nwords of a language are listed alphabetically ... .''\\nThe word dictionary is thus linked to the words\\nbook, word, language, and alphabetical.\\n\\n\\nA word vector is defined as the list of distances from a word to a\\ncertain set of selected words, which we call origins.\\nThe words in Fig. marked with Oi (unit, book,\\nand people) are assumed to be origin words.\\nIn principle, origin words can be freely chosen.\\nIn our experiments we used middle frequency words:\\nthe 51st to 1050th most frequent words\\nin the reference Collins English Dictionary (CED).\\n\\n\\nThe distance vector for dictionary is derived as follows:\\n\\n\\n\\n\\n\\n\\n\\nThe i-th element is the\\ndistance (the length of the shortest path) between dictionary and\\nthe i-th origin, Oi.\\nTo begin, we assume every link has a constant length of 1.\\nThe actual definition for link length will be given later.\\n\\n\\nIf word A is used in the definition of word B, these words are expected\\nto be strongly related.\\nThis is the basis of our hypothesis that the distances in the reference\\nnetwork reflect the associative distances between words\\n . \\n\\n\\nUse of Reference Networks \\nReference networks have been successfully used as\\nneural networks (by Vronis and Ide \\nfor word sense disambiguation)\\nand as fields for artificial association, such as spreading activation\\n(by Kojima and Furugori  for\\ncontext-coherence measurement).\\nThe distance vector of a word can be considered to be a list of the\\nactivation strengths at the origin nodes when the word node is activated.\\nTherefore, distance vectors can be expected to convey almost the same\\ninformation as the entire network, and clearly they are much easier\\nto handle.\\n\\n\\nDependence on Dictionaries \\nAs a semantic representation of words, distance vectors are expected to\\ndepend very weakly on the particular source dictionary.\\nWe compared two sets of distance vectors,\\n one from LDOCE  and the other from  COBUILD , and verified that their difference is at least smaller than the difference of\\n the word definitions themselves . \\n\\n\\nWe will now describe some technical details about the derivation\\nof distance vectors.\\n\\n\\nLink Length \\nDistance measurement in a reference network depends on the\\ndefinition of link length.\\nPreviously, we assumed for simplicity that every link\\nhas a constant length.\\nHowever, this simple definition seems unnatural because it does not\\nreflect word frequency.\\nBecause a path through low-frequency words (rare words) implies\\na strong relation, it should be measured as a shorter path.\\nTherefore, we use the following definition of link length,\\nwhich takes account of word frequency.\\n\\n\\n\\nThis shows the length of the links between words W\\n\\ni (i=1,2)in Fig.,\\nwhere Ni denotes the total number of links from and to Wiand n denotes the number of direct links between these two words.\\n\\n\\n\\nNormalization \\nDistance vectors are normalized by first\\nchanging each coordinate into its deviation in the coordinate:\\n\\n\\n\\nwhere ai and \\n\\n\\n\\nare the average and the standard deviation\\nof the distances from the i-th origin.\\nNext, each coordinate is changed into its deviation in the vector:\\n\\n\\n\\nwhere \\n\\n\\n\\n\\nand \\n\\n\\n\\nare the average and the standard\\ndeviation of \\n\\n\\n\\n\\n .\\n\\n\\n  Co-occurrence Vectors \\n\\nWe use ordinary co-occurrence statistics and\\nmeasure the co-occurrence likelihood between two words, X and Y,\\n by the mutual information estimate : \\n\\n\\n\\nwhere \\n\\n\\n\\nis the occurrence density of word X in a whole corpus,\\nand the conditional probability \\n\\n\\n\\n\\nis the density of X\\nin a neighborhood of word Y.\\nHere the neighborhood is defined as 50 words before or after any\\nappearance of word Y.\\n(There is a variety of neighborhood definitions such as\\n ``100 surrounding words''  and ``within a distance of no more than 3 words ignoring function words''\\n .) \\n\\n\\nThe logarithm with `+' is defined to be 0 for an argument less than 1.\\nNegative estimates were neglected because they are mostly accidental\\n except when X and Y are frequent enough . \\n\\n\\nA co-occurence vector of a word is defined as the list of co-occurrence\\nlikelihood of the word with a certain set of origin words.\\nWe used the same set of origin words as for the distance vectors.\\n\\n\\n\\nCo-occurrence Vector.\\n\\n\\nWhen the frequency of X or Y is zero, we can not measure their\\nco-occurence likelihood, and such cases are not exceptional.\\nThis sparseness problem is well-known and serious in the co-occurrence\\nstatistics.\\nWe used as a corpus the 1987 Wall Street Journal in the CD-ROM I\\nACL-CD-ROM-1, which has a total of 20M words.\\nThe number of words which appeared at least once was about 50%\\nof the total 62K head words of CED, and the percentage of\\nthe word-origin pairs which appeared at least once was\\nabout 16% of total 62K \\n\\n\\n\\n  1K (=62M) pairs.\\nWhen the co-occurrence likelihood can not be measured,\\nthe value \\n\\n\\n\\n\\n  was set to 0.\\n\\n\\n  Experimental Results \\n\\nWe compared the two vector representations by using them for\\nthe following two semantic tasks.\\nThe first is word sense disambiguation (WSD) based on the similarity of\\ncontext vectors;\\nthe second is the learning of  or  meanings\\nfrom example words.\\n\\n\\nWith WSD, the precision by using co-occurrence vectors\\nfrom a 20M words corpus was higher than by using distance vectors\\nfrom the CED.\\n\\n  Word Sense Disambiguation \\n\\nWord sense disambiguation is a serious semantic problem.\\nA variety of approaches have been proposed for solving it.\\nFor example, Vronis and Ide \\nused reference networks as neural networks,\\nHearst  used (shallow) syntactic similarity\\nbetween contexts,\\nCowie et al.  used simulated\\nannealing for quick parallel disambiguation, and\\nYarowsky  used co-occurrence statistics\\nbetween words and thesaurus categories.\\n\\n\\nOur disambiguation method is based on the similarity of context vectors,\\nwhich was originated by Wilks et al. .\\nIn this method, a context vector is the sum of its constituent word\\nvectors (except the target word itself).\\nThat is, the context vector for context,\\n\\n\\n\\nis\\n\\n\\n\\nThe similarity of contexts is measured by the angle of their vectors\\n(or actually the inner product of their normalized vectors).\\n\\n\\n\\nLet word \\n\\n\\n\\n  have senses \\n\\n\\n\\n\\n , and\\neach sense have the following context examples.\\n\\n\\n\\nWe infer that the sense of word \\n\\n\\n\\n  in an arbitrary context \\n\\n\\n\\n is \\n\\n\\n\\n  if for some j the similarity, \\n\\n\\n\\n\\n ,\\nis maximum among all the context examples.\\n\\n\\nAnother possible way to infer the sense is to choose sense \\n\\n\\n\\n such that the average of \\n\\n\\n\\n\\n  over\\n\\n\\n\\n\\n  is maximum.\\nWe selected the first method because a peculiarly similar example is more\\nimportant than the average similarity.\\n\\n\\nFigure  (next page) shows the disambiguation precision\\nfor 9 words.\\nFor each word, we selected two senses shown over each graph.\\nThese senses were chosen because they are clearly different and\\nwe could collect sufficient number (more than 20) of context examples.\\nThe names of senses were chosen from the category names in\\nRoget's International Thesaurus, except organ's.\\n\\n\\nThe results using distance vectors are shown by dots\\n(\\n\\n\\n\\n\\n\\n ),\\nand using co-occurrence vectors from the 1987 WSJ (20M words)\\nby circles (  ).\\n\\n\\nA context size (x-axis) of, for example, 10 means 10 words before the\\ntarget word and 10 words after the target word.\\nWe used 20 examples per sense; they were taken from the 1988 WSJ.\\nThe test contexts were from the 1987 WSJ:\\nThe number of test contexts varies from word to word (100 to 1000).\\nThe precision is the simple average of the respective precisions for\\nthe two senses.\\n\\n\\nThe results of Fig. show that the precision by\\nusing co-occurrence vectors are higher than that by using distance\\nvectors except two cases, interest and customs.\\nAnd we have not yet found a case where the distance vectors give higher\\nprecision. Therefore we conclude that co-occurrence vectors are\\nadvantageous over distance vectors to WSD based on the context similarity.\\n\\n\\nThe sparseness problem for co-occurrence vectors is not serious in this\\ncase because each context consists of plural words.\\n\\n\\n  Learning of -or- \\n\\nAnother experiment using the same two vector representations\\nwas done to measure the learning of  or  meanings.\\nFigure  shows the changes in the precision\\n(the percentage of agreement with the authors' combined judgement).\\nThe x-axis indicates the number of example words for each\\n or  pair.\\nJudgement was again done by using the nearest example.\\nThe example and test words are shown in Tables  and\\n, respectively.\\n\\n\\n\\nIn this case, the distance vectors were advantageous.\\nThe precision by using distance vectors increased to about 80% and then\\nleveled off,\\nwhile the precision by using co-occurrence vectors stayed around 60%.\\nWe can therefore conclude that the property of -or- is\\nreflected in distance vectors more strongly than in co-occurrence vectors.\\nThe sparseness problem is supposed to be a major factor in this case.\\n\\n\\n\\n  Supplementary Data \\n\\nIn the experiments discussed above, the corpus size for co-occurrence\\nvectors was set to 20M words ('87 WSJ)\\nand the vector dimension for both co-occurrence and distance vectors was\\nset to 1000.\\nHere we show some supplementary data that support these parameter\\nsettings.\\n\\n\\na. Corpus size (for co-occurrence vectors)\\n\\n\\nFigure  shows the change in disambiguation precision\\nas the corpus size for co-occurrence statistics increases from 200\\nwords to 20M words.\\n(The words are suit, issue and race,\\nthe context size is 10,\\nand the number of examples per sense is 10.)\\nThese three graphs level off after around 1M words.\\nTherefore, a corpus size of 20M words is not too small.\\n\\n\\n\\nb. Vector Dimension\\n\\n\\nFigure  (next page) shows the dependence of disambiguation\\nprecision on the vector dimension for (i) co-occurrence and\\n(ii) distance vectors.\\nAs for co-occurrence vectors, the precision levels off near a dimension\\nof 100. Therefore, a dimension size of 1000 is sufficient or even\\nredundant.\\nHowever, in the distance vector's case, it is not clear whether\\nthe precision is leveling or still increasing around 1000 dimension.\\n\\n\\n\\n  Conclusion \\n\\n\\n\\nA comparison was made of co-occurrence vectors from large text corpora\\nand of distance vectors from dictionary definitions.\\n\\n\\nFor the word sense disambiguation based on the context similarity,\\nco-occurrence vectors from the 1987 Wall Street Journal (20M total words)\\nwas advantageous over distance vectors from the Collins English Dictionary\\n( head words +  definition words).\\n\\n\\nFor learning  or  meanings from example words,\\ndistance vectors gave remarkably higher precision than co-occurrence\\nvectors.\\nThis suggests, though further investigation is required, that distance\\nvectors contain some different semantic information from\\nco-occurrence vectors.\\n\\n\\n\\n\\nBibliography \\n\\nKenneth W. Church and Patrick Hanks.\\n1989.\\nWord association norms, mutual information, and lexicography.\\nIn Proceedings of the 27th Annual Meeting of the Association\\nfor Computational Linguistics, pages 76-83, Vancouver, Canada.\\n\\n\\nJim Cowie, Joe Guthrie, and Louise Guthrie.\\n1992.\\nLexical disambiguation using simulated annealing.\\nIn Proceedings of COLING-92, pages 359-365, Nantes, France.\\n\\n\\nIdo Dagan, Shaul Marcus, and Shaul Markovitch.\\n1993.\\nContextual word similarity and estimation from sparse data.\\nIn Proceedings of the 31st Annual Meeting of the Association\\nfor Computational Linguistics, pages 164-171, Columbus, Ohio.\\n\\n\\nJames Deese.\\n1962.\\nOn the structure of associative meaning.\\nPsychological Review, 69(3):161-175.\\n\\n\\nMarti A. Hearst.\\n1991.\\nNoun homograph disambiguation using local context in large text\\n  corpora.\\nIn Proceedings of the 7th Annual Conference of the University of\\n  Waterloo Center for the New OED and Text Research, pages 1-22, Oxford.\\n\\n\\nHideki Kozima and Teiji Furugori.\\n1993.\\nSimilarity between words computed by spreading activation on an\\n  english dictionary.\\nIn Proceedings of EACL-93, pages 232-239, Utrecht, the\\n  Netherlands.\\n\\n\\nMark Liberman, editor.\\n1991.\\nCD-ROM I.\\nAssociation for Computational Linguistics Data Collection Initiative,\\n  University of Pennsylvania.\\n\\n\\nYoshihiko Nitta.\\n1988.\\nThe referential structure of the word definitions in ordinary\\n  dictionaries.\\nIn Proceedings of the Workshop on the Aspects of Lexicon for\\n  Natural Language Processing, LNL88-8, JSSST, pages 1-21, Fukuoka\\n  University, Japan.\\n(in Japanese).\\n\\n\\nYoshihiko Nitta.\\n1993.\\nReferential structure - a mechanism for giving word-definition in\\n  ordinary lexicons.\\nIn C. Lee and B. Kang, editors, Language, Information and\\n  Computation, pages 99-110. Thaehaksa, Seoul.\\n\\n\\nYoshiki Niwa and Yoshihiko Nitta.\\n1993.\\nDistance vector representation of words, derived from reference\\n  networks in ordinary dictionaries.\\nMCCS 93-253, Computing Research Laboratory, New Mexico State\\n  University, Las Cruces.\\n\\n\\nC. E. Osgood, G. F. Such, and P. H. Tannenbaum.\\n1957.\\nThe Measurement of Meaning.\\nUniversity of Illinois Press, Urbana.\\n\\n\\nFernando Pereira, Naftali Tishby, and Lillian Lee.\\n1993.\\nDistributional clustering of english words.\\nIn Proceedings of the 31st Annual Meeting of the Association for\\n  Computational Linguistics, pages 183-190, Columbus, Ohio.\\n\\n\\nPaul Procter, editor.\\n1978.\\nLongman Dictionary of Contemporary English (LDOCE).\\nLongman, Harlow, Essex, first edition.\\n\\n\\nHinrich Schtze.\\n1993.\\nWord space.\\nIn J. D. Cowan S. J. Hanson and C. L. Giles, editors, Advances\\n  in Neural Information Processing Systems, pages 895-902. Morgan Kaufmann,\\n  San Mateo, California.\\n\\n\\nJohn Sinclair, editor.\\n1987.\\nCollins COBUILD English Language Dictionary.\\nCollins and the University of Birmingham, London.\\n\\n\\nJean Vronis and Nancy M. Ide.\\n1990.\\nWord sense disambiguation with very large neural networks extracted\\n  from machine readable dictionaries.\\nIn Proceedings of COLING-90, pages 389-394, Helsinki.\\n\\n\\nYorick Wilks, Dan Fass, Cheng ming Guo, James E. McDonald, Tony Plate, and\\n  Brian M. Slator.\\n1990.\\nProviding machine tractable dictionary tools.\\nMachine Translation, 5(2):99-154.\\n\\n\\nDavid Yarowsky.\\n1992.\\nWord-sense disambiguation using statistical models of roget's\\n  categories trained on large corpora.\\nIn Proceedings of COLING-92, pages 454-460, Nantes, France.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nA comparison was made of vectors derived by using ordinary co-occurrence\\nstatistics from large text corpora and\\nof vectors derived by measuring the inter-word distances in dictionary\\ndefinitions.\\nThe precision of word sense disambiguation by\\nusing co-occurrence vectors from the 1987 Wall Street Journal\\n(20M total words) was higher than that by using\\ndistance vectors from the Collins English Dictionary\\n( head words +  definition words).\\nHowever, other experimental results suggest that distance vectors\\ncontain some different semantic information from co-occurrence vectors.\\n\\n'],\n",
              " ['\\n\\n  Introduction \\n  The problem \\n\\nWe want to account for the difference between the\\nfollowing kinds of dialogs:\\n\\n\\nDialog 1:\\n\\n\\n\\nDialog 2:\\n\\n\\n\\nThe first dialog is a task dialog.\\n(And there is rich literature on that topic\\n e.g. , , ). The second kind of dialog has been reported by Dyer\\n , whose program,  BORIS, was capable of \"in depth understanding of narratives\"\\n(but there were a whole series of such reports\\nin the 70s and early 80s by Schank and his students,\\n cf. , ). \\n\\n\\n Of course one can argue (e.g. ) that none of the programs\\ntruly understands any English. But even if they fake understanding,\\nthe question remains in what sense is the domain of marital relations\\nmore complex than the domain of appointment scheduling (if it really is);\\nwhat is behind these intuitions, and in what sense\\nthey are proved wrong by the existence of a program like  BORIS.\\n(Notice that the syntax of the first dialog is more complex than\\nthe syntax of the second one, but, intuitively, discussing\\ndivorce cases is more complicated than scheduling a meeting).\\n\\n\\nMore practically, we would like to be able to\\nmeasure the process of understanding natural language,\\nand in particular, to estimate the difficulty of a NLU task\\nbefore building a system for doing that task.\\n\\n\\n  Practical advantages of a small domain:  MINCAL \\n\\nWe have built a natural language interface,  MINCAL,\\n to an on-line calendar (). In this system the user can schedule, move and cancel appointments by talking to\\nthe computer or typing phrases. To perform an action,\\nthe system extracts slot values from the dialogs, e.g. for Dialog 1\\n\\n\\n\\nThe system is able to handle a whole range of grammatical\\nconstructions, including complex prepositional phrases.\\nThe problem of parsing sentences with prepositional phrases\\nis in general complex, but important, because\\nof the role of PPs in determining parameters of situations\\n (in the sense of ). The method we use () is a combination of three elements: (1)\\nlimiting structural ambiguities by using a grammar of constructions,\\nwhere forms, meanings and contexts are integrated in one data structure\\n ; (2) using background knowledge during parsing; (3) using discourse context during parsing\\n(including domain and application specific constraints).\\n\\n\\nThe method works because the domain is small. More specifically,\\n\\nOnly a small percent of constructions needed \\nFor instance, for the task of scheduling a room\\nwe need 5 out of 30 constructions with \"for\"\\n mentioned in ; and similarly for other prepositions. Note that among all prepositions the class of meanings that\\ncan be expressed using \"for\" is perhaps second least restricted,\\nthe least restricted consisting of PPs with \"of\", which however\\nis not needed for the task.\\n\\nThe number of semantic/ontological categories is small \\nThe second advantage of a limited domain lies in the\\nrelatively small number of semantic categories. For example,\\nfor the domain of calendars the number of concepts is less than 100;\\nfor room scheduling it is about 20. Even for a relatively\\ncomplex office application, say, WordPerfect Office 4.0, the number\\nof semantic categories is between 200 and 500 (the number depends\\nwhat counts as a category, and what is merely a feature).\\nWhy this is important? Because not only do we need a set of\\nsemantic categories, but also we have to encode background knowledge\\nabout them. For instance, given the concept of \"range\" with its\\n\"beginning\", \"end\" and \"measure\" (e.g. hours)\\nsmaller than the value of \"end\". We should know that two different\\nmeetings cannot occupy the same room in overlapping periods of time,\\nwe should know the number of days in any month, and that meetings\\nare typically scheduled after the current date, etc.\\n\\nBackground knowledge is bounded \\nOne should ask how many such facts we need? There is evidence\\n (, ,  ) that the ratio of the number of words to the number of facts necessary to understand\\nsentences with them is about 10:1. In the absence of large bodies\\nof computer accessible common-sense knowledge,\\nthis makes the enterprise of building\\nnatural language understanding systems for small domains\\nfeasible. Thus the advantage of limited domains lies in the fact that\\nbackground knowledge about them can be organized, hand-coded and\\n tested (cf. ). \\n\\n\\n\\n\\n  But what is a \"small domain\"? \\n\\n If we compare  BORIS (, ) with  MINCAL we notice some clear parallels.\\nFirst, they have an almost identical vocabulary size of about 350 words.\\nSecondly, they have a similar number of background knowledge facts.\\nNamely,  BORIS uses around 50 major knowledge structures such as\\nScripts, TAUs, MOPs, Settings, Relationships etc.; on average,\\nthe size of each such structure would not exceed 10 Prolog clauses\\n(and no more than 4 predicates with 2-3 variables each per clause)\\nif it were implemented in Prolog.\\nIf we apply a similar metrics to  MINCAL, we get about 200 facts\\nexpressing background knowledge about time, events and the calendar,\\nplus about 100 grammatical constructions, many of them dealing with\\ntemporal expressions, others with agents, actions etc. Clearly then\\nthe two systems are of about the same size. Finally, the main algorithms\\ndo not differ much in their complexities (as measured by size and\\nwhat they do).\\nSo the question remains: why is the domain of scheduling meetings\\n\"easier\" than the domain of discussing divorce experiences?\\nHow could we measure the open-ended character of the latter?\\n\\n\\n\\n  Semantic complexity: from intuitions to meaning automata \\n\\nWe are now ready to introduce the concept of semantic complexity\\nfor sets of sentences and natural language understanding tasks,\\ni.e. numbers measuring how complicated they are.\\nTo factor in the \"degree of understanding\",\\nthose numbers will be computed relative to some semantic types.\\nThen, for example, if\\nwe examine the semantic complexity of two sets of 24 sentences,\\none consisting of very simple time expressions, and the other of\\na set of idioms,\\nit turns out - surprisingly - that from a certain perspective they have\\nidentical complexities, but from another perspective they do not.\\n\\n  Two sets of 24 sentences and their intuitive complexity \\n\\nLet us consider the meanings of the following two constructions:\\n\\n\\npp \\n\\n\\n\\n\\n  at X pm/am\\n\\n\\npp \\n\\n\\n\\n\\n  at noun(bare)\\nFor each construction we will consider 24 cases.\\nFor the first construction these are the numbers 1-12 followed\\nby am or pm; for the second construction these are\\nexpressions such as at work, at lunch, at school, ....\\nOf course the construction \\n\\nat noun(bare) is open ended, but\\nfor the sake of comparison, we will choose 24 examples.\\nFor simplicity, we will consider the two constructions simply\\nas sets of sentences.\\nWe have then two 24-element sets of sentences:\\nThe set T contains sentences\\n\\n\\nThe meeting is at X PM_or_AM \\nwhere X ranges from 1 to 12, and PM_or_AM is either\\nam or pm. The set S contains 24 sentences of the type\\n\\n\\n\\nIntuitively, accounting for the semantics of the latter is more\\ncomplicated, because in order to explain the meaning of the expression\\nJohn is at work we have to have as the minimum the concept of\\nworking, of the place of work being a different place\\nthan the current discourse location, and of a habitual activity.\\nIn other words, a whole database of facts must be associated with\\nit. Furthermore, as the bare noun changes, e.g. into\\nJohn is at liberty, this database of facts has to change, too.\\nThis is not the case for at 7 am, and 8 pm.\\nHere, we simply map the expression X pm\\ninto \\n\\nhour(X+12) (ignoring everything else).\\n\\n\\n  Meaning automata and their complexity \\n\\nIn order to prove or disprove the intuitions described in the\\npreceding few paragraphs we need some tools. One of the tools for\\nmeasuring complexity widely used in theoretical computer science is\\nKolmogorov complexity.\\n\\n\\nKolmogorov complexity of a string x is defined as as the size of\\nthe shortest string y from which a certain universal Turing machine\\nproduces x. Intuitively, y measures the amount of information\\nnecessary to describe x, i.e. the information content of x.\\n (cf.  for details and a very good survey of Kolmogorov complexity and related concepts).\\nHowever, for our purposes\\nin this paper, any of the related definitions of complexity\\nwill work. For example,\\nit could be defined as the size of the smallest Turing\\nmachine that generates x (from an empty string); or we could use\\nthe Minimum Description Length of Rissanen\\n ( and ), or  the size of a grammar (as in ), or the number of states of an automaton.\\n\\n\\nWe could define semantic complexity of a set of sentences S\\nas its Kolmogorov complexity, i.e. as the size\\n(measured by the number of states) of the simplest\\nmachine M, such that for any sentence s\\nin S its semantics is given by M(s).\\nHowever this definition is problematic, because it assumes that there\\nis one correct semantics for any sentence, and\\nwe believe that this is not so.\\nIt is also problematic because the function K assigning its\\nKolmogorov complexity to a string is not computable.\\n\\n\\nThus, instead, we will\\ndefine  Q-complexity of a set of sentences S\\nas the size of the simplest model scheme M=MS,\\nsuch that any sentence s\\nin S its semantics is given by M(s), and M(s) correctly\\nanswers all questions about s contained in Q.\\n\\n\\nThe words \"model scheme\" can stand for either \"Turing machine\", or\\n\"Prolog program\", or \"description\", or a related notion. In this paper\\nwe think of M as a Turing machine that computes the\\nsemantics of the sentences in S, and measure its size by the\\nnumber of states. Of course, there can be more than one measure\\nof the size of the simplest model scheme M; and in\\npractice we will deal not with\\nthe simplest model scheme, but with the simplest we are\\nable to construct. And to take care of the possible\\nnon-computability of the function computing Q-complexity\\nof a set of sentences, we can put some restriction on\\nthe Turing machine, e.g. requiring it to be finite state or\\na stack automaton.\\n\\n\\nWe can now define the concept of meaning automaton\\n(M-automaton) as follows. Let Q be a set of questions. Formally,\\nwe treat each question as a (partial)\\nfunction from sentences to a set of answers A:\\n\\n\\n\\nIntuitively, each question examines a sentence for a piece of\\nrelevant information. Under this assumption the semantics of\\na sentence (i.e. a formal string) is not given by its truth\\nconditions or denotation but by a set of answers:\\n\\n\\n\\nNow, given a set of sentences S and a set of questions Q, their\\nmeaning automaton is a function\\n\\n\\n\\nwhich satisfies the constraint\\n\\n\\nM (s,q) =  q(s)\\n\\n\\ni.e. a function which gives a correct answer to every question.\\nWe call it a meaning automaton because for any sentence s\\n\\n\\n\\nFinally, the\\nQ-complexity of the set S is the size of the smallest such M.\\n\\n\\nNote that the idea of a meaning automaton as a question\\nanswer map allows us\\nto bypass all subtle semantics questions without\\ndoing violence to them. And it has some hope of being a computationally\\ntractable approach.\\n\\n\\n  Measuring semantic complexity \\n\\nWe can measure the semantic complexity of a set of sentences\\nby the size of the smallest model that answers all relevant questions\\nabout those sentences\\n(in practice, the simplest we are able to construct).\\nBut how are we going to decide\\nwhat relevant questions can be asked about the content of the set, e.g.\\nabout: Mary is at work, and\\nJohn is at liberty. Before we attempt to solve this problem,\\nwe can examine the types of questions.\\nA simple classification of questions\\n given by  (pp.191-2) is based on the type of answer they expect:\\n(1) those that expect affirmation or rejection -- yes-no questions;\\n(2) those that expect a reply supplying an item of information --\\n Wh questions; and\\n(3) those that expect as the reply one of two or more options\\npresented in the question -- alternative questions.\\n\\n\\n\\n  Semantic complexity classes \\n\\nWe now want to examine a few measures of semantic complexity:\\nyes/no-complexity,\\nand \"what is\"-complexity. We also analyze the complexity of  ELIZA\\n  as Q-complexity, and argue that defining semantic complexity of\\nNL interfaces as Q-complexity makes sense. In the second subsection\\nwe discuss the complexities of  MINCAL and  BORIS.\\n\\n  yes/no, \"what-is\" and other complexities \\n  yes-no complexities of T and S are the same  \\n\\nWe now can measure the yes-no-complexity of both T and\\nS. Let\\n\\n\\n\\nbe the mapping from\\n\\n\\n\\n\\n ,\\nwhere\\n\\n\\n\\nand\\n\\n\\n\\n\\n ,\\nif X=Y, and no otherwise.\\n(\\n\\n\\n\\n\\n ,\\nand we identify the time\\nexpressions with numbers for the sake of simplicity).\\nClearly, under this mapping all the questions\\ncan be correctly answered (remember that\\nquestion q13 returns yes for\\n\\n\\n\\n\\n\\n\\n\\n\\n ,\\nand no otherwise).\\n\\n\\n\\nis a similar mapping: we choose arbitrary 24 tokens,\\nand map the sentences of S into them in a 1-1 fashion.\\nAs before, for each s in S,\\n\\n\\n\\n\\nis well defined, and each question of the type\\nIs John at breakfast/.../at age? can be truthfully answered.\\n\\n\\nIf we measure the semantic complexity by the number of pairs in\\nthe \\n\\n\\n\\nfunctions,\\nthe yes-no complexities of both sets are the same and equal 24[2].\\nIf we measure it by the number of states of their respective Turing\\nmachines, because the two problems are isomorphic, their\\nyes-no complexity will again be identical. For example, we can\\nbuild a two state, 4-tape Turing machine. It would scan symbols\\non two input tapes, and print no on the output tape if the two\\ninput symbols are not equal. The third input tape would contain\\nfive 1\\'s and be used as a counter (the binary string\\ntwxyz represents the number \\n\\n1t+2w+4x+8y+8z+1).\\nThe machine moves always to the right, scanning the symbols.\\nIf it terminates with\\naccept and the empty output tape, it means yes;\\nif it terminates with\\naccept and the no on the output tape, it means no.\\nThis machine can be described as a \\n\\n\\n\\n\\ntable, hence we\\ncan assign the complexity of 30 to it. \\n\\n\\nWe arrive at a surprising conclusion that a set of idiomatic\\nexpressions with complicated meanings\\nand a trivial construction about time can have the same\\nsemantic complexity.\\n(From the perspective of answering yes/no questions).\\n\\n\\n  \"what is?\"-complexity \\n\\nLet U be a finite set of tokens.\\nConsider the following semantic machine MU: For any token u in\\nU, if the input is \"what is u\" the output is a definition of u.\\nFor simplicity, assume that the output is one token, i.e. can be\\nwritten in one move; let assume also that the input also consists\\nonly of one token, namely u, i.e. the question is implicit. Then,\\nthe size of MU is the measure of \"what is\"-complexity of U.\\nNow, consider T and S as sets of tokens.\\nFor T we get the \"what is\" complexity measure of 12+4=16,\\nas we can ask about every number, the meeting, the word \"is\", and the\\ntokens \"am\" and \"pm\". (We assume \"the meeting\" to be a single word).\\nFor S we get 24+2=26,\\nas we can ask about every X in \"at X\", about \"is\", and about \"John\".\\n\\n\\nThus, the semantic \"what is\"-complexity of\\nS is greater than the \"what is\"-complexity of\\nT. But, interestingly, the\\n\"what is\"-complexity of T is smaller than its yes/no-complexity.\\n\\n\\n  Complexity of NL interfaces as Q-complexity \\n\\nWe note that the definition of Q-complexity makes sense not only\\nfor declarative\\nsentences but also for commands. Consider, e.g.,  a NL interface\\nto a calendar. The set Q consists of questions about\\nparameters of calendar events: event_time?, event_name?,\\nalarm_on?, event_topic?, event_participants?.\\nIn general, in the context of a set of commands, we can identify\\nQ with the set of queries about the required and optional\\nparameters of actions described by those commands. \\n\\n\\nSimilarly, we can compute the semantic complexity of  ELIZA\\n  as Q-complexity. Namely, we can identify Q with the set of key-words for which  ELIZA\\nhas rules for transforming input sentences (including a rule for\\nwhat to do if an input sentence contains no key-word).\\nSince,  ELIZA had\\nnot more than 2 key list structures for each of the about\\n50 keywords, and its control mechanism had 18 states, its Q-complexity\\nwas no more than 118.\\n\\n\\n  Iterated \"what is?\"-complexity \\n\\nWhat would happen if one would like to play the game of asking\\n\"what is\" questions with a machine. How complex\\nsuch a machine would have to be? Again, using the results of\\n  and  about the roughly 10:1 ratio of the number of words to the number of facts necessary to\\nunderstand sentences with them, we get that\\nfor the set T we need about 20 facts for two\\nrounds of questions. However for\\nS we would need about 250 for two\\nrounds of questions. And these numbers are closer to our intuitive\\nunderstanding of the semantic complexity of the two sets.\\n(Notice that for iterated \"what is\"-complexity we assume\\nthat an explanation of a term is not one token, but roughly\\nten tokens).\\n\\n\\n\\n  Semantical simplicity of  MINCAL and  BORIS \\n\\nIn the previous subsection we have introduced some natural Q-complexity\\nmeasures, such as yes/no-complexity with\\n\\n\\n\\n\\nand\\n\\n\\n\\n\\n ,\\nor \"what-is\"-complexity with\\n\\n\\n\\n\\n ,\\nand the answers perhaps given by\\nsome reference works:\\n\\n\\n\\n\\n .\\nWe have shown how these two kinds of complexity measures distinguish\\nbetween the two sets of sentences with \"at\".\\nWe have also argued that semantic complexities of NL interfaces\\ncan be measured in a similar fashion. For instance, for\\na calendar interface we could use\\n\\n\\n\\n\\nand\\n\\n\\n\\n\\nand for  ELIZA-type programs:\\n\\n\\n\\n\\n ,\\nand\\n\\n\\n\\n\\n .\\n\\n\\nHowever we have not yet explained the difference in the apparent\\nsemantic complexities of  BORIS and  MINCAL. We will do it now.\\nFirst, as we noticed in Section 1.3, their vocabulary sizes and\\nthe sizes of their respective knowledge bases are almost identical.\\nThus, their \"what-is\"-complexities are roughly the same.\\n\\n\\nBut now our theory can give an explanation of why the sentence\\nThe meeting is at 5 seems simpler than\\nSarah cheated on Paul. Namely, for the last sentence we assume\\nnot only the ability to derive and discuss\\nthe immediate consequences of that fact\\nsuch as \"broken obligation \" or \"is Paul aware of it?\", but also\\nsuch related topics as \"Sarah\\'s emotional life\" , \"sexually\\ntransmitted diseases\", \"antibiotics\", \"germs\", \"flu\", and\\n\"death of grandmother\". In other words, the real complexity\\nof discussing a narrative is at least the complexity of\\n\"iterated-what-is\" combined with \"iterated-why\"\\n(and might as well include alternative questions).\\nBy the arguments of the preceding section\\nthis would require\\nreally extensive background knowledge, and\\nthe Q-complexity would range between 10[5] and 10[7].\\nIn contrast, the Q-complexity of  MINCAL is less than\\n10[4].\\n\\n\\nNow, obviously, one can argue that this analysis is immaterial, because\\nboth programs only fake understanding, and that real understanding\\nof the concept of a meeting with a VIP would include e.g. accompanying\\nemotions or its possible consequences for a project.\\nThis is a valid point, but the analysis stands, because\\nchanging topics, discussing whys and whats\\nis typical for discussing a story, but does not fit into\\nthe \"conversation for action\" paradigm.\\n\\n\\n\\n  How to build a complex system from semantically  simple components? \\n\\nWhat is the significance of the numbers we computed in the previous\\nsections? It is an argument showing that it\\nis possible to analyze some cases of semantic\\ncomplexity of some natural language understanding\\ntask before building systems for doing them (e.g. yes/no and\\nwhat-is complexities). Now, we want to argue\\nthat systems that exhibit (or can be attributed) complex behavior\\ncan be built from semantically simple components, where semantic\\nsimplicity is measured by Q-complexity.\\n\\n\\n\"What is\"-complexity:\\nA natural language understanding\\nsystem has to deal with a set of basic objects.\\nFor our domains of interest, these are\\nactions (typically, given by VPs),\\nobjects of actions (given by NPs), and its parameters (described by\\nPPs). These basic objects combine into possibly quite complex\\nentities to describe properties of situations (e.g. parameters\\nof a meeting).\\n\\n\\nIt can be argued that \"what is\"-complexity is a reasonable measure\\nof how complex is the set of those basic objects. Namely,\\n\"what is\"-complexity and \"twice-iterated-what-is\"\\n-complexity measures the size of the database of\\nbackground knowledge facts. Intuitively, this is a reasonable measure\\nof their semantic complexity.\\n\\n\\nComplexity of grammatical constructions:\\nIn many cases the complexity of a new construction\\nis not much greater than the complexity\\nof the subconstructions they are built from.\\nThis is the case of the simple imperative construction\\nS(imp) \\n\\n\\n\\n\\n  VP NP. In this case, and in general,\\nthere is a trade-off between letting\\nthe grammar overgeneralize, e.g. allowing \"schedule a cafeteria\",\\nand increasing the complexity of the grammar, e.g.\\nby increasing the number of noun categories\\nnp(event), np(place) etc.\\n\\n\\nSimilarly, as new constructions introduce more complexities, for example,\\nS(imp) \\n\\n\\n\\n\\n  VP NP PP,\\nwe can increase the number of constructions.\\nIn S(imp) \\n\\n\\n\\n\\n  VP NP PP,\\nPP can modify either the NP or the VP, and the complexity\\nof deciding the meaning of the sentence is a product of\\nall possible combinations of meanings of VPs and NPs.\\nTo reduce the number of combinations we split\\nS(imp) \\n\\n\\n\\n\\n  VP NP PP into\\nS(imp) \\n\\n\\n\\n\\n  VP NP(event) PP(at, time),\\nS(imp) \\n\\n\\n\\n\\n  VP NP(event) PP(at, place),\\nand use defaults and filters to exclude less plausible combinations\\n(such as places modifying actions\\nin the calendar context). Thus, roughly, the complexity\\nof the grammar can be estimated by the number of grammatical\\nconstructions, defaults and filters.\\n\\n\\nBut what about seemingly more complex constructions such as quantifiers.\\nWouldn\\'t they introduce new sorts of complexities?\\nJ. van Benthem has shown how to handle them in the spirit of\\nmeaning automata;\\n in  he used different types of automata to compute semantics of some quantified phrases. Thus,\\na very simple automaton can compute the semantics of \"all\", as in\\nCancel all my meetings today.\\nA more complex automaton can deal with more complex quantifiers, such as\\n\"most\". The basic idea is simple:\\nto decide whether most A are B is true, we can use a push-down\\nstore automaton. Its input consist of a word in \\n\\n\\n\\n\\n ,\\ne.g. abbab, where abbabdescribes the enumeration of the elements of A under which\\na is assigned to an element in A-B and b is assigned to an element\\nin \\n\\n\\n\\n ;\\nthe stack is used to store the elements; an element is\\nremoved from the stack if the next element is different; the automaton\\naccepts a sequence if at the end only b\\'s are left on the stack.\\nNotice that the meanings of A and B is ignored here; hence from\\nthe point of view of semantic complexity, the semantics of most\\nA are B would be very simple (5 states is enough). \\n\\n\\nThe complexity of discourse:\\nDespite the simplicity of  ELIZA, people were willing to attribute to\\nit a much more complex behavior. The reasons are discussed in\\n ,  and also in , where Winograd and Flores also argue that\\nthe basic conversation for action machine has only 9 states.\\n In his classification Bunt  lists 18 basic dialog control functions and dialog acts.\\nOne can of course argue about the adequacy of either model,\\nbut the fact remains that for simple tasks\\ndialog complexity is limited by a small number of basic states.\\n\\n\\n  Conclusions \\n\\nWhat are the contributions of this paper? 1. We have defined\\nsemantic complexity by connecting the concept of Kolmogorov\\ncomplexity with the types of questions that can\\napply to a sentence (a string). We have introduced the concept\\nof a meaning automaton i.e. an abstract machine for answering\\nquestions of interest.\\n2. We have analyzed semantic\\ncomplexities of simple examples involving prepositional\\nphrases and of larger NLU programs. 3. We have introduced a new\\nconcept of meaning of a string, identifying it with the set\\nof values for a fixed set of questions. 4. We have presented some\\narguments to the effect that\\nintuitively complex NLU tasks can be done by combining simple\\nsemantic automata.\\n\\n\\nSince this is all new, there are many open questions about the\\napproach. For instance:\\n(1) How useful is the new concept of meaning?\\nWhat about compositional semantics?\\nNotice that the appeal of\\ncompositionality at least partly lies in reducing the complexity\\nof the meaning automaton -- at a price of high \"what-is\"-complexity\\n(i.e. the complex semantic descriptions of words) we get a very simple\\nautomaton whose only move is functional application.\\n (See  and  for a discussion of compositionality).\\n\\n\\n(2) Can we estimate semantic complexities by statistical means?\\nThis is possible for some cases of \"what-is\"-complexity, e.g.\\nby estimating the number of technical terms in a corpus.\\n\\n\\n(3) Can we express semantic complexity of a NLU task as a function\\nof the complexity of an automaton partially solving the task and\\nthe description (or a corpus) of the whole task.\\nThis would be a most welcome result. It would mean that given\\ne.g. a corpus of phrases and a prototype that successfully\\nassigns semantics to 22% of them we could say that a complete\\nsystem would be, say, two orders of magnitude more complex.\\n\\n\\nOf course, we are aware of the fact that without some constraints on the type\\nof the corpus/description and the type of automata this kind of\\nproblem is undecidable, but the point is to find appropriate constraints.\\nFor instance, for \"what is\"-complexity such a result is\\ntrivially holds: the size of the corpus determines\\nthe size of the explanation table.\\n\\n\\n(4) It would be interesting to see under what circumstances the iteration\\nof \"what is\" questions would result in fixed points, e.g. for sets\\nT and S,\\nand what would these fixpoints be (excluding \"everything\").\\nSimilarly iterations of why questions might eventually result\\nin a fix point. But when?\\n\\n\\n(5) If we measure the semantic complexity by the number of pairs in\\nthe \\n\\n\\n\\nfunctions, the yes-no complexities of the two sets\\nT and S were the same and equal to 24[2],\\nsimilarly if we use Turing machines. But\\nnotice there are simpler automata for the same task\\nif we permit overgeneralizations, e.g. in our case we only need\\na machine with two input tapes performing a comparison\\n(i.e. with the complexity of 25, not 30)\\nit behaves almost like the yes-no machine of Section 3.1.1,\\nexcept that it will also accept pairs \\n\\n(qi, si), for i ] 24.\\nThe trade-offs between overgeneralization and simplicity\\ncan perhaps be investigated along the lines of\\n . For instance, at the price of additional states in the dialog/discourse machine, one could significantly simplify the\\ngrammar. We believe that both theoretical and empirical study of\\nthe matter is needed.\\n\\n\\nAcknowledgments. I\\'d like to thank\\nD. Kanevsky for our discussions of semantic\\ncomplexity, and W. Savitch for comments on an earlier draft. \\n\\nBibliography \\n\\nE. Bilange and J-Y. Magadur.\\nA robust approach for handling oral dialogues.\\nProc. Coling\\'92, pages 799-805, 1992.\\n\\n\\nH. Bunt.\\nContext and dialogue control.\\nThink, 3(May):19-31, 1994.\\n\\n\\nE.J. Crothers.\\nParagraph Structure Inference.\\nAblex Publishing Corp., Norwood, New Jersey, 1979.\\n\\n\\nK. Devlin.\\nLogic and Information.\\nCambridge University Press, Cambridge, 1991.\\n\\n\\nM.G. Dyer.\\nIn-Depth Understanding.\\nMIT Press, Cambridge, MA, 1983.\\n\\n\\nA.C. Graesser.\\nProse Comprehension Beyond the Word.\\nSpringer, New York, NY, 1981.\\n\\n\\nW. Lehnert, M.G.Dyer, P.N.Johnson, C.J.Yang, and S. Harley.\\nBoris - an experiment in in-depth understanding of narratives.\\nArtificial Intelligence, 20(1):15-62, 1983.\\n\\n\\nM. Li and P.M.B.Vitanyi.\\nInductive reasoning and kolmogorov complexity.\\nJournal of Commputer and System Sciences, 44(2):343-384, 1992.\\n\\n\\nJ. Rissanen.\\nA universal prior for integers and estimation by minimum description\\n  length.\\nAnnals of Statistics, 11:416-431, 1982.\\n\\n\\nR.Quirk and S.Greenbaum.\\nA Concise Grammar of Contemporary English.\\nHarcourt Brace Jovanovich, Inc., New York, NY, 1973.\\n\\n\\nW. J. Savitch.\\nWhy it might pay to assume that languages are infinite.\\nAnnals of Mathematics and Artificial Intelligence,\\n  8(1,2):17-26, 1993.\\n\\n\\nR. C. Schank, editor.\\nConceptual Information Processing.\\nAmerical Elsevier, New York, NY, 1975.\\n\\n\\nJ.A. Simpson and E.S.C. Weiner, editors.\\nThe Oxford English Dictionary.\\nClarendon Press, Oxford, England, 1989.\\n\\n\\nJ. Sinclair, editor.\\nCollins-Cobuild English Language Dictionary.\\nCollins ELT, London, 1987.\\n\\n\\nJ. van Benthem.\\nTowards a computational semantics.\\nIn Peter Gardenfors, editor, Generalized Quantifiers,\\npages   31-71. D.Reidel, Dordrecht, Holland, 1987.\\n\\n\\nJ. Weizenbaum.\\nEliza.\\nCommunications of the ACM, 9(1):36-45, 1966.\\n\\n\\nR. Wilensky, D.N. Chin, M. Luria, J. Martin, J. Mayfield, and D. Wu.\\nThe Berkeley Unix consultant project.\\nComputational Linguistics, 14(4):35-84, 1988.\\n\\n\\nT. Winograd and F. Flores.\\nUnderstanding Computers and Cognition.\\nAblex, Norwood, NJ, 1986.\\n\\n\\nW. Zadrozny.\\nOn compositional semantics.\\nProc. Coling\\'92, pages 260-266, 1992.\\n\\n\\nW. Zadrozny.\\nReasoning with background knowledge - a three-level theory.\\nComputational Intelligence 10, 2 (1994).\\n\\n\\nW. Zadrozny.\\nFrom compositional to systematic semantics.\\nLinguistic and Philosophy, 17(4) (1994).\\n\\n\\nW. Zadrozny.\\nFrom utterances to situations: Parsing prepositional phrases in a\\n  small domain.\\nProc. 4th Conference on Situation Theory and its Applications,\\n  1994.\\n\\n\\nW. Zadrozny and K. Jensen.\\nSemantics of paragraphs.\\nComputational Linguistics, 17(2):171-210, 1991.\\n\\n\\nW. Zadrozny and A. Manaster-Ramer.\\nThe significance of constructions.\\n(Manuscript from 1993)\\nIBM Research Technical Report  RC 20002(88492), 1995.\\n\\n\\nW. Zadrozny, M. Szummer, S. Jarecki, D. E. Johnson, and L.\\nMorgenstern.\\nNL understanding with a grammar of constructions.\\nProc. Coling\\'94, 1994.\\n\\nFootnotes\\n\\n  to appear in Proc. BISFAI\\'95,\\nThe Fourth Bar-Ilan Symposium on\\nFoundations of Artificial Intelligence,\\nJune 20-22, 1995,\\nRamat-Gan and Jerusalem, Israel\\n\\n\\n\\n\\n\\n',\n",
              "  '\\n\\nWe define semantic complexity using a new concept of\\nmeaning automata. We measure the semantic complexity of\\nunderstanding of prepositional phrases, of an \"in depth\\nunderstanding system\", and of a natural language interface to an on-line\\ncalendar. We argue that it is possible to measure some semantic\\ncomplexities of natural language processing systems before building them,\\nand that systems that exhibit relatively  complex behavior\\ncan be built from semantically simple components.\\n\\n'],\n",
              " [\"\\n\\n Introduction \\n\\nOne source of unnaturalness in the output of many text-to-speech systems\\nstems from the involvement of algorithmically generated default\\nintonation contours, applied under minimal control from syntax and\\nsemantics.  The intelligibility of the speech produced by these\\nsystems is a tribute to both the resilience of human language\\nunderstanding and the ingenuity of the algorithms' inventors.  It\\nhas often been noted, however, that the results frequently sound\\nunnatural when taken in context, and may on occasion mislead the\\nhearer.\\nIt is for this reason that a number of discourse-model-based speech\\ngeneration systems have been proposed, in which intonation contour is\\ndetermined from context or the model.  Work in this area includes an\\nearly study by Young and Fallside ([]), and studies by\\nTerken ([]), Houghton ([]), Isard and\\nPearson ([]), Davis and\\nHirschberg ([]), Hirschberg ([]), and\\nZacharski et al.\\n([]), although the representations of information structure\\nand its relation to syntax employed by these authors are rather different\\nfrom those proposed here.\\n Consider the exchange shown in , which is an artificial example modeled on the domain of TraumAID, a medical expert system in\\nthe context of which we are investigating spoken language\\n output. This particular example is slightly unrealistic in that TraumAID acts purely as a\\ncritiquing device and does not possess such an interactive query\\nsystem for its knowledge base; nor is it likely that such a query\\nsystem would be of practical use in the trauma surgery.  However,\\nsuch examples are useful for present purposes since they force\\nunambiguously contrastive contexts that motivate intonational focus\\nand contrastive stress.\\n In example , capitals indicate stress and brackets informally indicate the intonational phrasing.  The intonation contour\\nis indicated more formally using a version of Pierrehumbert's notation\\n(cf. Pierrehumbert [], Pierrehumbert and Hirschberg\\n []). In this notation, L+H* and H* are different high pitch accents. LH% (and its relative LH$) and\\nL (and its relatives LL% and LL$) are rising and low boundaries\\nrespectively.  The difference between members of sets like L, LL% and\\nLL$ boundaries embodies Pierrehumbert and Beckman's ([])\\ndistinction between intermediate phrase boundaries, intonational\\n phrase boundaries, and utterance boundaries. We shall skate over the former distinction here, noting only that utterance boundaries are distinguished from the\\nothers by a greater degree of lengthening and pausing.\\n The other annotations in  indicate that the intonational tunes L+H* LH% (or the related L+H* LH$) and H* L (or the related\\nH* LL$) convey two distinct kinds of\\ndiscourse information.  First, both H* and L+H* pitch accents mark the\\nword that they occur on (or rather, some element of its\\ninterpretation) for ``focus'', which in\\n the context of such simple queries as example  usually implies contrast of some kind.  Second, the tunes as a whole mark the\\nconstituent that bears them (or rather, its interpretation) as having\\na particular function in the discourse.  We have argued at length\\nelsewhere that, at least in this same restricted class of dialogues,\\nthe function of the L+H* LH% and L+H* LH$ tunes is to mark the\\n``theme'' - that\\nis, ``what the participants have agreed to talk about''.  The\\nH* L(L%/$) tune marks the ``rheme'' - that is, ``what the speaker has\\nto say'' about the theme.  This phenomenon is a strong one: the same\\nintonation contour sounds quite anomalous in the context of a question\\nthat does not establish an appropriate theme, such as ``which\\nprocedure is needed for the persistent  PNEUMOTHORAX?''.  The\\nadvantage for present purposes of Pierrehumbert's system, like other\\nautosegmental approaches, is that the entire tune\\ncan be defined independently of the particular string that it occurs\\nwith, by interpolation of pitch contour between the pitch-accent(s)\\nand the boundary for those parts bearing no tonal annotation.  It\\nwill be notationally convenient to speak of the latter as bearing\\n``null tone''.  (Of course such elements may\\nbear pitch and even secondary accent, and the\\nspecification of such details of the interpolated contour is by no\\nmeans a trivial matter.\\nHowever, we do not believe that anything hangs crucially on our use of\\nthis theory of intonation, rather than some other.)\\n\\n[Q:] I know that a  LEFT thoracostomy is needed for the  SIMPLE\\npneumothorax,\\n\\n[A:]\\n\\n\\n\\n\\n Combinatory Prosody \\n\\nFrom the example in the preceding section, it is clear that\\nintonational units corresponding to theme or rheme\\nneed not always correspond to a traditional syntactic\\nconstituent.  Since many problems in the analysis and synthesis of spoken\\nlanguage result from this apparent independence of syntactic and\\nintonational phrase boundaries, we have chosen to base our system\\non Combinatory Categorial Grammar (CCG), a formalism that\\ngeneralizes the notion of surface constituency, allowing multiple\\nderivations and constituent structures for sentences, including ones in\\nwhich the subject and verb of a transitive sentence can exist as a\\nconstituent, complete with an interpretation.\\nCCG (Steedman []) is an extension of\\nCategorial Grammar (CG).  Elements like verbs are associated with a\\nsyntactic ``category'' which identifies them as functions, and\\nspecifies the type and directionality of their arguments and the type\\nof their result.  We use a notation in which a rightward-combining\\nfunctor over a domain \\n\\n\\n\\n  into a range \\n\\n\\n\\n  is written \\n\\n\\n\\n\\n, while the corresponding leftward-combining functor is written\\n\\n\\n\\n\\n .  \\n\\n\\n\\n  and \\n\\n\\n\\n  may themselves be\\nfunction categories.   For example, a transitive verb is a function\\nfrom (object) NPs into predicates - that is, into functions from\\n(subject) NPs into S, written as follows:\\n\\n\\n\\n\\nWe also need the following two rules of functional application, where\\nX and Y are variables over categories:\\n FUNCTIONAL APPLICATION:\\n\\n\\n\\n These rules allow the function category   is a simple example: \\nTraumaid   recommends   lavage\\n--------   ----------   ------\\n   NP      (S\\\\NP)/NP      NP\\n           -------------------]\\n                   S\\\\NP\\n-----------------------[\\n           S\\nThe syntactic types in this derivation are simply a reflection of the\\ncorresponding semantic\\ntypes, apart from the addition of directional information.  If we\\n expand the category  to express the semantics of the transitive verb,\\nthe same context-free derivation can be made to build a\\ncompositional interpretation, \\n\\n\\n\\n\\n.  One way of writing such an interpreted category that is\\nparticularly convenient for translating into unification-based\\nprogramming languages like Prolog is the following:\\n\\n\\n\\n\\n In , syntactic types are paired with a semantic interpretation via the colon operator, and the category is that of a\\nfunction from NPs (with interpretation \\n\\n\\n\\n ) to functions\\nfrom NPs (with interpretation \\n\\n\\n\\n ) to Ss (with\\ninterpretation \\n\\n\\n\\n\\n ).   Constants in\\ninterpretations bear primes, variables do not, and there is a\\nconvention of left-associativity, so that \\n\\nrecommend' x y is\\nequivalent to \\n\\n(recommend' x) y.\\nCCG extends this strictly context-free categorial base in two respects.  First,\\nall arguments, such as NPs, bear only type-raised categories, such as\\n\\n\\n\\n\\n .  That is to say that the category of an NP,\\nrather than being that of a simple argument, is that of a function\\nover functions-over-such-arguments, namely verbs and the like.\\nSimilarly, all functions into such categories, such as\\ndeterminers, are functions into the raised categories, such as\\n\\n\\n\\n\\n .  For example, subject NPs bear the\\nfollowing category in the full notation:\\ntraumaid := \\n\\n\\n\\n\\nThe derivation of the same simple transitive sentence using\\n type-raised categories is illustrated in example   in the abbreviated notation. \\nTraumaid  recommends        lavage\\n--------  ----------  ------------------\\nS/(S\\\\NP)  (S\\\\NP)/NP   (S\\\\NP)\\\\((S\\\\NP)/NP)\\n          ------------------------------[\\n                     S\\\\NP\\n-------------------------]\\n           S\\nSecond, the combinatory rules are extended to include functional\\ncomposition, as well as application:\\n FORWARD COMPOSITION (]B):\\n\\n\\n\\nThis rule allows a second syntactic derivation for the above sentence, as\\n shown in example . \\nTraumaid  recommends   lavage\\n--------  ----------  --------\\nS/(S\\\\NP)  (S\\\\NP)/NP   S\\\\(S/NP)\\n--------------------]B\\n        S/NP\\n        ----------------------[\\n                 S\\nThe original reason for making these moves was to capture the fact\\nthat fragments like Traumaid recommends, which in traditional\\nterms are not regarded as syntactic constituents, can nevertheless\\n take part in coordinate constructions, like , and form the residue of relative clause formation, as in\\n . \\n\\n[a.] You propose, and Traumaid recommends, lavage.\\n\\n[b.] The treatment that Traumaid recommends\\n\\n\\nThe full extent of this theory (which covers unbounded rightward and\\nleftward ``movement'', and a number of other types of supposedly\\n``non-constituent'' coordination), together with the general class of\\nrules from which the composition rule is drawn, and the problem of\\nprocessing in the face of such associative rules, is discussed in the\\nearlier papers, and need not concern us here.  The point for present\\npurposes is that the partition of the sentence into the object and a\\nnon-standard constituent (\\n\\n\\n\\n\\n ) makes\\nthis theory structurally and semantically perfectly suited to the\\ndemands of intonation, as exhibited in exchanges like the\\n following: \\n\\n[Q:] I know that the surgeon recommends a left thoracotomy,\\nbut what does Traumaid recommend?\\n\\n[A:] \\n( T RAUMAID recommends) ( LA VAGE.)\\nL+H* \\t\\t LH% \\t\\t  H* LL$ \\n\\n\\nWe can therefore directly incorporate intonational constituency in\\nsyntax, as follows (cf. Steedman []).  First,\\nwe assign to each constituent an autonomous prosodic category,\\nexpressing its potential for combination with other prosodic\\ncategories.  Then we lock these two structural systems together via\\nthe following principle, which says that syntactic and prosodic\\nconstituency must be isomorphic:\\n PROSODIC CONSTITUENT CONDITION:\\nCombination of two syntactic categories via a syntactic combinatory\\nrule is only allowed if their prosodic categories can also combine\\nvia a prosodic combinatory rule.\\nOne way to accomplish this is to give pitch accents the category of\\nfunctions from boundaries to intonational/intermediate phrases.\\nAs in CCG, categories consist of a (prosodic) structural type, and an\\n(information structural) interpretation, associated via a colon.  The pitch\\n accents have the following functional types: \\nWe further assume, following Bird ([]),\\nthat the presence of a pitch accent causes some element(s) in the\\ntranslation of the category to be marked as focused, a matter which\\nwe will for simplicity assume to occur at the level of the lexicon.  For\\nexample, when recommends bears a pitch accent, its category will\\nbe written as follows:\\n\\n\\n\\n\\nWe depart from earlier versions of this theory in assuming that\\nboundaries are not simply arguments of such functions, but are\\n rather akin to type-raised arguments, as follows: \\nThese categories closely correspond to Pierrehumbert's distinction\\nbetween  various levels of phonological phrases.  For example, the\\nboundary L maps an\\nH* pitch accent into an intermediate phrase rheme, p:rheme.\\nThe LH% boundary maps an L+H* pitch accent onto a full intonation\\nphrase, which it is convenient for present purposes to write as\\np:theme.   (In a fuller notation we would make the distinction\\nbetween intermediate and intonational phrases explicit, but for\\npresent purposes it is irrelevant).  The LH$ boundary\\nmaps the same L+H* pitch accent\\ninto an utterance-level thematic phrase, written u:theme.\\nThe categories that result from the combination of a pitch\\naccent and a boundary may or may not\\nconstitute entire prosodic phrases, since there may be\\nprenuclear material bearing null tone.  There may also be\\nmaterial bearing null tone separating the\\npitch accent(s) from the boundary.  (Both possibilities are\\n illustrated in ).   We therefore assign the following category to the null tone, which can thereby\\napply to the right to any non-functional category of the form\\nX:Y, and compose to the right with any function into such a\\ncategory, including another null tone, to yield the same category:\\nIt is this omnivorous category that allows intonational tunes to be\\nspread over arbitrarily large constituents, since it allows the pitch\\naccent's desire for a boundary to propagate via composition into the\\nnull tone category, as in the earlier papers.\\nIn order to allow the derivation to proceed above the level of\\ncomplete prosodic phrases identifying themes and rhemes, we need the two\\n unary category-changing rules shown in  to change the phonological category of complete themes  and rhemes. \\nThese rules change the prosodic category either to\\nutterance, or to an endocentric function over that category.\\nThese types capture the fact that the\\nLL$ and LH$ boundaries can only occur at the end of a sentence, thereby\\ncorrecting an overgeneration in some early versions of this theory\\nnoted by Bird ([]).  The fact that\\nutterance is an atom rather than a\\nterm of the form X:Y is important, since it means that it can\\nunify only with another utterance.   This is vital to the preservation of the\\n intonation structure. \\nThe application of the above two rules to a complete intonational\\nphrase should be thought of as precipitating a side-effect whereby a\\ncopy of the category \\n\\n\\n\\n  is associated with the clause as its\\ntheme or rheme.  (We gloss over details of how\\nthis is done, as well as a number of further\\ncomplications arising in sentences with more than one rheme).\\nIn Steedman ([]), a related set of rules of which\\nthe present ones form a subset are shown to be well-behaved with a\\n wide range of examples.  Example . \\n      Traumaid                  recommends             lavage\\n        L+H*                        LH%                  H* LL$\\n-----------------------  --------------------------  -----------\\nS:s/(S:s\\\\NP:*traumaid') (S:recommend'x y\\\\NP:y)/NP:x  NP:*lavage'\\n     p:theme/b:lh          p:theme\\\\(p:theme/b:lh)      u:rheme\\n------------------------SYN-----------------------]B\\n------------------------PHON----------------------[\\n           S:recommend' x *traumaid'/NP:x\\n                      p:theme\\n           =============PHON=============            ====PHON===\\n           S:recommend' x *traumaid'/NP:x            NP:*lavage'\\n                 utterance/utterance                  utterance\\n           -------------------------SYN-------------------------]\\n           -------------------------PHON------------------------]\\n                      S: recommend' *lavage' *traumaid'\\n                                 utterance\\n\\n[Theme:] \\n\\nS:recommend' z *traumaid'/NP:z\\n\\n[Rheme:] \\n\\nNP:*lavage\\n\\n\\nNote that it is the identification of the theme and rheme at\\nthe stage before the final reduction\\nthat determines the information structure for the response, for it is\\nat this point that\\ndiscourse elements like the theme of the answer can be defined, and can be\\nused in semantically-driven synthesis of intonation contour directly\\nfrom the grammar.\\nOf course, such effusively informative intonation contours are\\ncomparatively rare in normal dialogues.  A more usual response to the\\n question ``What does Traumaid recommend?'' in  would put low pitch - that is, the null tone in Pierrehumbert's terms - on\\neverything except the focus of the rheme, lavage, as in\\n . \\nTraumaid recommends  LA VAGE.\\nH* LL$ \\nSuch an utterance is of course ambiguous as to whether the theme is\\ntraumaid or what traumaid recommends.  The earlier papers show\\nthat such ``unmarked'' themes, which include no primary pitch accent because\\nthey are entirely background, can be captured by a ``Null Theme\\nPromotion Rule'', as follows:\\n\\n\\n\\nThis rule says that any sequence bearing the null tone can be\\nregarded as an ``unmarked'' intermediate phrase theme.\\n\\n\\nModeling Contrast\\n\\nThe preceding remarks about the ambiguity of unmarked themes should\\nmake it clear that in general the information structure of the\\nresponse to a query cannot be identified on the basis of the question\\nalone, but requires information from the discourse model as well, to\\n which we now turn. \\nThis remark applies even more strongly to the assignment of focus and\\nthe corresponding pitch accents in the generation of the response, as\\nDavis and Hirschberg ([]), and Hirschberg\\n([]), among others, have pointed out.  That is, while it\\nmight appear as though pitch-accents could be assigned on some basis\\nsuch as the mention or non-mention of the relevant words in the theme\\nof the query, such an expedient will often break down.  Consider the\\nfollowing example, which might be produced by such a strategem, since\\nthe words ``left'' and ``thoracotomy'' do not occur in the theme Which\\n incision: Q:\\nWhich incision does  TRAUMAID prefer?\\nA:\\n( T RAUMAID prefers) (a  L EFTthora C OTomy.)\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t H* LL$ \\nIn some contexts, including the null context, this intonation contour\\nwill indeed be appropriate.  However, in any context where thoracotomy\\nprocedures are already established as the set of procedures in question,\\nthe pitch accent on thoracotomy in\\nthe response will be inappropriate and perhaps even misleading.\\n For example, in  below, the noun thoracotomy must remain unstressed while the adjective left must be accented in\\nthe response, despite having been explicitly mentioned in the text of\\n the question. Here the question itself establishes a contextual set.  The fact that\\nthe entity that is referenced in the response must be contrasted with\\nother alternatives in this set on the relevant property requires the\\nassignment of a pitch accent to the corresponding word.\\nQ:\\nDoes Traumaid prefer a  LEFT thoracotomy or a \\nRIGHT thoracotomy?\\nA:\\n(Traumaid prefers) (a  LEFT thoracotomy.)The mere fact that alternatives are contrasted on a given\\nproperty is not enough however to mandate the inclusion of a pitch accent on\\nthe corresponding linguistic material.   The property in question\\nmust restrict contrastively at the relevant point in the semantic\\nevaluation, before a pitch accent is forced.\\nThus, in a situation in which the choices include a left thoracotomy,\\na right thoracotomy, a left thoracostomy and a right thoracostomy,\\n the response to question , in which the adjective is unstressed, is perfectly\\n appropriate: Q:\\nDoes Traumaid prefer a  LEFT thora COTomy or a\\n RIGHT thora COSTomy?\\nA:\\n(Traumaid prefers) (a left thora COTomy).This example suggests that the set that is being considered by the\\ntime the adjective is semantically evaluated is no longer the entire\\nset including the left and right thoracotomy and thoracostomy\\nprocedures.  In fact, it is not even the set containing only the left\\nthoracotomy and right thoracostomy procedures, but rather the set\\ncontaining only the left thoracotomy procedure, which by definition\\ndoes not stand in contrast to any other thoracotomy procedure by\\nvirtue of the property of being performed on the left side.  This set\\narises because the noun thoracotomy restricts over the set including\\nthe left thoracotomy and the right thoracostomy procedures.\\nTo see this, consider the next exchange, uttered in the same\\nsituation.\\nQ:\\nDoes Traumaid prefer a  LEFT thora COTomy, a \\nRIGHT thora COTomy or a  LEFT thora COSTomy?\\nA:\\n(Traumaid prefers) (a  LEFT thora COTomy).Here the set established by the question is restricted by the noun in\\nthe rheme of the answer to be a set of two thoracotomy procedures\\n(both left and right).  Since they are distinguished by the property\\nleft, the corresponding linguistic material must be accented.\\nThe algorithm for determining which items are to be stressed for\\n reasons of contrast works as follows. For a given object x, we associate a set of properties which are essential for\\nconstructing an expression that uniquely refers to x, as well as a\\nset of objects (and their referring properties) which might be\\nconsidered alternatives to x with respect to the database\\nunder consideration.  The set of alternatives is restricted by\\nproperties or objects explicitly mentioned in the theme of the\\nquestion.  Then for each property of x in turn, we restrict the set\\nof alternatives to include only those objects having the given\\nproperty.  If imposing this restriction decreases the size of the set\\nof alternatives, then the given property serves to distinguish xfrom its alternatives, suggesting that the corresponding linguistic\\nmaterial should be stressed.\\nBesides determining the location of primary sentence stress,\\ncontrastive properties may also necessitate adopting non-standard\\nlexical stress patterns.  For example, in the following\\nquestion/answer pair, the normal lexical stress on thor switches\\nto pneu in pneumothorax because pneumothorax stands\\nin contrast to hemothorax.\\nQ:\\nI know which procedure is recommended for the simple\\nhemothorax.\\nBut which condition is a left  THORACOSTOMY\\nrecommended for?\\nA:\\nA left  THORACOSTOMY is recommended for the\\nsimple  PNEUmothorax.In the current implementation, such lexical stress shift is handled by\\nidentifying the lexical contrast properties in the alternative set\\nrepresentations and supplying separate pronunciations in the lexicon.\\nHowever, when such properties are determined to stand in contrast to one\\nanother, the alternate pronunciation could in principle be generated by\\nemploying the methods described above within the lexicon.\\n\\n\\n The Implementation\\n\\nThe present paper is an attempt to apply the theories outlined in the\\npreceding sections to the task of specifying contextually\\nappropriate intonation for natural language responses to database\\n queries.  The architecture of the system (shown in Figure ) identifies the key modules of the system, their relationships to the\\ndatabase and the underlying grammar, and the dependencies among their\\ninputs and outputs.\\nThe process begins with a fully segmented and prosodically annotated\\nrepresentation of a spoken query, as shown in example\\n .  We employ a simple bottom-up shift-reduce parser, making direct use of the combinatory\\nprosody theory described above, to identify the semantics of the\\nquestion.  The inclusion of prosodic categories in the grammar allows\\nthe parser to identify the information structure within the question\\n as well, marking ``focused'' items with *, as shown in . For the moment, unmarked themes are handled by taking the longest\\nunmarked constituent permitted by the syntax.\\nI know what the  CAT scan is for, \\nbut  W HICH condition does  URIN ALYSIS address? \\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nProposition:\\n\\n\\n\\n\\nTheme:\\n\\n\\n\\n\\n\\n(s:address(*urinalysis, x)/np:x)\\nRheme:\\n\\ns:address(*urinalysis, x)/np:x\\nThe content generation module, which has the task of determining the\\nsemantics and information structure of the response, relies on several\\nsimplifying assumptions.  Foremost among these is the notion that the\\nrheme of the question is the sole determinant of the theme of the\\nresponse, including the specification of focus (although the type of\\npitch accent that eventually marks the focus will be different in the\\nresponse).  The overall semantic structure of the response can be\\ndetermined by instantiating the variable in the lambda expression\\ncorresponding to the wh-question with a simple Prolog query.\\nGiven the syntactic and focus-marked semantic representation for the\\nresponse, along with the syntactic and focus-marked semantic\\nrepresentation for the theme of the response, a representation for the\\nrheme of the response can be worked out from the CCG rules.  The\\nassignment of focus for the rheme of the response (i.e.  the\\ninstantiated variable) must be worked out from scratch, on the basis\\nof the alternative sets in the database, as described in section 3.\\n For the question given in , the content generator produces the following:\\nProposition:\\n\\ns:address(*urinalysis, *hematuria)\\nTheme:\\n\\ns:address(*urinalysis, x)/np:x\\nRheme:\\n\\nnp:*hematuria\\nFrom the output of the content generator, the CCG generation\\nmodule produces a string of words and Pierrehumbert-style markings\\n representing the response, as shown in . \\nurinalysis@lhstar addresses@lh hematuria@hstarllb\\nThe final aspect of generation involves translating such a string into\\na form usable by a suitable speech synthesizer.  The current\\nimplementation uses the Bell Laboratories TTS system (Liberman and\\nBuchsbaum []) as a post-processor to synthesize the speech\\nwave itself.\\n\\n\\nResults\\n\\nThe system described above produces quite sharp and natural-sounding\\ndistinctions of intonation contour in minimal pairs of queries like\\n those in examples , which should be read as concerning a single patient with multiple wounds.  These examples\\nillustrate the system's capability for producing appropriately\\ndifferent intonation contours for a single string of words under the\\ncontrol of discourse context. If the responses in these examples are\\ninterchanged, the results sound distinctly unnatural in the given\\n contexts. \\n Examples  illustrate the necessity of the theme/rheme distinction.  Although the pitch accent locations in\\nthe responses in these examples are identical, occurring on thoracostomy and simple, the alternation in the\\ntheme and rheme tunes is necessary to convey the intended proposition\\nin the given contexts.\\n Examples  show that the system makes appropriate distinctions in focus placement\\nwithin themes and rhemes based on context.  Although the responses in these two\\nsentences possess the same intonational tunes, the pitch accent\\nlocation is crucial for conveying the appropriate contrastive\\nproperties.\\n Examples  manifest the eight basic combinatorial possibilities for pitch accent placement and tune\\nselection produced by our program for the given sentence.\\nThe inclusion of contrastive lexical stress shift increases the\\nnumber of intonational possibilities even more, as exemplified in\\n . \\nQ:\\nI know what's recommended for the  pneumothorax,\\nbut which procedure   is recommended for the  SI pneumothorax?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA left  THORAC OST OMY   isrecommended for the  SI MPLE pneumothorax.\\nH* \\t\\t L \\t\\t L+H* \\t\\t LH$ \\nQ:\\nI know what's recommended for the  PERSISTENTpneumothorax,\\nbut which pneumothorax   is a left  THORAC OSTOMYrecommended for?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA left  THORAC OSTOMY is recommendedfor  the S IMPLE pneumothorax.\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nQ:\\nI know what's recommended for the  PERITONITIS,\\nbut which procedure   is recommended for the simplepneumo T HORax?\\nL+H* \\t\\t LH% \\t\\t H* LL$ \\nA:\\nA left  THORAC OST OMY   isrecommended for the simple pneumo THORax.\\nH* \\t\\t L \\t\\t L+H* LH$ \\nQ:\\nI know what's recommended for the  PERITONITIS,\\nbut which condition   is a left  THORAC OSTOMYrecommended for?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA left  THORAC OSTOMY is recommendedfor  thesimple pneumo THORax.\\nL+H* \\t\\t LH% \\t\\t H* LL$ \\nQ:\\nA  RIGHT thoracostomy is recommended for the  pneumothorax,\\nbut which thoracostomy   is recommended for the  S pneumothorax?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA  L EFT thoracostomy   isrecommended for the  S IMPLE pneumothorax.\\nH* \\t\\t L \\t\\t L+H* \\t\\t LH$ \\nQ:\\nA  RIGHT thoracostomy is recommended for the pneumothorax,\\nbut which pneumothorax   is a  L EFT thoracostomyrecommended for?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA  LEFT thoracostomy is recommended for  the S IMPLE pneumothorax.\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nQ:\\nA  RIGHT thoracostomy is recommended for somecondition,\\nbut which thoracostomy   is recommended for the simplepneumo THORax?\\nL+H* \\t\\t LH% \\t\\t H* LL$ \\nA:\\nA  L EFT thoracostomy   isrecommended for the simple pneumo THORax.\\nH* \\t\\t L \\t\\t L+H* LH$ \\nQ:\\nA  RIGHT thoracostomy is recommended for somecondition,\\nbut which condition   is a  L EFTthoracostomy recommended for?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA  LEFT thoracostomy is recommended for  thesimple pneumo THORax.\\nL+H* \\t\\t LH% \\t\\t H* LL$ \\nQ:\\nI know which procedure is recommended for the simplehemothorax,\\nbut which procedure   is recommended for the simple P NEUmothorax?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA left  THORAC OST OMY   isrecommended for the simple  PNEUmothorax.\\nH* \\t\\t L \\t\\t L+H* \\t\\t LH$ \\nQ:\\nI know which procedure is recommended for the simplehemothorax,\\nbut which condition   is a left  THORAC OSTOMYrecommended for?\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\nA:\\nA left  THORAC OSTOMY is recommendedfor  thesimple  P NEUmothorax.\\nL+H* \\t\\t LH% \\t\\t H* \\t\\t LL$ \\n\\n\\nConclusions\\n\\nThe results show that is possible to generate synthesized spoken\\nresponses with contextually appropriate intonational contours in a\\ndatabase query task.  Many important problems remain, both because of\\nthe limited range of discourse-types and intonational tunes considered\\nhere, and because of the extreme oversimplification of the discourse\\nmodel (particularly with respect to the ontology, or variety of types\\nof discourse entities).  Nevertheless, the system presented here has a\\nnumber of properties that we believe augur well for its extension to\\nricher varieties of discourse, including the types of monologues and\\ncommentaries that are more appropriate for the actual TraumAID domain.\\nForemost among these is the fact that the system and the underlying\\ntheory are entirely modular.  That is, any of its components can be\\nreplaced without affecting any other component because each is\\nentirely independent of the particular grammar defined by the lexicon\\nand the particular knowledge base that the discourse concerns.  It is\\nonly because CCG allows us to unify the structures implicated\\nin syntax and semantics on the one hand, and intonation and discourse\\ninformation on the other, that this modular structure can be so simply\\nattained.\\n\\n\\nAcknowledgments\\n\\nPreliminary versions of some sections in the present paper were\\npublished as Prevost and Steedman ([]).  We are\\ngrateful to the audiences at those meetings, to ATT Bell\\nLaboratories for allowing us access to the TTS speech synthesizer, to\\nMark Beutnagel, Julia Hirschberg, and Richard Sproat for patient\\nadvice on its use, to Abigail Gertner for advice on Traumaid, to\\nJanet Pierrehumbert for discussions on notation, and to the anonymous\\nreferees for many helpful suggestions.  The usual\\ndisclaimers apply.  The research was supported in part by NSF grant\\nnos. IRI90-18513, IRI90-16592, IRI91-17110 and CISE IIP-CDA-88-22719,\\nDARPA grant no.  N00014-90-J-1863, ARO grant no.  DAAL03-89-C0031, and\\ngrant no.  R01-LM05217 from the National Library of Medicine.\\n\\n\\nFootnotes\\n\\n  The examples used throughout the paper are based on a\\nthe domain of TraumAID, which is currently under development at the\\nUniversity of Pennsylvania (Webber et al. []).  The\\nlay reader may find it useful to know that a thoracostomy is the\\ninsertion of a tube into the chest, and pneumothorax refers to\\nthe presence of air or gas in the pleural cavity.\\n  A brief summary of Pierrehumbert's notation\\ncan be found in Steedman ([]).\\n  Since utterance\\nboundaries always coincide with an intonational phrase boundary, this\\ndistinction is often left implicit in the literature, both being\\nwritten with % boundaries.  For purposes of synthesis, however, the\\ndistinction is important.\\n  It may be helpful for the\\nreader to know that lavage refers to the therapeutic cleansing\\nof an organ.\\n  It is important to\\nrealize that the semantics of the type raised categories and of the\\napplication rules ensures that this derivation yields an S with the\\n same interpretation as the earlier derivation , namely \\n\\n\\n\\n\\n .  At first glance, it looks as\\nthough type-raising will expand the lexicon alarmingly.  One way round\\nthis problem is discussed in\\nSteedman ([]).\\n  As before, it is important to realize that the semantics of the\\ncategories and of the new rule of functional composition guarantee\\nthat the S\\nyielded in this derivation bears exactly the same interpretation as\\n the original purely applicative derivation . \\n  A similar argument in a related categorial\\nframework is made by Moortgat ([]).\\n  Here we are ignoring the possibility of multiple pitch accents in the\\nsame prosodic phrase, but cf. Steedman ([]).\\n  Note\\nagain that $ boundaries are often conflated with % intonational\\nphrase boundaries in the literature.  These categories, which in some\\nsense imply that boundaries are phonological heads, constitute a\\nmodification to previous versions of the present theory that brings it\\nmore closely into line with the proposals in Pierrehumbert and\\nHirschberg ([]).  The idea that boundaries are functors\\nhas been independently proposed by Kirkeby-Garstad and Polgardi\\n(p.c.).\\n  These rules represent another minor\\ndeparture from the earlier papers.\\n  The category has the effect of preventing further composition into the\\nnull tone achieved in the earlier papers by a restriction on forward\\nprosodic composition.\\n  Note that since the raised\\nobject category is not crucial, it has been replaced by NP for ease of reading\\ncomprehension.  Also note the focus-marking effect of the pitch accents.\\n  See Prevost and Steedman ([])\\nfor an investigation of how much one can get away with on the basis of\\nthe question alone.\\n  It may be helpful to point out that a thoracotomy is a surgical incision of the chest wall, and a thoracostomy is the insertion of a tube into the chest.\\n  In using these examples to motivate the\\ntreatment of contrast in the system, we go beyond the class of\\ndiscourses that are actually handled by the system as currently\\nimplemented.  We are in fact glossing over a number of subtle problems\\nconcerning the theme-rheme structures that are involved, and the\\nprecise reflection of these information structures in intonation.\\n  That is not to claim that the adjective cannot carry a pitch\\naccent, of course.\\n  We omit a more detailed\\ndescription of the algorithm and its associated data structures for\\nthe sake of brevity.  A more detailed account and numerous examples\\nare given in Prevost and Steedman ([]).\\n  We stress that we do not start\\nwith a speech wave, but a representation that one might obtain from a\\nhypothetical system that translates such a wave into strings of words\\nwith Pierrehumbert-style intonation markings.\\n  Full\\ndescriptions of the CCG generation algorithm are given in\\nPrevost and Steedman ([]).\\n  The first line of each query is for reader\\nassistance only, and is not processed by the system described here.\\nThe waves files corresponding to the examples in this section\\nare available by anonymous ftp from ftp.cis.upenn.edu, under the\\ndirectory /pub/prevost/speechcomm.\\n\\n\\n\\n\",\n",
              "  \"\\n\\nThis paper presents a theory and a computational implementation for\\ngenerating prosodically appropriate synthetic speech in response to\\ndatabase queries.  Proper distinctions of contrast and emphasis are\\nexpressed in an intonation contour that is synthesized by rule under\\nthe control of a grammar, a discourse model, and a knowledge base.\\nThe theory is based on Combinatory Categorial Grammar, a formalism\\nwhich easily integrates the notions of syntactic constituency,\\nsemantics, prosodic phrasing and information structure.  Results from\\nour current implementation demonstrate the system's ability to\\ngenerate a variety of intonational possibilities for a given sentence\\ndepending on the discourse context.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\n For an agent  to be able to perform an action, it must satisfy both the physical and\\n knowledge preconditions of that action ,. For example, for an agent to pick up a particular tower of blocks, it\\nmust (1) know how to pick up towers in general, (2) be able to\\nidentify the tower in question, and (3)  have satisfied the (physical)\\npreconditions or constraints associated with picking up towers (e.g.,\\nit must have a free hand).  These conditions must hold whether the\\nagent is planning an action on its own or is involved in a\\ncollaborative planning effort with other agents.\\n\\n\\nIn this paper, we provide an axiomatization of knowledge preconditions\\nfor the SharedPlan model of collaborative activity\\n ,,.  This model draws upon  past work ,, but adapts it to the collaborative situation.  We briefly describe the SharedPlan framework\\n in Section , and then, in Section , present our  axiomatization of knowledge preconditions.  In Section , we demonstrate the use of knowledge preconditions in accounting for\\n information-seeking subdialogues, such as those in Figure .  We then compare our approach to the alternative accounts\\n ,,. \\n\\n\\n    SharedPlans\\n\\n\\nThe SharedPlan formalism is a mental-state model of collaborative\\nplans with roots in Pollack's  work on\\nsingle-agent plans.  For a group of agents GR to have a full\\nSharedPlan (FSP) for an act \\n\\n\\n\\n ,\\nthey must satisfy the\\n requirements given in Figure .  When the agents have satisfied only a subset of these requirements, they are said to have a\\n partial SharedPlan (PSP).  The bracketed  terms in Figure     indicate the operators used by Grosz and Kraus  to formalize each requirement.\\n\\n\\n Requirement (1) in Figure  refers to the agents' recipe  for  \\n\\n\\n\\n .\\nRecipes are modeled in Grosz\\nand Kraus's definitions as sets of constituent acts and constraints.\\nTo perform an act \\n\\n\\n\\n ,\\nan agent must perform each constituent act\\nin \\n\\n\\n\\n 's recipe according to the constraints of that recipe.\\nActions themselves may be further decomposed into act-types and\\nparameters.  We will represent an action \\n\\n\\n\\nas a term of the\\nform \\n\\n\\n\\n\\nwhere \\n\\n\\n\\n\\nrepresents\\nthe act-type of the action and the pi its parameters.\\n\\n\\n    Knowledge Preconditions\\n\\n\\nGrosz and Kraus  use the operators BCBA\\n(read ``believes can bring about'') and MBCBAG (read ``mutually\\nbelieve can bring about group'') to formalize respectively\\n requirements (2b) and (3a) in Figure .  Although these operators are intended to specify the conditions under which an agent\\nis able to perform an action, their definitions explicitly require\\nonly that an agent satisfy the physical preconditions or constraints\\nassociated with an action to be able to perform it.  Because an agent\\nis not truly capable of performing an act unless it possesses the\\nappropriate knowledge, the definitions of BCBA and MBCBAG must be\\naugmented with an axiomatization of knowledge preconditions.  The\\nfollowing observations made by Morgenstern ,\\nbut recast in our terminology, must be represented in such an\\naxiomatization:\\n1.\\nAgents need to know recipes for the acts they\\nperform.\\n2.\\nAll agents have some primitive acts in their\\nrepertoire.\\n3.\\nAgents must be able to identify the parameters of the acts they\\nperform.\\n4.\\nAgents may know only some descriptions of an act.\\n5.\\nAgents know that the knowledge necessary for complex acts\\nderives from that necessary for their component acts.\\n\\n\\nOur axiomatization of knowledge preconditions is based on\\nMorgenstern's observations, but adapted to the requirements of\\n individual and shared mental-state plans.  We use the predicates has.recipe and id.params to\\nrepresent explicitly observations (1) and (3) above.  The remaining\\nobservations are implicitly represented by the way in which these two\\nknowledge precondition relations are defined.  Observation (2) is\\nmodeled as the base case of has.recipe, and observation (5) is\\nmodeled by the use of has.recipe within the recursive plan\\ndefinitions.\\n\\n\\nObservation (4) requires that the knowledge precondition relations be\\nintensional, rather than extensional; within their scope it should not\\nbe possible to freely substitute one representation of an action for\\nanother.  We thus define has.recipe and id.params to\\nhold of action descriptions, rather than actions.  Action\\ndescriptions are intensional objects; one action description can be\\nsubstituted for another only if the descriptions are the same.  For\\nexample, although 555-1234 and phone-\\n\\nnumber(speech-lab) may\\nbe extensionally equivalent, the descriptions \\n\\n\\n\\n\\n -\\n\\n\\n\\n\\n and \\n\\n\\n\\n\\n -\\n\\nnumber(speech-\\n\\n\\n\\n\\nare not.  By\\nconvention, we will omit the corner quote notation in what follows and\\nsimply take the appropriate arguments of the predicates to represent\\naction descriptions rather than actions.\\n\\n\\nAlthough Morgenstern's observations are most naturally expressed\\ninformally in terms of knowledge, we formalize them using belief to\\nallow for the possibility of an agent's being incorrect.  Although it\\nis true that an agent cannot successfully act unless its beliefs about\\nrecipes and parameters are correct, having to know the recipes\\nand parameters is too strong a requirement for collaborating agents\\n . \\n\\n    Determining Recipes: has.recipe\\n\\n\\nFor an agent to be able to perform an act \\n\\n\\n\\n ,\\nit must know how to perform \\n\\n\\n\\n ;\\ni.e., it must have a recipe for the act.\\nThe relation \\n\\n\\n\\n\\nis used to represent that\\nagent G has a recipe \\n\\n\\n\\nfor an act \\n\\n\\n\\nat time T.  It is\\nformalized as follows:\\n\\n\\n\\n         (1)    [\\n\\n\\n\\n\\n        (2)    \\n\\n\\n\\n         (2a) \\t\\t\\n\\n\\n\\n         (2a1) \\t\\t    \\n\\n\\n\\n\\n         (2a2) \\t\\t\\t\\t\\n\\n\\n\\nClause (1) of the definition models Morgenstern's second observation,\\nnamely that agents do not need a recipe to perform a basic-level\\naction, i.e., one executable at will\\n . For non-basic-level actions (Clause (2)), the agent of \\n\\n\\n\\n(either a single agent (2a1) or a\\ngroup of agents (2a2)) must believe that some set of acts, \\n\\n\\n\\n ,\\nand constraints, \\n\\n\\n\\n ,\\nconstitute a recipe for\\n\\n\\n\\n .\\n\\n\\n    Identifying Parameters: id.params\\n\\n\\nAn agent must also be able to identify the parameters of an act\\n\\n\\n\\nto be able to perform it.  For example, if an agent is told\\n ``remove the flywheel,'' as in the dialogue of Figure , the agent must be able to identify the flywheel in question.  The\\nrelation \\n\\n\\n\\n\\nis used to represent that agent Gcan identify the parameters of act \\n\\n\\n\\nat time T.  If \\n\\n\\n\\n is of the form \\n\\n\\n\\n\\n ,\\nthen\\n\\n\\n\\n\\nis true if G can identify each of the pi.\\nTo do so, G must have a description of each pi that is suitable\\nfor \\n\\n\\n\\n\\n .\\nThe relation id.params is defined as\\nfollows:\\n\\n\\n\\n\\nThe ability to identify an object is highly context dependent.  For\\nexample, as Appelt points out , ``the\\ndescription that one must know to carry out a plan requiring the\\nidentification of `John's residence' may be quite different depending\\non whether one is going to visit him, or mail him a letter.''  The\\nfunction \\n\\n\\n\\nin the above definition is an oracle function\\nintended to model the context-dependent nature of parameter\\nidentification.  This function returns a suitable identification\\n constraint  for a parameter pi in the context of an act-type \\n\\n\\n\\n\\n .\\nFor example, in the case of\\nsending a letter to John's residence, the constraint produced by the\\noracle function would be that John's residence be described by a\\npostal address.\\n\\n\\nThe relation \\n\\nhas.sat.descr(G,P,C,T) holds of an agent G, a\\nparameter description P, an identification constraint C, and a\\ntime T, if G has a suitable description, as determined by C, of\\nthe object described as P at time T.  To formalize this relation,\\nwe utilize Kronfeld's  notion of an\\nindividuating set.  An agent's individuating set for an object is a\\nmaximal set of terms such that each term is believed by the agent to\\ndenote that object.  For example, an agent's individuating set for\\nJohn's residence might include its postal address as well as an\\nidentifying physical description such as ``the only yellow house on\\nCherry Street.''  To model individuating sets, we introduce a function\\nIS(G,P,T); the function returns an agent G's individuating set at\\ntime T for the object that G believes can be described as P.\\nThis function is based on similar elements of the formal language that\\nAppelt and Kronfeld  introduce as part\\nof their theory of referring.  The function returns a set that\\ncontains P as well as the other descriptions that G has for the\\nobject that it believes P denotes.\\n\\n\\nFor an agent to suitably identify a parameter described as P, the\\nagent must have a description, \\n\\n\\n\\n ,\\nof the parameter such that\\n\\n\\n\\nis of the appropriate sort.  For example, for an agent to\\nvisit John's residence, it is not sufficient for the agent to believe\\nthat the description ``John's residence'' refers to the place where\\nJohn lives.  Rather, the agent needs another description of John's\\nresidence, one such as ``the only yellow house on Cherry Street,''\\nthat is appropriate for the purpose of visiting him.  To model an\\nagent's ability to identify a parameter (described as P) for some\\npurpose, we thus require that the agent have an individuating set for\\nthe parameter that contains a description \\n\\n\\n\\nsuch that\\n\\n\\n\\nsatisfies the identification constraint that derives from\\nthe purpose.  The definition of has.sat.descr is thus as\\n follows:     \\n\\n\\n\\n\\n [\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe predicate \\n\\n\\n\\n\\nis true if the\\nconstraint C applies to the parameter description \\n\\n\\n\\n .\\nThe\\noracle function \\n\\n\\n\\n\\nin id.params\\nproduces the appropriate identification constraint on pi given\\n\\n\\n\\n\\n .\\n\\n\\n\\n    The Role of Knowledge Preconditions in Language Processing\\n\\n\\nWe now show how the requirements of knowledge preconditions can be\\nused in discourse processing.  Our model of discourse processing is\\nbased on the theory of discourse structure proposed by Grosz and\\nSidner .  According to their theory,\\ndiscourse structure consists of three interrelated components: a\\nlinguistic structure, an attentional state, and an intentional\\nstructure.  The linguistic structure consists of discourse\\n segments and an embedding relationship among  them; the bold rule in Figure     indicates the linguistic structure of that discourse.  Attentional state is an abstraction of\\nthe discourse participants' focus of attention; it serves as a record\\nof those entities that are salient at any point in the discourse.\\nIntentional structure is comprised of discourse segment purposes and\\ntheir interrelationships, particularly that of dominance.  A\\ndiscourse segment purpose, or DSP, is a\\nGricean-like  intention that leads to the\\ninitiation of a discourse segment.  One DSP is dominated by another if\\nthe satisfaction of the first provides part of the satisfaction of the\\nsecond.\\n\\n\\nIntentional structure plays a central role in discourse processing; an\\nagent's comprehension of the utterances in a discourse relies on the\\n recognition of this structure .  Grosz and Sidner  proposed SharedPlans to provide a\\nbasis for recognizing intentional structure.  They argued that\\ndiscourses are fundamentally collaborative, and hence that a model of\\nshared plans provides a more appropriate basis for discourse\\nprocessing than a model of single-agent plans.  However, the\\nconnection between SharedPlans and intentional structure was never\\nspecified.\\n\\n  SharedPlans as Intentional Structure \\n\\nWe have developed a model of discourse processing that provides that\\n connection .  Figure 3 illustrates the role of SharedPlans in modeling intentional structure.  Each segment of a\\ndiscourse has an associated SharedPlan.  The purpose of the segment is\\n taken to be intention that (Int.Th ) the discourse participants form that plan.  This intention is held by the\\nagent who initiates the segment.  In what follows, we will refer to\\nthat participant as the initiating conversational participant or ICP;\\n the other participant is the OCP .  Dominance relationships between DSPs are modeled using subsidiary\\nrelationships between SharedPlans.  One plan is subsidiary to another\\nif the completion of the first plan contributes to the completion of\\nthe second.  Subsidiary relationships are discussed in more detail in\\n Section \\n\\n\\nThe utterances of a discourse are understood in terms of their\\ncontribution to the SharedPlans associated with the segments of the\\ndiscourse.  Those segments that have been completed at the time of\\nprocessing an utterance have a full SharedPlan (FSP) associated with\\nthem (e.g., segment (2) in Figure 3), while those that have not have a\\npartial SharedPlan (PSP) (e.g., segments (1) and (3) in Figure 3).\\n\\n\\n\\n\\n\\n\\nFigure 3: Modeling Intentional Structure\\n\\n\\nFor each utterance of a discourse, an agent must determine whether the\\nutterance begins a new segment of the discourse, contributes to the\\n current segment, or completes it .  For an utterance to begin a new segment, it must indicate the initiation of a\\nsubsidiary plan.  This case is described in further detail below.  For\\nan utterance to contribute to the current segment, it must advance the\\npartial SharedPlan associated with the segment towards completion.\\nThat is, it must establish one of the beliefs or intentions required\\nfor the discourse participants to have a full SharedPlan, but missing\\nfrom their current partial SharedPlan.  For an utterance to complete\\nthe current segment, it must indicate that the purpose of that segment\\nhas been satisfied.  For that to be the case, the SharedPlan\\nassociated with the segment must be an FSP rather than a PSP.  That\\nis, all of the beliefs and intentions required of an FSP, as indicated\\n in Figure , must have been established over the course of the segment.\\n\\n\\nA detailed description of the implemented algorithms used in modeling\\n each of these cases can be found elsewhere .  Here, we focus on the use of knowledge preconditions in accounting for the\\ninitiation of information-seeking subdialogues.  We use the dialogue\\n in Figure  as an example and assume the role of the Expert  (participant ``E'') in analyzing the discourse.  The dialogue in Figure     was extracted from a larger discourse in which the Expert and\\nApprentice (participant ``A'') are collaborating on removing the pump\\nof an air compressor.  We thus take the purpose of the larger\\ndiscourse to be\\n\\n\\n   DSP1=\\n\\n\\n\\n        where ac1 represents the air compressor the agents        are working on. \\n\\n\\n    Accounting for the Initiation of New Discourse Segments\\n\\n\\nTo make sense of an utterance, an agent must provide an\\nexplanation for it in the form of an answer to the question, ``Why did\\n the speaker say that to me?'' .  An OCP must provide a similar explanation for an ICP's initiation of a new\\ndiscourse segment.  This explanation takes the form of an answer to\\nthe question ``Why does the ICP want to engage in a segment with\\npurpose DSPj at this point in our discourse?''; i.e., ``How is\\nDSPj related to what we were talking about before?''  Subsidiary\\nrelationships between SharedPlans provide the basis for modeling the\\nOCP's reasoning.\\n\\n\\nOne plan is subsidiary to another if the completion of the first plan\\ncontributes to the completion of the second.  The most basic example\\nof this relationship occurs within the FSP definition itself.  As\\n indicated in Figure , a full plan for an act  \\n\\n\\n\\n includes full plans for each subact in \\n\\n\\n\\n 's recipe as components\\n(requirements (2c) and (3b)).  The plans for the subacts thus\\ncontribute to the plan for \\n\\n\\n\\nand are therefore subsidiary to\\nit.\\n\\n\\nSubsidiary relationships may also arise in response to the other\\nrequirements of the FSP definition.  For example, as discussed in\\n Section , the BCBA operator used to model requirement (2b) specifies that to be able to perform an act \\n\\n\\n\\n ,\\nan agent must (1) have a recipe for \\n\\n\\n\\n(\\n\\nhas.recipe), (2) be\\nable to identify the parameters of \\n\\n\\n\\n(\\n\\nhas.sat.descr), and\\n(3) have satisfied the constraints associated with performing\\n\\n\\n\\n .\\nThe first of these requirements provides an explanation for\\n the first subdialogue in Figure . \\n\\n\\n The purpose of this subdialogue is represented as      DSP2=\\n\\n\\n\\n \\t\\t  Achieve(has.recipe(a,\\t\\t\\t\\t\\n\\n\\n\\nand can be glossed as ``the Apprentice\\nintends that the agents collaborate on his obtaining a recipe for the\\nact of removing the flywheel of the air compressor.''  To account for\\nthe Apprentice's initiation of this subdialogue, the Expert must\\ndetermine the relationship of DSP2 to the purpose of the agents'\\npreceding discourse, namely DSP1. In this case, the Expert can\\nreason that the Apprentice wants to engage in the subdialogue to\\nobtain a recipe for the act of removing the flywheel so that he will\\nbe able to perform that act as part of the agents' SharedPlan to\\nremove the pump.  The plan in DSP2 is thus subsidiary to that in\\nDSP1 by virtue of a knowledge precondition requirement of the\\nlatter plan.\\n\\n\\n Figure  illustrates this analysis.  Each box in the figure corresponds to a discourse segment and contains the SharedPlan\\nused to model the segment's purpose.  The SharedPlans are labeled so\\nas to be co-indexed with the DSPs discussed above.  The arrows\\nindicate subsidiary relationships between SharedPlans, as explained by\\nthe text that adjoins them.  When plan Pj is subsidiary to plan Pi,\\nDSPj is dominated by DSPi.\\n\\n\\nThe information represented within each SharedPlan in\\n Figure  is separated into two parts.  Those beliefs and intentions that have been established at the time of the analysis\\nare shown above the dotted line, while those that remain to be\\nestablished, but that are used in determining subsidiary\\nrelationships, are shown below the line.  The index in square brackets\\nto the right of each constituent indicates the FSP requirement from\\nwhich the constituent arose.\\n\\n\\n As indicated in Figure , the initiation of the second  subdialogue in Figure  is explained similarly. This time, however, it is the need to identify parameters of acts\\n(requirement (2) above) that leads to the initiation of the\\nsubdialogue.  In addition, the parameter in question is a parameter of\\nan act in a subsidiary individual plan of the Apprentice's.\\n\\n\\n  Discussion \\n\\nIn this paper, we have shown that information-seeking subdialogues may\\nbe explained on the basis of knowledge precondition requirements.  Our\\naccount of such subdialogues fits within a general framework for\\ndiscourse processing in which the purpose of a subdialogue is modeled\\nusing a SharedPlan and is related to the purposes of other\\nsubdialogues based on the requirements of the FSP definition.\\n Elsewhere , we show that correction and subtask subdialogues, among others, may also be accounted for in this manner.\\n\\n\\nIn contrast, alternative plan-based accounts of dialogue understanding\\nintroduce multiple types of plans to account for the utterances in a\\ndiscourse.  For example, Litman and Allen ,\\npropose the use of two types of plans to model clarification and\\ncorrection subdialogues: discourse plans and domain\\nplans.  Domain plans represent knowledge about a task, while\\ndiscourse plans represent conversational relationships between\\nutterances and plans.  Litman and Allen provide operators for the\\nfollowing discourse plans:\\n\\nINTRODUCE-PLAN: introduce a new plan for discussion\\n\\nCONTINUE-PLAN: execute the next action in a plan\\n\\nTRACK-PLAN: talk about the execution of an action\\n\\nMODIFY-PLAN: introduce a new plan by modifying a previous one\\n\\nCORRECT-PLAN: correct a plan\\n\\nIDENTIFY-PARAMETER: identify a parameter of an action in a plan\\n\\n\\n\\n\\nUnder our approach, the recognition of discourse plans is unnecessary.\\nThe fact that a speaker is using an utterance to, for example,\\nintroduce a plan, or track a plan, or identify a parameter, need not\\nbe explicitly recognized for the purposes of utterance interpretation.\\nFurthermore, we would argue that such facts are not intended to be\\nrecognized (cf. Grice ).  Rather, they simply fall\\nout of recognizing the relationship of an utterance to the current\\ndiscourse structure, i.e., the currently active SharedPlans.  For\\nexample, INTRODUCE-PLAN corresponds to initiating a new discourse\\nsegment, CONTINUE- or TRACK-PLAN to contributing to the current\\nsegment, and IDENTIFY-PARAMETER to initiating a new segment to satisfy\\na \\n\\nhas.sat.descr knowledge precondition requirement.  Although the\\ninitiation of a new SharedPlan corresponds to the initiation of a new\\ndiscourse segment under our approach, it is the SharedPlan that must\\nbe recognized and not a discourse plan that refers to that SharedPlan.\\n\\n\\nLambert and Carberry  have extended\\nLitman and Allen's approach by introducing a third type of plan.  Problem-solving plans, such as BUILD-PLAN and INSTANTIATE-VARS, are\\nused to model the process by which agents construct domain plans.\\nUnder our approach, the need to explicitly recognize problem-solving\\nplans is also avoided.  The fact that an agent is building a plan or\\ninstantiating a variable is a byproduct of understanding an utterance\\nby relating it to the current discourse structure.  BUILD-PLAN\\ncorresponds to initiating a new discourse segment to satisfy a\\n\\nhas.recipe knowledge precondition requirement, while\\nINSTANTIATE-VARS corresponds to initiating one to satisfy a\\n\\nhas.sat.descr requirement.  Unlike Lambert and Carberry's approach,\\nhowever, and Litman and Allen's as well, our approach actually\\nrecognizes this structure.  The other approaches are essentially\\nutterance-to-utterance based and thus do not recognize discourse\\nsegments as separate units.\\n\\n\\nRamshaw  has added a different third type of\\nplan, exploration plans, to Litman and Allen's two types.\\nExploration plans are intended to model the process by which agents\\nexplore courses of actions.  Although we have not yet incorporated\\nsuch reasoning into our model, we hypothesize that the exploration of\\nplans can be modeled, without the introduction of a new plan type, by\\nreasoning about an agent's potential intentions and the process\\nby which they become full-fledged intentions\\n ,. \\n\\n\\nThese alternative approaches share an important property that\\ndistinguishes them from our approach; they take a data-structure\\nview of plans, rather than a mental phenomenon view\\n .  Whereas data-structure plans are essentially ``recipes-for-action,'' mental phenomenon plans are a ``structured\\ncollection of beliefs and intentions''\\n , pg. 77].  Data-structure plans thus describe what an agent is doing with an utterance, but not why the agent is doing it.  For\\nexample, although the constraints of Litman and Allen's\\nIDENTIFY-PARAMETER discourse plan force the plan to be related to\\nanother plan that involves the parameter to be identified,\\nIDENTIFY-PARAMETER does not explain why this information is desired;\\nit does not capture that agents need to know parameters to be\\n able to perform acts involving them.  It thus fails to model the essential knowledge precondition nature of identifying a parameter.  Although it is\\npossible to impose a mental phenomenon interpretation on top of a\\ndata-structure plan, doing so does not result in a mental phenomenon\\n plan .  Saying that G1 intends to IDENTIFY a PARAMETER fails to address why G1 intends to do so.\\n\\n\\nThe need to explain an utterance is not unique to interpretation.\\nMoore and Paris  have shown that a similar\\nneed exists in generation.  In particular, they have argued that\\nRST-based text plans must be augmented with intentional structure.\\nOtherwise, a system has no record of why it said what it did and is\\nthus unable to respond effectively if a hearer does not understand or\\naccept its utterances.\\n\\n\\n\\n  Conclusion \\n\\nIn this paper, we have presented an axiomatization of knowledge\\npreconditions for the SharedPlan model of collaborative activity\\n .  We have also shown how the requirements of knowledge preconditions can be used to account for information-seeking\\nsubdialogues in discourse.  Our account of this phenomenon fits within\\na general framework for discourse processing in which SharedPlans and\\nrelationships among them are used to model the intentional component\\nof Grosz and Sidner's  theory of discourse\\nstructure.  Unlike the alternative approaches, our approach recognizes\\nand makes use of discourse structure.  In addition, it does not require\\nthe introduction of new plan types.\\n\\nBibliography \\n\\nD. Appelt and A. Kronfeld.\\nA computational model of referring.\\nIn Proceedings of IJCAI-87, pages 640-647, Milan, Italy, 1987.\\n\\n\\nD. E. Appelt.\\nSome pragmatic issues in the planning of definite and indefinite noun\\n  phrases.\\nIn Proceedings of the 23rd Annual Meeting of the ACL, pages\\n  198-203, Chicago, IL, 1985.\\n\\n\\nM. E. Bratman, D. J. Israel, and M. E. Pollack.\\nPlans and resource-bounded practical reasoning.\\nComputational Intelligence, 14:349-355, 1988.\\n\\n\\nH. P. Grice.\\nUtterer's meaning and intentions.\\nPhilosophical Review, 68(2):147-177, 1969.\\n\\n\\nB. J. Grosz and S. Kraus.\\nCollaborative plans for group activities.\\nIn Proceedings of IJCAI-93, pages 367-373, Chambery, Savoie,\\n  France, 1993.\\n\\n\\nB. J. Grosz and C. L. Sidner.\\nAttention, intentions, and the structure of discourse.\\nComputational Linguistics, 12(3):175-204, 1986.\\n\\n\\nB. J. Grosz and C. L. Sidner.\\nPlans for discourse.\\nIn P. R. Cohen, J. L. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 417-444. MIT Press, Cambridge, MA, 1990.\\n\\n\\nB. J. Grosz [Deutsch].\\nThe structure of task-oriented dialogs.\\nIn IEEE Symposium on Speech Recognition: Contributed Papers,\\n  pages 250-253, Pittsburgh, PA, 1974.\\n\\n\\nJ. R. Hobbs.\\nOntological promiscuity.\\nIn Proceedings of the 23rd Annual Meeting of the ACL, pages\\n  61-69, Chicago, IL, 1985.\\n\\n\\nA. Kronfeld.\\nDonnellan's distinction and a computational model of reference.\\nIn Proceedings of the 24th Annual Meeting of the ACL, pages\\n  186-191, New York, NY, 1986.\\n\\n\\nL. Lambert and S. Carberry.\\nA tripartite plan-based model of dialogue.\\nIn Proceedings of the 29th Annual Meeting of the ACL, pages\\n  47-54, Berkeley, CA, 1991.\\n\\n\\nD. J. Litman and J. F. Allen.\\nA plan recognition model for subdialogues in conversations.\\nCognitive Science, 11:163-200, 1987.\\n\\n\\nK. E. Lochbaum, B. J. Grosz, and C. L. Sidner.\\nModels of plans to support communication: An initial report.\\nIn Proceedings of AAAI-90, pages 485-490, Boston, MA, 1990.\\n\\n\\nK. E. Lochbaum.\\nUsing Collaborative Plans to Model the Intentional Structure of\\n  Discourse.\\nPhD thesis, Harvard University, 1994.\\n\\n\\nJ. D. Moore and C. L. Paris.\\nPlanning text for advisory dialogues: Capturing intentional and\\n  rhetorical information.\\nComputational Linguistics, 19(4):651-694, December 1993.\\n\\n\\nR. C. Moore.\\nA formal theory of knowledge and action.\\nIn J. R. Hobbs and R. C. Moore, editors, Formal Theories of the\\n  Commonsense World, pages 319-358. Ablex Publishing Corp., Norwood, NJ,\\n  1985.\\n\\n\\nL. Morgenstern.\\nKnowledge preconditions for actions and plans.\\nIn Proceedings of IJCAI-87, pages 867-874, Milan, Italy, 1987.\\n\\n\\nL. Morgenstern.\\nFoundations of a Logic of Knowledge, Action, and Communication.\\nPhD thesis, New York University, 1988.\\n\\n\\nM. E. Pollack.\\nPlans as complex mental attitudes.\\nIn P. R. Cohen, J. L. Morgan, and M. E. Pollack, editors,   Intentions in Communication, pages 78-104. MIT Press, Cambridge, MA, 1990.\\n\\n\\nL. A. Ramshaw.\\nA three-level model for plan exploration.\\nIn Proceedings of the 29th Annual Meeting of the ACL, pages\\n  39-46, Berkeley, CA, 1991.\\n\\n\\nC. L. Sidner and D. J. Israel.\\nRecognizing intended meaning and speakers' plans.\\nIn Proceedings of IJCAI-81, pages 203-208, Vancouver, British\\n  Columbia, Canada, 1981.\\n\\nFootnotes\\n\\n  This work was done as part of my dissertation\\nresearch at Harvard University, and was supported by a Bellcore\\ngraduate fellowship and by U S WEST Advanced Technologies.  I would\\nlike to thank Barbara Grosz, Stuart Shieber, and Candy Sidner for\\ntheir helpful comments, discussions, and insights on this work.\\n  Unless otherwise indicated, we will use the term\\n``agent'' to refer to both individual agents and sets of agents.\\n  This description of a PSP is\\nonly a rough, though useful, approximation to the formal definition\\ngiven by Grosz and Kraus .\\n  A comparison of our\\nformalization with those of Morgenstern  and\\n Moore  can be found elsewhere . \\n  Basic-level actions are by their\\nnature single-agent actions.\\n  A more precise account of what it means to be able\\nto identify an object is beyond the scope of this paper; for further\\ndetails, see the discussions by Hobbs , Appelt and\\nKronfeld , and\\nMorgenstern .\\n  The term discourse segment is a generalization of\\nthe term subdialogue.  Whereas the term discourse segment applies to\\nall types of discourse, the term subdialogue is reserved for segments\\nthat occur within dialogues.\\n  We have omitted the time parameters for simplicity of\\nexposition.\\n  For\\nsimplicity of exposition, we will take participant ``E'' to be female\\nand participant ``A'' to be male.\\n  We describe\\n a method for recognizing DSPs of this form elsewhere . \\n  Although Lambert and\\nCarberry  adopt\\nPollack's  terminology in presenting their\\ntheory, their ``plans'' are not mental state plans in Pollack's\\nsense.\\n  Although Litman and\\nAllen describe IDENTIFY-PARAMETER as providing ``a suitable\\ndescription for a term instantiating a parameter of an action such\\nthat the hearer is then able to execute the action''\\n , pg. 173], the IDENTIFY-PARAMETER operator itself does not include a formalization of the last condition,\\ni.e., that the parameter description should enable the execution of\\nthe action.\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nIf an agent does not possess the knowledge needed to perform an\\naction, it may privately plan to obtain the required information on\\nits own, or it may involve another agent in the planning process by\\nengaging it in a dialogue.  In this paper, we show how the\\nrequirements of knowledge preconditions can be used to account for\\ninformation-seeking subdialogues in discourse.  We first present an\\naxiomatization of knowledge preconditions for the SharedPlan model of\\n collaborative activity , and then provide an analysis of information-seeking subdialogues within a general\\nframework for discourse processing.  In this framework, SharedPlans\\nand relationships among them are used to model the intentional\\ncomponent of Grosz and Sidner's  theory of\\ndiscourse structure.\\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nWithin the cooperative parallel grammar project  PARGRAM\\n(IMS-Stuttgart, Xerox-Palo Alto, Xerox-Grenoble), the analysis and\\nrepresentation of structures in the grammars must be viewed from a\\nmore global perspective than that of the individual languages (German,\\nEnglish, French).  One major goal of  PARGRAM is the development\\nof broad coverage grammars which are also modular and easy to\\nmaintain.  Another major goal is the construction of parallel\\nanalyses for sentences of the same type in German, English, and\\nFrench.  If this can be achieved, the problem faced by machine\\ntranslation (MT) could be greatly reduced.  Due to the recent\\ndevelopment of a faster and more powerful version of the LFG\\n(Lexical-Functional-Grammar) based Grammar Writer's Workbench\\n(Kaplan and Maxwell 1993) at Xerox, the implementation of a\\nlinguistically adequate, broad coverage grammar appears viable.  Given\\nthe flexible projection-based architecture of LFG (Dalrymple et\\nal. 1995) and the MT approach presented in Kaplan et\\n al. (1989), a robust MT system is already in place. \\n\\n\\nIn this paper, we concentrate on two issues within the broader\\nperspective of  PARGRAM: the treatment of auxiliaries and the\\ntransparent representation of multiple genitive NPs in German.  These\\nphenomena represent two areas for which generally accepted proposals\\nexist, but whose implementation in the context of parallel grammar\\ndevelopment throws up questions as to their wider, crosslinguistic,\\nfeasibility.  With respect to auxiliaries, the standard raising\\napproach that is usually adopted yields undesirable structural\\ncomplexity and results in idiosyncratic, language particular analyses\\nof the role of auxiliaries.  With regard to genitive NPs, the standard\\nanalysis for German yields structures which are too ambiguous for a\\nsuccesful application of machine translation.  The following sections\\npresent a solution in that morphological wellformedness\\nconditions are stated at a separate component, the morphology\\n  projection. Furthermore, a representation of argument structure is\\nimplemented that is related to, but not identical to the\\nrepresentation of grammatical functions.  Language particular\\nidiosyncratic requirements are thus separated out from the language\\nuniversal information required for further semantic interpretation, or\\nmachine translation.\\n\\n\\n  The Formalism \\n\\nThe architecture of LFG assumed here is the ``traditional''\\narchitecture described in Bresnan (1982), as well as the newer\\nadvances within LFG (Dalrymple et al. , 1995). A grammar is viewed as\\na set of correspondences expressed in terms of projections\\nfrom one level of representation to another.  Two fundamental levels\\nof representations within LFG are the c(onstitutent)-structure and the\\nf(unctional)-structure.  The c-structure encodes idiosyncratic phrase\\nstructural properties of a given language, while the f-structure\\nprovides a language universal representation of grammatical functions\\n(e.g.,  SUBJect,  OBJect), complementation, tense, binding,\\netc.  The correspondence between c-structure and f-structure is\\nnot onto or one-to-one, but many-to-one, allowing an abstraction over\\nidiosyncratic c-structure properties of a language (e.g.,\\ndiscontinuous constituents).\\n\\n\\nIn addition, several proposals exploring possible representations of a\\ns(emantic)-structure have been made over the years (e.g. Halvorsen and\\nKaplan (1988), Dalrymple et al.  (1993)).  As the \\nrealization of a separate semantic component is only planned for the\\nlatter stages within  PARGRAM, no further discussion of possible\\nformalisms will take place here.  It should be noted, however, that\\nrudimentary semantic information, such as argument structure\\ninformation (lexical semantics), is encoded within the f-structures in\\norder to facilitate transfer in some cases.  A case in point is\\npresented in the section on German genitive NPs.  \\n\\n\\n  Auxiliaries -- a flat approach \\n  The Received Wisdom \\n\\nAuxiliaries have given rise to lively debates concerning their exact\\nsyntactic status (e.g. Chomsky (1957), Ross (1967), Pullum and Wilson\\n(1977), Akmajian et al. (1979), Gazdar et al. (1982)): are they simply\\nmain verbs with special properties, or should they instantiate a\\nspecial category  AUX?  Within current lexical approaches\\n(Lexical-Functional-Grammar (LFG), Head-driven Phrase Structure\\nGrammar (HPSG)), auxiliaries (e.g. have, be) and modals\\n(e.g. must, should) are treated as raising verbs,\\nwhich are marked as special in some way: in HPSG through an [\\nAUX: +] feature (Pollard and Sag 1994), in LFG (Bresnan 1982) by a\\n difference in  PRED value.  However, newer work within LFG (Bresnan 1995, T.H. King 1995) has been moving away\\nfrom the raising approach towards an analysis where auxiliaries are\\nelements which contribute to the clause only tense/aspect, agreement,\\nor voice information, but not a subcategorization frame.  This view is\\nalso in line with approaches within GB (Government-Binding), which see\\nauxiliaries simply as possible instantiations of the functional\\ncategory I (see also Halle and Marantz (1993)). \\n\\n\\nThe ``traditional'' treatment of auxiliaries in both HPSG (Pollard and Sag\\n1994) and LFG has its roots in Ross's (1967) proposal to treat\\n auxiliaries and modals on a par with main verbs. In particular, auxiliaries are treated as a subclass of raising verbs (e.g. Pollard\\nand Sag (1994), Falk (1984)).  For example, a simple sentence like (1)\\nwould correspond to the c-structure and f-structure shown in (2)\\nand (3), respectively. Note that the level of embedding in the\\nf-structure exactly mirrors the c-structure: each verbal element takes\\na complement.  \\n\\n\\n\\nThe main reasons to treat auxiliaries as complement taking verbs in\\nEnglish are: 1) an account of VP-ellipsis, VP-topicalization, etc. \\nfollows immediately; 2) restrictions on the nature of the verbal\\ncomplement (progressive, past participle, etc.) following the\\nauxiliary can be stated straightforwardly (Pullum and Wilson (1977),\\nAkmajian et al. (1979), Gazdar et al. (1982)).  The latter point holds\\nfor German as well, and in fact, without some sort of a hierarchical\\nstructure, stating wellformedness conditions on a string of multiple\\nauxiliaries becomes wellnigh impossible in light of the greater\\nordering possibilities granted by the flexible German word order.\\nThere are also major reasons, however, for not adopting this analysis:\\n1) linguistic adequacy; 2) unmotivated structural complexity; 3)\\nnon-parallel analyses for predicationally equivalent sentences.\\nConsider the French equivalent of (-2) in (1).\\n\\n\\n\\nAs argued by Akmajian et al. (1979), crosslinguistic evidence\\nindicates that elements bearing only tense, mood, or voice should\\nbelong to a distinct syntactic category.  In many languages, like\\nFrench or Japanese, the information carried by will (future), or\\nhave (perfect) is realized morphologically rather than\\nperiphrastically.  The analysis in (0) thus effectively claims\\nthat there exists a deep difference in the predicational structure of\\n auxiliaries like will and have and the French   aura. This is not desirable from a crosslinguistic point of view, nor is it helpful for MT.  \\n\\n\\n  Alternative Implementation \\n\\nThe approach adopted here is a flat analysis of auxiliaries at\\nf-structure ((1)).\\n\\n\\n\\nThe auxiliaries wird `will' and haben `have' now only\\ncontribute information as to the overall tense, but do not\\nsubcategorize for complements.  Structural phenomena like VP-ellipsis,\\ncoordination, or topicalization can, however, still be accounted for\\nin terms of an appropriate embedding at c-structure (cf. (-3)).\\nThe role of auxiliaries in natural language is now adequately modeled,\\nin particular with respect to a more realistic treatment of tense\\n(compare (-2) and (0)), as the French (-1) has\\n essentially the same f-structure as (0). \\n\\n\\nHowever, the flat f-structure in (0) provides no room for a\\nstatement of selectional requirements, allowing massive overgeneration\\n(e.g. nothing blocks the presence of two haben in (-4)).\\nNeither can the particular order of auxiliaries be regulated.  Our\\nsolution takes advantage of LFG's flexible projection-based\\narchitecture by implementing a projection which models the\\nhierarchical selectional requirements of auxiliaries, yet does not\\ninterfere with the subcategorizational properties of verbs, as would\\nbe the case under a raising analysis.\\n\\n\\n\\nIn LFG, the flexible word order of German is handled via   functional uncertainty, which characterizes long-distance\\ndependencies without resorting to movement analyses (Netter (1988),\\nZaenen and Kaplan (1995)).  As in (0), which illustrates our\\nalternative solution, functional uncertainty is represented by the\\n Kleene Star (*). The annotation on the NPs indicates that they could fulfill the role of any possible grammatical\\nfunction (GF), e.g.  SUBJ or  OBJ, and that the level of\\nembedding ranges from zero to infinite.  With every auxiliary\\nsubcategorizing for an  XCOMP, the two NPs could conceivably be\\narguments of three different verbs: wird, haben, or   gedreht.  Thus, the greater structural complexity unnecessarily\\nincreases the search space for the determination of a verb's\\narguments. In (0), however, the m-structure is projected from the\\nc-structure parallel to the f-structure through annotations similar to\\n the usual f-structure annotations. Statements about ``morphological'' dependents ( DEP) are thus decoupled from\\nfunctional uncertainty: the relation of NP arguments to their\\npredicator now does not extend through various layers of artificial\\nstructural complexity ( XCOMPs).  For VP-topicalization or\\nextraposition an unbounded long-distance dependency must still be\\nassumed.  However, as the functional uncertainty path for auxiliaries\\nis distributed only over the m-structure of the verb complex ((\\n\\n\\n\\n\\n ), and does not involve the resolution of\\nthe role of NP arguments, there are in fact differing paths of\\nfunctional uncertainty involved.  The dependencies between predicators\\nand their arguments and auxiliaries and their dependents are thus\\nneatly factored out. The m-structure corresponding to the matrix VP in\\n(0) is (1).  The desired flat f-structure resulting from the\\nusual \\n\\n\\n\\nand \\n\\n\\n\\nannotations is as in (-1).\\n\\n\\n\\nLike the f-structure, the m-structure is an attribute-value matrix.\\nIt encodes language-specific information about idiosyncratic\\nconstraints on morphological forms.  The m-structure is not derived\\nfrom the f-structure.  Rather, both representations are in\\nsimultaneous correspondence with the c-structure.  The following\\n(abbreviated) lexical entry exemplifies the pieces of information\\nneeded.  The disjunctive lexical entry for wird `will' in\\n(1) takes the various combinatory possibilities of auxiliaries\\nand main verbs into account, and provides the appropriate tense\\nfeature.  For example, it requires that the embedded  VFORM be  BASE,\\nand that there be no passive involved for a simple future like   wird drehen.  \\n\\n\\n\\nFeatures needed only to ensure language particular wellformedness are\\nno longer unified into the f-structure, cluttering a representation\\nthat is meant to be language independent.  In our analysis, only\\nfeatures needed for further semantic interpretation, MT, or for the\\nexpression of language universal syntactic generalizations are\\nrepresented at f-structure.  For example, morphologically encoded\\ninformation like case, gender, or agreement is needed for statements\\nas to binding, predicate-argument relations, or the determination of\\ncomplex clause structures (given that agreement is generally\\nclause-bounded), and is therefore represented at f-structure.\\nWellformedness conditions on adjective inflection or relative pronoun\\nagreement, however, can now be stated on the m-structure as\\nidiosyncratic, language particular information which can be ignored\\nfor purposes of MT or semantic interpretation.\\n\\n\\n\\n\\nSyntactic Analyses for Parallel Grammars:\\n  Auxiliaries and Genitive NPs\\n\\nMiriam Butt  - Christian Fortmann  - Christian Rohrer \\nInstitut fr Maschinelle Sprachverarbeitung  Universitt\\nStuttgart  \\nAzenbergstr. 12  70174 Stuttgart, Germany \\n  {mutt|fortmann|rohrer}@ims.uni-stuttgart.de \\n\\n\\nAbstract:\\nThis paper focuses on two disparate aspects of German syntax from\\n  the perspective of parallel grammar development.  As part of a\\n  cooperative project, we present an innovative approach to\\n  auxiliaries and multiple genitive NPs in German.  The LFG-based\\n  implementation presented here avoids unnessary structural complexity\\n  in the representation of auxiliaries by challenging the traditional\\n  analysis of auxiliaries as raising verbs.  The approach developed\\n  for multiple genitive NPs provides a more abstract, language\\n  independent representation of genitives associated with nominalized\\n  verbs.  Taken together, the two approaches represent a step towards\\n  providing uniformly applicable treatments for differing languages,\\n  thus lightening the burden for machine translation.\\n\\n\\n  Introduction \\n\\nWithin the cooperative parallel grammar project  PARGRAM\\n(IMS-Stuttgart, Xerox-Palo Alto, Xerox-Grenoble), the analysis and\\nrepresentation of structures in the grammars must be viewed from a\\nmore global perspective than that of the individual languages (German,\\nEnglish, French).  One major goal of  PARGRAM is the development\\nof broad coverage grammars which are also modular and easy to\\nmaintain.  Another major goal is the construction of parallel\\nanalyses for sentences of the same type in German, English, and\\nFrench.  If this can be achieved, the problem faced by machine\\ntranslation (MT) could be greatly reduced.  Due to the recent\\ndevelopment of a faster and more powerful version of the LFG\\n(Lexical-Functional-Grammar) based Grammar Writer's Workbench\\n(Kaplan and Maxwell 1993) at Xerox, the implementation of a\\nlinguistically adequate, broad coverage grammar appears viable.  Given\\nthe flexible projection-based architecture of LFG (Dalrymple et\\nal. 1995) and the MT approach presented in Kaplan et\\n al. (1989), a robust MT system is already in place. \\n\\n\\nIn this paper, we concentrate on two issues within the broader\\nperspective of  PARGRAM: the treatment of auxiliaries and the\\ntransparent representation of multiple genitive NPs in German.  These\\nphenomena represent two areas for which generally accepted proposals\\nexist, but whose implementation in the context of parallel grammar\\ndevelopment throws up questions as to their wider, crosslinguistic,\\nfeasibility.  With respect to auxiliaries, the standard raising\\napproach that is usually adopted yields undesirable structural\\ncomplexity and results in idiosyncratic, language particular analyses\\nof the role of auxiliaries.  With regard to genitive NPs, the standard\\nanalysis for German yields structures which are too ambiguous for a\\nsuccesful application of machine translation.  The following sections\\npresent a solution in that morphological wellformedness\\nconditions are stated at a separate component, the morphology\\n  projection. Furthermore, a representation of argument structure is\\nimplemented that is related to, but not identical to the\\nrepresentation of grammatical functions.  Language particular\\nidiosyncratic requirements are thus separated out from the language\\nuniversal information required for further semantic interpretation, or\\nmachine translation.\\n\\n\\n  The Formalism \\n\\nThe architecture of LFG assumed here is the ``traditional''\\narchitecture described in Bresnan (1982), as well as the newer\\nadvances within LFG (Dalrymple et al. , 1995). A grammar is viewed as\\na set of correspondences expressed in terms of projections\\nfrom one level of representation to another.  Two fundamental levels\\nof representations within LFG are the c(onstitutent)-structure and the\\nf(unctional)-structure.  The c-structure encodes idiosyncratic phrase\\nstructural properties of a given language, while the f-structure\\nprovides a language universal representation of grammatical functions\\n(e.g.,  SUBJect,  OBJect), complementation, tense, binding,\\netc.  The correspondence between c-structure and f-structure is\\nnot onto or one-to-one, but many-to-one, allowing an abstraction over\\nidiosyncratic c-structure properties of a language (e.g.,\\ndiscontinuous constituents).\\n\\n\\nIn addition, several proposals exploring possible representations of a\\ns(emantic)-structure have been made over the years (e.g. Halvorsen and\\nKaplan (1988), Dalrymple et al.  (1993)).  As the \\nrealization of a separate semantic component is only planned for the\\nlatter stages within  PARGRAM, no further discussion of possible\\nformalisms will take place here.  It should be noted, however, that\\nrudimentary semantic information, such as argument structure\\ninformation (lexical semantics), is encoded within the f-structures in\\norder to facilitate transfer in some cases.  A case in point is\\npresented in the section on German genitive NPs.  \\n\\n\\n  Auxiliaries -- a flat approach \\n  The Received Wisdom \\n\\nAuxiliaries have given rise to lively debates concerning their exact\\nsyntactic status (e.g. Chomsky (1957), Ross (1967), Pullum and Wilson\\n(1977), Akmajian et al. (1979), Gazdar et al. (1982)): are they simply\\nmain verbs with special properties, or should they instantiate a\\nspecial category  AUX?  Within current lexical approaches\\n(Lexical-Functional-Grammar (LFG), Head-driven Phrase Structure\\nGrammar (HPSG)), auxiliaries (e.g. have, be) and modals\\n(e.g. must, should) are treated as raising verbs,\\nwhich are marked as special in some way: in HPSG through an [\\nAUX: +] feature (Pollard and Sag 1994), in LFG (Bresnan 1982) by a\\n difference in  PRED value.  However, newer work within LFG (Bresnan 1995, T.H. King 1995) has been moving away\\nfrom the raising approach towards an analysis where auxiliaries are\\nelements which contribute to the clause only tense/aspect, agreement,\\nor voice information, but not a subcategorization frame.  This view is\\nalso in line with approaches within GB (Government-Binding), which see\\nauxiliaries simply as possible instantiations of the functional\\ncategory I (see also Halle and Marantz (1993)). \\n\\n\\nThe ``traditional'' treatment of auxiliaries in both HPSG (Pollard and Sag\\n1994) and LFG has its roots in Ross's (1967) proposal to treat\\n auxiliaries and modals on a par with main verbs. In particular, auxiliaries are treated as a subclass of raising verbs (e.g. Pollard\\nand Sag (1994), Falk (1984)).  For example, a simple sentence like (1)\\nwould correspond to the c-structure and f-structure shown in (2)\\nand (3), respectively. Note that the level of embedding in the\\nf-structure exactly mirrors the c-structure: each verbal element takes\\na complement.  \\n\\n\\n\\nThe main reasons to treat auxiliaries as complement taking verbs in\\nEnglish are: 1) an account of VP-ellipsis, VP-topicalization, etc. \\nfollows immediately; 2) restrictions on the nature of the verbal\\ncomplement (progressive, past participle, etc.) following the\\nauxiliary can be stated straightforwardly (Pullum and Wilson (1977),\\nAkmajian et al. (1979), Gazdar et al. (1982)).  The latter point holds\\nfor German as well, and in fact, without some sort of a hierarchical\\nstructure, stating wellformedness conditions on a string of multiple\\nauxiliaries becomes wellnigh impossible in light of the greater\\nordering possibilities granted by the flexible German word order.\\nThere are also major reasons, however, for not adopting this analysis:\\n1) linguistic adequacy; 2) unmotivated structural complexity; 3)\\nnon-parallel analyses for predicationally equivalent sentences.\\nConsider the French equivalent of (-2) in (1).\\n\\n\\n\\nAs argued by Akmajian et al. (1979), crosslinguistic evidence\\nindicates that elements bearing only tense, mood, or voice should\\nbelong to a distinct syntactic category.  In many languages, like\\nFrench or Japanese, the information carried by will (future), or\\nhave (perfect) is realized morphologically rather than\\nperiphrastically.  The analysis in (0) thus effectively claims\\nthat there exists a deep difference in the predicational structure of\\n auxiliaries like will and have and the French   aura. This is not desirable from a crosslinguistic point of view, nor is it helpful for MT.  \\n\\n\\n  Alternative Implementation \\n\\nThe approach adopted here is a flat analysis of auxiliaries at\\nf-structure ((1)).\\n\\n\\n\\nThe auxiliaries wird `will' and haben `have' now only\\ncontribute information as to the overall tense, but do not\\nsubcategorize for complements.  Structural phenomena like VP-ellipsis,\\ncoordination, or topicalization can, however, still be accounted for\\nin terms of an appropriate embedding at c-structure (cf. (-3)).\\nThe role of auxiliaries in natural language is now adequately modeled,\\nin particular with respect to a more realistic treatment of tense\\n(compare (-2) and (0)), as the French (-1) has\\n essentially the same f-structure as (0). \\n\\n\\nHowever, the flat f-structure in (0) provides no room for a\\nstatement of selectional requirements, allowing massive overgeneration\\n(e.g. nothing blocks the presence of two haben in (-4)).\\nNeither can the particular order of auxiliaries be regulated.  Our\\nsolution takes advantage of LFG's flexible projection-based\\narchitecture by implementing a projection which models the\\nhierarchical selectional requirements of auxiliaries, yet does not\\ninterfere with the subcategorizational properties of verbs, as would\\nbe the case under a raising analysis.\\n\\n\\n\\nIn LFG, the flexible word order of German is handled via   functional uncertainty, which characterizes long-distance\\ndependencies without resorting to movement analyses (Netter (1988),\\nZaenen and Kaplan (1995)).  As in (0), which illustrates our\\nalternative solution, functional uncertainty is represented by the\\n Kleene Star (*). The annotation on the NPs indicates that they could fulfill the role of any possible grammatical\\nfunction (GF), e.g.  SUBJ or  OBJ, and that the level of\\nembedding ranges from zero to infinite.  With every auxiliary\\nsubcategorizing for an  XCOMP, the two NPs could conceivably be\\narguments of three different verbs: wird, haben, or   gedreht.  Thus, the greater structural complexity unnecessarily\\nincreases the search space for the determination of a verb's\\narguments. In (0), however, the m-structure is projected from the\\nc-structure parallel to the f-structure through annotations similar to\\n the usual f-structure annotations. Statements about ``morphological'' dependents ( DEP) are thus decoupled from\\nfunctional uncertainty: the relation of NP arguments to their\\npredicator now does not extend through various layers of artificial\\nstructural complexity ( XCOMPs).  For VP-topicalization or\\nextraposition an unbounded long-distance dependency must still be\\nassumed.  However, as the functional uncertainty path for auxiliaries\\nis distributed only over the m-structure of the verb complex ((\\n\\n\\n\\n\\n ), and does not involve the resolution of\\nthe role of NP arguments, there are in fact differing paths of\\nfunctional uncertainty involved.  The dependencies between predicators\\nand their arguments and auxiliaries and their dependents are thus\\nneatly factored out. The m-structure corresponding to the matrix VP in\\n(0) is (1).  The desired flat f-structure resulting from the\\nusual \\n\\n\\n\\nand \\n\\n\\n\\nannotations is as in (-1).\\n\\n\\n\\nLike the f-structure, the m-structure is an attribute-value matrix.\\nIt encodes language-specific information about idiosyncratic\\nconstraints on morphological forms.  The m-structure is not derived\\nfrom the f-structure.  Rather, both representations are in\\nsimultaneous correspondence with the c-structure.  The following\\n(abbreviated) lexical entry exemplifies the pieces of information\\nneeded.  The disjunctive lexical entry for wird `will' in\\n(1) takes the various combinatory possibilities of auxiliaries\\nand main verbs into account, and provides the appropriate tense\\nfeature.  For example, it requires that the embedded  VFORM be  BASE,\\nand that there be no passive involved for a simple future like   wird drehen.  \\n\\n\\n\\nFeatures needed only to ensure language particular wellformedness are\\nno longer unified into the f-structure, cluttering a representation\\nthat is meant to be language independent.  In our analysis, only\\nfeatures needed for further semantic interpretation, MT, or for the\\nexpression of language universal syntactic generalizations are\\nrepresented at f-structure.  For example, morphologically encoded\\ninformation like case, gender, or agreement is needed for statements\\nas to binding, predicate-argument relations, or the determination of\\ncomplex clause structures (given that agreement is generally\\nclause-bounded), and is therefore represented at f-structure.\\nWellformedness conditions on adjective inflection or relative pronoun\\nagreement, however, can now be stated on the m-structure as\\nidiosyncratic, language particular information which can be ignored\\nfor purposes of MT or semantic interpretation.\\n\\n\\n\\n  Multiple Genitive NPs \\n\\nThe differing surface realization of genitives within NPs in English\\n(preverbal NPs, postverbal PPs), French (postverbal PPs), and German\\n(preverbal NPs, postverbal PPs or NPs), poses a particular challenge\\nfor a parallel grammar development project like  PARGRAM.  In\\nthis paper, we suggest a treatment of multiple genitive NPs which not\\nonly accounts for some restrictions on their distribution within\\nGerman, but also allows a language independent (universal)\\nrepresentation, thus facilitating MT.\\n\\n\\nIn general, the distribution of multiple NPs within NPs is an area of\\nGerman syntax which has not received a satisfactory account to date\\n(e.g., Pollard and Sag (1994), Bhatt (1990), Haider (1988)). In\\nGerman, nouns generally have at most one genitive which may occur in\\na prenominal or postnominal position adjacent to the noun. Both kinds of\\ngenitives have the same morphological shape. However, nominalizations\\nthat are derived from a transitive verb allow for two genitives, one\\nin the prenominal, the other in the postnominal position.\\n\\n\\nThe function of a genitive is generally expressed as indicating a\\npossessor:  POSS within LFG. However, in the case of two\\ngenitives, the assignment of two  POSS values violates the\\nuniqueness-condition on f-structures and is furthermore insufficient\\nto distinguish the two differing kinds of genitives.  We therefore\\npropose the utilization of two functions named  GEN1 and \\n  GEN2 in order to avoid association with any specific semantic role.\\nFurthermore, as genitives in the NP are generally optional, they are\\ntaken to express no governed functions, i.e., they are not\\nsubcategorized for by the noun.  So  GEN1 and  GEN2 are \\nsemantic functions in LFG on a par with, say, adjuncts. The\\n NP rule for German then is (1). \\n\\n\\n\\nIf the head-noun is not derived from, say, a verb, the single genitive\\nin either position is interpreted as a possessor. In case of a derived\\nnominal, however, a genitive is interpreted according to the thematic roles\\nassigned to the arguments of the verbal base. That means the functions\\n GEN1 and sc gen2 have to be linked to the appropriate roles.\\nNeither of the two functions is in principle restricted to any\\nspecific role. But if both genitives are present they must be \\ninterpreted according to a thematic role hierarchy.\\n\\n\\nAs (1) shows, if only one genitive is present, its prenominal\\ninterpretation may be as agent or as patient. A postnominal (single)\\ngenitive is interpreted as agent if the head noun is derived from an\\nintransitive, and as a patient/theme if derived from a transitive.\\n\\n\\n\\nHowever, if two genitives occur, as in (1), the prenominal\\ngenitive is restricted to an agent, and the postnominal one to patient. \\nThis restriction must be encoded at some level, but does not follow\\nfrom the distiction between  GEN1 and  GEN2, which are functions\\nthat do not bear any semantic content on their own.\\n\\n\\n\\nTo obtain the correct linking, the argument structure of the verbal\\nbase must be available.  Since MT is based on f-structures within \\n  PARGRAM, the argument structure has to be present at this level of\\n representation. Nominalization is therefore implemented as a morphologically driven process (lexical rule) which\\neliminates  SUBJ and  OBJ from the verb's subcategorization\\nframe and enters the verb's argument structure into the lexical entry\\nof the noun.  This yields the optionality of genitives while\\npreserving the underlying semantics, as shown in (1).  The\\nassociation of  GEN1 and  GEN2 then is determined according\\nto a hierarchical order of arguments (Bresnan, 1995).\\n\\n\\nThis approach also provides a means of handling certain cases of\\ncategorial shift. For instance, in German temporal and conditional\\nadjuncts may be realized as PPs dominating an NP headed by a deverbal\\nnoun. English does not have this option, but employs an adjunct-clause\\ninstead. Here, the  GEN1 and  GEN2 functions of the German\\nf-structure have to be related correctly to the  SUBJ and \\n  OBJ functions of the English f-structure.\\n\\n\\n\\nHere the linking of the  GEN1 and  GEN2 functions to the\\nappropriate thematic role in the German f-structure drives the\\ntransfer of these functions to the  SUBJ and  OBJ functions\\nof the English f-structure. \\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper focuses on two disparate aspects of German syntax from\\n  the perspective of parallel grammar development.  As part of a\\n  cooperative project, we present an innovative approach to\\n  auxiliaries and multiple genitive NPs in German.  The LFG-based\\n  implementation presented here avoids unnessary structural complexity\\n  in the representation of auxiliaries by challenging the traditional\\n  analysis of auxiliaries as raising verbs.  The approach developed\\n  for multiple genitive NPs provides a more abstract, language\\n  independent representation of genitives associated with nominalized\\n  verbs.  Taken together, the two approaches represent a step towards\\n  providing uniformly applicable treatments for differing languages,\\n  thus lightening the burden for machine translation.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nTyped feature structures (TFSs) serve as a means for the specification\\nof linguistic information in current linguistic formalisms such as\\n HPSG (,) or Categorial Grammar (). Generalizing first-order terms (FOTs), TFSs are also used to specify\\n logic programs and constraint systems in LOGIN (), LIFE  (), ALE (,), TFS () and others.  General frameworks that are completely independent of any\\nlinguistic theory can be used to specify grammars for natural\\nlanguages. Indeed, most of the above mentioned languages were used for\\nspecifying HPSG grammars.\\n\\n\\nLinguistic formalisms (in particular, HPSG) use TFSs as the basic\\nblocks for representing linguistic data: lexical items, phrases and\\nrules. Usually, no mechanism for manipulating TFSs (e.g., parsing\\nalgorithm) is specified.  Current approaches for processing HPSG\\ngrammars either translate the grammar to Prolog (e.g.,\\n ,) or specify it as a general constraint system. Using general solvers for a specific application, namely parsing,\\nresults in disappointing performance. Clearly, efficient processing\\ncalls for a different method.\\n\\n\\nResearch in the semantics of programming language has undergone much\\nprogress in recent years. At the same time, linguistic theories have\\nbecome more formal and grammars for natural languages are nowadays\\nspecified with rigor, resembling computer programs. The interaction of\\ncomputer science and linguistics enables the use of techniques and\\nresults of the former to be applied to the latter.\\n\\n\\nWe present an approach for processing TFSs that guarantees both an\\nexplicit definition and high efficiency. Our main aim is to provide an\\noperational semantics for TFS-based linguistic formalisms, especially\\nHPSG. We adopt an abstract machine approach for the compilation of\\ngrammars, in a formalism that is a subset of ALE.  Such approaches\\nwere used for processing procedural and functional languages, but they\\ngained much popularity for logic programming languages since the\\nintroduction of the Warren Abstract Machine (WAM - see\\n ). Most current implementations of Prolog, as well as of other logic languages, are based on abstract machines.  The\\nincorporation of such techniques usually leads to very efficient\\ncompilers in terms of both space and time requirements.  The abstract\\nmachine is composed of data structures and a set of instructions,\\naugmented by a compiler from the TFS formalism to the abstract\\ninstructions. The effect of each instruction is defined using a\\nlow-level language that can be executed on ordinary\\nhardware. Recently, a similar approach was applied to LIFE\\n (), which is a general purpose logic programming language; however, due to differences in the motivation and in the formalisms,\\nour machine is much different. Moreover, the LIFE machine is limited\\nto term unification, whereas our machine includes a control module\\nthat enables manipulation of whole grammars.\\n\\n\\nThe abstract machine ensures that a grammars specified using our\\nsystem are endowed with well defined meaning. It enables, for example,\\nto formally verify the correctness of a compiler for HPSG, given an\\nindependent definition.  The design of such an abstract architecture\\nmust be careful enough to compromise two, usually conflicting,\\nrequirements: the closer the machine is to common architectures, the\\nharder it is to develop compilers for it; on the other hand, if such a\\nmachine is too complex, then while a compiler for it is easier to\\nproduce, it becomes more complicated to execute its language on normal\\narchitectures.\\n\\n\\nThe next section sketches the framework for which our machine is\\n designed and defines some basic notions. Section  describes the abstract machine core design along with the compilation scheme. In\\n section  control structures are added to enable parsing. A conclusion and plans for further research are given in\\n section .  Due to lack of space, the description is  rather general. Refer to  for more details. \\n\\n\\n    The Framework\\n\\n  Fundamental Notions \\n\\nWe briefly review the basic notions we use (thoroughly described in\\n ,). An HPSG grammar consists of a type specification and grammar rules\\n(including principles and lexical rules).  The basic entity of HPSG is\\nthe (typed) feature structure (TFS), which is a connected, directed,\\nlabeled, possibly cyclic, finite graph, whose nodes are decorated with\\ntypes and whose edges are labeled by features.  A TFS is\\nreentrant if it contains two different paths that lead to the\\nsame node.  The types are ordered according to an inheritance\\nhierarchy where higher types inherit features from their super-types.\\n\\n\\nMany different formalizations of TFS systems exist; we basically\\n follow the definitions of (,).  The set of types includes both \\n\\n\\n\\n ,\\nthe least type, and \\n\\n\\n\\n ,\\nthe greatest\\none. Types are ordered by subsumption (\\n\\n\\n\\n )\\naccording to\\ntheir information content, not set inclusion of their\\ndenotation. Hence, \\n\\n\\n\\nis the most general type, subsuming every\\nother, and \\n\\n\\n\\nis the contradictory type, subsumed by every other.\\n\\n\\nThe inheritance hierarchy is required to be bounded complete: every\\nset of consistent types \\n\\n\\n\\n\\nmust have a unique least\\nupper bound \\n\\n\\n\\n\\n .\\nEvery partial\\norder can be naturally extended to a bounded complete one. The\\nappropriateness function \\n\\nApprop(t,f) is required to be monotone and\\n to comply with the feature introduction condition.  However, we allow appropriateness specifications to contain loops.\\n\\n\\nThe basic operation performed on TFSs is unification\\n(\\n\\n\\n\\n ). There are various definitions for TFS unification, and we\\nbase our unification algorithm on the definition given in\\n . Two TFSs A and B are inconsistent if their unification results in failure, denoted by \\n\\n\\n\\n\\n .\\n\\n\\nThe TFSs with which we deal are required to be totally\\n well-typed, for more efficient processing. This might be problematic for the users who may prefer to\\nspecify only partial information about linguistic entities. Therefore,\\nsome description language must be provided, allowing partial\\ndescriptions from which totally well-typed feature structures can be\\nautomatically deduced.  As there are efficient algorithms to deduce\\nstructures from their descriptions, we prefer not to commit ourselves\\nto one description language. We define our system over explicit\\nrepresentations of TFS, as will be clear from\\n section . \\n\\n\\n  Type Specification \\n\\nA program (or a grammar) contains a type specification, consisting of\\na type hierarchy and an appropriateness specification. We adopt ALE's\\n format () for this specification: it is a sequence of statements of the form:\\nt sub \\n\\n\\n\\n\\nintro \\n\\n\\n\\n\\n .\\nwhere \\n\\n\\n\\n\\nare types,\\n\\n\\n\\n\\nare features and \\n\\n\\n\\n .\\nThis statement,\\nwhich is said to characterize t, means that \\n\\n\\n\\n\\n are (immediate) subtypes of t (i.e., for every \\n\\n\\n\\n\\n\\nappropriate for it. Moreover, these features are introduced by t, i.e., they are not appropriate for any type t'such that \\n\\n\\n\\n\\n .\\nFinally, the statement specifies that\\n\\nApprop(t,fi) = ri for every i.  Each type (except \\n\\n\\n\\nand\\n\\n\\n\\n )\\nmust be characterized by exactly one statement. The arity of a type t, Ar(t), is the number of features appropriate for it.\\n\\n\\nThe full subsumption relation is the reflexive transitive closure of\\nthe immediate relation determined by the characterization\\nstatements. If this relation is not a bounded complete partial order,\\nthe specification is rendered invalid. The same is true in case it is\\nnot an appropriateness specification.\\n\\n\\n We use the type hierarchy in figure  as a running example, where bot stands for \\n\\n\\n\\n .\\nThe type \\n\\n\\n\\n is systematically omitted from type specifications.\\n\\n\\n\\n\\n    Representation of Feature Structures\\n\\n\\nThe most convenient graphical representation of TFSs is\\nattribute-value matrices (AVMs). However, to represent a (totally\\nwell-typed) feature structure linearly we use an FOT-like notation,\\nbased upon At-Kaci's \\n\\n\\n\\n  -terms (,), where the type plays a similar role to that of a function symbol and the\\nfeatures are listed in a fixed order. Reentrancy is implied by\\nattaching identical tags to reentrant TFSs. A term is normal if\\nall its types are tagged, and if the same tag appears more than once,\\nthen only its first occurrence carries information.\\n See  for the details. \\n\\n\\nTotal well-typedness implies that the names of the features in a TFS can\\nbe coded by their position in the argument list of a type, and thus\\nfeature-names are omitted from the linear representation.  Assuming\\nthat the feature names are ordered alphabetically, the linear\\n representation of an example TFS is given in figure . \\n\\n\\n\\n\\n\\n    A TFS Unification Engine\\n\\n  First-Order Terms vs. Feature Structures \\n\\nWhile TFSs resemble FOTs in many aspects, it is important to note the\\ndifferences between them. First, TFSs are typed, as opposed to\\n(ordinary) FOTs.  TFSs are interpreted over more specific domains than\\nFOTs.  In addition, TFSs label the arcs by feature names, whereas FOTs\\nuse a positional encoding for argument structure. More importantly,\\nwhile FOTs are essentially trees, with possibly shared leaves, TFSs\\nare directed graphs, within which variables can occur anywhere.\\nMoreover, our system doesn't rule out cyclic structures, so that\\ninfinite terms can be represented, too.  FOTs are consistent only if\\nthey have the same functor and the same arity. TFSs, on the contrary,\\ncan be unified even if their types differ (as long as they have a\\nnon-degenerate least upper bound). Moreover, their arity can differ,\\nand the arity of the unification result can be greater than that of\\nany of the unificands.  Consequently, many diversions from the\\noriginal WAM were necessary in our design. In the following sections\\nwe try to emphasize the points where such diversions were made.\\n\\n\\n  Processing Scheme \\n\\nThe machine's engine is designed for unifying two TFSs: a program and a query.  The program is compiled once to produce\\nmachine instructions.  Each query is compiled before its execution;\\nthe resulting code is executed prior to the execution of the compiled\\nprogram.  Processing a query builds a graph representation of the\\nquery in the machine's memory. The processing of a program produces\\ncode that, during run-time, unifies the program with a query already\\nresident in memory.  The result of the unification is a new TFS,\\nrepresented as a graph in the machine's memory.  In what follows we\\ninterleave the description of the machine, the TFS language it is\\ndesigned for and the compilation of programs in this language.\\n\\n\\n  Memory Representation of Feature Structures \\n\\nFollowing the WAM, we use a global, one-dimensional array called \\nHEAP of data cells.  A global register H points to the\\n(current) top element of HEAP.  Data cells are tagged: STR cells\\ncorrespond to nodes, and store their types, while REF cells represent\\narcs, and contain the address of their targets. The number of arcs\\nleaving a node of type t is Ar(t), fixed due to total\\nwell-typedness.  Hence, we can keep the WAM's convention of storing\\nall the outgoing arcs from a node consecutively following the\\nnode. Given a type t and a feature f, the position of the arc\\ncorresponding to f (f-arc) in any TFS of type t can be\\nstatically determined; the subgraph that is the value of f can be\\naccessed in one step.  This is a major difference from the approach\\n presented in , which leads to a more time-efficient system without harming the elegance of the machine design.\\n\\n\\nIt is important to note that STR cells differ from their WAM analogs\\nin that they can be dereferenced when a type is becoming more\\nspecific. In such cases, a chain of REF cells leads to the\\ndereferenced STR cell.  Thus, if a TFS is modified, only its STR cell\\nhas to be changed in order for all pointers to it to `feel' the\\nmodification automatically. The use of self-referential REF cells is\\ndifferent, too: there are no real (Prolog-like) variables in our\\nsystem, and such cells stand for features whose values are temporarily\\nunknown.\\n\\n\\nOne cell is required for every node and arc, so for representing a\\ngraph of n nodes and m arcs, n+m cells are needed. Of course,\\nduring unification nodes can become more specific and a chain of REF\\ncells is added to the count, but the length of such a chain is bounded\\nby the depth of the type hierarchy and path compression during\\n dereferencing cuts it occasionally. As an example, figure  depicts a possible heap representation of the TFS\\nb(b(1d,1),d).\\n\\n\\n\\n\\n  Flattening Feature Structures \\n\\nBefore processing a TFS, its linear representation is transformed to a\\nset of ``equations'', each having a flat (nesting free) format.  To\\nfacilitate this a set of registers \\n\\n\\n\\nthat store addresses of TFSs in memory is used.  A register Reg[j] is\\nassociated with each tag j of a normal term.  The flattening\\nalgorithm is straight-forward and similar to the WAM's.\\n Figure  depicts examples of the equations corresponding to two TFSs.\\n\\n\\n\\n\\n  Processing of a Query \\n\\nWhen processing an equation of the form \\n\\n\\n\\n\\nrepresenting part of a query, two different instructions are\\ngenerated. The first is put_node t/n, Xi0, where n =\\nAr(t). Then, for every argument Xij, an instruction\\nof the form put_arc Xi0, j, Xij is\\ngenerated. put_node creates a representation of a node of type\\nt on top of the heap and stores its address in Xi0; it also\\nincrements H to leave space for the arcs. put_arc fills\\nthis space with REF cells.\\n\\n\\nIn order for put_arc to operate correctly, the registers it\\nuses must be initialized. Since only put_node sets the\\nregisters, all put_node instructions must be executed before\\nany put_arc instruction is. Hence, the machine maintains two\\nseparate streams of instructions, one for put_node and one for\\nput_arc, and executes the first completely before moving to the\\nother. This compilation scheme is called for by the cyclic character\\n of TFSs: as explained in , the original single-streamed WAM scheme would fail on cyclic terms.  Consequently, the order of the\\nequations becomes irrelevant, and in the actual implementation they\\nmight be processed in any order.\\n\\n\\nThe effect of the two instructions is given in\\n figure  lists the result of compiling the term b(b(1d,1),d). When this code is\\nexecuted (first the put_node instructions, then the \\nput_arc ones), the resulting representation of the TFS in memory is\\n the one shown above in figure . \\n\\n\\n\\n\\n\\n    Compilation of the Type Hierarchy\\n\\n\\nOne of the reasons for the efficiency of our compiler is that it\\nperforms an important part of the unification during compile-time: the\\ntype unification.  The WAM's equivalent of this operation is a simple\\nfunctor and arity comparison. It is due to the nature of a typed\\nsystem that this check has to be replaced by a more complex\\ncomputation.  Efficient methods were suggested for performing\\nleast-upper-bound computation during run time\\n (see ), but clearly computing during compilation time is preferable.  Since type unification adds information by\\nreturning the features of the unified type, this operation builds new\\nstructures, in our design, that reflect the added knowledge. Moreover,\\nthe WAM's special register S is here replaced by a stack. S is used by\\nthe WAM to point to the next sub-term to be matched against, but in\\nour design, as the arity of the two terms can differ, there might be a\\nneed to hold the addresses of more than one such sub-term.  These\\naddresses are stored in the stack.  When the type hierarchy is\\nprocessed, the (full) subsumption relation is computed.  Then, a table\\nis generated which stores, for every two types t1,t2, the least\\nupper bound \\n\\n\\n\\n\\n .\\nMoreover, this table lists also the\\narity of t, its features and their `origin': whether they are\\nappropriate for t1, t2, both or none of them.  Out of this table\\na series of abstract machine language functions are generated. The\\nfunctions are arranged as a two-dimensional array called \\nunify_type, indexed by two types t1,t2. Each such function\\nreceives one parameter, the address of a TFS on the heap. When\\nexecuted, it builds on the heap a skeleton for the unification result:\\nan STR cell of the type \\n\\n\\n\\n\\n ,\\nand a REF cell for each\\nappropriate feature of it.\\n\\n\\nConsider unify_type[t1,t2](addr) where addr is the\\naddress of a TFS A (of type t2) in memory.  Let \\n\\n\\n\\n\\n ,\\nand let f be some feature appropriate for t.  If f is\\ninherited from t2 only, the value of the REF cell is simply set to\\npoint to the f-arc in A.  If f is inherited from t1 only, a\\nself-referential REF cell is created. But the information that the\\nactual value for this cell is yet to be seen must be recorded.  This\\nis done by means of the global stack S, every element of which is a\\npair [action,addr], where action is either `copy' or\\n`unify'. In the case we describe, the action is `copy' and the address\\nis that of the REF cell. If f is appropriate for both t1 and\\nt2, a REF cell with the address of the f-arc in A is created,\\nand a `unify' cell is pushed onto the stack. Finally, if f is\\nintroduced by t, a VAR cell is created, with \\n\\nApprop(t,f) as its\\n value. VAR cells are explained in section .  As an  example, we list below (figure ) the resulting code for the unification the two types a and b.\\n\\n\\n\\nThis example code is rather complex; often the code is much simpler:\\nfor example, when t2 is subsumed by t1, nothing has to be\\ndone. As another example, if t1 is subsumed by t2, then\\nadditional features of the program term have to be added to A. But\\nif no such features exist, the only required effect is a change of the\\ntype of A.  Another case is when t1 and t2 are\\nnot compatible: unify_type[t1,t2] returns `fail'.\\nThis leads to a call to the function fail, which aborts\\nthe unification.\\n\\n\\n  Processing of a Program \\n\\nThe program is stored in a special memory area, the CODE area.\\nUnlike the WAM, in our framework registers that are set by the\\nexecution of a query are not helpful when processing a program. The\\nreason is that there is no one-to-one correspondence between the\\nsub-terms of the query and the program, as the arities of the TFSs can\\ndiffer.  The registers are used, but (with the exception of X1)\\ntheir old values are not retained during execution of the program.\\n\\n\\nThree kinds of machine instructions are generated when processing a\\nprogram equation of the form Xi0 =\\nt(Xi1,...,Xin). The first instruction is \\nget_structure t/n,Xi0, where n = Ar(t).  For each argument\\nXij of t an instruction of the form unify_variable\\nXij is generated if Xij is first seen; if it was\\nalready seen, unify_value Xij is generated.  For\\nexample, the machine code that results from compiling the program\\n a(3d1,3) is depicted in figure . The implementation of these three instructions is given in\\n figure . \\n\\n\\n\\n\\n\\nThe get_structure instruction is generated for a TFS Ap (of\\ntype t) which is associated with a register Xi. It matches Apagainst a TFS Aq that resides in memory using Xi as a pointer to\\nAq. Since Aq might have undergone some type inference or\\nprevious binding (for example, due to previous unifications caused by\\nother instructions), the value of Xi must first be\\ndereferenced. This is done by the function deref which follows a\\nchain of REF cells until it gets to one that does not point to\\nanother, different REF-cell. The address of this cell is the value it\\nreturns.\\n\\n\\nThe dereferenced value of Xi, addr, can either be a\\nself-referential REF cell or an STR cell. In the first case, the TFS\\nhas to be built by the program. A new TFS is being built on top of the\\nheap (using code similar to that of put_structure) with \\naddr set to point to it.  For every feature of this structure, a\\n`copy' item is pushed onto the stack.  The second case, in which Xipoints to an existing TFS of type t', is the more interesting one.\\nAn existing TFS has to be unified with a new one whose type is\\nt. Here the pre-compiled unify_type[t,t'] is invoked.\\n\\n\\nThe unify_variable instruction resembles very much its WAM\\nanalog, in the case of read mode. There is no equivalent of the\\nWAM's write mode as there are no real variables in our\\nsystem. However, in unify_value there is some similarity to the\\nWAM's modes, where the `copy' action corresponds to write mode and the\\n`unify' action to read mode. In this latter case the function \\nunify is called, just like in the WAM.  This function\\n (figure ) is based upon unify_type.  In contrast to the latter, the two TFS arguments of unify are in\\nmemory, and full unification is performed. The first difference is the\\nreason for removing an item from the stack S and using it as a part of\\nthe unification process; the second is realized by recursive calls to\\nunify for subgraphs of the unified graphs.\\n\\n\\n\\nWhen a sequence of instructions that were generated for some TFS is\\nsuccessfully executed on some query, the result of the unification of\\nboth structures is built on the heap and every register Xi stores\\nthe value of its corresponding node in this graph. The stack S is\\nempty.\\n\\n\\n    Lazy Evaluation of Feature Structures\\n\\n\\nOne of the drawbacks of maintaining total structures is that when two\\nTFSs are unified, the values of features that are introduced by the\\nunified type have to be built.  For example, unify_type[a,b]\\n (figure ) has to build a TFS of type bot, which is the value of the f4 feature of type c. This is expensive\\nin terms of both space and time; the newly built structure might not\\nbe used at all. Therefore, it makes sense to defer it.\\n\\n\\nTo optimize the design in this aspect, a new kind of heap cells,\\nVAR-cells, is introduced. A VAR cell whose contents is a type \\nt stands for the most general TFS of type t. VAR cells are\\ngenerated by the various unify_type functions for introduced\\nfeatures; they are expanded only when the explicit values of such\\nfeatures are needed: either during the execution of \\nget_structure, where the dereferenced value is a VAR cell, or during\\nunify. In both cases the TFS has to be built, by means of\\nexecuting the pre-compiled function build_most_general_fs\\nwith the contents of the VAR cell as an argument. This function (which\\nis automatically generated by the type hierarchy compiler) builds a\\nTFS of the designated type on the heap, with VAR cells instead of REF\\ncells for the features. These cells will, again, only be expanded when\\nneeded. We thus obtain a lazy evaluation of TFSs that weakly resembles\\nGtz's notion of unfilled feature structures\\n (). Moreover, we gain another important property, namely that our type hierarchies can now contain loops, since\\nappropriateness loops can only cause non termination when introduced\\nfeatures are fully constructed.\\n\\n\\n\\n    Parsing\\n\\n\\nThe previous section delineated a very simple abstract machine,\\ncapable of unifying two simple TFSs. We now add to this machine\\ncontrol structures that will enable parsing.  We define rules,\\ngrammars and parsing, and then describe how the basic machine is\\nextended to accommodate the application of a single rule. We sketch\\nthe extensions necessary for manipulating a whole grammar (program).\\nThese extensions were not tested yet.\\n\\n  Grammars \\n\\nA multi-rooted structure (MRS) is a directed, labeled, finite\\ngraph with an ordered non-empty set of distinguished nodes, roots, from which all the nodes are reachable.  A rule is a\\nMRS, where the graph that is reachable from the last root is the\\n rule's head, and the ones  that are reachable from the rest of the roots form its body. A MRS is linearly represented as a sequence of terms, separated by\\ncommas, where two occurrences of the same tag, even within two\\ndifferent terms, denotes reentrancy (that is, the scope of the tags is\\nthe entire sequence of terms). The head is preceded by `\\n\\n\\n\\n\\n '\\n rather than by a comma.  See  for the formal details.\\n\\n\\nApplication of a rule amounts to unifying its body with a MRS resident\\nin memory and producing its head as a result.  When two TFSs A1 and\\nA2 are parts of MRSs \\n\\n\\n\\nand \\n\\n\\n\\n ,\\nrespectively, the\\nunification of A1 and A2 in the context of\\n\\n\\n\\nand \\n\\n\\n\\nis defined just like ordinary unification,\\nbut \\n\\n\\n\\nand \\n\\n\\n\\nmight be affected by the process.  As an\\nexample, the rule \\n\\n\\n\\n\\nconsists of a MRS of length three. When it is applied\\nto the MRS \\n\\n\\n\\n\\n ,\\nthe result is a new MRS\\nwhose head is a(d2,d1). \\n\\n\\n\\n 's head is modified even though\\nit does not participate directly in the unification, as it is part of\\nthe context.\\n\\n\\nA grammar is a finite set \\n\\n\\n\\nof rules together with a\\nstart feature structure As.  The lexicon associates with\\n every word w a TFS Aw, its category, by means of special rules of the form  \\n\\n\\n\\nThe input for the parser, therefore, is a\\nMRS rather than a string of words.  A MRS \\n\\n\\n\\nis derived by\\nsome TFS A if there exists a rule \\n\\n\\n\\n\\nsuch that Ais obtained by unifying \\n\\n\\n\\nwith \\n\\n\\n\\n 's body in the context of\\n\\n\\n\\n 's head. We abuse the term `derive' to denote also the reflexive\\ntransitive closure of this relation. A is a category if it\\n derives a substring of some input.  The language generated by the grammar is the set of strings of words \\n\\n\\n\\n\\nsuch that the category of wi is Ai for \\n\\n\\n\\n\\n and As derives \\n\\n\\n\\n\\n .\\n\\n\\nA dotted rule (or edge) is a MRS that is more specific\\nthan some rule in the grammar, with an additional dot,\\nindicating a location preceding some element in the MRS.  An edge is\\ncomplete if its dot precedes the head and is active\\notherwise.  We denote dotted rules by \\n\\n\\n\\n\\nInformally, such\\na dotted rule asserts that each of \\n\\n\\n\\n\\nderives a string\\n\\n\\n\\n\\nsuch that \\n\\n\\n\\n\\nis a\\nsubstring of the input. \\n\\n\\n\\n\\nstill have to derive\\n\\n\\n\\n\\nin order for A0 to be a category\\nderiving \\n\\n\\n\\n\\n .\\n\\n\\n    Parsing as Operational Semantics\\n\\n\\nWe view parsing as a computational process along the lines of\\n . Given a grammar (R,As), an item is a triple \\n\\n\\n\\n\\n where i,j are natural numbers and \\n\\n\\n\\nis a dotted rule. A state is a finite set of items.  A computation is triggered by some\\ninput string of words \\n\\n\\n\\n\\nof length n ] 0.  The\\ninitial state, \\n\\n\\n\\n ,\\nis \\n\\n\\n\\n\\nwhere Ai is the\\ncategory of wi and \\n\\n\\n\\n\\n .\\nFor any state S, the next state S' is constructed by the following\\ntransition relation `\\n\\n\\n\\n ' (the fundamental rule):\\nFor every \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\nsuch that \\n\\n\\n\\n\\n ,\\n add  \\n\\n\\n\\nto S',\\nwhere \\n\\n\\n\\n\\nis obtained by\\nunifying Band B'' in the contexts of \\n\\n\\n\\n\\nand \\n\\n\\n\\n\\nrespectively.\\n\\n\\nA computation is an infinite sequence of states \\n\\n\\n\\n\\n ,\\nsuch\\nthat \\n\\n\\n\\n\\nand for every \\n\\n\\n\\n ,\\n\\n\\n\\n\\n .\\nA\\ncomputation is terminating if there exists some \\n\\n\\n\\nfor which\\n\\nSm = Sm+1 (i.e., a fixed-point is reached). A successful\\ncomputation is a terminating one, the final state of which contains an\\nitem of the form \\n\\n\\n\\n\\nwhere \\n\\n\\n\\n\\n ;\\notherwise, the computation fails.\\nThe presence of more than one such item in the final state indicates\\nthat the input can be analyzed in more than one way.\\n\\n\\nTo represent a state of the computation the machine uses a chart, structured as a two-dimensional array storing, in the (i,j)entry, all the dotted rules \\n\\n\\n\\nsuch that \\n\\n\\n\\n\\nis a member\\nof the state.  Items are added to the chart by means of an agenda that controls the order of addition.\\n\\n\\n    Application of a Single Rule\\n\\n\\nTo allow the application of a single rule, the syntax of queries is\\nextended from simple TFSs to MRSs. The same code is generated for the\\nqueries, with additional advance instructions preceding each TFS\\nof the query.  The advance instruction simply increments the\\nindices of the chart item being manipulated. As a result of executing\\nthe query, the (i,i+1) diagonal of the chart is initialized with\\nsingleton sets of edges.\\n\\n\\nThe syntax of programs is extended, too, from a TFS to a single\\nMRS. Again, the same code is generated for the TFSs of a program:\\nprogram-code for each element of the rule's body and query-code for\\nthe head.  Before the first TFS, a start_rule instruction is\\ngenerated. A move_dot and next_item instructions are\\ngenerated between two consecutive structures, and after the last one,\\nthe head, an end_rule instruction concludes the generated code.\\n\\n\\nTo understand the effect of these instructions, one must understand\\nthe non-uniform internal representation of dotted rules.  Each such\\nrule is represented by a record, edge, containing three\\nfields. The seen field is a list of pointers to the roots of an\\nMRS, for the part of the dotted rule preceding the dot. The \\nto_see field is a pointer to the code area, for the rest of the\\nrule.  A complete edge is represented as a single TFS, its head, since\\nthe rest of the structures (that are unaccessible from the head) are\\nirrelevant.  An edge with an initial dot is simply a pointer to code.\\n\\n\\nSince the rules are applied incrementally, a TFS at a time, care must\\nbe taken of reentrancies. The rules manipulate registers which must\\ncontain the right values when used. To that end the values of the\\nregisters are stored after execution of a part of a rule (that is,\\nbefore moving the dot), and the right values are loaded prior to each\\nsuch execution (after moving the dot). The field regs of\\nan edge stores the saved registers.\\n\\n\\nstart_rule sets the stage for the application of the rule: it\\nstores the address of the beginning of the query in X1, where \\nget_structure expects to find it. It also records the values of\\ni,j and k of the current edges.  move_dot is executed after\\nthe successful unification of one TFS; it copies the newly created\\nedge, including the values of the registers, to the chart (and\\ninteracts with the agenda).  next_item just restores the\\nregisters' values and resumes execution.  end_rule is executed\\nonce a complete edge is constructed; it adds the edge to the chart and\\nselects the next edge to work on.\\n\\n\\n    Control Structures\\n\\n\\nWhen designing the control module, three parameters\\nhave to be set:\\nthe order of searching chart entries that can combine with a complete\\nedge e;\\nthe order of searching active edges within this chart entry;\\nand the search strategy: are all the edges that can combine with ecomputed first, and then their consequences (BFS), or rather all the\\nconsequences of the first such edge, then the next etc. (DFS).\\nThe order the chart is searched for active edges is right to left:\\nfrom (i,i) to (0,i).  There is no way to decide that a certain\\nedge in the chosen chart entry is appropriate save by trying to unify\\nit with the complete edge that was just entered.  Hence all edges in a\\nchart entry are considered a disjunctive value, and each of them is\\ntried in turn.  Furthermore, upon initialization each entry on the\\ndiagonal (i,i) of the chart is set to be a disjunction of all the\\nrules in the grammar.  As for the search strategy, we chose to employ\\nBFS; some way to record all the edges that were added as consequences\\nof e is needed, in order to compute their consequences next.\\n\\n\\nDetermining the values of these parameters is program-independent: the\\nmaintenance of the chart is fixed. This fact results from the nature\\nof the process the machine implements, namely parsing, and has a\\ndesirable consequence: one might change some of these parameters\\neasily without having to modify the compiler or even the set of\\nmachine instructions. What has to be changed is the data structures\\nthat support the control mechanism.\\nFor lack of space we don't detail the control module. Essentially, it\\nemploys a list of edges, agenda, and interacts with the machine\\ninstructions described above through designated functions.\\n\\n\\n\\n    Conclusion\\n\\n\\nAs linguistic formalisms become more rigorous, the necessity of well\\ndefined semantics for grammar specifications increases. We presented\\nan operational semantics for TFS-based formalisms, making use of\\nan abstract machine specifically tailored for  this kind of\\napplications. In addition, we described a compiler for a general\\nTFS-based language. The compiled code, in terms of abstract machine\\ninstructions, can be interpreted and executed on ordinary hardware.\\nThe use of abstract machine techniques is expected to result in highly\\nefficient processing.\\n\\n\\nThe TFS unification engine and the type hierarchy compiler were\\nalready implemented; the control module will be implemented shortly.\\nWe then plan to enhance the machine by adding specific\\nvalues for lists (and perhaps sets). The implementation will\\nserve as a platform for developing an HPSG grammar for the Hebrew language.\\n\\n\\n  Acknowledgments \\n\\nPart of the work described herein was done while the first author was\\nvisiting the Seminar fr Sprachwiessenschaft in Tbingen, Germany.\\nWe wish to thank the Minerva Stipendien Komitee for funding\\nthis visit, and the members of the SFB-340 B4 project\\nin Tbingen, especially Paul King, Thilo Gtz and John Griffith,\\nfor stimulating discussions.\\nWe also wish to thank Bob Carpenter for his help during this\\nproject, and the anonymous referees for enlightening comments.\\nThis work is supported by a grant from the Israeli Ministry of\\nScience: ``Programming Languages Induced Computational Linguistics''.\\nThe work of the second author was also partially supported by the Fund\\nfor the Promotion of Research in the Technion.\\n\\nBibliography \\n\\nH. At-Kaci.\\nWarren's Abstract Machine: A Tutorial Reconstruction.\\nLogic Programming. The MIT Press, Cambridge, Massachusetts, 1991.\\n\\n\\nH. At-Kaci.\\nAn introduction to LIFE - programming with logic, inheritance,\\n  functions and equations.\\nIn D. Miller, editor, Logic Programming - Proceedings of the\\n  1993 International Symposium, pages 52-68. MIT Press, 1993.\\n\\n\\nH. At-Kaci, R. Boyer, P. Lincoln, and R. Nasr.\\nEfficient implementation of lattice operations.\\nACM TOPLAS,\\n  11(1):115-146, January 1989.\\n\\n\\nH. At-Kaci and R. Di Cosmo.\\nCompiling order-sorted feature term unification.\\nPRL Technical Note 7, Digital Paris Research Laboratory, December\\n  1993.\\n\\n\\nH. At-Kaci and R. Nasr.\\nLOGIN: a logic programming language with built-in inheritance.\\nJournal of Logic Programming, 3:185-215, 1986.\\n\\n\\nH. At-Kaci, A. Podelski, and S. C. Goldstein.\\nOrder-sorted feature theory unification.\\nIn D. Miller, editor, Logic Programming - Proceedings of the\\n  1993 International Symposium, pages 506-524, Cambridge, Mass., 1993. MIT\\n  Press.\\n\\n\\nB. Carpenter.\\nALE - the attribute logic engine: User's guide.\\nTechnical report, Laboratory for Computational Linguistics,\\n  Philosophy Department, Carnegie Mellon University, Pittsburgh, PA 15213, Dec.\\n  1992.\\n\\n\\nB. Carpenter.\\nThe Logic of Typed Feature Structures.\\nCambridge Tracts in Theoretical Computer Science. Cambridge\\n  University Press, 1992.\\n\\n\\nB. Carpenter, C. Pollard, and A. Franz.\\nThe specification and implementation of constraint-based unification\\n  grammars.\\nIn Proceedings of the Second International Workshop on Parsing\\n  Technology, Cancun, Mexico, 1991.\\n\\n\\nG. Erbach.\\nProFIT: Prolog with features, inheritance and templates.\\nCLAUS Report 42, Computerlinguistik, Universitt des Saarlandes,\\n  July 1994.\\n\\n\\nT. W. Gtz.\\nA normal form for types feature structures.\\nMaster's thesis, Eberhard-Karls Universitt, Tbingen, March\\n  1994.\\n\\n\\nN. Haddock, E. Klein, and G. Morill, editors.\\nCategorial Grammar, Unification and Parsing, volume 1 of   Working Papers in Cognitive Science.\\nUniversity of Edingburgh, Center for Cognitive Science, 1987.\\n\\n\\nG. Penn.\\nA comprehensive HPSG grammar in ALE.\\nTechnical report, Laboratory for Computational Linguistics, Carnegie\\n  Mellon University, Pittsburgh, PA, 1993.\\n\\n\\nC. Pollard and I. A. Sag.\\nInformation Based Syntax and Semantics.\\nNumber 13 in CSLI Lecture Notes. CSLI, 1987.\\n\\n\\nC. Pollard and I. A. Sag.\\nHead-Driven Phrase Structure Grammar.\\nUniversity of Chicago Press and CSLI Publications, 1994.\\n\\n\\nS. M. Shieber, Y. Schabes, and F. C. N. Pereira.\\nPrinciples and implementation of deductive parsing.\\nTechnical Report TR-11-94, Center for Research in Computing\\n  Technology, Division of Applied Sciences, Harvard University, 1994.\\n\\n\\nS. Wintner and N. Francez.\\nAbstract machine for typed feature structures.\\nTechnical Report LCL 94-8, Laboratory for Computational\\n  Linguistics, Technion, Israel Institute of Technology, Haifa 32000, Israel,\\n  July 1994.\\n\\n\\nR. Zajac.\\nInheritance and constraint-based grammar formalisms.\\nComputational Linguistics, 18(2):159-182, 1992.\\n\\nFootnotes\\n\\n  This\\ncondition states that every feature is introduced by some least type\\nand is appropriate for all the types it subsumes.\\n  A TFS is totally well-typed if it contains all\\nand only the features that are appropriate for its type, and each\\nfeature bears an appropriate value. This requirement will be slightly\\n relaxed; see section . \\n   We use the operator `*' to refer to\\nthe contents of an address or a register.\\n  This use of head must not be confused\\nwith the linguistic one, the core features of a phrase.\\n  Notice that the traditional direction is\\nreversed. Note also that the head and the body need not be disjoint.\\n  Words can have more\\nthan one category, but we ignore ambiguity for the sake of\\nsimplicity.\\n  Categories are TFSs rather\\nthan the atomic symbols of Context Free Grammars.\\n  If \\n\\n\\n\\n\\nthen \\n\\n\\n\\n\\n .\\n\\n\\n\\n\\n\\n\",\n",
              "  \"\\n\\nThis paper describes an abstract machine for linguistic formalisms\\nthat are based on typed feature structures, such as HPSG.  The core\\ndesign of the abstract machine is given in detail, including the\\ncompilation process from a high-level language to the abstract machine\\nlanguage and the implementation of the abstract instructions.  The\\nmachine's engine supports the unification of typed, possibly cyclic,\\nfeature structures.  A separate module deals with control structures\\nand instructions to accommodate parsing for phrase structure grammars.\\nWe treat the linguistic formalism as a high-level declarative\\nprogramming language, applying methods that were proved useful in\\ncomputer science to the study of natural languages: a grammar\\nspecified using the formalism is endowed with an operational\\nsemantics. \\n\\n\"],\n",
              " [\"\\n\\n  Introduction \\n\\nThis work is part of an effort to develop a robust, domain-independent\\nsyntactic parser capable of yielding the one correct analysis for\\nunrestricted naturally-occurring input. Our goal is to develop a\\nsystem with performance comparable to extant part-of-speech taggers,\\nreturning a syntactic analysis from which predicate-argument structure\\ncan be recovered, and which can support semantic interpretation. The\\nrequirement for a domain-independent analyser favours statistical\\ntechniques to resolve ambiguities, whilst the latter goal favours a\\nmore sophisticated grammatical formalism than is typical in\\nstatistical approaches to robust analysis of corpus material.\\n\\n\\nBriscoe and Carroll (1993) describe a probablistic parser using a\\nwide-coverage unification-based grammar of English written in the\\nAlvey Natural Language Tools (ANLT) metagrammatical formalism (Briscoe\\net al., 1987), generating around 800 rules in a syntactic\\nvariant of the Definite Clause Grammar formalism (DCG, Pereira \\nWarren, 1980) extended with iterative (Kleene) operators. The ANLT grammar is\\nlinked to a lexicon containing about 64K entries for 40K lexemes,\\nincluding detailed subcategorisation information appropriate for the\\ngrammar, built semi-automatically from a learners' dictionary (Carroll\\n Grover, 1989).  The resulting parser is efficient, capable of\\nconstructing a parse forest in what seems to be roughly quadratic time, and\\nefficiently returning the ranked n-most likely analyses (Carroll, 1993,\\n1994).  The probabilistic model is a refinement of probabilistic context-free\\ngrammar (PCFG) conditioning CF `backbone' rule application on LR state\\nand lookahead item. Unification of the `residue' of features not\\nincorporated into the backbone is performed at parse time in\\nconjunction with reduce operations. Unification failure results in the\\nassociated derivation being assigned\\na probability of zero. Probabilities are assigned to transitions in\\nthe LALR(1) action table via a process of supervised training based on\\ncomputing the frequency with which transitions are traversed in a\\ncorpus of parse histories. The result is a probabilistic parser which,\\nunlike a PCFG, is capable of probabilistically discriminating\\nderivations which differ only in terms of order of application of the\\nsame set of CF backbone rules, due to the parse context defined by the\\nLR table.\\n\\n\\nExperiments with this system revealed three major problems which our\\ncurrent research is addressing.  Firstly, although the system is able to\\nrank parses with a 75% chance that the correct analysis will be the\\nmost highly ranked, further improvement will require a `lexicalised'\\nsystem in which (minimally) probabilities are associated with\\nalternative subcategorisation possibilities of individual lexical\\nitems. Currently, the relative frequency of subcategorisation\\npossibilities for individual lexical items is not recorded in\\nwide-coverage lexicons, such as ANLT or COMLEX (Grishman et al., 1994).\\nSecondly, removal of punctuation from the input (after segmentation\\ninto text sentences) worsens performance as punctuation both reduces\\nsyntactic ambiguity (Jones, 1994) and signals non-syntactic\\n(discourse) relations between text units (Nunberg, 1990). Thirdly, the\\nlargest source of error on unseen input is the omission of appropriate\\nsubcategorisation values for lexical items (mostly verbs), preventing the\\nsystem from finding the correct analysis. The current coverage of this system\\non a general corpus (e.g. Brown or LOB) is estimated to be around 20% by\\nBriscoe (1994). We have developed a variant probabilistic LR parser which does\\nnot rely on subcategorisation and uses punctuation to reduce ambiguity.\\nThe analyses produced by this parser could be utilised for phrase-finding\\napplications, recovery of subcategorisation frames, and other `intermediate'\\nlevel parsing problems.\\n\\n\\n  Part-of-speech Tag Sequence Grammar \\n\\nSeveral robust parsing systems exploit the comparative success of\\npart-of-speech (PoS) taggers, such as Fidditch (Hindle, 1989) or MITFP\\n(de Marcken, 1990), by reducing the input to a determinate sequence of\\nextended PoS labels of the type which can be practically disambiguated\\nin context using a (H)MM PoS tagger (e.g. Church, 1988). Such approaches,\\nby definition, cannot exploit subcategorisation, and probably achieve\\nsome of their robustness as a result. However, such parsers typically\\nalso employ heuristic rules, such as `low' attachment of PPs to\\nproduce unique `canonical' analyses. This latter step complicates the\\nrecovery of predicate-argument structure and does not integrate with a\\nprobabilistic approach to parsing.\\n\\n\\nWe utilised the ANLT metagrammatical formalism to develop a\\nfeature-based, declarative description of PoS label sequences for\\nEnglish. This grammar compiles into a DCG-like grammar of\\napproximately 400 rules. It has been designed to enumerate possible\\nvalencies for predicates (verbs, adjectives and nouns) by including\\nseparate rules for each pattern of possible complementation in\\nEnglish. The distinction between arguments and adjuncts is expressed,\\nfollowing X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction\\nof adjuncts to maximal projections (XP \\n\\n\\n\\n\\nXP\\nAdjunct\\n\\n\\n  Text Grammar and Punctuation \\n\\nNunberg (1990) develops a partial `text' grammar for English\\nwhich incorporates many constraints that (ultimately) restrict\\nsyntactic and semantic interpretation. For example, textual adjunct\\nclauses introduced by colons scope over following punctuation, as\\n(1a) illustrates; whilst textual adjuncts introduced by\\ndashes cannot intervene between a bracketed adjunct and the textual\\nunit to which it attaches, as in (2b).\\n\\n\\n\\nWe have developed a declarative grammar in the ANLT metagrammatical\\nformalism, based on Nunberg's procedural description.  This grammar\\ncaptures the bulk of the text-sentential constraints described by\\nNunberg with a grammar which compiles into 26 DCG-like\\nrules. Text\\ngrammar analyses are useful because they demarcate some of the\\nsyntactic boundaries in the text sentence and thus reduce ambiguity,\\nand because they identify the units for which a syntactic analysis\\nshould, in principle, be found; for example, in (3), the\\nabsence of dashes would mislead a parser into seeking a syntactic\\nrelationship between three and the following names, whilst in\\nfact there is only a discourse relation of elaboration between this\\ntext adjunct and pronominal three.\\n\\n\\n\\nThe rules of the text grammar divide into three groups: those\\nintroducing text-sentences, those defining text adjunct introduction\\nand those defining text adjuncts (Nunberg, 1990). An example of each\\ntype of rule is given in (4a-c).\\n\\n\\n\\nThese rules are phrase structure schemata employing iterative\\noperators, optionality and disjunction, preceded by a mnemonic name.\\nNon-terminal categories are text sentences, units or adjuncts which\\ncarry features mostly representing the punctuation marks which occur\\nas daughters in the rules (e.g. +sc represents presence of a\\nsemi-colon marker), whilst terminal punctuation is represented as +pxx\\n(e.g. +pda, dash). (4a) states that a text sentence can contain zero\\nor more text units with a semi-colon at their right boundary followed by\\na text unit optionally followed by a question or exclamation\\nmark. (4b) states that a text unit not containing a semi-colon can\\nconsist of a text unit or adjunct not containing dashes, colons or\\nsemi-colons followed by a text adjunct introduced by a dash. This type\\nof `unbalanced' adjunct can only be expanded by (4c) which states\\nthat it consists of a single opening dash followed by a text unit\\nwhich does not itself contain dashes or semi-colons. The features on\\nthe first daughter of (4b) force dash adjuncts to have lower\\nprecedence and narrower scope than colons or semi-colons, blocking\\ninterpretations of multiple dashes as sequences of `unbalanced'\\nadjuncts.\\n\\n\\nNunberg (1990) invokes rules of (point) absorption which delete\\npunctuation marks (inserted according to a simple context-free text\\ngrammar) when adjacent to other `stronger' punctuation marks. For\\ninstance, he treats all dash interpolated text adjuncts as\\nunderlyingly balanced, but allows a rule of point absorption to\\nconvert (5a) into (6b).\\n\\n\\n\\nThe various rules of absorption introduce procedurality into the\\ngrammatical framework and require the positing of underlying forms\\nwhich are not attested in text. For this reason, `absorption' effects\\nare captured through propagation of featural constraints in parse\\ntrees. For instance, (6a) is blocked by including distinct\\nrules for the introduction of balanced and unbalanced text adjuncts\\nand only licensing the latter text sentence finally.\\n\\n\\nThe text grammar has been tested on Susanne and covers 99.8% of\\nsentences. (The failures are mostly text segmentation problems). The\\nnumber of analyses varies from one (71%) to the thousands (0.1%).\\nJust over 50% of Susanne sentences contain some punctuation, so\\naround 20% of the singleton parses are punctuated. The major source\\nof ambiguity in the analysis of punctuation concerns the function of\\ncommas and their relative scope as a result of a decision to\\ndistinguish delimiters and separators (Nunberg 1990:36). Therefore, a\\ntext sentence containing eight commas (and no other punctuation) will\\nhave 3170 analyses. The multiple uses of commas cannot be resolved\\nwithout access to (at least) the syntactic context of occurrence.\\n\\n\\n  The Integrated Grammar \\n\\nDespite Nunberg's observation that text grammar is distinct from\\nsyntax, text grammatical ambiguity favours interleaved application of\\ntext grammatical and syntactic constraints.  The integration of text\\nand PoS sequence grammars is straightforward and remains modular, in\\nthat the text grammar is `folded into' the PoS sequence grammar, by\\ntreating text and syntactic categories as overlapping and dealing with\\nthe properties of each using disjoint sets of features, principles of\\nfeature propagation, and so forth. The text grammar rules are\\nrepresented as left or right branching rules of `Chomsky-adjunction'\\nto lexical or phrasal constituents. For example, the simplified rule\\nfor combining NP appositional or parenthetical text adjuncts is\\nN2[+ta] \\n\\n\\n\\n\\nH2 Ta[+bal]\\n\\n\\n  Parsing the Susanne and SEC Corpora \\n\\nThe integrated grammar has been used to parse Susanne and the quite\\ndistinct SEC Corpus (Taylor  Knowles, 1988), a 50K word treebanked\\ncorpus of transcribed British radio programmes punctuated by the\\ncorpus compilers. Both corpora were retagged with determinate\\npunctuation and PoS labelling using the Acquilex HMM tagger (Elworthy,\\n1993, 1994) trained on text tagged with a slightly modified version of\\nCLAWS-II labels (Garside et al., 1987).\\n\\n  Coverage and Average Ambiguity \\n\\nTo examine the efficiency and coverage of the grammar we applied it to\\nour retagged versions of Susanne and SEC. We used the ANLT chart\\nparser (Carroll, 1993), but modified just to count the number of\\npossible parses in the parse forests (Billot  Lang, 1989) rather\\nthan actually unpacking them. We also imposed a per-sentence time-out\\nof 30 seconds CPU time, running in Franz Allegro Common Lisp 4.2 on an\\nHP PA-RISC 715/100 workstation with 96 Mbytes of physical memory.\\n\\n\\nWe define the `coverage' of the grammar to be the inverse of the\\nproportion of sentences for which no analysis was found--a weak measure\\nsince discovery of one or more global analyses does not entail that the\\ncorrect analysis is recovered. For both corpora, the majority of sentences\\nanalysed successfully received under 100 parses, although there is a long tail\\nin the distribution.  Monitoring this distribution is helpful during\\ngrammar development to ensure that coverage is increasing but the\\nambiguity rate is not. A more succinct though less intuitive measure\\nof ambiguity rate for a given corpus is what we call the average\\nparse base (APB), defined as the geometric mean over all sentences in the\\ncorpus of \\n\\n\\n\\n\\n ,\\nwhere n is the number of words in a sentence, and\\n p, the number of parses for that sentence. Thus, given a sentence n tokens long, the APB raised to the nth power gives the number of analyses\\nthat the grammar can be expected to assigned to a sentence of\\nthat length in the corpus. Table\\n  gives these measures for all of the sentences in Susanne and in SEC.\\n\\n\\n\\n\\nAs the grammar was developed solely with reference to Susanne,\\ncoverage of SEC is quite robust. The two corpora differ considerably\\nsince the former is drawn from American written text whilst the latter\\nrepresents British transcribed spoken material. The corpora overall contain\\nmaterial drawn from widely disparate genres / registers, and are more\\ncomplex than those used in DARPA ATIS tests and more diverse than those used\\nin MUC. The APBs for Susanne and SEC of 1.256\\nand 1.239 respectively indicate that sentences of average\\nlength in each corpus could be expected to be assigned of the order of 97 and\\n126 analyses (i.e. \\n\\n1.256[20.1] and \\n\\n1.239[22.6]). Black et al.(1993:156)\\nquote a parse base of 1.35 for the IBM grammar for computer manuals\\napplied to sentences 1-17 words long. Although, as mentioned above, Black's\\nmeasure may not be exactly the same as our APB measure, it is probable that\\nthe IBM grammar assigns more analyses than ours for sentences of the same\\nlength. Black achieves a coverage of around 95%, as opposed to our coverage\\nrate of 67-74% on much more heterogeneous data and longer sentences.\\n\\n\\nThe parser throughput on these tests, for sentences successfully\\nanalysed, is around 45 words per CPU second on an HP PA-RISC\\n715/100. Sentences of up to 30 tokens (words plus punctuation) are\\nparsed in an average under 0.6 seconds each, whilst those around 60\\ntokens take on average 4.5 seconds. Nevertheless, the relationship\\nbetween sentence length and processing time is fitted well by a\\nquadratic function, supporting the findings of Carroll (1994) that in\\npractice NL grammars do not evince worst-case parsing complexity.\\n\\n  Coverage, Ambiguity and Punctuation \\n\\nWe have also run experiments to\\nevaluate the degree to which punctuation is contributing useful\\ninformation. Intuitively, we would expect the exploitation of text\\ngrammatical constraints to both reduce ambiguity and extend coverage\\n(where punctuation cues discourse rather than syntactic relations between\\nconstituents). Jones (1994) reports a preliminary experiment evaluating\\nreduction of ambiguity by punctuation. However, the grammar he uses was\\ndeveloped only to cover the test sentences, drawn entirely from\\nthe SEC corpus which was punctuated post hoc by the corpus developers\\n(Taylor and Knowles, 1988).\\n\\n\\nWe took all in-coverage sentences from Susanne\\nof length 8-40 words inclusive containing internal punctuation; a total\\nof 2449 sentences. The APB for this set was 1.273, mean length\\n22.5 words, giving an expected number of analyses for an average\\nsentence of 225. We then removed all sentence-internal punctuation from this\\nset and re-parsed it. Around 8% of sentences now failed to receive an\\nanalysis. For those that did (mean length 20.7 words), the APB was now\\n1.320, so an average sentence would be assigned 310 analyses, 38% more than\\nbefore. On closer inspection, the increase in ambiguity is due to two\\nfactors: a) a significant proportion of sentences that previously received\\n1-9 analyses now receive more, and b) there is a much more substantial tail\\nin the distribution of sentence length vs. number of parses, due to some\\nlonger sentences being assigned many more parses. Manual examination of 100\\ndepunctuated examples revealed that in around a third of cases, although the\\nsystem returned global analyses, the correct one was not in this set (Briscoe\\n Carroll, 1994).  With a more constrained (subcategorised) syntactic\\ngrammar, many of these examples would not have received any global\\nsyntactic analysis.\\n\\n\\n\\n  Parse Selection \\n\\nA probabilistic LR parser was trained with the integrated grammar by\\nexploiting the Susanne treebank bracketing. An LR parser (Briscoe and\\nCarroll, 1993) was applied to unlabelled bracketed sentences from the\\nSusanne treebank, and a new treebank of 1758 correct and complete analyses\\nwith respect to the integrated grammar was constructed semi-automatically by\\nmanually resolving the remaining ambiguities. 250 sentences from the new\\ntreebank were kept back for testing. The remainder, together with a further\\nset of analyses from 2285 treebank sentences that were not checked manually,\\nwere used to train a probabilistic version of the LR parser, using\\nGood-Turing smoothing to estimate the probability of unseen transitions in\\nthe LALR(1) table (Briscoe and Carroll, 1993; Carroll, 1993).  The\\nprobabilistic parser can then return a ranking of all possible analyses for a\\nsentence, or efficiently return just the n-most probable (Carroll,\\n1993).\\n\\n\\nThe probabilistic parser was tested on the 250 sentences held out from the\\nmanually-created treebank (with mean length 18.2 tokens, mean number of parses\\nper sentence 977, and APB 1.252); in this test 85 sentences (34%) had the\\n correct analysis ranked in the top three. This figure rose to 51% for sentences of less than 20 words. Considering\\njust the highest ranked analysis for each sentence, in Sampson, Haigh\\n Atwell's (1989) measure of correct rule application the parser\\n scored a mean of 83.5% correct over all 250 sentences. Table  shows the results of this test--with respect to the original Susanne\\nbracketings--using the Grammar Evaluation Interest Group scheme (GEIG, see\\ne.g. Harrison et al., 1991). This compares unlabelled bracketings\\nderived from corpus treebanks with those derived from parses for the same\\nsentences by computing recall, the ratio of matched brackets over all\\nbrackets in the treebank; precision, the ratio of matched brackets over\\nall brackets found by the parser; `crossing' brackets, the number of times\\na bracketed sequence output by the parser overlaps with one from the\\ntreebank but neither is properly contained in the other; and minC, the number of sentences for which all of the analyses had one\\nor more crossings.\\n\\n\\n\\n\\nThe table also gives an indication of the best and worst possible\\nperformance of the disambiguation component of the system, showing the\\nresults obtained when parse selection is replaced by a simple random\\nchoice, and the results of evaluating the manually-created treebank\\nagainst the corresponding Susanne bracketings. In this latter figure, the mean\\nnumber of crossings is greater than zero mainly because of compound noun\\nbracketing ambiguity which our grammar does not attempt to resolve,\\nalways returning a right-branching binary analysis.\\n\\n\\nBlack (1993:7) uses the crossing brackets measure to define a notion\\nof structural consistency, where the structural consistency rate for\\nthe grammar is defined as the proportion of sentences for which at\\nleast one analysis contains no crossing brackets, and reports a rate\\nof around 95% for the IBM grammar tested on the computer manual\\ncorpus. The problem with the GEIG scheme and with structural\\nconsistency is that both are still weak measures (designed to avoid\\nproblems of parser/treebank representational compatibility) which lead\\nto unintuitive numbers whose significance still depends heavily on\\ndetails of the relationship between the representations compared\\n(c.f. the compound noun issue mentioned above).\\n\\n\\nSchabes et al. (1993) and Magerman (1995) report results using the GEIG\\nevaluation scheme which are numerically superior to ours. However, their\\nexperiments are not strictly compatible because they both utilise\\nmore homogeneous and probably simpler corpora. In addition, Schabes et al. do not recover tree labelling, whilst Magerman has developed a parser\\ndesigned to produce identical analyses to those used in the Penn Treebank,\\nremoving the problem of spurious errors due to grammatical incompatibility.\\nBoth these approaches achieve better coverage by constructing the grammar\\nfully automatically. No one has yet shown that any robust parser\\nis practical and useful for some NLP task. However, it seems likely that\\nsay rule-to-rule semantic interpretation will be easier with hand-constructed\\ngrammars with an explicit, determinate ruleset.\\nA more meaningful comparison will require application\\nof different parsers to an identical and extended test suite and\\nutilisation of a more stringent standard evaluation procedure sensitive to\\nnode labellings.\\n\\n  Parse Selection and Punctuation \\n\\nIn order to assess the contribution of punctuation to the selection of\\nthe correct analysis, we applied the same trained version of the\\nintegrated grammar to the 106 sentences from the test set which\\ncontain internal punctuation, both with and without the punctuation\\nmarks in the input. A comparison of the GEIG evaluation metrics for\\nthis set of sentences punctuated and unpunctuated gives a measure of\\nthe contribution of punctuation to parse selection on this data. (The\\nresults for the unpunctuated set were computed against a version of\\nthe Susanne treebank from which punctuation had also been removed.) As\\n table  shows, recall declines by 10%, precision by 5% and there are an average of 1.27 more crossing brackets per sentence.\\nThese results indicate clearly that punctuation and text grammatical\\nconstraints can play an important role in parse selection.\\n\\n\\n\\n\\n\\n\\n  Conclusions \\n\\nBriscoe and Carroll (1993) and Carroll (1993) showed that the LR\\nmodel, combined with a grammar exploiting subcategorisation constraints,\\ncould achieve good parse selection accuracy but at the expense of\\npoor coverage of free text. The results reported here suggest that improved\\ncoverage of heterogeneous text can be achieved by exploiting textual and\\ngrammatical constraints on PoS and punctuation sequences. The\\nexperiments show that grammatical coverage can be\\ngreatly increased by relaxing subcategorisation constraints, and that\\ntext grammatical or punctuation-cued constraints can reduce\\nambiguity and increase coverage during parsing.\\n\\n\\nTo our knowledge these are the first experiments which objectively\\ndemonstrate the utility of punctuation for resolving syntactic\\nambiguity and improving parser coverage. They extend work by\\nJones (1994) and Briscoe and Carroll (1994) by applying a\\nwide-coverage text grammar to substantial quantities of\\nnaturally-punctuated text and by quantifying the contribution of\\npunctuation to ambiguity resolution in a well-defined probabilistic\\nparse selection model.\\n\\n\\nAccurate enough parse selection for practical applications will\\nrequire a more lexicalised system.  Magerman's (1995) parser is an\\nextension of the history-based parsing approach developed at IBM\\n(e.g. Black, 1993) in which rules are conditioned on lexical and\\nother (essentially arbitrary) information available in the parse\\nhistory. In future work, we intend to explore a more restricted and\\nsemantically-driven version of this approach in which, firstly,\\nprobabilities are associated with different subcategorisation\\npossibilities, and secondly, alternative predicate-argument structures\\nderived from the grammar are ranked probabilistically. However, the\\nmassively increased coverage obtained here by relaxing subcategorisation\\nconstraints underlines the need to acquire accurate and complete\\nsubcategorisation frames in a corpus-driven fashion, before such\\nconstraints can be exploited robustly and effectively with free text.\\n\\n\\n  References \\n\\n\\nBillot, S. and Lang, B. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the\\n27th Meeting of Association for Computational Linguistics, 143-151. Vancouver, Canada.\\n\\nBlack, E., Garside, R. and Leech, G. (eds.) 1993. Statistically-Driven Computer Grammars of English: The\\nIBM/ Lancaster Approach. Rodopi, Amsterdam.\\n\\nBriscoe, E. 1994. Prospects for practical parsing of unrestricted text: robust\\nstatistical parsing techniques. In Oostdijk, N  de Haan, P. eds. Corpus-based Research into Language. Rodopi, Amsterdam: 97-120.\\n\\nBriscoe, E. and Carroll, J. 1993. Generalised probabilistic LR parsing for unification-based grammars. Computational Linguistics 19.1: 25-60.\\n\\nBriscoe, E. and Carroll, J. 1994. Parsing (with) Punctuation. Rank Xerox Research Centre, Grenoble, MLTT-TR-007.\\n\\nBriscoe, E., Grover, C., Boguraev, B. and Carroll, J. 1987. A formalism and environment for the development of a large\\n          grammar of English. In Proceedings of the\\n10th International Joint Conference on Artificial Intelligence, 703-708. Milan, Italy.\\n\\nCarroll, J. 1993. Practical unification-based parsing of natural language. Cambridge University, Computer Laboratory, TR-314.\\n\\nCarroll, J. 1994. Relating complexity to practical performance in parsing with\\nwide-coverage unification grammars. In Proceedings of the\\n32nd Meeting of Association for Computational Linguistics, 287-294. Las Cruces, NM.\\n\\nCarroll, J. and Grover, C. 1989. The derivation of a large computational lexicon for English\\n          from LDOCE. In Boguraev, B. and Briscoe, E. eds. Computational Lexicography for Natural Language Processing. Longman, London: 117-134.\\n\\nChurch, K. 1988. A stochastic parts program and noun phrase parser for\\nunrestricted text. In Proceedings of the\\n2nd Conference on Applied Natural Language Processing, 136-143. Austin, Texas.\\n\\nElworthy, D. 1993. Part-of-speech tagging and phrasal tagging. Acquilex-II Working Paper 10, Cambridge University Computer\\nLaboratory (can be obtained from cide@cup.cam.ac.uk).\\n\\nElworthy, D. 1994. Does Baum-Welch re-estimation help taggers?. In Proceedings of the\\n4th Conf. Applied NLP. Stuttgart, Germany.\\n\\nGarside, R., Leech, G. and Sampson, G. 1987. Computational analysis of English. Longman, London.\\n\\nGrishman, R., Macleod, C. and Meyers, A. 1994. Comlex syntax: building\\na computational lexicon. In Proceedings of the\\nInternational Conference on Computational\\nLinguistics, COLING-94, 268-272. Kyoto, Japan.\\n\\nGrover, C., Carroll, J. and Briscoe, E. 1993. The Alvey Natural Language Tools Grammar (4th Release). Cambridge University Computer Laboratory, TR-284.\\n\\nHarrison, P., Abney, S., Black, E., Flickenger, D., Gdaniec,\\nC., Grishman, R., Hindle, D., Ingria, B., Marcus, M., Santorini, B.\\nand Strzalkowski, T. 1991. Evaluating syntax performance of parser/grammars of English. In Proceedings of the\\nWorkshop on Evaluating Natural Language Processing Systems. ACL.\\n\\nHindle, D. 1989. Acquiring disambiguation rules from text. In Proceedings of the\\n27th Annual Meeting of the Association for Computational Linguistics, 118-25. Vancouver, Canada.\\n\\nJackendoff, R 1977. X-bar Syntax. MIT Press; Cambridge, MA..\\n\\nJones, B 1994. Can punctuation help parsing?. In Proceedings of the\\nColing94. Kyoto, Japan.\\n\\nMagerman, D. 1995. Statistical decision-tree models for parsing. In Proceedings of the\\n33rd annul Meeting of the Association for Computational Linguistics. Boston, MA.\\n\\nde Marcken, C. 1990. Parsing the LOB corpus. In Proceedings of the\\n28th Annual Meeting of the Association for Computational Linguistics, 243-251. New York.\\n\\nNunberg, G. 1990. The linguistics of punctuation. CSLI Lecture Notes 18, Stanford, CA.\\n\\nPereira, F. and Warren, D. 1980. Definite clause grammars for language analysis - a survey\\n          of the formalism and a comparison with augmented transition\\n          networks. Artificial Intelligence 13.3: 231-278.\\n\\nSampson, G. 1994. Susanne: a Doomsday book of English grammar. In Oostdijk, N  de Haan, P. eds. Corpus-based Research into Language. Rodopi, Amsterdam: 169-188.\\n\\nSampson, G., Haigh, R., and Atwell, E. 1989. Natural language analysis by stochastic optimization: a\\n          progress report on Project APRIL. Journal of Experimental and Theoretical Artificial Intelligence 1: 271-287.\\n\\nSchabes, Y., Roth, M. and Osborne, R. 1993. Parsing of the Wall Street Journal with the inside-outside\\nalgorithm. In Proceedings of the\\nMeeting of European Association for Computational\\nLinguistics. Utrecht, The Netherlands.\\n\\nTaylor, L. and Knowles, G. 1988. Manual of information to accompany the SEC corpus:\\nthe machine-readable corpus of spoken English. University of Lancaster, UK, Ms..\\n\\n\\n\\nFootnotes\\n\\n  Black\\net al.(1993:13) define an apparently similar measure, parse base, as the\\n``geometric mean of the number of parses per word for the entire corpus'', but\\nin the immediately following sentence talk about raising it to the power of\\nthe number of words in a sentence, which is inappropriate for a simple\\nratio.\\n  This is a strong\\nmeasure, since it not only accounts for structural identity between\\ntrees, but also correct rule application at every node.\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nWe describe an approach to robust domain-independent syntactic parsing\\nof unrestricted naturally-occurring (English) input. The technique involves\\nparsing sequences of part-of-speech and punctuation labels using a\\nunification-based grammar coupled with a probabilistic LR parser. We describe\\nthe coverage of several corpora using this grammar and report the results of a\\nparsing experiment using probabilities derived from bracketed training data.\\nWe report the first substantial experiments to assess the contribution of\\npunctuation to deriving an accurate syntactic analysis, by parsing identical\\ntexts both with and without naturally-occurring punctuation marks.\\n\\n'],\n",
              " [\"\\n\\n  Introduction \\n\\nHidden Markov Models are commonly used for statistical language\\nmodels, e.g. in part-of-speech tagging and speech recognition\\n . The models need a large set of parameters which are induced from a (text-) corpus. The parameters should be optimal in the\\nsense that the resulting models assign high probabilities to seen\\ntraining data as well as new data that arises in an application. \\n\\n\\nThere are several methods to estimate model parameters. The first\\none is to use each word (type) as a state and estimate the transition\\nprobabilities between two or three words by using the relative\\nfrequencies of a corpus. This method is commonly used in speech\\nrecognition and known as word-bigram or word-trigram model. The relative\\nfrequencies have to be smoothed to handle the sparse data problem and to\\navoid zero probabilities. \\n\\n\\nThe second method is a variation of the first method. Words are\\nautomatically grouped, e.g. by similarity of distribution in the corpus\\n . The relative frequencies of pairs or triples of groups (categories, clusters) are used as model parameters, each group is\\nrepresented by a state in the model. The second method has the advantage\\nof drastically reducing the number of model parameters and thereby\\nreducing the sparse data problem; there is more data per group than per\\nword, thus estimates are more precise.\\n\\n\\nThe third method uses manually defined categories. They are\\nlinguistically motivated and usually called parts-of-speech. An\\nimportant difference to the second method with automatically derived\\ncategories is that with the manual definition a word can belong to more\\nthan one category. A corpus is (manually) tagged with the categories and\\ntransition probabilities between two or three categories are estimated\\nfrom their relative frequencies. This method is commonly used for\\n part-of-speech tagging . \\n\\n\\nThe fourth method is a variation of the third method and is also\\nused for part-of-speech tagging. This method does not need a\\npre-annotated corpus for parameter estimation. Instead it uses a lexicon\\nstating the possible parts-of-speech for each word, a raw text corpus,\\nand an initial bias for the transition and output probabilities. The\\nparameters are estimated by using the Baum-Welch algorithm\\n . The accuracy of the derived model depends heavily on the initial bias, but with a good choice results are comparable to those of\\n method three . \\n\\n\\nThis paper investigates a fifth method for estimating natural\\nlanguage models, combining the advantages of the methods mentioned\\nabove. It is suitable for both speech recognition and part-of-speech\\ntagging, has the advantage of automatically deriving word categories\\nfrom a corpus and is capable of recognizing the fact that a word belongs\\nto more than one category. Unlike other techniques it not only induces\\ntransition and output probabilities, but also the model topology, i.e.,\\nthe number of states, and for each state the outputs that have a\\nnon-zero probability. The method is called model merging and was\\n introduced by . \\n\\n\\nThe rest of the paper is structured as follows. We first give a short\\nintroduction to Markov models and  present the model merging technique.\\nThen, techniques for reducing the time complexity are presented and we\\nreport two experiments using these techniques. \\n\\n\\n    Markov Models\\n\\n\\nA discrete output, first order Markov Model\\nconsists of\\n\\na finite set of states \\n\\n\\n ,\\n\\n\\nwith qs        the start state, and qe the end state;\\n\\na finite output alphabet \\n\\n ;\\n\\na \\n\\n\\nmatrix, specifying the\\n        probabilities of state transitions p(q'|q) between states q        and q' (there are no transitions into qs, and no\\n        transitions originating in qe);\\n        for each state \\n\\n\\n ,\\nthe\\n        sum of the outgoing transition probabilities is 1, \\n\\n\\n ;\\n\\na \\n\\n\\nmatrix, specifying the output\\n        probabilities \\n\\n\\nof state q emitting output \\n\\n ;\\n        for each state \\n\\n\\n ,\\nthe\\n        sum of the output probabilities is 1,\\n\\n\\n .\\n\\n\\n\\n\\nA Markov model starts running in the start state qs, makes a\\ntransition at each time step, and stops when reaching the end state\\nqe. The transition from one state to another is done according to the\\nprobabilities specified with the transitions. Each time a state is\\nentered (except the start and end state) one of the outputs is chosen\\n(again according to their probabilities) and emitted. \\n\\n  Assigning Probabilities to Data \\n\\nFor the rest of the paper, we are interested in the probabilities\\nwhich are assigned to sequences of outputs by the Markov models.\\nThese can be calculated in the following way.\\n\\n\\nGiven a model M, a sequence of outputs \\n\\n\\n\\n\\nand a sequence of states \\n\\n\\n\\n\\n(of same length),\\nthe probability that the model running through the sequence of states\\nand emitting the given outputs is\\n\\n\\n\\n(with q0 = qs).\\nA sequence of outputs can be emitted by more than one sequence of\\nstates, thus we have to sum over all sequences of states with the given\\nlength to get the probability that a model emits a given sequence of\\noutputs:\\n\\n\\n\\nThe probabilities are calculated very efficiently with the Viterbi\\n algorithm . Its time complexity is linear to the sequence length despite the exponential growth of the search space.\\n\\n\\n  Perplexity \\n\\nMarkov models assign rapidly decreasing probabilities to output\\nsequences of increasing length. To compensate for different lengths and\\nto make their probabilities comparable, one uses the perplexity \\n\\n\\n\\nof\\nan output sequence instead of its probability. The perplexity is defined\\nas\\n\\n\\n\\nThe probability is normalized by taking the k[th] root (k is the\\nlength of the sequence). Similarly, the log perplexity \\n\\n\\n\\nis\\ndefined:\\n\\n\\n\\nHere, the log probability is normalized by dividing by the length of the\\nsequence. \\n\\n\\n\\nand \\n\\n\\n\\nare defined such that higher perplexities (log\\nperplexities, resp.) correspond to lower probabilities, and vice versa.\\nThese measures are used to determine the quality of Markov models. The\\nlower the perplexity (and log perplexity) of a test sequence, the higher\\nits probability, and thus the better it is predicted by the model.\\n\\n\\n\\n    Model Merging\\n\\n\\nModel merging is a technique for inducing model parameters for\\nMarkov models from a text corpus. It was introduced in \\n  and  to induce models for regular languages from a few samples, and adapted to natural language models in\\n . Unlike other techniques it not only induces transition and output probabilities from the corpus, but also the model topology,\\ni.e., the number of states and for each state the outputs that have\\nnon-zero probability. In n-gram approaches the topology is fixed.\\nE.g., in a pos-n-gram model, the states are mostly syntactically\\nmotivated, each state represents a syntactic category and only words\\nbelonging to the same category have a non-zero output probability in a\\nparticular state. However the n-gram-models make the implicit\\nassumption that all words belonging to the same category have a similar\\ndistribution in a corpus. This is not true in most of the cases. \\n\\n\\nBy estimating the topology, model merging groups words into\\ncategories, since all words that can be emitted by the same state form a\\ncategory. The advantage of model merging in this respect is that it can\\nrecognize that a word (the type) belongs to more than one category, while\\neach occurrence (the token) is assigned a unique category. This\\nnaturally reflects manual syntactic categorizations, where a word can\\nbelong to several syntactic classes but each occurrence of a word is\\nunambiguous. \\n\\n  The Algorithm \\n\\nModel merging induces Markov models in the following way. Merging\\nstarts with an initial, very general model. For this purpose, the\\nmaximum likelihood Markov model is chosen, i.e., a model that exactly\\nmatches the corpus. There is one path for each utterance in the corpus\\nand each path is used by one utterance only. Each path gets the same\\nprobability 1/u, with u the number of utterances in the corpus. This\\nmodel is also referred to as the trivial model. Figure\\n .a shows the trivial model for a corpus with words a, b, c and utterances \\n\\nab, ac, abac. It has one path for each of the\\nthree utterances ab, ac, and abac, and each path gets the same\\nprobability 1/3. The trivial model assigns a probability of\\n\\n\\n\\n\\nto the corpus. Since the model makes an\\nimplicit independence assumption between the utterances, the corpus\\nprobability is calculated by multiplying the utterance's probabilities,\\nyielding \\n\\n\\n\\n\\n .\\n\\n\\nNow states are merged successively, except for the start and end\\nstate. Two states are selected and removed and a new merged state is\\nadded. The transitions from and to the old states are redirected to the\\nnew state, the transition probabilities are adjusted to maximize the\\nlikelihood of the corpus; the outputs are joined and their probabilities\\nare also adjusted to maximize the likelihood. One step of merging can be\\n seen in figure .b. States 1 and 3 are removed, a combined state 1,3 is added, and the probabilities are adjusted.\\n\\n\\nThe criterion for selecting states to merge is the probability of\\nthe Markov model generating the corpus. We want this probability to stay\\nas high as possible. Of all possible merges (generally, there are\\nk(k-1)/2 possible merges, with k the number of states exclusive\\nstart and end state which are not allowed to merge) we take the merge\\nthat results in the minimal change of the probability. For the trivial\\nmodel and u pairwise different utterances the probability is\\n\\np(S|Mtriv) = 1/u[u]. The probability either stays constant, as in\\n Figure .b and c, or decreases, as in  .d and e. The probability never increases because the trivial model is the maximum likelihood model, i.e., it\\nmaximizes the probability of the corpus given the model.\\n\\n\\nModel merging stops when a predefined threshold for the corpus\\nprobability is reached. Some statistically motivated criteria for\\n termination using model priors are discussed in . \\n\\n\\n\\n  Using Model Merging \\n\\nThe model merging algorithm needs several optimizations to be\\napplicable to large natural language corpora, otherwise the amount of\\ntime needed for deriving the models is too large. Generally, there are\\nO(l[2]) hypothetical merges to be tested for each merging step (l is\\nthe length of the training corpus). The probability of the training\\ncorpus has to be calculated for each hypothetical merge, which is O(l)with dynamic programming. Thus, each step of merging is O(l[3]). If we\\nwant to reduce the model from size l+2 (the trivial model, which\\nconsists of one state for each token plus initial and final states) to\\nsome fixed size, we need O(l) steps of merging. Therefore, deriving a\\nMarkov model by model merging is O(l[4]) in time. \\n\\n\\n discuss several computational shortcuts and approximations:\\n1.\\nImmediate merging of identical initial and final states of\\ndifferent utterances. These merges do not change the corpus\\n        probability and thus are the first merges anyway.\\n2.\\nUsage of the Viterbi path (best path) only instead of summing up\\n        all paths to determine the corpus probability.\\n3.\\nThe assumption that all\\n        input samples retain their Viterbi path after merging.\\n        Making this approximation, it is no\\n        longer necessary to re-parse the whole corpus for each\\n        hypothetical merge.\\n\\n\\nWe use two additional strategies to reduce the time complexity of the\\nalgorithm: a series of cascaded constraints on the merges and the\\nvariation of the starting point.\\n\\n  Constraints \\n\\nWhen applying model merging one can observe that first mainly states\\nwith the same output are merged. After several steps of merging, it is\\nno longer the same output but still mainly states that output words of\\nthe same syntactic category are merged. This behavior can be exploited\\nby introducing constraints on the merging process. The constraints allow\\nonly some of the otherwise possible merges. Only the allowed merges are\\ntested for each step of merging.\\n\\n\\nWe consider constraints that divide the states of the current model into\\nequivalence classes. Only states belonging to the same class are allowed\\nto merge. E.g., we can divide the states into classes generating the\\nsame outputs. If the current model has N states and we divide them\\ninto k]1 nonempty equivalence classes \\n\\n\\n\\n\\n ,\\nthen, instead of\\nN(N-1)/2, we have to test\\n\\n\\n\\nmerges only.\\n\\n\\nThe best case for a model of size N is the division into N/2classes of size 2. Then, only N/2 merges must be tested to find the\\nbest merge.\\n\\n\\nThe best division into k ] 1 classes for some model of size N is the\\ncreation of classes that all have the same size N/k (or an\\napproximation if \\n\\n\\n\\n\\n ). Then, \\n\\n\\n\\nmust be tested for each step of merging.\\n\\n\\nThus, the introduction of these constraints does not reduce the order of\\nthe time complexity, but it can reduce the constant factor significantly\\n(see section about experiments). \\n\\n\\nThe following equivalence classes can be used for constraints \\nwhen using untagged corpora:\\n1.\\nStates that generate the same outputs (unigram constraint)\\n2.\\nunigram constraint, and additionally all predecessor states must\\ngenerate the same outputs (bigram constraint)\\n3.\\ntrigrams or higher, if the corpora are large enough\\n4.\\na variation of one: states that output words belonging to one\\n        ambiguity class, i.e. can be of a certain number of syntactic\\n        classes. \\n\\n\\nMerging starts with one of the constraints. After a number of merges\\nhave been performed, the constraint is discarded and a weaker one is\\nused instead.\\n\\n\\nThe standard n-gram approaches are special cases of using model\\nmerging and constraints. E.g., if we use the unigram constraint, and\\nmerge states until no further merge is possible under this constraint,\\nthe resulting model is a standard bigram model, regardless of the order\\nin which the merges were performed.\\n\\n\\nIn practice, a constraint will be discarded before no further merge is\\npossible (otherwise the model could have been derived directly, e.g., by\\nthe standard n-gram technique). Yet, the question when to discard a\\nconstraint to achieve best results is unsolved.\\n\\n\\n  The Starting Point \\n\\nThe initial model of the original model merging procedure is the maximum\\nlikelihood or trivial model. This model has the advantage\\nof directly representing the corpus. But its disadvantage is its huge\\nnumber of states. A lot of computation time can be saved by choosing an\\ninitial model with fewer states. \\n\\n\\nThe initial model must have two properties:\\n1.\\nit must be larger than the intended model, and\\n2.\\nit must be easy to construct.\\nThe trivial model has both properties. A class of models that\\ncan serve as the initial model as well are n-gram models.\\nThese models are smaller by one or more orders of magnitude than the\\ntrivial model and therefore could speed up the derivation of a model\\nsignificantly. \\n\\n\\nThis choice of a starting point excludes a lot of solutions which are\\nallowed when starting with the maximum likelihood model. Therefore,\\nstarting with an n-gram model yields a model that is at most\\nequivalent to one that is generated when starting with the trivial\\nmodel, and that can be much worse. But it should be still better than\\nany n-gram model that is of lower of equal order than the initial\\nmodel.\\n\\n\\n\\n    Experiments\\n\\n  Model Merging vs. Bigrams \\n\\nThe first experiment compares model merging with a standard bigram\\nmodel. Both are trained on the same data. We use \\n\\nNtrain = 14,421words of the Verbmobil corpus. The corpus consists of transliterated\\n dialogues on business appointments. The models are tested on  \\nNtest=2,436words of the same corpus. Training and test parts are disjunct.\\n\\n\\nThe bigram model yields a Markov model with 1,440 states. It assigns a\\nlog perplexity of 1.20 to the training part and 2.40 to the\\ntest part.\\n\\n\\nModel merging starts with the maximum likelihood model for the training\\npart. It has 14,423 states, which correspond to the 14,421 words (plus\\nan initial and a final state). The initial log perplexity of the\\ntraining part is 0.12. This low value shows that the initial model is\\nvery specialized in the training part. \\n\\n\\nWe start merging with the same-output (unigram) constraint to reduce\\ncomputation time.  After 12,500 merges the constraint is discarded and\\nfrom then on all remaining states are allowed to merge. The constraints\\nand the point of changing the constraint are chosen for pragmatic\\nreasons. We want the constraints to be as week as possible to allow the\\nmaximal number of solutions but at the same time the number of merges\\nmust be manageable by the system used for computation (a SparcServer1000\\nwith 250MB main memory). As the following experiment will show, the\\nexact points of introducing/discarding constraints is not important for\\nthe resulting model. \\n\\n\\nThere are \\n\\n\\n\\n\\nhypothetical first merges\\nin the unconstraint case. This number is reduced to \\n\\n\\n\\n\\n when using the unigram constraint, thus by a factor of \\n\\n\\n\\n .\\nBy\\nusing the constraint we need about a week of computation time on a\\nSparcServer 1000 for the whole merging process. Computation would not\\nhave been feasible without this reduction.\\n\\n\\n Figure  shows the increase in perplexity during merging. There is no change during the first 1,454 merges. Here, only\\nidentical sequences of initial and final states are merged (compare\\n figure .a to c). These merges do not influence the probability assigned to the training part and thus do not change the\\nperplexity.\\n\\n\\nThen, perplexity slowly increases. It can never decrease: the maximum\\nlikelihood model assigns the highest probability to the training part\\nand thus the lowest perplexity. \\n\\n\\n Figure  also shows the perplexity's slope. It is low until about 12,000 merges, then drastically increases. At about this\\npoint, after 12,500 merges, we discard the constraint. For this reason,\\nthe curve is discontinuous at 12,500 merges. The effect of further\\nretaining the constraint is shown by the thin lines. These stop after\\n12,983 merges, when all states with the same outputs are merged (i.e.,\\nwhen a bigram model is reached). Merging without a constraint continues\\nuntil only three states remain: the initial and the final state plus one\\nproper state.\\n\\n\\nNote that the perplexity changes very slowly for the largest part, and\\nthen changes drastically during the last merges. There is a constant\\nphase between 0 and 1,454 merges. Between 1,454 and \\n\\n\\n\\n 11,000 merges\\nthe log perplexity roughly linearly increases with the number of merges,\\nand it explodes afterwards.\\n\\n\\nWhat happens to the test part? Model merging starts with a very special\\nmodel which then is generalized. Therefore, the perplexity of some\\nrandom sample of dialogue data (what the test part is supposed to be)\\nshould decrease during merging. This is exactly what we find in the\\nexperiment. \\n\\n\\n Figure  shows the log perplexity of the test part during merging. Again, we find the discontinuity at the point where the\\nconstraint is changed. And again, we find very little change in\\nperplexity during about 12,000 initial merges, and large changes during\\nthe last merges.\\n\\n\\nModel merging finds a model with 113 states, which assigns a log\\nperplexity of 2.26 to the test part. Thus, in addition to finding a\\nmodel with lower log perplexity than the bigram model (2.26 vs. 2.40),\\nwe find a model that at the same time has less than 1/10 of the states\\n(113 vs. 1,440).\\n\\n\\nTo test if we found a model that predicts new data better than the\\nbigram model and to be sure that we did not find a model that is simply\\nvery specialized to the test part, we use a new, previously unseen part\\nof the Verbmobil corpus. This part consists of 9,784 words. The bigram\\nmodel assigns a log perplexity of 2.78, the merged model with 113 states\\n assigns a log perplexity of 2.41 (see table ). Thus, the model found by model merging can be regarded generally better than the\\nbigram model.\\n\\n\\n  Improvements \\n\\nThe derivation of the optimal model took about a week although the size\\nof the training part was relatively small. Standard speech applications\\ndo not use 14,000 words for training as we do in this experiment, but\\n100,000, 200,000 or more. It is not possible to start with a model of\\n100,000 states and to successively merge them, at least it is not\\npossible on today's machines. Each step would require the test of\\n\\n\\n\\n\\nmerges. \\n\\n\\nIn the previous experiment, we abandoned the same-output constraint\\nafter 12,500 merges to keep the influence on the final result as small\\nas possible. It can not be skipped from the beginning because somehow\\nthe time complexity has to be reduced. But it can be further retained,\\nuntil no further merge under this constraint is possible. This yields a\\nbigram model. The second experiment uses the bigram model with 1,440\\nstates as its starting point and imposes no constraints on the merges.\\n The results are shown in figure .  \\n\\n\\nWe see that the perplexity curves approach very fast their counterparts\\nfrom the previous experiment. The states differ from those of the\\npreviously found model, but there is no difference in the number of\\nstates and corpus perplexity in the optimal point. So, one could in\\nfact, at least in the shown case, start with the bigram model without\\nloosing anything. Finally, we calculate the perplexity for the\\nadditional test part. It is 2.39, thus again lower than the perplexity\\n of the bigram model (see table ). It is even slightly lower than in the previous experiment, but most probably due to random\\nvariation.\\n\\n\\nThe derived models are not in any case equivalent (with respect to\\nperplexity), regardless whether we start with the trivial model or the\\nbigram model. We ascribe the equivalence in the experiment to the\\nparticular size of the training corpus. For a larger training corpus,\\nthe optimal model should be closer in size to the bigram model, or even\\nlarger than a bigram model. In such a case  starting with bigrams does\\nnot lead to an optimal model, and a trigram model must be used.\\n\\n\\n\\n    Conclusion\\n\\n\\nWe investigated model merging, a technique to induce Markov models from\\ncorpora. The original procedure is improved by introducing constraints\\nand a different initial model. The procedures are shown to be applicable\\nto a transliterated speech corpus. The derived models assign lower\\nperplexities to test data than the standard bigram model derived from\\nthe same training corpus. Additionally, the merged model was much\\nsmaller than the bigram model. \\n\\n\\nThe experiments revealed a feature of model merging that allows for\\nimprovement of the method's time complexity. There is a large initial\\npart of merges that do not change the model's perplexity w.r.t. the\\ntest part, and that do not influence the final optimal model. The time\\nneeded to derive a model is drastically reduced by abbreviating these\\ninitial merges. Instead of starting with the trivial model, one can\\nstart with a smaller, easy-to-produce model, but one has to ensure that\\nits size is still larger than the optimal model.\\n\\n\\n  Acknowledgements \\n\\nI would like to thank Christer Samuelsson for very useful comments on\\nthis paper. This work was supported by the Graduiertenkolleg\\nKognitionswissenschaft, Saarbrcken.\\n\\nBibliography \\n\\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.\\n1983.\\nA maximum likelihood approach to continuous speech recognition.\\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\\n  5(2):179-190.\\n\\n\\nLeonard E. Baum, Ted Petrie, George Soules, and Norman Weiss.\\n1970.\\nA maximization technique occuring in the statistical analysis of\\n  probabilistic functions in markov chains.\\nThe Annals of Methematical Statistics, 41:164-171.\\n\\n\\nThorsten Brants.\\n1995.\\nEstimating HMM topologies.\\nIn Tbilisi Symposium on Language, Logic, and Computation, Human\\n  Communication Research Centre, Edinburgh, HCRC/RP-72.\\n\\n\\nKenneth Ward Church.\\n1988.\\nA stochastic parts program and noun phrase parser for unrestricted\\n  text.\\nIn Proc. Second Conference on Applied Natural Language\\n  Processing, pages 136-143, Austin, Texas, USA.\\n\\n\\nDoug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun.\\n1992.\\nA practical part-of-speech tagger.\\nIn Proceedings of the 3rd Conference on Applied Natural Language\\n  Processing (ACL), pages 133-140.\\n\\n\\nF. Jelinek.\\n1990.\\nSelf-organized language modeling for speech recognition.\\nIn A. Waibel and K.-F. Lee, editors, Readings in Speech\\n  Recognition, pages 450-506. Kaufmann, San Mateo, CA.\\n\\n\\nS. M. Omohundro.\\n1992.\\nBest-first model merging for dynamic learning and recognition.\\nIn J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors,   Advances in Neural Information Processing Systems 4, pages 958-965.\\n  Kaufmann, San Mateo, CA.\\n\\n\\nFernando Pereira, Naftali Tishby, and Lillian Lee.\\n1993.\\nDistributional clustering of english words.\\nIn Proceedings of the 31st ACL, Columbus, Ohio.\\n\\n\\nL. R. Rabiner.\\n1989.\\nA tutorial on hidden markov models and selected applications in\\n  speech recognition.\\nIn Proceedings of the IEEE, volume 77(2), pages 257-285.\\n\\n\\nAndreas Stolcke and Stephen M. Omohundro.\\n1994.\\nBest-first model merging for hidden markov model induction.\\nTechnical Report TR-94-003, International Computer Science Institute,\\n  Berkeley, California, USA.\\n\\n\\nA. Viterbi.\\n1967.\\nError bounds for convolutional codes and an asymptotically optimum\\n  decoding algorithm.\\nIn IEEE Transactions on Information Theory, pages 260-269.\\n\\nFootnotes\\n\\n  Many thanks to the Verbmobil\\nproject for providing these data. We use dialogues that were recorded in\\n1993 and 94, and which are now available from the Bavarian Archive for\\nSpeech Signals BAS (http://www.phonetik.uni-muenchen.de/\\nBas/BasHomeeng.html).\\n\\n\\n\\n\\n\\n\",\n",
              "  '\\n\\nThis paper investigates model merging, a technique for deriving Markov\\nmodels from text or speech corpora. Models are derived by starting with\\na large and specific model and by successively combining states to build\\nsmaller and more general models. We present methods to reduce the time\\ncomplexity of the algorithm and report on experiments on deriving\\nlanguage models for a speech recognition task. The experiments show \\nthe advantage of model merging over the standard bigram\\napproach. The merged model assigns a lower perplexity to the test set\\nand uses considerably fewer states. \\n\\n']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX2HOwiUTfQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d18258c-6088-4ea5-e2c1-8379d4a7f314"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 13.3MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 12.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 9.7MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 8.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/9e/5b80becd952d5f7250eaf8fc64b957077b12ccfe73e9c03d37146ab29712/transformers-4.6.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 52.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 42.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SZ01FU20AR7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "fde2e5c5de9e46329ffefb1d4f6166f2",
            "e04c35ae1911484193bcf1329082d91a",
            "18976a3185b14210af4512e5da86e4f7",
            "3f2e53b77edf4006a2637d92c39bc39d",
            "ac53b686560c4cc591694a549d52bd72",
            "bb507455410142a5a8c3f396b0a52097",
            "e8ad901f84db4342883ff3f854879ea0",
            "0516be42fefe428e9c8735a280c905da",
            "c3a16be9b28245e5ba0166953dbe263b",
            "04604891240a469695169e8e0c4c23be",
            "6749f713499e42ef9dc32fed5ea40341",
            "28dce52515374383ad6831bfbf9cc24a",
            "9cee349163c24884ae6ea5b46539fb84",
            "d01c7bcbfd3248d2b2ca8fd5155cc14d",
            "835f373bcda14a1fbcba85196fec89bb",
            "f79e67fcdd3d4eb7b407996922d5c9b9",
            "f134c294d7604531a589277f8a23f7a8",
            "e2eabdd59a7e425a9d1dc3ae4fa7e8c3",
            "c5b661321a4c4f9fabc3596c6922d7b8",
            "24348a33e2004af38e9030141168573e",
            "44c9608dbe474bf9968dbcb73f897dce",
            "f7e134ab862348ec9da36de84d65fc55",
            "4eda6f0cd23841efbff3dc848cc90ff6",
            "1d7f48d13fd94da3b747eadf63dc961c",
            "4e82ddf69fd541868ed80a662c8578dc",
            "e69006e78aa34e5ba19f1029c6dc6071",
            "917ace664f4943e8813bc7f0e47448f6",
            "404097575bb14739a585af44180bc83c",
            "e7f59289080443c1ab7d28b480e57ce4",
            "54467de49847407f8fc995ffb8a12343",
            "c3b4bdcad5894e768ef2699d81727a29",
            "f9736c584b3d4536af2e1016b4a0a86a",
            "75481d0da5e448b8b3a0ab5f46df8a16",
            "daa46470f17b45aba225106d655221ad",
            "41ef7656c7c74e0194f5178c793142cd",
            "686da3701d4643e8bbd1f1394fb0be77",
            "dfa8062755454e71987304b42a2845ad",
            "289a475d651d4f1e85b73d2b48e35aff",
            "c027c1c80fe246e78d70d8f3ce54e5ec",
            "25e836b22b3648ee9f95f3cc003f5f68"
          ]
        },
        "outputId": "310462b1-166c-4bc9-ff7f-580164eb1661"
      },
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "import transformers\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'google/pegasus-arxiv'\n",
        "tokenizer_pegasus = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fde2e5c5de9e46329ffefb1d4f6166f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1912529.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3a16be9b28245e5ba0166953dbe263b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=65.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f134c294d7604531a589277f8a23f7a8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=88.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e82ddf69fd541868ed80a662c8578dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1120.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75481d0da5e448b8b3a0ab5f46df8a16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2275327883.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlFJ8eyxQODu"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "tokenized_train_data = []\n",
        "for idx in range(len(train_data)):\n",
        "  art = tokenizer.encode(train['Articles'][idx], truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  summ = tokenizer.encode(train_data['Summary'][idx], max_length=100,  truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  tokenized_train_data.append([art,summ])\n",
        "\n",
        "tokenized_test_data = []\n",
        "for idx in range(110, len(data)):\n",
        "  art = tokenizer.encode(test_data['Articles'][idx], truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  summ = tokenizer.encode(test_data['Summary'][idx], max_length=100,  truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  tokenized_test_data.append([art,summ])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdjt1sOIoAsS"
      },
      "source": [
        "tokenized_train_data = []\n",
        "for idx in range(len(train)):\n",
        "  art = tokenizer_pegasus.encode(train['Articles'][idx], truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  summ = tokenizer_pegasus.encode(train['Summary'][idx], max_length=100,  truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  tokenized_train_data.append([art,summ])\n",
        " \n",
        "tokenized_test_data = []\n",
        "for idx in range(110, len(data)):\n",
        "  art = tokenizer_pegasus.encode(test['Articles'][idx], truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  summ = tokenizer_pegasus.encode(test['Summary'][idx], max_length=100,  truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  tokenized_test_data.append([art,summ])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NczPOrfBoc82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe58f91-0bb7-4980-9cb3-c13ca9327df4"
      },
      "source": [
        "for ipt, lbl in tokenized_test_data:\n",
        "  print(ipt.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n",
            "torch.Size([1, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI1mJSNMrb9O"
      },
      "source": [
        "import tensorflow as tf\n",
        "from statistics import mean\n",
        "import numpy as np"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxTGRDT-ngqn"
      },
      "source": [
        "def train_simple_network(mdl, loss_func, train_loader, tok, spacy_tok, vcb, val_loader=None, score_funcs=None, epochs=10, device=\"cuda\", checkpoint_file=None, lr=0.001):\n",
        "    \"\"\"Train simple neural networks\n",
        "    \n",
        "    Keyword arguments:\n",
        "    model -- the PyTorch model / \"Module\" to train\n",
        "    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n",
        "    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs. \n",
        "    val_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n",
        "    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n",
        "    epochs -- the number of training epochs to perform\n",
        "    device -- the compute lodation to perform training\n",
        "    \n",
        "    \"\"\"\n",
        "    if score_funcs == None:\n",
        "        score_funcs = {}#Empty set \n",
        "    \n",
        "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
        "    if val_loader is not None:\n",
        "        to_track.append(\"val loss\")\n",
        "    for eval_score in score_funcs:\n",
        "        to_track.append(\"train \" + eval_score )\n",
        "        if val_loader is not None:\n",
        "            to_track.append(\"val \" + eval_score )\n",
        "        \n",
        "    total_train_time = 0 #How long have we spent in the training loop? \n",
        "    results = {}\n",
        "    #Initialize every item with an empty list\n",
        "    for item in to_track:\n",
        "        results[item] = []\n",
        "    \n",
        "    # Adafactor optimizer - Built and optimized upon Adam\n",
        "    optimizer = transformers.Adafactor(mdl.parameters())\n",
        "\n",
        "    #Place the model on the correct compute resource (CPU or GPU)\n",
        "    mdl.to(device)\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "    \n",
        "        mdl = mdl.train()#Put our model in training mode\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # y_true = []\n",
        "        # y_pred = []\n",
        "        bleu = []\n",
        "\n",
        "        start = time.time()\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Train Batch\", leave=False):\n",
        "\n",
        "            src = tok.encode(inputs, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "            tar = tok.encode(labels, max_length=100, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "      \n",
        "            #Move the batch to the device we are using. \n",
        "            inputs = src.to(device)\n",
        "            labs = tar.to(device)\n",
        "            \n",
        "            #batch_size = labels.shape[0]\n",
        "            max_length_labels = labs.shape[1]\n",
        "\n",
        "            # PyTorch stores gradients in a mutable data structure. So we need to set it to a clean state before we use it. \n",
        "            #Otherwise, it will have old information from a previous iteration\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = mdl.generate(inputs, max_length=max_length_labels) #this just computed f_Θ(x(i))\n",
        "            y_hat1 = tf.one_hot(y_hat.detach().cpu(), len(vcb))\n",
        "            y_hat1 = torch.tensor(y_hat1.numpy(),requires_grad=True).squeeze(0).to(device)\n",
        "            # max_length_inputs = y_hat1.shape[0]\n",
        "            shape = y_hat1.shape[0]\n",
        "\n",
        "            print(y_hat1.shape)\n",
        "            # tar_1 = tok.encode(labels, max_length=max_length_inputs, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "            # labels_1 = tar_1.to(device)\n",
        "            print(labs.squeeze(0).shape)\n",
        "            \n",
        "            # if max_length_inputs == max_length_labels:\n",
        "            #     labl = labs\n",
        "            # else:\n",
        "            #     labl = labels_1\n",
        "\n",
        "            if shape == max_length_labels:\n",
        "                n1 = 'None'\n",
        "            else:\n",
        "                labs = labs[0][:shape]\n",
        "\n",
        "            # Compute loss.\n",
        "            loss = loss_func(y_hat1, labs.squeeze(0))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            #Now we just need to update all the parameters! \n",
        "            optimizer.step()\n",
        "\n",
        "            #Now we are just grabbing some information we would like to have\n",
        "            running_loss += loss.item() #* batch_size\n",
        "            \n",
        "            #moving labels & predictions back to CPU for computing / storing predictions\n",
        "            labs = labs.detach().cpu().numpy()\n",
        "            y_hat1 = y_hat1.detach().cpu().numpy()\n",
        "            # decoded_pred = tok.batch_decode(y_hat1, skip_special_tokens=True)[0]\n",
        "            # decoded_labels = tok.batch_decode(labs, skip_special_tokens=True)[0]\n",
        "\n",
        "            # for name, score_func in score_funcs.items():\n",
        "            #   bleu.append(score_func(spacy_tok(decoded_pred),spacy_tok(decoded_labels)))\n",
        "            #   name = name\n",
        "\n",
        "\n",
        "            # for i in range(batch_size):\n",
        "            #     y_true.append(labels[i])\n",
        "            #     y_pred.append(y_hat[i,:])\n",
        "        #end training epoch\n",
        "        end = time.time()\n",
        "        total_train_time += (end-start)\n",
        "        \n",
        "        results[\"epoch\"].append( epoch )\n",
        "        results[\"total time\"].append( total_train_time )\n",
        "        results[\"train loss\"].append( running_loss )\n",
        "        \n",
        "        # y_pred = np.asarray(y_pred)\n",
        "        \n",
        "        # if y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
        "        #     y_pred = np.argmax(y_pred, axis=1)\n",
        "        # results[\"train \" + name].append(mean(bleu))\n",
        "        # for name, score_func in score_funcs.items():\n",
        "        #     results[\"train \" + name].append( score_func(y_true, y_pred) )\n",
        "      \n",
        "        if val_loader is None:\n",
        "            pass\n",
        "        else:#Lets find out validation performance as we go!\n",
        "            mdl = mdl.eval() #Set the model to \"evaluation\" mode, b/c we don't want to make any updates!\n",
        "\n",
        "            # y_true = []\n",
        "            # y_pred = []\n",
        "            bleu = []\n",
        "            \n",
        "            running_loss = 0.0\n",
        "\n",
        "            for inputs, labels in val_loader:\n",
        "\n",
        "                src = tok.encode(inputs, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "                tar = tok.encode(labels, max_length=100, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "        \n",
        "                #Move the batch to the device we are using. \n",
        "                inputs = src.to(device)\n",
        "                labs = tar.to(device)\n",
        "                \n",
        "                # batch_size = labels.shape[0]\n",
        "                max_length_labels = labs.shape[1]\n",
        "        \n",
        "                y_hat = mdl.generate(inputs, max_length=max_length_labels)\n",
        "                y_hat1 = tf.one_hot(y_hat.detach().cpu(), len(vcb))\n",
        "                y_hat1 = torch.tensor(y_hat1.numpy(), requires_grad=True).squeeze(0).to(device)\n",
        "                shape = y_hat1.shape[0]\n",
        "\n",
        "                # max_length_inputs = y_hat1.shape[0]\n",
        "\n",
        "                # tar_1 = tok.encode(labels, max_length=max_length_inputs, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "                # labels_1 = tar_1.to(device)\n",
        "                # if max_length_inputs == max_length_labels:\n",
        "                #     labl = labs\n",
        "                # else:\n",
        "                #     labl = labels_1\n",
        "                if shape == max_length_labels:\n",
        "                    n1 = 'None'\n",
        "                else:\n",
        "                    labs = labs[0][:shape]\n",
        "\n",
        "                loss = loss_func(y_hat1, labs.squeeze(0))\n",
        "                # labels = tar.to(device)\n",
        "                #Now we are just grabbing some information we would like to have\n",
        "                running_loss += loss.item() #* batch_size\n",
        "\n",
        "                #moving labels & predictions back to CPU for computing / storing predictions\n",
        "                labs = labs.detach().cpu().numpy()\n",
        "                y_hat1 = y_hat1.detach().cpu().numpy()\n",
        "\n",
        "                # decoded_pred = tok.batch_decode(y_hat1, skip_special_tokens=True)[0]\n",
        "                # decoded_labels = tok.batch_decode(labs, skip_special_tokens=True)[0]\n",
        "\n",
        "                # for name, score_func in score_funcs.items():\n",
        "                #     bleu.append(score_func(spacy_tok(decoded_pred),spacy_tok(decoded_labels)))\n",
        "                #     name = name\n",
        "                # for i in range(batch_size):\n",
        "                #     y_true.append(labels[i])\n",
        "                #     y_pred.append(y_hat[i,:])\n",
        "                        \n",
        "            results[\"val loss\"].append( running_loss )\n",
        "\n",
        "            # y_pred = np.asarray(y_pred)\n",
        "\n",
        "            # if y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
        "            #     y_pred = np.argmax(y_pred, axis=1)\n",
        "            \n",
        "            # results[\"val \" + name].append(mean(bleu))\n",
        "\n",
        "            # for name, score_func in score_funcs.items():\n",
        "            #     results[\"val \" + name].append( score_func(y_true, y_pred) )\n",
        "                \n",
        "        if checkpoint_file is not None:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'results' : results\n",
        "                }, checkpoint_file)\n",
        "\n",
        "    return pd.DataFrame.from_dict(results)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo2YJV7Bt8Yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9238e412-1c68-4c0e-829a-0fbfc7904815"
      },
      "source": [
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import spacy\n",
        "eng_words = spacy.load('en')\n",
        "def tokenize_words(text):\n",
        "  return [tok.text for tok in eng_words.tokenizer(text)]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import spacy\n",
        "import numpy as np\n",
        "# eng_words = spacy.load('en')\n",
        "# def tokenize_words(text):\n",
        "#   return [tok.text for tok in eng_words.tokenizer(text)]\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "score_funcs = {'BLEU':sentence_bleu}\n",
        "results_df = train_simple_network(mdl=model, loss_func=criterion, train_loader=train_data, spacy_tok=tokenize_words, val_loader=test_data, tok=tokenizer_pegasus, vcb = tokenizer_pegasus.get_vocab())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:08<14:32,  8.00s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:10<11:38,  6.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:13<09:34,  5.37s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:15<07:43,  4.37s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:16<05:59,  3.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:19<05:34,  3.22s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:22<05:10,  3.02s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:25<05:01,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:27<04:52,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:30<04:44,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:33<04:43,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:36<04:32,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:38<04:13,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([34, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:41<04:17,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:43<04:18,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:46<04:17,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:49<04:08,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:51<04:04,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:54<03:50,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:56<03:54,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:59<03:57,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:02<03:59,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:05<03:59,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:08<03:58,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:10<03:46,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:12<03:36,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:15<03:34,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:18<03:39,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:21<03:41,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:24<03:41,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:26<03:29,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:29<03:31,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:31<03:10,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:33<03:02,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:35<02:55,  2.34s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:37<02:40,  2.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:39<02:30,  2.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:41<02:39,  2.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:44<02:49,  2.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:47<02:46,  2.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:49<02:54,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:52<02:58,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:55<02:59,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:58<03:00,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:01<03:00,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:03<02:46,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:05<02:34,  2.45s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:08<02:39,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:11<02:42,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:14<02:43,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:17<02:43,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:19<02:42,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:22<02:42,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:25<02:39,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:28<02:36,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:31<02:34,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:34<02:32,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:23,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:23,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:42<02:16,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:45<02:16,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:48<02:15,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:51<02:15,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:54<02:13,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:56<02:08,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:00<02:19,  3.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:03<02:11,  3.05s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:06,  3.02s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:09<01:58,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:11<01:55,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:17<01:44,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([34, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:35,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:21<01:33,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:24<01:34,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:27<01:34,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:30<01:25,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:32<01:25,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:18,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:38<01:19,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:40<01:19,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:43<01:14,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:44<01:00,  2.26s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:47<01:03,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:49<00:59,  2.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:52<01:00,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:55<01:01,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:57<00:53,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:00<00:52,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:03<00:52,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:05<00:49,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:08<00:49,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:11<00:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:13<00:42,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:16<00:41,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:19<00:38,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:22<00:36,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:24<00:31,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:27<00:27,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:29<00:23,  2.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:30<00:19,  2.19s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:33<00:19,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:36<00:18,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:39<00:16,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:41<00:12,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:44<00:09,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:46<00:07,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:49<00:05,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:52<00:02,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:54<00:00,  2.40s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  10%|█         | 1/10 [07:57<1:11:41, 477.96s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:17,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([36, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:14,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:12,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:10<04:44,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<03:58,  2.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:15,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:21,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:31,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:35,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:38,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:40,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:34,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:20,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:23,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:26,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:26,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:18,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:48<04:12,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<03:59,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:53<04:04,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:56<04:04,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [00:59<04:07,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:02<04:08,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:05<04:08,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:54,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:10<03:43,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:42,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:45,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:47,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:48,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:24<03:35,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:27<03:37,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:29<03:14,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:31<03:08,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:34<03:00,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:35<02:45,  2.24s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:37<02:36,  2.14s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:40<02:44,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:43<02:53,  2.45s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:45<02:51,  2.45s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:48<02:58,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:51<03:02,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:54<03:03,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:57<03:04,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:00<03:04,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:02<02:50,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:04<02:37,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:07<02:43,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:10<02:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:13<02:47,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:16<02:46,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:19<02:45,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:22<02:44,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:25<02:42,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:28<02:39,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:31<02:37,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:33<02:35,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:26,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:25,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:42<02:18,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:44<02:18,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:47<02:16,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:50<02:17,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:53<02:14,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:56<02:09,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:00<02:20,  3.18s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:03<02:12,  3.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:08,  3.05s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:09<01:59,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:11<01:56,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:47,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:17<01:45,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:36,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:21<01:33,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:24<01:34,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:27<01:34,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:30<01:25,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:32<01:26,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:19,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:38<01:20,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:41<01:19,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:43<01:14,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:44<01:01,  2.26s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:47<01:04,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:49<00:59,  2.37s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:52<01:00,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:55<01:01,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:57<00:53,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:00<00:52,  2.51s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:03<00:52,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:05<00:49,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:08<00:48,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:11<00:46,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:13<00:42,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:16<00:40,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:19<00:39,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:22<00:36,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:24<00:31,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:27<00:27,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:29<00:23,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:31<00:19,  2.19s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:33<00:19,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:36<00:17,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:39<00:16,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:41<00:12,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:44<00:09,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:46<00:07,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:49<00:05,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:52<00:02,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:54<00:00,  2.38s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  20%|██        | 2/10 [15:55<1:03:43, 477.93s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:20,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:15,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:13,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:10<04:46,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<03:59,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:15,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:21,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:31,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:37,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:39,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:41,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:32,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:17,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:22,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:26,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:27,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:18,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:48<04:14,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:00,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:04,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:56<04:05,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [00:59<04:08,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:02<04:08,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:05<04:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:54,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:10<03:43,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:41,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:45,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:46,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:47,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:24<03:34,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:27<03:36,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:29<03:14,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:31<03:06,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:33<02:59,  2.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:35<02:44,  2.22s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:37<02:35,  2.13s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:40<02:43,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:43<02:55,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:45<02:52,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:48<02:59,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:51<03:03,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:54<03:04,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:57<03:05,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:00<03:05,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:02<02:51,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:04<02:37,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:07<02:42,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:10<02:45,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:13<02:46,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:16<02:46,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:19<02:45,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:22<02:44,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:25<02:42,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:28<02:39,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:31<02:37,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:33<02:34,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:25,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:25,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:42<02:19,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:45<02:18,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:47<02:16,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:50<02:17,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:53<02:14,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:56<02:10,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:00<02:20,  3.20s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:03<02:13,  3.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:08,  3.07s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:09<01:59,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:11<01:56,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:47,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:17<01:46,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:36,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:22<01:34,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:25<01:35,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:27<01:34,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:30<01:26,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:33<01:26,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:19,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:38<01:20,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:41<01:19,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:43<01:14,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:45<01:01,  2.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:47<01:04,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:50<00:59,  2.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:53<01:01,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:56<01:01,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:57<00:53,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:00<00:52,  2.51s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:03<00:53,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:06<00:50,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:09<00:49,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:11<00:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:14<00:42,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:17<00:40,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:20<00:39,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([81, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:23<00:36,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:25<00:31,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:27<00:27,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:29<00:24,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:31<00:19,  2.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:34<00:19,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:37<00:18,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:40<00:16,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:42<00:12,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:44<00:09,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:47<00:07,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:50<00:05,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:53<00:02,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:54<00:00,  2.41s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  30%|███       | 3/10 [23:54<55:46, 478.06s/it]  \n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:20,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:15,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:14,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:10<04:46,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:00,  2.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:16,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:22,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:31,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:36,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:40,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:41,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:34,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:19,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:23,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:25,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:26,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:17,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:48<04:12,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<03:59,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:04,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:56<04:06,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [00:59<04:07,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:02<04:07,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:05<04:07,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:54,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:10<03:44,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:12<03:25,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([45, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:15<03:34,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:18<03:39,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:21<03:42,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:23<03:31,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:26<03:34,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:28<03:12,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:31<03:06,  2.45s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:33<02:59,  2.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:35<02:46,  2.25s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:37<02:37,  2.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:39<02:45,  2.30s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:42<02:56,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:45<02:52,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:48<03:00,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:51<03:05,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:54<03:06,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:57<03:07,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [01:59<03:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:02<02:52,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:04<02:39,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:07<02:44,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:10<02:47,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:13<02:48,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:16<02:47,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:19<02:46,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:22<02:46,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:25<02:43,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:27<02:40,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:30<02:37,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:33<02:34,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:25,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:24,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:41<02:18,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:44<02:18,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:47<02:16,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:50<02:17,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:53<02:14,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:56<02:10,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:00<02:20,  3.20s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:03<02:13,  3.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:08,  3.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:08<02:00,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:11<01:56,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:48,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:17<01:46,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:37,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:21<01:34,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:24<01:34,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:27<01:34,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:30<01:26,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:32<01:26,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:19,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:38<01:20,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:41<01:20,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:43<01:14,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:44<01:01,  2.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:47<01:04,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:50<00:59,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:52<01:01,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:55<01:01,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:57<00:53,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:00<00:53,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:03<00:52,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:06<00:50,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:09<00:49,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:11<00:46,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:14<00:42,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:17<00:40,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:20<00:39,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:23<00:36,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:25<00:31,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:27<00:27,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:29<00:24,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:31<00:19,  2.20s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:34<00:19,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:37<00:18,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:40<00:16,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:42<00:12,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:44<00:09,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:47<00:07,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:50<00:05,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:52<00:02,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:54<00:00,  2.40s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  40%|████      | 4/10 [31:52<47:48, 478.13s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:21,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:18,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:15,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:11<04:47,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:00,  2.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:16,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:22,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:32,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:37,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:38,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:41,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:34,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:19,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:23,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:27,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:28,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:20,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:48<04:14,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:01,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:04,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:57<04:06,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([65, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:00<04:08,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:02<04:07,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:05<04:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:53,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:10<03:44,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:42,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:46,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:48,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:47,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:24<03:35,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:27<03:36,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:29<03:14,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:31<03:07,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:34<03:00,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:36<02:45,  2.23s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:37<02:36,  2.14s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:40<02:44,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:43<02:55,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:45<02:52,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:48<02:59,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:51<03:03,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:54<03:05,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:57<03:06,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:00<03:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:02<02:52,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:05<02:38,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:07<02:43,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:10<02:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:13<02:47,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:16<02:47,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:19<02:46,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:22<02:45,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:25<02:43,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:28<02:40,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:31<02:38,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:34<02:35,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:26,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:25,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:42<02:18,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:45<02:18,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:48<02:16,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:51<02:17,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:54<02:14,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:57<02:10,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:01<02:21,  3.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:04<02:13,  3.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:08,  3.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:09<01:59,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:12<01:56,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:47,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:17<01:46,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([33, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:36,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:22<01:34,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:25<01:35,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:28<01:34,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:30<01:26,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:33<01:26,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:19,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:38<01:20,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:41<01:20,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:44<01:14,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:45<01:01,  2.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:48<01:04,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:50<01:00,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:53<01:01,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:56<01:01,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:58<00:53,  2.45s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:01<00:52,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:04<00:53,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:06<00:50,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:09<00:49,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:12<00:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:14<00:42,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:17<00:40,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:20<00:39,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:23<00:36,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:25<00:31,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:28<00:27,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:30<00:23,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:31<00:19,  2.19s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:34<00:19,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:37<00:18,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:40<00:16,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:42<00:12,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:45<00:09,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:47<00:07,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:50<00:05,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:53<00:02,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:55<00:00,  2.40s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  50%|█████     | 5/10 [39:51<39:52, 478.40s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:21,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:17,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:15,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:11<04:47,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:00,  2.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:17,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:22,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:31,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:37,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:38,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:41,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:35,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:20,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:25,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:28,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:29,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:21,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:49<04:14,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:01,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:06,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:57<04:06,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:00<04:08,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:03<04:07,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:05<04:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:54,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:10<03:44,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:42,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:46,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:48,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:48,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:24<03:35,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:27<03:36,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:29<03:14,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:32<03:07,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:34<02:59,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:36<02:45,  2.23s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:38<02:36,  2.14s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:40<02:44,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:43<02:55,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:46<02:52,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:47<02:38,  2.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([39, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:50<02:49,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:53<02:54,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:56<02:59,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [01:59<03:01,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:01<02:49,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:04<02:37,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:07<02:41,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:09<02:45,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:12<02:47,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:15<02:46,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:18<02:45,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:21<02:45,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:24<02:42,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:27<02:40,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:30<02:37,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:33<02:35,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:25,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:26,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:41<02:18,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:44<02:18,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:47<02:16,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:50<02:17,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:53<02:14,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:56<02:10,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:00<02:21,  3.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:03<02:13,  3.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:08,  3.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:08<02:00,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:11<01:57,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:48,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:16<01:46,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:37,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:21<01:34,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:24<01:35,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:27<01:34,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:29<01:26,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:32<01:26,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:19,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:37<01:20,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:40<01:20,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:43<01:15,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:44<01:01,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:47<01:04,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:49<00:59,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:52<01:01,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:55<01:02,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:57<00:54,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:00<00:52,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:03<00:53,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:06<00:50,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:08<00:49,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:11<00:46,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:14<00:42,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:17<00:40,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:18<00:34,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([57, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:21<00:33,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:23<00:29,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:26<00:26,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:28<00:23,  2.33s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:30<00:19,  2.15s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:32<00:18,  2.36s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:35<00:17,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:38<00:16,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:40<00:12,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:43<00:09,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:46<00:07,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:49<00:05,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:51<00:02,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:53<00:00,  2.42s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  60%|██████    | 6/10 [47:48<31:52, 478.02s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:21,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:17,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:15,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:10<04:47,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:00,  2.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:16,  2.47s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:22,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:31,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:37,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:39,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:41,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:34,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:19,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:24,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:27,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:28,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:19,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:48<04:14,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:00,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:06,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:57<04:08,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:00<04:08,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:02<04:08,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:05<04:07,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:54,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:10<03:44,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:42,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:46,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:47,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:47,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:24<03:33,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:27<03:35,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:29<03:13,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:31<03:07,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:34<02:59,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:35<02:44,  2.22s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:37<02:35,  2.13s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:40<02:43,  2.27s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:43<02:54,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:45<02:51,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:48<02:58,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:51<03:02,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:54<03:04,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:57<03:05,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:00<03:04,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([43, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:02<02:51,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:04<02:38,  2.51s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:07<02:43,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:10<02:45,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:13<02:47,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:16<02:46,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:19<02:46,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:22<02:45,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:25<02:43,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:28<02:40,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:31<02:37,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:34<02:34,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:36<02:25,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:39<02:24,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:42<02:18,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:45<02:18,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:48<02:16,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:51<02:17,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:54<02:14,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:56<02:10,  2.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:00<02:20,  3.20s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:03<02:13,  3.11s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:06<02:08,  3.06s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:09<02:00,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:12<01:56,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:14<01:47,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:17<01:46,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:19<01:36,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:22<01:34,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:25<01:35,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:28<01:35,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:30<01:26,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:33<01:26,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:35<01:19,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:38<01:20,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:41<01:20,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:44<01:14,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:45<01:01,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:48<01:04,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:50<01:00,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:53<01:01,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:56<01:01,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [03:58<00:53,  2.44s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:01<00:52,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:03<00:52,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:06<00:50,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:09<00:49,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:12<00:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:14<00:42,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:17<00:40,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:20<00:39,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:23<00:37,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:25<00:31,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:28<00:27,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:30<00:24,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:31<00:19,  2.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:34<00:19,  2.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:37<00:18,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:40<00:16,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:42<00:12,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:45<00:09,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:48<00:07,  2.59s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:51<00:05,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:54<00:02,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:55<00:00,  2.49s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  70%|███████   | 7/10 [55:48<23:55, 478.66s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:24,  2.97s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:21,  2.98s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:18,  2.98s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:11<04:49,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:02,  2.31s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:18,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:18<04:23,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:32,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:36,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:39,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:42,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:34,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:20,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:25,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:29,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:28,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:20,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:49<04:15,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:01,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:06,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:57<04:07,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:00<04:09,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:03<04:08,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:06<04:07,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:55,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:11<03:45,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:42,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:47,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:49,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:50,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:25<03:36,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:28<03:38,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:30<03:16,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:32<03:09,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:34<03:02,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:36<02:46,  2.25s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:38<02:37,  2.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:41<02:46,  2.32s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:44<02:57,  2.51s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:46<02:55,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:49<03:03,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:52<03:07,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:55<03:08,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:58<03:09,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:01<03:09,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:03<02:55,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:06<02:42,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:09<02:47,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:12<02:49,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:15<02:50,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:17<02:49,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:20<02:48,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:23<02:47,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:26<02:44,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:29<02:41,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:32<02:38,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:35<02:35,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:38<02:26,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:41<02:26,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:43<02:19,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:46<02:19,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:49<02:17,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:52<02:18,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:55<02:15,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:58<02:11,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:02<02:22,  3.23s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:05<02:14,  3.13s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:08<02:09,  3.09s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:11<02:00,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:14<01:57,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:16<01:49,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:19<01:47,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:21<01:37,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:24<01:35,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:27<01:35,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:30<01:35,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:32<01:26,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:35<01:27,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:37<01:19,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:40<01:20,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:43<01:20,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:45<01:15,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:47<01:01,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:50<01:04,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:52<01:00,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:55<01:01,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:58<01:01,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [04:00<00:54,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:03<00:53,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:06<00:53,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:08<00:50,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:11<00:49,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:14<00:46,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:16<00:42,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:19<00:40,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:22<00:39,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:25<00:36,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:27<00:31,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:30<00:27,  2.54s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:32<00:24,  2.41s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:33<00:19,  2.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:36<00:19,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:39<00:18,  2.60s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:42<00:16,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:44<00:12,  2.51s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:47<00:09,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:50<00:07,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:53<00:05,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:55<00:02,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:57<00:00,  2.45s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  80%|████████  | 8/10 [1:03:51<15:59, 479.74s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:23,  2.97s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:18,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:16,  2.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:11<04:48,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:02,  2.31s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:18,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:18<04:25,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:21<04:34,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:40,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:43,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:46,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:37,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:35<04:22,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([41, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:25,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:41<04:29,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:29,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:20,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:49<04:15,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:01,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:07,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:57<04:07,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:00<04:10,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:03<04:11,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:06<04:10,  2.92s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:56,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:11<03:47,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:14<03:45,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:17<03:48,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:20<03:50,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:23<03:50,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:25<03:37,  2.75s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:28<03:38,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:30<03:16,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:32<03:09,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:35<03:01,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:36<02:47,  2.26s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:38<02:37,  2.16s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:41<02:46,  2.31s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:44<02:57,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:46<02:53,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:49<03:01,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:52<03:05,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:55<03:06,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:58<03:06,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:01<03:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:04<02:52,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:06<02:39,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:09<02:44,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:12<02:47,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:14<02:48,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:17<02:48,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:20<02:47,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:24<02:47,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:26<02:45,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:29<02:42,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:32<02:39,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:35<02:36,  2.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:38<02:27,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:41<02:27,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:43<02:20,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:46<02:20,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:49<02:18,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:53<02:19,  2.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:55<02:11,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([72, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:58<02:08,  2.86s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:02<02:20,  3.20s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:05<02:13,  3.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:08<02:08,  3.07s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:11<02:00,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:13<01:57,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:16<01:48,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:19<01:46,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:21<01:37,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:24<01:34,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:27<01:35,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:30<01:35,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:32<01:27,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:35<01:27,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:37<01:20,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:40<01:21,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:43<01:21,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:46<01:15,  2.71s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:47<01:02,  2.30s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:50<01:05,  2.51s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:52<01:00,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:55<01:01,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:58<01:01,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [04:00<00:53,  2.45s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:03<00:52,  2.52s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:06<00:53,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:08<00:50,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:11<00:49,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:14<00:46,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:16<00:42,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:19<00:41,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:22<00:39,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:25<00:37,  2.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:27<00:31,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:30<00:28,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:32<00:24,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:33<00:19,  2.22s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:35<00:16,  2.10s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([33, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:38<00:16,  2.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:41<00:15,  2.57s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:43<00:11,  2.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:46<00:09,  2.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:48<00:07,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:51<00:05,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:54<00:02,  2.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:56<00:00,  2.41s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:  90%|█████████ | 9/10 [1:11:51<07:59, 479.94s/it]\n",
            "Train Batch:   0%|          | 0/110 [00:00<?, ?it/s]\u001b[A\n",
            "Train Batch:   1%|          | 1/110 [00:02<05:24,  2.97s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   2%|▏         | 2/110 [00:05<05:18,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   3%|▎         | 3/110 [00:08<05:16,  2.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   4%|▎         | 4/110 [00:10<04:47,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([69, 96103])\n",
            "torch.Size([69])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▍         | 5/110 [00:12<04:00,  2.29s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   5%|▌         | 6/110 [00:15<04:18,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   6%|▋         | 7/110 [00:17<04:23,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   7%|▋         | 8/110 [00:20<04:33,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   8%|▊         | 9/110 [00:23<04:38,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:   9%|▉         | 10/110 [00:26<04:40,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  10%|█         | 11/110 [00:29<04:43,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  11%|█         | 12/110 [00:32<04:36,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  12%|█▏        | 13/110 [00:34<04:21,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([79, 96103])\n",
            "torch.Size([79])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  13%|█▎        | 14/110 [00:37<04:26,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  14%|█▎        | 15/110 [00:40<04:29,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▍        | 16/110 [00:43<04:30,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  15%|█▌        | 17/110 [00:46<04:21,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  16%|█▋        | 18/110 [00:49<04:16,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  17%|█▋        | 19/110 [00:51<04:02,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  18%|█▊        | 20/110 [00:54<04:06,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  19%|█▉        | 21/110 [00:57<04:07,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  20%|██        | 22/110 [01:00<04:08,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  21%|██        | 23/110 [01:03<04:10,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  22%|██▏       | 24/110 [01:06<04:09,  2.91s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  23%|██▎       | 25/110 [01:08<03:56,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  24%|██▎       | 26/110 [01:11<03:45,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▍       | 27/110 [01:13<03:43,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([92, 96103])\n",
            "torch.Size([92])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  25%|██▌       | 28/110 [01:16<03:46,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  26%|██▋       | 29/110 [01:19<03:48,  2.82s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  27%|██▋       | 30/110 [01:22<03:49,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  28%|██▊       | 31/110 [01:25<03:36,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  29%|██▉       | 32/110 [01:28<03:39,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  30%|███       | 33/110 [01:30<03:16,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([61, 96103])\n",
            "torch.Size([61])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  31%|███       | 34/110 [01:32<03:09,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  32%|███▏      | 35/110 [01:34<03:02,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  33%|███▎      | 36/110 [01:36<02:46,  2.25s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([58, 96103])\n",
            "torch.Size([58])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  34%|███▎      | 37/110 [01:38<02:37,  2.15s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▍      | 38/110 [01:41<02:45,  2.30s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  35%|███▌      | 39/110 [01:44<02:56,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  36%|███▋      | 40/110 [01:46<02:53,  2.48s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([82, 96103])\n",
            "torch.Size([82])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  37%|███▋      | 41/110 [01:49<03:00,  2.62s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  38%|███▊      | 42/110 [01:52<03:04,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  39%|███▉      | 43/110 [01:55<03:06,  2.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  40%|████      | 44/110 [01:58<03:07,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  41%|████      | 45/110 [02:01<03:06,  2.87s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  42%|████▏     | 46/110 [02:03<02:52,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([75, 96103])\n",
            "torch.Size([75])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  43%|████▎     | 47/110 [02:05<02:39,  2.53s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 96103])\n",
            "torch.Size([70])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  44%|████▎     | 48/110 [02:08<02:44,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([46, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▍     | 49/110 [02:11<02:46,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  45%|████▌     | 50/110 [02:14<02:48,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  46%|████▋     | 51/110 [02:17<02:47,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  47%|████▋     | 52/110 [02:20<02:47,  2.89s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  48%|████▊     | 53/110 [02:23<02:47,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  49%|████▉     | 54/110 [02:26<02:44,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  50%|█████     | 55/110 [02:29<02:41,  2.94s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  51%|█████     | 56/110 [02:32<02:39,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  52%|█████▏    | 57/110 [02:35<02:36,  2.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  53%|█████▎    | 58/110 [02:37<02:27,  2.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([85, 96103])\n",
            "torch.Size([85])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  54%|█████▎    | 59/110 [02:40<02:27,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▍    | 60/110 [02:43<02:19,  2.79s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([87, 96103])\n",
            "torch.Size([87])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  55%|█████▌    | 61/110 [02:46<02:19,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  56%|█████▋    | 62/110 [02:49<02:18,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  57%|█████▋    | 63/110 [02:52<02:18,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  58%|█████▊    | 64/110 [02:55<02:15,  2.96s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  59%|█████▉    | 65/110 [02:58<02:11,  2.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  60%|██████    | 66/110 [03:02<02:22,  3.23s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([99, 96103])\n",
            "torch.Size([99])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  61%|██████    | 67/110 [03:05<02:14,  3.13s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  62%|██████▏   | 68/110 [03:08<02:09,  3.08s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  63%|██████▎   | 69/110 [03:10<02:01,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([88, 96103])\n",
            "torch.Size([88])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  64%|██████▎   | 70/110 [03:13<01:58,  2.95s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([98, 96103])\n",
            "torch.Size([98])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▍   | 71/110 [03:16<01:49,  2.80s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  65%|██████▌   | 72/110 [03:19<01:46,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  66%|██████▋   | 73/110 [03:21<01:37,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  67%|██████▋   | 74/110 [03:24<01:35,  2.65s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  68%|██████▊   | 75/110 [03:26<01:35,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  69%|██████▉   | 76/110 [03:29<01:35,  2.81s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  70%|███████   | 77/110 [03:32<01:27,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([71, 96103])\n",
            "torch.Size([71])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  71%|███████   | 78/110 [03:35<01:27,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  72%|███████▏  | 79/110 [03:37<01:17,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  73%|███████▎  | 80/110 [03:40<01:19,  2.64s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  74%|███████▎  | 81/110 [03:42<01:19,  2.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▍  | 82/110 [03:45<01:14,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([84, 96103])\n",
            "torch.Size([84])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  75%|███████▌  | 83/110 [03:46<01:01,  2.28s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([38, 96103])\n",
            "torch.Size([38])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  76%|███████▋  | 84/110 [03:49<01:04,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  77%|███████▋  | 85/110 [03:52<01:00,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([73, 96103])\n",
            "torch.Size([73])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  78%|███████▊  | 86/110 [03:55<01:01,  2.58s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  79%|███████▉  | 87/110 [03:58<01:02,  2.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  80%|████████  | 88/110 [04:00<00:54,  2.49s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60, 96103])\n",
            "torch.Size([60])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  81%|████████  | 89/110 [04:02<00:53,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([91, 96103])\n",
            "torch.Size([91])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  82%|████████▏ | 90/110 [04:05<00:53,  2.69s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  83%|████████▎ | 91/110 [04:08<00:50,  2.68s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  84%|████████▎ | 92/110 [04:11<00:49,  2.77s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▍ | 93/110 [04:14<00:46,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([93, 96103])\n",
            "torch.Size([93])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  85%|████████▌ | 94/110 [04:16<00:42,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([78, 96103])\n",
            "torch.Size([78])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  86%|████████▋ | 95/110 [04:19<00:41,  2.76s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  87%|████████▋ | 96/110 [04:22<00:39,  2.84s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  88%|████████▊ | 97/110 [04:25<00:37,  2.88s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  89%|████████▉ | 98/110 [04:27<00:31,  2.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([66, 96103])\n",
            "torch.Size([66])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  90%|█████████ | 99/110 [04:30<00:28,  2.56s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([80, 96103])\n",
            "torch.Size([80])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  91%|█████████ | 100/110 [04:32<00:24,  2.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([68, 96103])\n",
            "torch.Size([68])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  92%|█████████▏| 101/110 [04:33<00:19,  2.22s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([53, 96103])\n",
            "torch.Size([53])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  93%|█████████▎| 102/110 [04:36<00:19,  2.43s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  94%|█████████▎| 103/110 [04:39<00:18,  2.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▍| 104/110 [04:42<00:16,  2.74s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  95%|█████████▌| 105/110 [04:44<00:12,  2.50s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([62, 96103])\n",
            "torch.Size([62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  96%|█████████▋| 106/110 [04:47<00:09,  2.46s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([77, 96103])\n",
            "torch.Size([77])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  97%|█████████▋| 107/110 [04:49<00:07,  2.55s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 96103])\n",
            "torch.Size([95])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  98%|█████████▊| 108/110 [04:53<00:05,  2.70s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 96103])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch:  99%|█████████▉| 109/110 [04:55<00:02,  2.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([89, 96103])\n",
            "torch.Size([89])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Batch: 100%|██████████| 110/110 [04:57<00:00,  2.41s/it]\u001b[A\n",
            "                                                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([56, 96103])\n",
            "torch.Size([56])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 10/10 [1:19:52<00:00, 479.29s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IjU59KYdGhN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "77690476-e01f-4b0c-a801-aeb623412d71"
      },
      "source": [
        "results_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>total time</th>\n",
              "      <th>train loss</th>\n",
              "      <th>val loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>294.308738</td>\n",
              "      <td>1259.761312</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>588.600380</td>\n",
              "      <td>1259.896636</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>883.471948</td>\n",
              "      <td>1259.379808</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1178.149083</td>\n",
              "      <td>1259.815345</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1473.541455</td>\n",
              "      <td>1259.672971</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1767.173297</td>\n",
              "      <td>1259.468701</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>2063.064078</td>\n",
              "      <td>1260.073024</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>2360.764199</td>\n",
              "      <td>1259.740105</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>2657.263782</td>\n",
              "      <td>1259.800835</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>2954.707612</td>\n",
              "      <td>1259.555510</td>\n",
              "      <td>778.638506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch   total time   train loss    val loss\n",
              "0      0   294.308738  1259.761312  778.638506\n",
              "1      1   588.600380  1259.896636  778.638506\n",
              "2      2   883.471948  1259.379808  778.638506\n",
              "3      3  1178.149083  1259.815345  778.638506\n",
              "4      4  1473.541455  1259.672971  778.638506\n",
              "5      5  1767.173297  1259.468701  778.638506\n",
              "6      6  2063.064078  1260.073024  778.638506\n",
              "7      7  2360.764199  1259.740105  778.638506\n",
              "8      8  2657.263782  1259.800835  778.638506\n",
              "9      9  2954.707612  1259.555510  778.638506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYyoD4jedGnk"
      },
      "source": [
        "train_loss = results_df['train loss']/110"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUrLtv7bdGrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5cd533-f449-4b29-eae4-69de3fedc58e"
      },
      "source": [
        "train_loss"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    11.452376\n",
              "1    11.453606\n",
              "2    11.448907\n",
              "3    11.452867\n",
              "4    11.451572\n",
              "5    11.449715\n",
              "6    11.455209\n",
              "7    11.452183\n",
              "8    11.452735\n",
              "9    11.450505\n",
              "Name: train loss, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "hossTj8h9Idw",
        "outputId": "84c1fdce-a7c7-4fba-b345-2adb89d321d2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(results_df['train loss'])\n",
        "plt.ylabel('Loss + 1259')\n",
        "plt.xlabel('Number of epochs')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Number of epochs')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_E9mxXl-U13",
        "outputId": "2699151b-5555-4abe-889a-c7c046896bd2"
      },
      "source": [
        "bleu = []\n",
        "model.eval()\n",
        "test_data = sorted(test_data, key=len)\n",
        "for inputs, labels in test_data:\n",
        "  inputs = tokenizer_pegasus.encode(inputs, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  target = tokenizer_pegasus.encode(labels, max_length=100, truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "  # print(inputs)\n",
        "  # print(target)\n",
        "  len_labels = target.shape[1]\n",
        "  preds = model.generate(inputs, max_length=len_labels)\n",
        "  decoded_preds = tokenizer_pegasus.batch_decode(preds, skip_special_tokens=True)\n",
        "  decoded_target = tokenizer_pegasus.batch_decode(target, skip_special_tokens=True)\n",
        "  bleu.append(sentence_bleu(tokenize_words(decoded_preds[0]),tokenize_words(decoded_target[0])))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivYT-8NngGWN"
      },
      "source": [
        "rand = tokenizer_pegasus.encode(test_data[40][0], truncation=True, return_tensors=\"pt\").to(torch_device)\n",
        "preds = model.generate(rand, max_length=100)\n",
        "decoded_preds = tokenizer_pegasus.batch_decode(preds, skip_special_tokens=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_n3poDjN_NO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "086a4f21-a535-493f-816d-3f392c34e196"
      },
      "source": [
        "decoded_preds[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the aim of this paper is to deal with constraints that are needed to develop a based on unification grammar so that our parser can deal with some types of sentences in variety. <n> the main result is the following. <n> _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "rHyvEkCEhwOC",
        "outputId": "fb51375f-9cc7-4a7d-d418-ff4561249c4c"
      },
      "source": [
        "import re\n",
        "a = test_data[40][0]\n",
        "articles_processed = re.sub(re.compile('<.*?>'),'',a)\n",
        "articles_processed = re.sub('[^A-Za-z0-9]+',' ',articles_processed)\n",
        "articles_processed = articles_processed.lower()\n",
        "articles_processed"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' introduction our aim is to formalize constraints that are needed to develop a parser based on unification grammar called ug henceforth so that our parser can deal with variety of types of sentences in japanese however just parsing syntactically is not enough for natural language understanding one important and necessary task to be done when a parser processes a discourse in japanese is the so called zero anaphora resolution all of syntactic semantic and pragmatic constraints are to be involved to resolve zero anaphora of course some of omitted pronouns are syntactically resolved for instance vp with suffix te is not regarded as a clause but a conjunct vp therefore the subject of the vp with te which is possibly omitted from surface should corefer with the subject of the sentence one example is hanako felt cold and closed the window where both of zero subjects and refer to the sentential topic hanako in this example one of the possible accounts for this interpretation is the following zero subject of te phrase is anaphoric pronominal or pro in gb term as the result is controlled by the subject of the main vp which is also zero subject is in gb term anaphoric pronominal or pro the sentential topic hanako is the only possible antecedent of this zero subject in this example however in complex sentences things are quite different consider the following sentence 1 since hanako behaved like feeling cold i closed the window 2 since i behaved like feeling cold hanako closed the window if contextually we can take only hanako and the speaker of this sentence as candidates of antecedent of or intuitively the following two interpretations are equally likely a hanako speaker b speaker hanako therefore and are both pro in fact this fact is well known among japanese linguists i e as a result zero anaphora resolution of complex sentence is not only to be done syntactically but also to be done pragmatically and or semantically one of the promising candidate for this is the centering theory to apply the centering theory that is originally for a sequence of sentences namely discourse we regard the subordinate clause and the main clause as a segment of discourse respectively moreover hanako who is marked by wa is regarded as the topic for these two clauses then the topic hanako is the strongest candidate for the backward center of the subordinate clause therefore the backward center of the subordinate clause is hanako and consequently zero subject refers to hanako by the same way as the subordinate clause case is dealt with the zero subject of the main clause is known to refer to hanako too this result is neither interpretation a nor b shown above another candidate is the property sharing thoery in her theory since the both of zero subjects share the subjecthood both of them finally are known to refer to hanako that is the topic for both of these clauses therefore the property sharing theory also fails to account for the intuitive interpretations then we shift our attention to more microscopic one in which roughly speaking the important part of semantics of complex sentence is formalized as relations among semantic roles that appear in the main clause or the subordinate clause at the first glance the constraints about these relations are not local in terms of main or subordinate clauses in other words semantic roles that appear in subordinate clause and semantic roles that appear in the main clause seem to be directly constrained by the constraints of complex sentence however looking more carefully we find that the constraints of subordinate clause and the constraints of main clause are represented as local constraints by introducing the new notion of motivated which is characterized as a person who has enough reason to act as the main clause describes more precisely motivated is one of the pragmatic roles that appear in a subordinate clause and the constraints in subordinate clause are stated as identity relations between motivated and other semantic pragmatic roles appearing in subordinate clause therefore these constraints are local in subordinate clause the constraints in main clause are stated as identity relations between motivated which comes from subordinate clause and other semantic roles appearing in main clause therefore in understanding the main clause we don t have to be care about semantic pragmatic roles in subordinate clause other than a motivated in this sense the constraints in the main clause can be treated as almost local constraints of the main clause the next question is how to represent the semantics of complex sentence in feature structure called fs henceforth for this we should write down the constraints about these relations among semantic pragmatic roles in a feature structure formalism due to the space limitation in this paper we mainly pursue the constraints about semantic feature structures hierarchical structure of complex sentence we pay our attention to the general structure of japanese utterance which is helpful to represent semantics of complex sentence several japanese linguists have already proposed the general structure of japanese utterances mikami categorized clauses into three classes namely open semi open and closed this categorization indicates how freely the content of clause interacts with the outside of clause for instance they are categorized by the degree of possibilities of coreference between zero pronouns inside the subordinate clause and nominal or topic that appear in the main clause following mikami s idea minami proposed four levels namely level a b c and d which correspond roughly to vp proposition sentence without communication mood and utterance which takes into account a hearer respectively divided level a into two levels one of them corresponds to vp the other corresponds to vp a certain kind of subject which is called objective subject gunji proposed the more detailed structure in which starting from predicate say verb and adjective objects voice subject aspect tense modality topic and mood are or might be sequentially added to make an informationally more fulfilled sentence component finally it ends up with an utterance in gunji s structure some node can have more than two daughter nodes to make more complex sentence following them the structure of the so called cluase level complex sentence is the following shown in fig 1 in fig 1 sub clause and conjunct mean subordinate clause and conjunctive particle respectively note that fig 1 represents not only the hierarchical structure but also the word order of a complex sentence in japanese the structure is almost the same as gunji s structure except for explicitly showing complex proposition subordinate clause and conjunctive particle that are newly added to deal with complex sentences note that comment appearing in sub clause has the same structure as comment appearing just below judgement that is to say comment is recursively defined however in practice the more the level of depth of recursively appearing comment is the less comprehensible the sentence is subordinate clause in this section at first we show the predicate categories used in the subordinate clauses that we deal with in this paper in table in each category of 2 3 4 5 and 6 exists there a person who is affected by the situation described by the subordinate clause on the contrary in category 1 there is not necessarily an explicit affected person in our theory this affected person plays a key role for semantics of complex sentence as the result in general we cannot derive a useful result for category 1 in our theory therefore we don t deal with category 1 in this paper at this moment we should explain the nature of the so called subjective predicate mentioned in table in short a subjective predicate describes the experiencer s inner state which can exclusively be known by the experiencer him herself next we focus on verbal suffix garu firstly we show garu s syntax garu is the present form and its root form is gar therefore inflections are as follows gar re gar i etc in addition garu has an allophonic root form gat and gat ta past form gat teiru progressive form and so on are derived from gat some of these forms will appear in our examples next we talk about the semantics of garu garu roughly means show a sign of or behave like ing also in its semantics is informally explained however our proposal is to formalize garu s semantics in ug or more generally in computational linguistics for this first of all we introduce a new pragmatic role called observer definition 1 observer observer is a person who directly observes or is indirectly informed the situation described by the proposition part therefore an observer has a certain evidence to be convinced that that situation actually happens although this notion of observer shares a large part with pivot of our notion of observer is introduced only by garu therefore it is much narrower notion as you will see later this newly introduced role is playing a key role which bridges semantic roles of subordinate clause to semantic roles of main clause as for an observer introduced by garu one of the widely known consequence about the nature of subjective predicate is the following in a sentence if a subjective adjective is used without being followed by a verbal suffix garu the experiencer of the subjective adjective should be the speaker of the sentence the next thing we should do about a newly introduced notion of observer is to make clear the way to deal with it in fs first of all in our fs a semantic content sem is basically a soa state of affair form of situation semantics however we use semantic role like agent patient experiencer and so on as argument roles of soa since an observer observes the situation which is characterized by a soa if we know that there exists an observer the observed soa is embedded in observing situation which in turn is embedded in the whole semantic content in this sense the observed soa s argument role is observed but as far as we have no confusion we omit role name observed henceforth a typical schema of sem of fs of this type is the following note that we use garu as a value of the relation feature meant by rel the english gross of this relation garu is observe sem rel garu observer 11 soa rel ragent 12experiencer 13 patient 14 now we explain the semantics of clause which consists of subjective adjective with garu or ta garu that are in categories 4 and 5 these categories forms are p garu or its past form p gat ta where p is a subjective adjective category 4 in table or is a verb followed by ta gar category 5 in table and is the experiencer of p which is possibly zero in these categories there exist observers who are not the experiencer of p and observe that experience the sem feature of p garu gat ta is the following rel garuobserver 11 where soa rel pexp 13 where means not token identical in our fs constraints for tokens like are written with where as shown in this fs since constraint satisfaction method in ug has been and is developed by many researchers recently i e our theory will be able to be implemented in systems like theirs if the sentence finishes just after garu gat ta the important points are 1 an introduced observer is the speaker and consequently 2 the experiencer cannot be the speaker if a clause with garu gat ta is a subordinate clause the experiencer cannot be identified with a semantic role corresponding to the subject of main clause or higher clause as for category 2 subjective verbs like kurusimu feel sick and kanasimu feel sadness that describe subjective and or emotional experience in verb form are used like the case of garu an observer who observers the experience can be introduced however this observer is not obligatory therefore unlike the garu gat ta case the experiencer also can be an obligatory semantic role of higher clause as well as the speaker complex sentence feature structure according to the hierachical structure of japanese sentence shown in fig 1 the essential part of hierarchical structure of the following sentence is shown in fig 2 in this figure the structure just below each proposition is replaced with the corresponding parts of sentence since behaved like feeling cold closed the window basically the embedding structure of fs corresponds to the hierarchy shown in the hierarchical structure fig 1 to grasp the image of the relation between a hierarchical structure and the corresponding fs we show an example of fs of the above complex sentence analyzed based on this hierarchical structure in the following this fs is the result of the unification between the fss of subordinate clause and main clause where the contents of syntactic feature head namely is omitted morph samu gat ta node mado o sime ta head sem 20 rel sime agent 11 object windowtense past rel node motivated 11 soa rel garuobserver soa rel samu i experiencer 22 tense past where english grosses of relation name is the following sime close node because samu i feel cold the key point of the semantics of complex sentence is the role motivated that appears in which corresponds to the content of the subordinate clause the role motivated is the link between the content of subordinate clause and the main clause semantically motivated is characterized as the following definition 2 motivated motivated is a person who is affected by the situation described by the subordinate clause deeply enough to feel or act as the main clause describes the important and indispensable part of semantics of complex sentence is roughly speaking the relation between a subordinate clause and the main clause but if you look more closely this relation is actually the relations among semantic pragmatic roles appearing in the subordinate clause and those appearing in the main clause the newly introduced role of motivated gives the most important clue for this relation therefore in the rest of this paper our effort will be concentrated into whom a motivated refers to more precisely in fs our main concerns are which semantic role in the sem of subordinate clause the motivated can or cannot be unified with and which semantic role in the sem of main clause the motivated can or cannot be unified with constraints in this subsection we propose the constraints on complex sentence for this at first we categorize the relations between subordinate clause and main clause based on their semantics they are divided up to many types of complex sentence we show the most important and typical types in table where sc and mc mean subordinate clause and main clause respectively in this table the first column is for a name of sentence type the second column indicates a rough meaning of the relation between subordinate clause sc and main clause mc of complex sentence and the third column shows japanese conjunctive particles used to represent a type of complex sentence in the same row three vp adjuncts te tutu and nagara are usually used to express events ocurring simultaneously however if they are used with aspectual suffix i which means perfective for instance i nagara they are regarded as clause conjuncts and are to be interpreted as although we don t deal with type 4 because a temporal adverbial clause just describes an event that occurs before simultaneously or after another event which is described by the main clause therefore generally we don t expect essential information for relations among semantic roles appearing in adverbial or main clause from this type of sentence now we focus on type 1 2 and 3 where a motivated plays the key role in the constraints in table we show the constraints that say which semantic pragmatic role of subordinate clause can be a motivated table shows which semantic role of main clause can be unified with the motivated in these tables the first column of the first row is for constraint names the second column shows a set of sentence types for which the constraints shown in the second row apply the third column of table shows predicate patterns of subordinate clause and the third column of table shows semantic categories of predicate of main clause for them constraints written in the second row apply note that all of these constraints in table are local in a subordinate clause because both sides of of constraints are roles of subordinate clause in case of subjective adjective without garu the constraint motivated experiencer holds also for type 1 except for the case where directionally auxiliary verb yaru give kureru be given are used analysis for these cases is one of our future problem as for table is a state except for the case that there exists a third party who is a motivated puts the experiencer into that state for instance the experiencer is permitted to do something by the motivated since in this kind of case things are quite complicated we omit it here because of the limited space constraints in table are also local in a main clause because every semantic role that appeares in the righthand side of the constraints is defined within the main clause needless to say the influence from a subordinate clause comes only via role motivated where name means a name of each constraint in the rest of this section we show the examples that exemplify these constraints first we take of type 1 the constraints to be applied are s1 and m1 as you know from the contents of subordinate and main clause by combination of s1 and m1 zero agent of main clause is the observer of the situation described by the subordinate clause where behaved like feeling cold this interpretation coincides with native s intuition look at the following pair of example although behaved like feeling bad didn t take a medicine at last although wanted to stay finally forced him out in both of and the motivateds of subordinate clause are constrained by s2 namely motivateds can be either or the observer of subordinate clause constraint m1 says that in both cases is unified with the motivated intuitively in is on the other hand in is the observer both of these interpretations comply with constraints s2 and m1 since it is hot i am in trouble intuitively corefer with this interpretation is expected by constraint s3 and m2 that apply in this case as you know from these examples our constraints are not strong enough to identify the antecedent of uniquely but makes safe interpretations moreover disambiguation done by these constraints is useful for further inference that will be done with commonsense knowledge or with a special vocabulary like kekkyoku finally used in in case of s5 namely intransitive passive or adversity passive it is well known i e that there exists a person who is affected by the situation described by the passive sentence an example sentence is the following although his wife had gone doesn t show a bit of sadness the semantic role of this affected person in zero role whose wife was dead is an affected the intuitive interpretation that is expected by our constraints s5 of table and m1 of table on the contrary in case of s6 namely transitive passive generally we don t have an affected however in some context a transitive passive form may require the role affected which is inherent to adversity passive for instance s wallet was stolen in this case a person whose wallet was stolen is not explicit but regarded as an affected another case having an affected is that a relational noun is the subject of transitive passive then a person who is in the relation expressed by the relational noun is thought to be affected by that situation too here we take mother father daughter son supervisor and so forth as a relational noun a couple of example sentences are the following since his henchman was attacked the boss retaliated although his henchman was attacked the boss didn t retaliate who retaliated or didn t retaliate has a certain relation between the henchman who had been attacked for instance may be the boss of that henchman in since constraint s6 of table and m1 of table apply is an affected of attacking event described in the subordinate clause this interpretation coincides with native s intuition in sum with these constraints a constraint satisfaction process in ug based parsing can be done locally and consequently very efficiently in other words primarily a constraint satisfaction process of a subordinate clause can be done within the analysis of subordinate clause and that of the main clause can be done within it except for using motivated whose value has already been constrained in the subordinate clause related works and conclusions one of the relevant researches to ours is jpsg that has been developed by gunji and is further studied by the icot working group our focus is a more pragmatics oriented one than jpsg is many japanese linguists have already done the enormous amount of basic observations and proposed linguistic theories about the phenomena we deal with in this paper of course our research is based on their works and observations in it is said that if garu is used in a subordinate clause the subject of the main clause is not the experiencer of the subordinate clause in she says that 1 a cognizer that corresponds to our observer is introduced if garu is used and 2 if an observer is introduced in the subordinate clause the mentally responsible person appearing in the main clause is identical with the observer in linguistic phenomena these observations are similar to the constraint we propose here so what is new the answer is that 1 we explicitly state the semantics of complex sentence as the relations among semantic roles namely since we use semantic pragmatic roles instead of grammatical roles in constraints our constraints can account for zero anaphora in a sentence where the main clause is passive where an agent or an experiencer is not necessarily the subject like the following example since taro behaved like hating to go to school he was scolded where the intuitive reading is the following that is zero subject refers to taro and that is not the zero subject refers to taro s parents who are the observer and motivated of the subordinate clause 2 we formalize this theory in ug formalism even though the details are omitted due to the space limitation 3 we find that the constraints of complex sentences are actually local ones this localization of constraint was found by introducing new pragmatic roles observer and motivated and is extremely important for efficiency of ug based parsing this localization also makes the proposed constraints be compositional ones because in the case of deeply embedded complex sentence to identify the referent of each motivated that bridges between a subordinate clause and its main clause the constraints we proposed are resolved with computation confined within each clause analysis of case in which a directional auxiliary verb i e yaru kureru is used is left as the future problem finally we implemented a japanese language understanding system based on the theory we state in this paper but due to the space limitation we will report the detail of implementation in other place in the near future bibliography brennan s m walker friedman and c pollard 1987 a centering approach to pronouns 25th annual meeting of acl pp 155 162 gunji t 1987 japanese phrase structure grammar reidel dordrecht gunji t 1989 relevance of the formalization of phrase structure grammar to mechanical language processing report of tokutei kenkyu ministry of education and academy iida m and p sells 1988 discourse factors in the binding of zibun in japanese syntax ed w poser csli stanford kameyama m 1988 japanese zero pronominal binding where syntax and discourse meet in japanese syntax ed w poser csli stanford katagiri y 1991 perspectivity and japanese reflexive zibun in csli lecture notes no 26 situation theory and its applications vol 2 j barwise et al eds pp 425 447 kuno s 1973 the structure of the japanese language cambridge mit press kuno s 1978 danwa no bunpou taishukan tokyo ohye s 1975 nitieigo no hikakukenkyu taishukan tokyo saito r 1992 shinjou jutugo no goyouronteki bunseki pragmatic analysis about psychological predicates nihongogaku vol 11 no 6 pp 110 116 mikami a 1953 gendai gohou josetu kuroshio shuppan tokyo minami f 1974 gendai nihongo no kouzou taishukan tokyo palmer f r 1986 mood and modality cambridge university press cambridge sells p 1985 lectures on contemporary syntactic theories csli stanford takubo y 1987 tougokouzou to bunmyakujouhou syntactic structure and contextual information nihongogaku 1987 5 meiji shoin tokyo teramura h 1984 nihongo no sintakusu to imi ii japanese syntax and semantics ii kuroshio shuppan tokyo teramura h 1990 nihongo no sintakusu to imi iii japanese syntax and semantics iii kuroshio shuppan tokyo tsuda h hasida k sirai h 1989 jpsg parser on constraint logic programming 4th acl european chapter walker m m iida and s cote 1990 centering in japanese discourse coling 90 footnotes henceforth means zero where is either grammatical semantic or pragmatic role for instance means zero subject means zero agent means zero experiencer and so forth hanako is a typical girl s name the examples shown below are a tip of iceberg we actually analyzed of course we gather the data about native s intuitive interpretation from more than twenty natives around authors '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQAlB7Ql-U67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73e909d-accb-4e4a-c81a-a6307322d589"
      },
      "source": [
        "bleu_1 = []\n",
        "from gensim.summarization import summarize\n",
        "for inputs, labels in test_data:\n",
        "  label_tok = tokenize_words(labels)\n",
        "  s = summarize(inputs, len(label_tok))\n",
        "  input_tok = tokenize_words(s)\n",
        "  bleu_1.append(sentence_bleu(input_tok,label_tok))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ZUu6PADZtU"
      },
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lASf8ABiDZx-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "50804d01e0d6498d96acd88cd61f1290",
            "ed400546a28f48ab8e86d1290d4e6c62",
            "25e71d93fd024c2cabe32327ca6fc516",
            "4b505e270e15464fbbe806a44e0159dd",
            "10c9b3efe29643e6b4097ca23030bf8b",
            "86990031b52d43a6ba7aa4d06428a33f",
            "e604153598bf461ebbf986635b0bddca",
            "61996efd17f84c24a002cdabe5c5e2d6",
            "5d0e241f496e4819b43e8d4ed3a8c5cd",
            "e6d4226bf62c480ab9007458b0f02d8d",
            "49646509194a470fbf533866084888c3",
            "20711695cf404a3684512df984c0024b",
            "4e7588d0c3ef4d5a80e6bbec8f725a86",
            "dd57bcba915c46e7944216701386623c",
            "fd8ad292b18247e08a52ee5d69224e74",
            "d3925c311de244888be188e571560427",
            "994b910603284776bba5a75ae6403ee3",
            "c0b0f72e6a194d18be90a39ca39b777f",
            "185b3fa8b73847cd97aba364834a29b1",
            "05f606f386c248b3bf9c52197c80d59c",
            "42bc8195e862451d8b9e198d4648f26f",
            "b9bbe9bab53a4b30b3e05dc133626905",
            "09a127813df946f9b1ce222e40605258",
            "69478767346947dfa2bbfc7f09973c67",
            "1d5923d3b08348cc93ea75747c5e5aef",
            "389c47da17b54c489c685de44a9621fe",
            "163c5fc50af64a12b04bebe8d258a03c",
            "d0d08097b489431aa1945f66b252b6ac",
            "1bc744e920214d83bf474aea631dbbe5",
            "e3288340a5844820819deaa35bdffb3d",
            "2816b00d74ae4321a132b885616c2886",
            "5be7ab085ecb418faa5ec420b5650227"
          ]
        },
        "outputId": "9d319ec3-5d95-4342-f40c-23d0544756ab"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "device = torch.device('cpu')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50804d01e0d6498d96acd88cd61f1290",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d0e241f496e4819b43e8d4ed3a8c5cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "994b910603284776bba5a75ae6403ee3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d5923d3b08348cc93ea75747c5e5aef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19V5vDTUDZ0P",
        "outputId": "d40f464d-4a08-4723-c47d-25701c47a373"
      },
      "source": [
        "bleu_2 = []\n",
        "for inputs, labels in test_data:\n",
        "  preprocess_text = inputs.strip().replace(\"\\n\",\"\")\n",
        "  t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\",max_length=2048).to(device)\n",
        "  tok_label = tokenize_words(labels)\n",
        "  summary_ids = model.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=len(tok_label)-25,\n",
        "                                    max_length=len(tok_label)+25,\n",
        "                                    early_stopping=True)\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  output_tok = tokenize_words(output)\n",
        "  bleu_2.append(sentence_bleu(output_tok,tok_label))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z9YSqwrmos_E",
        "outputId": "38a62cab-6ae8-4b7b-e587-182dabf471d2"
      },
      "source": [
        "preprocess_text = a.strip().replace(\"\\n\",\"\")\n",
        "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\",max_length=2048).to(device)\n",
        "summary_ids = model.generate(tokenized_text,\n",
        "                                    num_beams=4,\n",
        "                                    no_repeat_ngram_size=2,\n",
        "                                    min_length=50,\n",
        "                                    max_length=60,\n",
        "                                    early_stopping=True)\n",
        "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "output"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"u.s. linguists have proposed a 'conjunctive particle' in the main clause. the subordinate clause is the subject of the VP withsuffix te, which is possibly omitted from surface, or pro\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "x16wltkTDZ3G",
        "outputId": "f2e3ca8b-d293-4539-ea57-a477740af6f9"
      },
      "source": [
        "result = pd.DataFrame({'PEGASUS':bleu,'Gensim-Extractive':bleu_1,'T5-Abstractive':bleu_2})\n",
        "result.plot()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe59218ec50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQRXgykZOvml",
        "outputId": "8e80fd71-380e-4b75-d3ef-2cbb78f6a4d3"
      },
      "source": [
        "result['PEGASUS'].mean()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.43807729335840623"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP4MTomjOh4A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "004131f5-8160-4e64-ed89-e8b5c5f4f8d1"
      },
      "source": [
        "updated_result = pd.DataFrame([result['PEGASUS'].mean(),result['Gensim-Extractive'].mean(),result['T5-Abstractive'].mean()],index=['PEGASUS','GENSIM','T5'])\n",
        "updated_result.plot.bar(legend=None)\n",
        "plt.ylabel('BLEU Score')\n",
        "# plt.get_legend().remove()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'BLEU Score')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfFwSGD7Lnxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "0ce7675a-2f34-4537-88d6-2b56ce2be3d0"
      },
      "source": [
        "result.plot()\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(18,9)\n",
        "_=plt.legend(bbox_to_anchor=(1.15,1),loc='upper right')\n",
        "_=fig=plt.title('BLEU Scores',fontsize=16)\n",
        "_=fig=plt.xlabel('Articles',fontsize=14)\n",
        "_=fig=plt.ylabel('BLEU Score',fontsize=14)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmLdmYDLyePn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}